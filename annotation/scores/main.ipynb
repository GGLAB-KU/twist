{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import statsmodels\n",
    "\n",
    "DOMAIN = 'COMP'\n",
    "\n",
    "GROUP_NAME = 'GROUP_16' # TODO: update\n",
    "# Load the JSON file\n",
    "file_path = 'json/group16.json' # TODO: update\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "counter_for_id = 1\n",
    "annotator_data_dict = {}\n",
    "for item in data:\n",
    "    for annotation in item['annotations']:\n",
    "        email = annotation['completed_by']['email']\n",
    "        if email not in annotator_data_dict:\n",
    "            annotator_data_dict[email] = {}\n",
    "        task_id = f'task_{counter_for_id}_full_annotation'\n",
    "        annotator_data_dict[email][task_id] = annotation\n",
    "    counter_for_id += 1\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(annotator_data_dict, orient='index')\n",
    "# create a new column for the mail\n",
    "df['mail'] = df.index\n",
    "# reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# mail must be the first column\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "\n",
    "\n",
    "JSON_FILE = 'json/group16.json' # TODO: update\n",
    "COLUMNS = df.columns.tolist()\n",
    "\n",
    "TOTAL_NUMBER_OF_TASKS = len(COLUMNS) - 1\n",
    "\n",
    "df[\"number_of_tasks_completed\"] = df[COLUMNS].notnull().sum(axis=1) - 1\n",
    "\n",
    "REMOVE_MAILS = ['gosahin@ku.edu.tr', 'efe.ozkara@metu.edu.tr', 'yigitcankarakas@gmail.com', 'abulut20@ku.edu.tr'] # TODO: update\n",
    "df = df[~df['mail'].isin(REMOVE_MAILS)]\n",
    "ANNOTATOR_MAILS = df['mail'].tolist()\n",
    "\n",
    "\n",
    "# columns are task_1_full_annotation, task_2_full_annotation, ..., \n",
    "# sometimes not all columns are filled with ground truth annotations, so we need to eliminate those columns\n",
    "new_columns = []\n",
    "task_columns = []\n",
    "new_columns.append(\"mail\")\n",
    "new_columns.append(\"number_of_tasks_completed\")\n",
    "new_total_number_of_tasks = 0\n",
    "for i in range(1, TOTAL_NUMBER_OF_TASKS+1):\n",
    "    column_name = f'task_{i}_full_annotation'\n",
    "    new_columns.append(column_name)\n",
    "    task_columns.append(column_name)\n",
    "    new_total_number_of_tasks += 1\n",
    "\n",
    "TOTAL_NUMBER_OF_TASKS = new_total_number_of_tasks\n",
    "df = df[new_columns]\n",
    "\n",
    "\n",
    "\n",
    "def read_tasks_as_json(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        task_data = json.load(f)\n",
    "    return task_data\n",
    "\n",
    "tasks_json = read_tasks_as_json(JSON_FILE)\n",
    "\n",
    "def extract_paragraph_data(task_data):\n",
    "    all_paragraph = task_data['data']['my_text_1']\n",
    "    sections = ['TR-1', 'EN-2', 'TR-2', 'EN-3', 'TR-3']\n",
    "    positions = [all_paragraph.find(section) for section in sections] + [len(all_paragraph)]\n",
    "    intervals = [(positions[i], positions[i+1]) for i in range(len(positions) - 1)]\n",
    "    return all_paragraph, intervals\n",
    "\n",
    "def separate_clean_data(paragraph):\n",
    "    lines = paragraph.strip().split(' \\n')\n",
    "    english_data, turkish_data = {}, {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('EN-'):\n",
    "            key, value = line.split(': ', 1)\n",
    "            english_data[key] = value\n",
    "        elif line.startswith('TR-'):\n",
    "            key, value = line.split(': ', 1)\n",
    "            turkish_data[key] = value\n",
    "    return english_data, turkish_data\n",
    "\n",
    "def count_words(data, keys):\n",
    "    all = sum(len(data[key].split()) for key in keys if key in data.keys())\n",
    "    return all\n",
    "\n",
    "def get_annotation_results(annotation, mail, intervals_for_sentences):\n",
    "    results = {'email': mail}\n",
    "    features = {\n",
    "        'english_detected_terms': [],\n",
    "        'turkish_detected_terms': [],\n",
    "        'turkish_detected_labels': [],\n",
    "        'turkish_detected_corrections': [],\n",
    "        'english_terimler_org_detected': []\n",
    "    }\n",
    "\n",
    "    for item in annotation['result']:\n",
    "        if 'value' in item and 'labels' in item['value']:\n",
    "            label = item['value']['labels'][0]\n",
    "            text, start, end = item['value']['text'], item['value']['start'], item['value']['end']\n",
    "            meta_text = item.get('meta', {}).get('text', [None])[0]\n",
    "            section = None\n",
    "\n",
    "            # Check if text is not empty before modifying it\n",
    "            if text:\n",
    "                # Remove non-alphanumeric characters from the end\n",
    "                if not text[-1].isalnum():\n",
    "                    text = text[:-1]\n",
    "                    end -= 1\n",
    "\n",
    "                # Remove non-alphanumeric characters from the beginning\n",
    "                if text and not text[0].isalnum():\n",
    "                    text = text[1:]\n",
    "                    start += 1\n",
    "                    \n",
    "            # trim 'a' like a term --> term, note that there should be a space after 'a'\n",
    "            if text and text[0].lower() == 'a' and text[1].isspace():\n",
    "                text = text[2:]\n",
    "                start += 2\n",
    "                \n",
    "            # trim 'an' like a term --> term, note that there should be a space after 'an'\n",
    "            if text and text[:2].lower() == 'an' and text[2].isspace():\n",
    "                text = text[3:]\n",
    "                start += 3\n",
    "                \n",
    "            # trim 'the' like a term --> term, note that there should be a space after 'the'\n",
    "            if text and text[:3].lower() == 'the' and text[3].isspace():\n",
    "                text = text[4:]\n",
    "                start += 4\n",
    "                \n",
    "            # trim bir terim --> terim, note that there should be a space after 'bir'\n",
    "            if text and text[:3].lower() == 'bir' and text[3].isspace():\n",
    "                text = text[4:]\n",
    "                start += 4\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            # Determine the section based on the start and end positions\n",
    "            if 0 <= start < end <= intervals_for_sentences[0][0]:\n",
    "                section_for_EN_1 = 'EN-1'\n",
    "            elif intervals_for_sentences[0][0] <= start <= end <= intervals_for_sentences[0][1]:\n",
    "                section = 'TR-1'\n",
    "            elif intervals_for_sentences[1][0] <= start <= end <= intervals_for_sentences[1][1]:\n",
    "                section = 'EN-2'\n",
    "            elif intervals_for_sentences[2][0] <= start <= end <= intervals_for_sentences[2][1]:\n",
    "                section = 'TR-2'\n",
    "            elif intervals_for_sentences[3][0] <= start <= end <= intervals_for_sentences[3][1]:\n",
    "                section = 'EN-3'\n",
    "            elif intervals_for_sentences[4][0] <= start <= end <= intervals_for_sentences[4][1]:\n",
    "                section = 'TR-3'\n",
    "\n",
    "\n",
    "            if text:  # Only process if text is not empty\n",
    "                if label == 'TERM':\n",
    "                    # features['english_detected_terms'].append((text, start, end, section))\n",
    "\n",
    "                    words = text.split()\n",
    "                    word_start = start\n",
    "\n",
    "                    for word in words:\n",
    "                        word_end = word_start + len(word)\n",
    "\n",
    "                        # Add each word as a separate term in the `english_detected_terms` list\n",
    "                        features['english_detected_terms'].append((word, word_start, word_end))\n",
    "\n",
    "                        # Move to the next word's start position\n",
    "                        word_start = word_end + 1  # assuming there's a space between words\n",
    "                        \n",
    "                    if meta_text:\n",
    "                        # lowercase meta_text\n",
    "                        meta_text = meta_text.lower()\n",
    "                        features['english_terimler_org_detected'].append((text, start, end, meta_text))\n",
    "                    else:\n",
    "                        features['english_terimler_org_detected'].append((text, start, end, \"EMPTY LINK\"))\n",
    "                elif label in ['CORRECT_TRANSLATION', 'WRONG_TRANSLATION']:\n",
    "                    # features['turkish_detected_terms'].append((text, start, end, section))\n",
    "                    # features['turkish_detected_labels'].append((text, start, end, section, label))\n",
    "                    \n",
    "                    words = text.split()\n",
    "                    word_start = start\n",
    "                    \n",
    "                    for word in words:\n",
    "                        word_end = word_start + len(word)\n",
    "                        \n",
    "                        # Add each word as a separate term in the `turkish_detected_terms` list\n",
    "                        features['turkish_detected_terms'].append((word, word_start, word_end))\n",
    "                        features['turkish_detected_labels'].append((word, word_start, word_end, label))\n",
    "                        \n",
    "                        # Move to the next word's start position\n",
    "                        word_start = word_end + 1                        \n",
    "                        \n",
    "                    if label == 'WRONG_TRANSLATION':\n",
    "                        if meta_text:\n",
    "                            # lowercase meta_text\n",
    "                            meta_text = meta_text.lower()\n",
    "                            features['turkish_detected_corrections'].append((text, start, end, meta_text))\n",
    "                        else:\n",
    "                            features['turkish_detected_corrections'].append((text, start, end, \"EMPTY CORRECTION\"))\n",
    "\n",
    "    results.update(features)\n",
    "    return results\n",
    "\n",
    "import re\n",
    "\n",
    "def word_positions_with_check(sentence):\n",
    "    # Words to exclude\n",
    "    exclude_words = {\"EN\", \"TR\", \"1\", \"2\", \"3\"}\n",
    "    \n",
    "    # Find all the words and their positions using regular expression\n",
    "    words = re.finditer(r'\\b\\w+\\b', sentence)\n",
    "    \n",
    "    # Create a list of tuples (word, start, end) excluding specified words\n",
    "    word_list = [\n",
    "        (match.group(), match.start(), match.end())\n",
    "        for match in words\n",
    "        if match.group() not in exclude_words\n",
    "    ]    \n",
    "    return word_list\n",
    "\n",
    "\n",
    "def get_lists(word_list, intervals_for_sentences):\n",
    "    english_list = []\n",
    "    turkish_list = []\n",
    "\n",
    "    for word in word_list:\n",
    "        start = word[1]\n",
    "        end = word[2]\n",
    "\n",
    "        if 0 <= start < end <= intervals_for_sentences[0][0]:\n",
    "            section = 'EN-1'\n",
    "            english_list.append(word)\n",
    "        elif intervals_for_sentences[0][0] <= start <= end <= intervals_for_sentences[0][1]:\n",
    "            section = 'TR-1'\n",
    "            turkish_list.append(word)\n",
    "        elif intervals_for_sentences[1][0] <= start <= end <= intervals_for_sentences[1][1]:\n",
    "            section = 'EN-2'\n",
    "            english_list.append(word)\n",
    "        elif intervals_for_sentences[2][0] <= start <= end <= intervals_for_sentences[2][1]:\n",
    "            section = 'TR-2'\n",
    "            turkish_list.append(word)\n",
    "        elif intervals_for_sentences[3][0] <= start <= end <= intervals_for_sentences[3][1]:\n",
    "            section = 'EN-3'\n",
    "            english_list.append(word)\n",
    "        elif intervals_for_sentences[4][0] <= start <= end <= intervals_for_sentences[4][1]:\n",
    "            section = 'TR-3'\n",
    "            turkish_list.append(word)\n",
    "\n",
    "    return english_list, turkish_list\n",
    "\n",
    "\n",
    "def term_position(word_list, term_list):\n",
    "    result_list = [\n",
    "    1 if (word, start, end) in term_list else 0\n",
    "    for word, start, end in word_list]\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def calculate_fleiss_kappa(annotators, num_items):\n",
    "    \"\"\"\n",
    "    Calculate Fleiss' Kappa for multiple annotators.\n",
    "    \n",
    "    :param annotators: A list of lists where each sublist represents the annotations by a single annotator.\n",
    "                       Each element in the sublist should be binary (0 for not detected, 1 for detected).\n",
    "    :param num_items: The total number of items (e.g., terms) being evaluated.\n",
    "    :return: Fleiss' kappa score.\n",
    "    \"\"\"\n",
    "    # Count the number of raters and items\n",
    "    num_raters = len(annotators)\n",
    "    \n",
    "    # Initialize an array to store the counts for each category per item\n",
    "    # We assume two categories: 0 (not detected) and 1 (detected)\n",
    "    category_counts = np.zeros((num_items, 2))\n",
    "    \n",
    "    # Populate the category_counts matrix with annotator decisions\n",
    "    for annotator in annotators:\n",
    "        for item_idx, decision in enumerate(annotator):\n",
    "            category_counts[item_idx, decision] += 1\n",
    "\n",
    "    # Compute the proportion of agreement for each item\n",
    "    P_i = (category_counts ** 2).sum(axis=1) - num_raters\n",
    "    P_i = P_i / (num_raters * (num_raters - 1))\n",
    "    \n",
    "    # Compute the mean agreement for all items\n",
    "    P_bar = np.mean(P_i)\n",
    "    \n",
    "    # Compute the proportion of decisions for each category (overall distribution)\n",
    "    p_j = category_counts.sum(axis=0) / (num_raters * num_items)\n",
    "    \n",
    "    # Compute the expected agreement by chance\n",
    "    P_e_bar = (p_j ** 2).sum()\n",
    "    \n",
    "    # Fleiss' kappa\n",
    "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar) if (1 - P_e_bar) != 0 else 0\n",
    "    \n",
    "    return kappa\n",
    "\n",
    "\n"
   ],
   "id": "bade7a69d7aba5fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "f46ee7817641e17e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # if some cells are NaN in the first row, fill them with the last row\n",
    "# \n",
    "# df.iloc[0] = df.iloc[0].fillna(df.iloc[-1])\n",
    "# \n",
    "# # If you want to update the DataFrame in place, this line does it\n",
    "# df.update(df.iloc[0])\n",
    "# \n",
    "# # drop the last row\n",
    "# df = df.drop(df.tail(1).index)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n"
   ],
   "id": "e92fad4a27d841fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "15e1c7e3ae93b257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "english_fleiss_kappas = []\n",
    "turkish_fleiss_kappas = []\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for task_column in task_columns:\n",
    "    \n",
    "    task_num = int(task_column.split('_')[1])\n",
    "    all_paragraph, intervals_for_sentences = extract_paragraph_data(tasks_json[task_num - 1])\n",
    "    english_data, turkish_data = separate_clean_data(all_paragraph)\n",
    "    num_words_en = count_words(english_data, ['EN-1', 'EN-2', 'EN-3'])\n",
    "    num_words_tr = count_words(turkish_data, ['TR-1', 'TR-2', 'TR-3'])\n",
    "    tasks_annotations = df[f'{task_column}'].to_list()\n",
    "    \n",
    "    word_list = word_positions_with_check(all_paragraph)\n",
    "    english_list, turkish_list = get_lists(word_list, intervals_for_sentences)\n",
    "    \n",
    "    \n",
    "    task_results = []\n",
    "    \n",
    "    \n",
    "    for annotation in tasks_annotations:\n",
    "        if annotation is None:\n",
    "            continue\n",
    "        mail = annotation['completed_by']['email']\n",
    "        results = get_annotation_results(annotation, mail, intervals_for_sentences)\n",
    "        results['task'] = task_column\n",
    "        results['term_position_eng'] = term_position(english_list, results['english_detected_terms'])\n",
    "        results['term_position_tr'] = term_position(turkish_list, results['turkish_detected_terms'])\n",
    "        task_results.append(results)\n",
    "                            \n",
    "                            \n",
    "    english_results = []\n",
    "    turkish_results = []\n",
    "    sum_of_english_results = [0] * len(english_list)\n",
    "    sum_of_turkish_results = [0] * len(turkish_list)\n",
    "    \n",
    "    for result in task_results:\n",
    "        english_results.append(result['term_position_eng'])\n",
    "        turkish_results.append(result['term_position_tr'])\n",
    "        \n",
    "        sum_of_english_results = [sum(x) for x in zip(sum_of_english_results, result['term_position_eng'])]\n",
    "        sum_of_turkish_results = [sum(x) for x in zip(sum_of_turkish_results, result['term_position_tr'])]\n",
    "        \n",
    "    \n",
    "    final_format_english = []\n",
    "    \n",
    "    \n",
    "    en_num_of_gold_terms_0_3 = 0\n",
    "    en_num_of_silver_terms_1_2 = 0\n",
    "    en_num_of_gold_nan_terms_3_0 = 0\n",
    "    en_num_of_silver_nan_terms_2_1 = 0\n",
    "    \n",
    "    \n",
    "    for category_2 in sum_of_english_results:\n",
    "        category_1 = 3 - category_2\n",
    "        new_tuple = [category_1, category_2]\n",
    "        final_format_english.append(new_tuple)\n",
    "        \n",
    "        \n",
    "        if new_tuple == [0, 3]:\n",
    "            en_num_of_gold_terms_0_3 += 1\n",
    "        elif new_tuple == [1, 2]:\n",
    "            en_num_of_silver_terms_1_2 += 1\n",
    "        elif new_tuple == [3, 0]:\n",
    "            en_num_of_gold_nan_terms_3_0 += 1\n",
    "        elif new_tuple == [2, 1]:\n",
    "            en_num_of_silver_nan_terms_2_1 += 1\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    final_format_turkish = []\n",
    "    \n",
    "    tr_num_of_gold_terms_0_3 = 0\n",
    "    tr_num_of_silver_terms_1_2 = 0\n",
    "    tr_num_of_gold_nan_terms_3_0 = 0\n",
    "    tr_num_of_silver_nan_terms_2_1 = 0\n",
    "    \n",
    "    for category_2 in sum_of_turkish_results:\n",
    "        category_1 = 3 - category_2\n",
    "        new_tuple = [category_1, category_2]\n",
    "        final_format_turkish.append(new_tuple)\n",
    "        \n",
    "        if new_tuple == [0, 3]:\n",
    "            tr_num_of_gold_terms_0_3 += 1\n",
    "        elif new_tuple == [1, 2]:\n",
    "            tr_num_of_silver_terms_1_2 += 1\n",
    "        elif new_tuple == [3, 0]:\n",
    "            tr_num_of_gold_nan_terms_3_0 += 1\n",
    "        elif new_tuple == [2, 1]:\n",
    "            tr_num_of_silver_nan_terms_2_1 += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    english_fleiss_kappa = calculate_fleiss_kappa(english_results, len(english_list))\n",
    "    turkish_fleiss_kappa = calculate_fleiss_kappa(turkish_results, len(turkish_list))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # SAVE RESULTS TO DATAFRAME\n",
    "    to_be_concat = pd.DataFrame({'group_name': [GROUP_NAME],\n",
    "                                    'task_name': [task_column],\n",
    "                                 'domain': [DOMAIN],\n",
    "                                 'word_list_en': [english_list],\n",
    "                                 'word_list_tr': [turkish_list],\n",
    "                                    'english_annotations': [english_results],\n",
    "                                    'turkish_annotations': [turkish_results],\n",
    "                                'final_format_english': [final_format_english],\n",
    "                                'final_format_turkish': [final_format_turkish], \n",
    "                                'en_num_of_gold_terms_0_3': [en_num_of_gold_terms_0_3],\n",
    "                                'en_num_of_silver_terms_1_2': [en_num_of_silver_terms_1_2],\n",
    "                                 'en_num_of_gold_nan_terms_3_0': [en_num_of_gold_nan_terms_3_0],\n",
    "                                    'en_num_of_silver_nan_terms_2_1': [en_num_of_silver_nan_terms_2_1],\n",
    "                                    'tr_num_of_gold_terms_0_3': [tr_num_of_gold_terms_0_3],\n",
    "                                    'tr_num_of_silver_terms_1_2': [tr_num_of_silver_terms_1_2],\n",
    "                                    'tr_num_of_gold_nan_terms_3_0': [tr_num_of_gold_nan_terms_3_0],\n",
    "                                    'tr_num_of_silver_nan_terms_2_1': [tr_num_of_silver_nan_terms_2_1], \n",
    "                                'english_fleiss_kappa_1': [english_fleiss_kappa],\n",
    "                                'turkish_fleiss_kappa_1': [turkish_fleiss_kappa]})\n",
    "    results_df = pd.concat([results_df, to_be_concat], ignore_index=True)"
   ],
   "id": "b40603cc5bcaf731",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchmetrics.nominal import FleissKappa\n",
    "fleiss_kappa_english = []\n",
    "fleiss_kappa_turkish = []\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    english_annotations = row['final_format_english']\n",
    "    turkish_annotations = row['final_format_turkish']\n",
    "    \n",
    "    # convert to long tensor\n",
    "    english_annotations = torch.tensor(english_annotations).long()\n",
    "    turkish_annotations = torch.tensor(turkish_annotations).long()\n",
    "    metric = FleissKappa(mode='counts')\n",
    "    english_kappa = metric(english_annotations)\n",
    "    turkish_kappa = metric(turkish_annotations)\n",
    "    \n",
    "    fleiss_kappa_english.append(english_kappa.item())\n",
    "    fleiss_kappa_turkish.append(turkish_kappa.item())\n",
    "    "
   ],
   "id": "ac4f7dcb9fe6702e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df['english_fleiss_kappa_2'] = fleiss_kappa_english\n",
    "results_df['turkish_fleiss_kappa_2'] = fleiss_kappa_turkish"
   ],
   "id": "7b7f30c2b345cd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_df.to_csv('results/' + GROUP_NAME + '_results.csv', index=False)",
   "id": "4506935d13304b79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_df\n",
   "id": "1453730ec86fe6eb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
