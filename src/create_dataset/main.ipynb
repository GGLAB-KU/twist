{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import deepl\n",
    "\n",
    "\n",
    "def get_wikipedia_page_details(title):\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts|info|categories|links\",\n",
    "        \"explaintext\": True,\n",
    "        \"inprop\": \"url\",\n",
    "        \"redirects\": True,\n",
    "        \"cllimit\": \"max\",\n",
    "        \"pllimit\": \"max\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return {\"page_id\": 'N/A', \"title\": title, \"summary\": 'Request failed', \"content\": '', \"lastrevid\": 'N/A', \"length\": 0, \"fullurl\": 'N/A', \"categories\": [], \"links\": []}\n",
    "\n",
    "    data = response.json()\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    page = next(iter(pages.values()))\n",
    "\n",
    "    if 'extract' not in page:\n",
    "        return {\"page_id\": 'N/A', \"title\": title, \"summary\": 'Not found', \"content\": '', \"lastrevid\": 'N/A', \"length\": 0, \"fullurl\": 'N/A', \"categories\": [], \"links\": []}\n",
    "\n",
    "    return {\n",
    "        \"page_id\": page.get('pageid', 'N/A'),\n",
    "        \"title\": page.get('title', title),\n",
    "        \"summary\": page.get('extract', 'Summary not found').split('\\n\\n')[0],\n",
    "        \"content\": page.get('extract', 'Content not found'),\n",
    "        \"lastrevid\": page.get('lastrevid', 'N/A'),\n",
    "        \"length\": len(page.get('extract', '')),\n",
    "        \"fullurl\": page.get('fullurl', 'N/A'),\n",
    "        \"categories\": [cat['title'] for cat in page.get('categories', [])],\n",
    "        \"links\": [link['title'] for link in page.get('links', [])]\n",
    "    }\n",
    "\n",
    "# Fetch glossary details for multiple subjects\n",
    "subjects = [\n",
    "    \"Glossary of physics\",\n",
    "    \"Glossary of areas of mathematics\",\n",
    "    \"Glossary of calculus\",\n",
    "    \"Glossary of computer science\",\n",
    "    \"Glossary of artificial intelligence\",\n",
    "    \"Machine learning\",\n",
    "    \"Deep learning\",\n",
    "    \"Natural language processing\"\n",
    "]\n",
    "\n",
    "glossary_data = [get_wikipedia_page_details(subject) for subject in subjects]\n",
    "glossary_df = pd.DataFrame(glossary_data)\n",
    "glossary_df.to_csv(\"outputs/intermediate_outputs/list_of_glossaries.csv\", index=False)\n",
    "\n",
    "# Extracting and processing the links from the dataset\n",
    "data = pd.read_csv(\"outputs/intermediate_outputs/list_of_glossaries.csv\")\n",
    "\n",
    "# Collecting all unique links into a dictionary\n",
    "all_links = {}\n",
    "for index, row in data.iterrows():\n",
    "    links = ast.literal_eval(row['links'])\n",
    "    for link in links:\n",
    "        all_links[link] = {\n",
    "            'source_title': row['title'],\n",
    "            'source_page_id': row['page_id']\n",
    "        }\n",
    "\n",
    "# Converting the dictionary to a DataFrame\n",
    "all_terms_df = pd.DataFrame.from_dict(all_links, orient='index').reset_index().rename(columns={'index': 'linked_page_title'})\n",
    "all_terms_df.sort_values(by=['source_page_id', 'linked_page_title'], inplace=True)\n",
    "all_terms_df.to_csv('outputs/intermediate_outputs/all_terms.csv', index=False)\n",
    "\n",
    "# Function to get paragraphs from a Wikipedia page\n",
    "def get_paragraphs(page_name):\n",
    "    try:\n",
    "        r = requests.get(f'https://en.wikipedia.org/api/rest_v1/page/html/{page_name}')\n",
    "        soup = bs4.BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        paragraphs = [\n",
    "            re.sub(r'(\\[[0-9]+\\])', '', unicodedata.normalize('NFKD', tag.text)).strip()\n",
    "            for tag in soup.find_all('p')\n",
    "            if '.' in tag.text and '\\n' not in tag.text\n",
    "        ]\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to merge short paragraphs into longer ones based on a length limit\n",
    "def merge_short_paragraphs(paragraphs, length_limit):\n",
    "    merged_paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if len(current_paragraph) + len(paragraph) + 1 <= length_limit:\n",
    "            current_paragraph += \" \" + paragraph if current_paragraph else paragraph\n",
    "        else:\n",
    "            if current_paragraph:\n",
    "                merged_paragraphs.append(current_paragraph)\n",
    "            current_paragraph = paragraph\n",
    "\n",
    "    if current_paragraph:\n",
    "        merged_paragraphs.append(current_paragraph)\n",
    "\n",
    "    return merged_paragraphs\n",
    "\n",
    "# Process each linked_page_title in the CSV file with a progress bar\n",
    "length_limit = 1000\n",
    "all_terms_df['paragraphs'] = [get_paragraphs(title) for title in tqdm(all_terms_df['linked_page_title'], desc=\"Fetching paragraphs\")]\n",
    "all_terms_df['samples'] = [merge_short_paragraphs(paragraphs, length_limit) for paragraphs in tqdm(all_terms_df['paragraphs'], desc=\"Merging paragraphs\")\n",
    "\n",
    "# Save the extended DataFrame to a new CSV file\n",
    "all_terms_df.to_csv('outputs/final_outputs/all_terms_with_paragraphs_and_samples.csv', index=False)\n",
    "\n",
    "# save as xlsx \n",
    "all_terms_df.to_excel('outputs/final_outputs/all_terms_with_paragraphs_and_samples.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Replace with your DeepL auth key\n",
    "auth_key = \"bc17660f-cf42-46a1-bd41-ff7e522ac749:fx\"  \n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'outputs/final_outputs/sample_2.5_all_terms_cleaned_en.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the string representation of lists in 'samples' column back to actual lists\n",
    "data['samples'] = data['samples'].apply(ast.literal_eval)\n",
    "\n",
    "# Function to translate a list of samples\n",
    "def translate_samples(samples, target_lang=\"TR\"):\n",
    "    translated = []\n",
    "    for sample in samples:\n",
    "        result = translator.translate_text(sample, source_lang=\"EN\", target_lang=target_lang)\n",
    "        translated.append(result.text)\n",
    "    return translated\n",
    "\n",
    "# Apply the translation to each row in the DataFrame\n",
    "data['translated_samples'] = data['samples'].apply(lambda x: translate_samples(x, target_lang=\"TR\"))\n",
    "\n",
    "# Save the DataFrame to a new Excel file to verify the results\n",
    "output_file_path = 'outputs/final_outputs/translated_samples.xlsx'\n",
    "data.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'outputs/final_outputs/translated_samples.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the string representation of a list to a list\n",
    "df['samples'] = df['samples'].apply(ast.literal_eval)\n",
    "df['translated_samples'] = df['translated_samples'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a JSON structure with sequential IDs\n",
    "json_data = []\n",
    "id_counter = 1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(len(row['samples'])):\n",
    "        json_data.append({\n",
    "            \"id\": id_counter,\n",
    "            \"term_id\": row[\"term_id\"],\n",
    "            \"term_name\": row[\"linked_page_title\"],\n",
    "            \"data\": {\n",
    "                \"my_text\": f\"ENGLISH: {row['samples'][i]} \\n \\n TURKISH: {row['translated_samples'][i]}\"\n",
    "            }\n",
    "        })\n",
    "        id_counter += 1\n",
    "\n",
    "# Save the JSON structure to a file\n",
    "json_file_path = 'label_studio/translated_samples.json'\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(json_data, json_file, ensure_ascii=False, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
