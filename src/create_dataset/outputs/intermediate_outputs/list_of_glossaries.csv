page_id,title,summary,content,lastrevid,length,fullurl,categories,links
36626070,Glossary of physics,"This glossary of physics is a list of definitions of terms and concepts relevant to physics, its sub-disciplines, and related fields, including mechanics, materials science, nuclear physics, particle physics, and thermodynamics. For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of astronomy, Glossary of areas of mathematics, and Glossary of engineering.","This glossary of physics is a list of definitions of terms and concepts relevant to physics, its sub-disciplines, and related fields, including mechanics, materials science, nuclear physics, particle physics, and thermodynamics. For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of astronomy, Glossary of areas of mathematics, and Glossary of engineering.


== A ==

ab initio
A mathematical model which seeks to describe atomic nuclei by solving the non-relativistic Schrödinger equation for all constituent nucleons and the forces that exist between them. Such methods yield precise results for very light nuclei but become more approximate for heavier nuclei.

Abbe number
Also called the V-number or constringence. 
In optics and lens design, a measure of a transparent material's dispersion (a variation of refractive index versus wavelength). High values of V indicate low dispersion.

absolute electrode potential
In electrochemistry, the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface).

absolute humidity
The ratio of the water vapor in a sample of air to the volume of the sample.

absolute motion

absolute pressure
Is zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure.

absolute scale
Any system of measurement that begins at a minimum, or zero point, and progresses in only one direction. The zero point of an absolute scale is a natural minimum, leaving only one direction in which to progress, whereas an arbitrary or ""relative"" scale begins at some point selected by a person and can progress in both directions.

absolute zero
The theoretical lowest possible temperature, understood by international agreement as equivalent to 0 Kelvin or −273.15 °C (−459.67 °F). More formally, it is the theoretical lower limit of the thermodynamic temperature scale, at which enthalpy and entropy of a cooled ideal gas reach their minimum values and the fundamental particles of nature have minimal vibrational motion.

absorption spectroscopy
Any of various spectroscopic techniques that measure the absorption of electromagnetic radiation due to its interaction with a sample. The sample absorbs energy, i.e. photons, from the radiating field. The intensity of the absorption varies as a function of frequency or wavelength, and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum.

absorptivity

accelerating expansion of the universe
The observation that the expansion of the universe is such that the velocity at which a distant galaxy is receding from the observer is continuously increasing with time.

acceleration
The rate at which the velocity of a body changes with time, also the rate of change of the rate at which the position of a body changes with time.

acceleration due to gravity
The acceleration on an object caused by the force of gravitation.

accelerometer
An instrument used to measure the proper acceleration of a body irrespective of other forces.

acoustics
The branch of physics dealing with the production, transmission, and effects of sound.

adhesion
adhesion is what makes things stick together.
It's the force that allows tape to stick to a surface or glue to hold two objects together. Contrast cohesion.

adiabatic cooling

adiabatic heating

adiabatic process
A process which occurs without transfer of heat or mass of substances between a thermodynamic system and its surroundings. In an adiabatic process, energy is transferred to the surroundings only as work. The adiabatic process provides a rigorous conceptual basis for the theory used to expound the first law of thermodynamics, and as such it is a key concept in thermodynamics.

aerodynamics
The study of the motion of air, particularly its interaction with a solid object, such as an airplane wing. It is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.

afocal system
An optical system that produces no net convergence or divergence of the beam, i.e. has an infinite effective focal length. This type of system can be created with a pair of optical elements where the distance between the elements is equal to the sum of each element's focal length (
  
    
      
        d
        =
        
          f
          
            1
          
        
        +
        
          f
          
            2
          
        
      
    
    {\displaystyle d=f_{1}+f_{2}}
  
).

air mass
1.  In meteorology, a volume of air that is defined by its temperature and water vapor content. Air masses may cover many hundreds or thousands of square miles and generally adapt to the characteristics of the surface below them. They are often classified according to their latitude and their source regions.
2.  In astronomy, the ""amount of air that one is looking through"" when observing a star or other celestial source from a vantage point that is within Earth's atmosphere. It is formulated as the integral of air density along the light ray.

air mass coefficient
Defines the direct optical path length through the Earth's atmosphere, expressed as a ratio relative to the path length vertically upwards, i.e. at the zenith. The air mass coefficient can be used to help characterize the solar spectrum after solar radiation has traveled through the atmosphere.

albedo
The fraction of the total light incident on a reflecting surface, especially a celestial body, which is reflected back in all directions.

alloy
A chemical mixture of a metal with one or more other metals or other elements.

alpha decay
Also α-decay.
A type of radioactive decay in which an atomic nucleus emits an alpha particle and thereby transforms or ""decays"" into a different atomic nucleus, with a mass number that is reduced by four and an atomic number that is reduced by two.

alpha particle (α)
Also symbolized by α2+, He2+, and 42He2+.
A type of subatomic particle consisting of two protons and two neutrons bound together into a particle identical to the nucleus of a helium-4 ion. It has a charge of +2 e and a mass of 4 u. Alpha particles are classically produced in the process of radioactive alpha decay, but may also be produced in other ways and given the same name.

alternating current (AC)
A form of electric current in which the movement of electric charge periodically reverses direction. Contrast direct current.

ammeter
An instrument that is used to measure electric current.

amorphous solid
A type of solid which does not have a definite geometric shape.

ampere (A)
Often abbreviated as amp.
The SI base unit of electric current, defined as one coulomb of electric charge per second.

amplifier
Also electronic amplifier or (informally) amp.
An electronic device that can increase the power of a signal (a time-varying voltage or current). It is a two-port electronic circuit that uses electric power from a power supply to increase the amplitude of a signal applied to its input terminals, producing a proportionally greater amplitude signal at its output. The amount of amplification provided by an amplifier is measured by its gain: the ratio of output voltage, current, or power to input. An amplifier is a circuit that has a power gain greater than one.

amplitude
The height of a wave as measured from its center (normal) position.

angle of incidence
In geometric optics, the angle between a ray incident on a surface and the line perpendicular to the surface at the point of incidence, called the normal. The ray can be formed by any wave: optical, acoustic, microwave, X-ray, etc.

angle of reflection
The change in direction of a wavefront at an interface between two different media so that the wavefront returns into the medium from which it originated. Common examples include the reflection of light, sound, and water waves. The law of reflection says that for specular reflection the angle at which the wave is incident on the surface equals the angle at which it is reflected. Mirrors exhibit specular reflection.

ångström (Å)
A unit of length primarily used to measure subatomic particles that is equal to 10−10 metres (one ten-billionth of a metre) or 0.1 nanometres.

angular acceleration
The time rate of change of angular velocity. In three dimensions, it is a pseudovector. In SI units, it is measured in radians per second squared (rad/s2), and is usually denoted by the Greek letter alpha (α). Just like angular velocity, there are two types of angular acceleration: spin angular acceleration and orbital angular acceleration, representing the time rate of change of spin angular velocity and orbital angular velocity, respectively. Unlike linear acceleration, angular acceleration need not be caused by a net external torque. For example, a figure skater can speed up her rotation (thereby obtaining an angular acceleration) simply by contracting her arms inwards, which involves no external torque.

angular displacement
The angle (in radians, degrees, or revolutions) through which a point revolving around a centre or line has been rotated in a specified sense about a specified axis.

angular frequency (ω)
Also angular speed, radial frequency, circular frequency, orbital frequency, radian frequency, and pulsatance.
A scalar measure of rotation rate. It refers to the angular displacement per unit time (e.g. in rotation) or the rate of change of the phase of a sinusoidal waveform (e.g. in oscillations and waves), or as the rate of change of the argument of the sine function. Angular frequency (or angular speed) is the magnitude of the vector quantity that is angular velocity. The term angular frequency vector 
  
    
      
        
          
            
              ω
              →
            
          
        
      
    
    {\displaystyle {\vec {\omega }}}
  
 is sometimes used as a synonym for the vector quantity angular velocity.

One revolution is equal to 2π radians, hence

  
    
      
        ω
        =
        
          
            
              2
              π
            
            T
          
        
        =
        
          2
          π
          f
        
        ,
      
    
    {\displaystyle \omega ={{2\pi } \over T}={2\pi f},}
  

where:
ω is the angular frequency or angular speed (measured in radians per second),
T is the period (measured in seconds),
f is the ordinary frequency (measured in hertz) (sometimes symbolised with ν).

angular momentum
Also (rarely) moment of momentum or rotational momentum.
The rotational equivalent of linear momentum. It is an important quantity in physics because it is a conserved quantity–that is, the total angular momentum of a closed system remains constant.

angular velocity (ω)
How fast an object rotates or revolves relative to another point, i.e. how fast the angular position or orientation of an object changes with time. There are two types of angular velocity: orbital angular velocity and spin angular velocity. Spin angular velocity refers to how fast a rigid body rotates with respect to its centre of rotation. Orbital angular velocity refers to how fast a rigid body's centre of rotation revolves about a fixed origin, i.e. the time rate of change of its angular position relative to the origin. In general, angular velocity is measured in angle per unit time, e.g. radians per second. The  SI unit of angular velocity is expressed as radians/sec with the radian having a dimensionless value of unity, thus the SI units of angular velocity are listed as 1/sec. Angular velocity is usually represented by the Greek letter omega (ω, sometimes Ω). By convention, positive angular velocity indicates counter-clockwise rotation, while negative is clockwise.

anion
A negatively charged ion. Contrast cation.

annihilation
In particle physics, the process that occurs when a subatomic particle collides with its respective antiparticle to produce other particles, such as an electron colliding with a positron to produce two photons. The total energy and momentum of the initial pair are conserved in the process and distributed among a set of other particles in the final state. Antiparticles have exactly opposite additive quantum numbers from particles, so the sums of all quantum numbers of such an original pair are zero. Hence, any set of particles may be produced whose total quantum numbers are also zero as long as conservation of energy and conservation of momentum are obeyed.

anode
The electrode through which a conventional electric current flows into a polarized electrical device; the direction of current flow is, by convention, opposite to the direction of electron flow, and so electrons flow out of the anode. In a galvanic cell, the anode is the negative terminal or pole which emits electrons toward the external part of an electrical circuit. However, in an electrolytic cell, the anode is the wire or plate having excess positive charge, so named because negatively charged anions tend to move towards it. Contrast cathode.

anti-gravity
A theory of creating a place or object that is free from the force of gravity. It does not refer to the lack of weight under gravity experienced in free fall or orbit, or to balancing the force of gravity with some other force, such as electromagnetism or aerodynamic lift.

antimatter

antineutron
The antiparticle of the neutron, with symbol n. It differs from the neutron only in that some of its properties have equal magnitude but opposite sign. It has the same mass as the neutron, and no net electric charge, but has opposite baryon number (+1 for neutron, −1 for the antineutron). This is because the antineutron is composed of antiquarks, while neutrons are composed of quarks. The antineutron consists of one up antiquark and two down antiquarks.

antiparticle
In particle physics, every type of particle has an associated antiparticle with the same mass but with opposite physical charges such as electric charge. For example, the antiparticle of the electron is the antielectron (which is often referred to as the positron). While the electron has a negative electric charge, the positron has a positive electric charge, and is produced naturally in certain types of radioactive decay. Some particles, such as the photon, are their own antiparticle. Otherwise, for each pair of antiparticle partners, one is designated as ""normal"" matter (the kind comprising all matter with which humans usually interact), and the other (usually given the prefix ""anti-"") as antimatter.

antiproton
 It is a subatomic particle of the same mass as a proton but having a negative electric charge and oppositely directed magnetic moment. It is the proton’s antiparticle. Antiprotons were first produced and identified in 1955 by Emilio Segrè, Owen Chamberlain

antiquark
For every quark flavor there is a corresponding type of antiparticle known as an antiquark that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign.

arc length

Archimedes' principle
A physical principle which states that the upward buoyant force that is exerted on a body immersed in a fluid, whether fully or partially submerged, is equal to the weight of the fluid that the body displaces and acts in the upward direction at the center of mass of the displaced fluid.

area moment of inertia

astrophysics
The branch of astronomy that deals with the physics of the Universe, especially with the compositional nature of celestial bodies rather than their positions or motions in space.

attenuation coefficient
The measure of how much the incident energy beam (e.g. ultrasound or x-rays) is weakened by the material it is passing through.

atom
A basic unit of matter that consists of a dense central nucleus surrounded by a cloud of negatively charged electrons. The atomic nucleus contains a mix of positively charged protons and electrically neutral neutrons.

atomic line filter

atomic mass

atomic mass unit
A deprecated term, usually referring to the unified atomic mass unit, a carbon-based standard, but historically referring to an oxygen-based standard.

atomic number (Z)
The number of protons found in the nucleus of an atom. It is most often used to classify elements within the periodic table.

atomic orbital

atomic packing factor

atomic physics
A branch of physics that studies atoms as isolated systems of electrons and an atomic nucleus. Compare nuclear physics.

atomic structure

atomic weight (A)
The sum total of protons (or electrons) and neutrons within an atom.

audio frequency
A periodic vibration whose frequency is in the band audible to the average human, the human hearing range.  It is the property of sound that most determines pitch, with a generally accepted standard hearing range for humans is 20 to 20,000 Hz.  Also known as audible frequency (AF)

Avogadro constant
The ratio of the number of constituent particles in a substance, usually atoms or molecules, to the amount of substance, of which the SI unit is the mole. It is defined as exactly 6.02214076×1023 mol−1.

Avogadro number
The total number of individual molecules in one mole of a substance, by definition equaling exactly 6.02214076×1023.

Avogadro's law
A physical law which states that volumes of gases which are equal to each other at the same temperature and pressure will contain equal numbers of molecules.

axion
A hypothetical subatomic particle postulated to account for the rarity of processes that break charge-parity symmetry. It is very light, electrically neutral, and pseudoscalar.

azimuthal quantum number
A quantum number for an atomic orbital that determines its orbital angular momentum and describes the shape of the orbital.


== B ==

Babinet's principle
A theorem concerning diffraction which states that the diffraction pattern from an opaque body is identical to that from a hole of the same size and shape except for the overall forward beam intensity.

background radiation
The ubiquitous ionizing radiation to which the general human population is exposed.

Balanced Forces
When all the forces acting upon an object balance each other, the object will be at equilibrium; it will not accelerate.

ballistics

Balmer series
Also Balmer lines.
In atomic physics, one of a set of six named series describing the spectral line emissions of the hydrogen atom. The Balmer series is calculated using the Balmer formula, an empirical equation discovered by Johann Balmer in 1885.

barometer
A scientific instrument used in meteorology to measure atmospheric pressure. Pressure tendency can forecast short-term changes in the weather.

baryon
A subatomic particle such as a proton or a neutron, each of which is made of (usually) three quarks. Nearly all matter humans are likely to encounter is baryonic matter.

battery
A combination of two or more electrical cells which produces electricity.

beam
A structural element that is capable of withstanding load primarily by resisting bending. Beams are traditionally descriptions of building or civil engineering structural elements, but smaller structures such as truck or automobile frames, machine frames, and other mechanical or structural systems contain beam structures that are designed and analyzed in a similar fashion.

bending
Also known as flexure. 
The behavior of a slender structural element subjected to an external load applied perpendicularly to a longitudinal axis of the element.

bending moment
The reaction induced in a structural element when an external force or moment is applied to the element, causing the element to bend. The simplest structural element subjected to bending moments is the beam.

Bernoulli equation

Bernoulli's principle
In fluid dynamics, a principle which states that an increase in the speed of a fluid occurs simultaneously with a decrease in pressure or a decrease in the fluid's potential energy.: Ch.3 : 156–164, § 3.5 

Bessel function
A canonical solution y(x) of Friedrich Bessel's differential equation

  
    
      
        
          x
          
            2
          
        
        
          
            
              
                d
                
                  2
                
              
              y
            
            
              d
              
                x
                
                  2
                
              
            
          
        
        +
        x
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        +
        
          (
          
            
              x
              
                2
              
            
            −
            
              α
              
                2
              
            
          
          )
        
        y
        =
        0
      
    
    {\displaystyle x^{2}{\frac {d^{2}y}{dx^{2}}}+x{\frac {dy}{dx}}+\left(x^{2}-\alpha ^{2}\right)y=0}
  

for an arbitrary complex number α, the order of the Bessel function. Although α and −α produce the same differential equation, it is conventional to define different Bessel functions for these two values in such a way that the Bessel functions are mostly smooth functions of α. The most important cases are when α is an integer or half-integer. Bessel functions for integer α are also known as cylinder functions or the cylindrical harmonics because they appear in the solution to Laplace's equation in cylindrical coordinates. Spherical Bessel functions with half-integer α are obtained when the Helmholtz equation is solved in spherical coordinates.

beta decay
Also β-decay.
In nuclear physics, a type of radioactive decay in which a beta particle is emitted from an atomic nucleus, transforming the original nuclide to its isobar.

beta particle
A high-energy, high-speed electron or positron emitted by certain types of radioactive atomic nuclei.

Big Bang
The prevailing cosmological model that describes the early development of the Universe.

binding energy
The mechanical energy required to disassemble a whole into separate parts. A bound system typically has a lower potential energy than the sum of its constituent parts.

binomial random variable

biocatalysis

biophysics
An interdisciplinary science using methods of and theories from physics to study biological systems.

black body
A hypothetical idealized physical body that completely absorbs all incident electromagnetic radiation, regardless of frequency or angle of incidence. Perfect black bodies are imagined as substitutes for actual physical bodies in many theoretical discussions of thermodynamics, and the construction of nearly perfect black bodies in the real world remains a topic of interest for materials engineers. Contrast white body.

black-body radiation
The type of electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an opaque and non-reflective body) held at constant, uniform temperature. The radiation has a specific spectrum and intensity that depends only on the temperature of the body.

block and tackle
A system of two or more pulleys with a rope or cable threaded between them, usually used to lift or pull heavy loads.

Bohr model

boiling point
The temperature at which a liquid undergoes a phase change into a gas; the vapour pressure of liquid and gas are equal at this temperature.

boiling point elevation
The phenomenon by which the boiling point of a liquid (a solvent) increases when another compound is added, meaning that the resulting solution has a higher boiling point than the pure solvent. This happens whenever a non-volatile solute, such as a salt, is added to a pure solvent, such as water. The boiling point can be measured accurately using an ebullioscope.

Boltzmann constant
A physical constant relating the average kinetic energy of the particles in a gas with the temperature of the gas. It is the gas constant R divided by the Avogadro constant NA.

Bose–Einstein condensate (BEC)

boson
A type of subatomic particle that behaves according to Bose–Einstein statistics and possesses integer spin. Bosons include elementary particles such as photons, gluons, W and Z bosons, Higgs bosons, and the hypothetical graviton, as well as certain composite particles such as mesons and stable nuclides of even mass number. Bosons constitute one of two main classes of particles, the other being fermions. Unlike fermions, there is no limit to the number of bosons that can occupy the same quantum state.

Boyle's law
A chemical law which states that the volume of a given mass of a gas at constant temperature is inversely proportional to its pressure.

Bra–ket notation

Bragg's law

bremsstrahlung
Radiation emitted by the acceleration of unbound charged particles.

Brewster's angle
Also called the polarization angle.
The angle of incidence at which light with a particular polarization is completely transmitted through a transparent dielectric surface, with no reflection. When unpolarized light is incident at this angle, the light that is reflected is consequently perfectly polarized.

british thermal unit (btu)
An Imperial unit of energy defined as the amount of energy needed to heat one pound of water by one degree Fahrenheit; 1 btu is equal to about 1,055 joules. In scientific contexts the btu has largely been replaced by the SI unit of energy, the joule.

brittleness
The tendency of a material to break without significant plastic deformation when subjected to stress. Brittle materials absorb relatively little energy prior to fracture, even those of high strength. Breaking is often accompanied by a snapping sound.

Brownian motion
Also called pedesis.
The presumably random movement of particles suspended in a fluid (liquid or gas) resulting from their bombardment by fast-moving atoms or molecules in the gas or liquid.

Bubble

Bulk modulus
A measure of a substance's resistance to uniform compression defined as the ratio of the infinitesimal pressure increase to the resulting relative decrease of the volume. Its base unit is the pascal.

buoyancy
An upward force exerted by a fluid that opposes the weight of an immersed object.


== C ==

calculus
A branch of mathematics that studies change and has two major sub-fields: differential calculus (concerning rates of change and slopes of curves), and integral calculus (concerning accumulation of quantities and the areas under and between curves). These two branches are related to each other by the fundamental theorem of calculus.

capacitance
The ratio of the change in the electric charge of a system to the corresponding change in its electric potential. There are two closely related notions of capacitance: self capacitance and mutual capacitance. Any object that can be electrically charged exhibits self capacitance. A material with a large self capacitance holds more electric charge at a given voltage than one with low capacitance. The notion of mutual capacitance is particularly important for understanding the operations of the capacitor, one of the three elementary linear electronic components (along with resistors and inductors).

capacitive reactance
An opposition to the change of voltage across an electrical circuit element. Capacitive reactance 
  
    
      
        
          
            
              X
              
                C
              
            
          
        
      
    
    {\displaystyle \scriptstyle {X_{C}}}
  
 is inversely proportional to the signal frequency 
  
    
      
        
          
            f
          
        
      
    
    {\displaystyle \scriptstyle {f}}
  
 (or angular frequency, ω) and the capacitance 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle \scriptstyle {C}}
  
.

capacitor
An electrical circuit element consisting of two conductors separated by an insulator (also known as a dielectric).

Carnot cycle
A theoretical ideal thermodynamic cycle proposed by French physicist Nicolas Léonard Sadi Carnot in 1824 and expanded upon by others in the 1830s and 1840s. It provides an upper limit on the efficiency that any classical thermodynamic engine can achieve during the conversion of heat into work, or conversely, the efficiency of a refrigeration system in creating a temperature difference by the application of work to the system. It is not an actual thermodynamic cycle but is a theoretical construct.

Cartesian coordinate system
A coordinate system that specifies each point uniquely in a plane by a set of numerical coordinates, which are the signed distances to the point from two fixed perpendicular oriented lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis (plural axes) of the system, and the point where they meet is called the origin, at ordered pair (0, 0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.

cathode
The electrode through which a conventional electric current flows out of a polarized electrical device; the direction of current flow is, by convention, opposite to the direction of electron flow, and so electrons flow into the cathode. In a galvanic cell, the cathode is the positive terminal or pole which accepts electrons flowing from the external part of an electrical circuit. However, in an electrolytic cell, the cathode is the wire or plate having excess negative charge, so named because positively charged cations tend to move towards it. Contrast anode.

cathode ray

cation
A positively charged ion. Contrast anion.

celestial mechanics

Celsius scale
Also centigrade scale.
A scale and unit of measurement of temperature.

center of curvature

center of gravity
The point in a body around which the resultant torque due to gravity forces vanish. Near the surface of the earth, where gravity acts downward as a parallel force field, the center of gravity and the center of mass are the same.

center of mass
Within a given distribution of mass, the unique point in space at which the weighted relative position of the distributed mass sums to zero.

center of pressure

centigrade
See Celsius scale.

central-force problem
A classic problem in potential theory involving the determination of the motion of a particle in a single central potential field. The solutions to such problems are important in classical mechanics, since many naturally occurring forces, such as gravity and electromagnetism, are central forces.

centrifugal force
The apparent outward force that draws a rotating body away from the centre of rotation. It is caused by the inertia of the body as the body's path is continually redirected.

centripetal force
A force which keeps a body moving with a uniform speed along a circular path and is directed along the radius towards the centre.

cGh physics
Any attempt in mainstream physics to unify existing theories of relativity, gravitation, and quantum mechanics, particularly by envisioning the three universal constants fundamental to each field – the speed of light (
  
    
      
        c
      
    
    {\displaystyle c}
  
), the gravitational constant (
  
    
      
        G
      
    
    {\displaystyle G}
  
), and the Planck constant (
  
    
      
        h
      
    
    {\displaystyle h}
  
) – as the edges of a three-dimensional cube, at each corner of which is positioned a major sub-field within theoretical physics according to which of the three constants are accounted for by that sub-field and which are ignored. One corner of this so-called ""cube of theoretical physics"", where all three constants are accounted for simultaneously, has not yet been satisfactorily described: quantum gravity.

chain reaction
A sequence of reactions in which a reactive product or byproduct causes additional similar reactions to take place.

change of base rule

charge carrier

chemical physics
A branch of chemistry and physics that studies chemical processes from the point of view of physics by investigating physicochemical phenomena using techniques from atomic and molecular physics and condensed matter physics.

chromatic aberration

circular motion

classical mechanics
Also called Newtonian mechanics.
A sub-field of mechanics concerned with the set of physical laws describing the motion of bodies under the collective actions of a system of forces.

coefficient of friction

coherence

cohesion
The tendency of similar particles or surfaces to cling to one another. Contrast adhesion.

cold fusion

complex harmonic motion

composite particle

Compton scattering
A type of light–matter interaction in which a photon is scattered by a charged particle, usually an electron, which results in part of the energy of the photon being transferred to the recoiling electron; a resulting decrease in the energy of the photon is called the Compton effect. The opposite phenomenon occurs in inverse Compton scattering, when a charged particle transfers part of its energy to a photon.

concave lens

condensation point

condensed matter physics
A branch of physics that studies the physical properties of condensed phases of matter.

conservation of momentum

conservation law

constructive interference

continuous spectrum

continuum mechanics

convection
The transfer of heat by the actual transfer of matter.

convex lens

coulomb (C)
The SI derived unit of electric charge, defined as the charge transported by a constant current of one ampere in one second.

Coulomb's law

converging lens

cosmic background radiation

creep

crest
The point on a wave with the maximum value or upward displacement within a cycle.

crest factor

critical angle

critical mass
The smallest amount of fissile material needed for a sustained nuclear chain reaction.

cube of theoretical physics
See cGh physics.

Curie temperature

current density

current length

curvilinear motion
The motion of a moving particle or object that conforms to a known or fixed curve. Such motion is studied with two coordinate systems: planar motion and cylindrical motion.

cyclotron
A type of particle accelerator in which charged particles accelerate outwards from the center along a spiral path.


== D ==

Dalton's law

damped vibration

Damping ratio
Any influence upon or within an oscillatory system that has the effect of reducing, restricting, or preventing its oscillations. Damping is a result of processes that dissipate the energy stored in the oscillation.

Darcy–Weisbach equation

dark energy

dark matter

DC motor
A mechanically commutated electric motor powered by direct current.

decibel

definite integral

deflection
The degree to which a structural element is displaced under a load. It may refer to an angle or a distance.

deformation
1.  (mechanics)
2.  (engineering)

density
Also called mass density.
A physical property of a substance defined as its mass per unit volume.

derivative
For a mathematical function of a real variable, a measurement of the sensitivity to change of the function value (output) with respect to a change in its argument (input); e.g. the derivative of the position of a moving object with respect to time is the object's velocity and measures how quickly the position of the object changes as time changes. Derivatives are a fundamental tool of calculus.

destructive interference

diamagnetism

dielectric
An electrical insulator that can be polarized by an applied electric field. When a dielectric material is placed in an electric field, electric charges do not flow through the material as they would in a conductor but only shift slightly from their equilibrium positions, with positive charges displaced in the direction of the field's flow and negative charges displaced in the opposite direction; this creates an internal electric field that reduces the larger field within the dielectric material.

diffraction

direct current (DC)

dispersion

displacement
1.  (fluid) Occurs when an object is immersed in a fluid, pushing it out of the way and taking its place. The volume of the immersed object will be exactly equal to the volume of the displaced fluid, so that the volume of the immersed object can be deduced if the volume of the displaced fluid is measured.
2.  (vector) The shortest distance from the initial to the final position of a point. Thus, it is the length of an imaginary straight path, typically distinct from the path actually travelled by.

distance
A numerical description of how far apart objects are.

drift velocity

Doppler effect
The change in frequency of a wave (or other periodic event) for an observer moving relative to its source. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.

drag
Forces which act on a solid object in the direction of the relative fluid flow velocity. Unlike other resistive forces, such as dry friction, which is nearly independent of velocity, drag forces depend on velocity.

ductility
A solid material's ability to deform under tensile stress; this is often characterized by the material's ability to be stretched into a wire.

dynamics
The branch of classical mechanics that studies forces and torques and their effects on motion, as opposed to kinematics, which studies motion without reference to these forces.

dyne


== E ==

econophysics

elastic collision

elastic energy

elastic instability

elastic modulus

elasticity
The tendency of a material to return to its original shape after it is deformed.

electric charge
A physical property of matter that causes it to experience a force when near other electrically charged matter. There are two types of electric charge: positive and negative.

electric circuit
An electrical network consisting of a closed loop, giving a return path for the current.

electric current
A flow of electric charge through a conductive medium.

electric displacement field

electric field
The region of space surrounding electrically charged particles and time-varying magnetic fields. The electric field represents the force exerted on other electrically charged objects by the electrically charged particle the field is surrounding.

electric field gradient

electric field intensity

electric generator

electric motor

electric potential

electric power
The rate at which electric energy is transferred by an electric circuit.

electrical conductor
Any material which contains movable electric charges and therefore can conduct an electric current under the influence of an electric field.

electrical insulator
Any material whose internal electric charges do not flow freely and which therefore does not conduct an electric current under the influence of an electric field.

electrical potential energy

electrical and electronics engineering

electrical network
An interconnection of electrical elements such as resistors, inductors, capacitors, voltage sources, current sources, and switches.

electrical resistance
The opposition to the passage of an electric current through an electrical element.

electricity
The set of physical phenomena associated with the presence and flow of electric charges.

electro-optic effect

electrochemical cell

electrodynamics

electrolytic cell

electromagnet
A type of magnet in which the magnetic field is produced by the flow of electric current.

electromagnetic field
Also abbreviated EM field or EMF.
A physical field produced by moving electrically charged objects.

electromagnetic induction

electromagnetic radiation
Also abbreviated EM radiation or EMR.
A form of energy emitted and absorbed by charged particles, which exhibits wave-like behavior as it travels through space.

electromagnetic spectrum

electromagnetic wave equation

electromagnetism

electromechanics

electromotive force (
  
    
      
        
          
            E
          
        
      
    
    {\displaystyle {\mathcal {E}}}
  
)
Also abbreviated emf.
The electrical intensity or ""pressure"" developed by a source of electrical energy such as a battery or generator and measured in volts. Any device that converts other forms of energy into electrical energy provides electromotive force as its output.

electron
A subatomic particle with a negative elementary electric charge.

electron capture

electron cloud

electron pair

electron paramagnetic resonance
Also called electron spin resonance (ESR) and electron magnetic resonance (EMR).
A method for studying materials with unpaired electrons which makes use of the Zeeman effect. It shares some basic principles with nuclear magnetic resonance (NMR).

electronvolt (eV)
A unit of energy equal to approximately 1.6×10−19 joule. By definition, it is the amount of energy gained by the charge of a single electron moved across an electric potential difference of one volt.

electronegativity
A chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself.

electronics
A field that deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, and integrated circuits as well as associated passive interconnection technologies.

electrostatics

electrostriction

elementary charge

elementary particle

emission spectrum

emissivity

energy
The ability to do work.

energy level

endothermic
An adjective used to refer to a process or reaction in which a system absorbs energy from its surroundings, usually in the form of heat but also in the form of light, electricity, or sound. Contrast exothermic.

engineering physics

enthalpy

entropy
A quantity which describes the randomness of a substance or system.

equilibrant force

equipartition

escape velocity
The velocity at which the kinetic energy plus the gravitational potential energy of an object is zero. It is the speed needed to ""escape"" from a gravitational field without further propulsion.

excited state

exothermic
An adjective used to refer to a process or reaction that releases energy from a system, usually in the form of heat but also in the form of light, electricity, or sound. Contrast endothermic.

experimental physics


== F ==

farad

falling bodies
Objects that are moving towards a body with greater gravitational influence, such as a planet.

faraday

Faraday constant

Fermat's principle

Fermi surface

fermion
A type of particle that behaves according to Fermi–Dirac statistics, obeys the Pauli exclusion principle, and possesses half-integer spin. Fermions include all quarks and leptons, as well as all composite particles made of an odd number of these (such as all baryons and many atoms and nuclei). Fermions constitute one of two main classes of particles, the other being bosons.

ferrimagnetism

ferromagnetism

field line

first law of thermodynamics

fission
Either a nuclear reaction or a radioactive decay process in which the nucleus of an atom splits into smaller parts (lighter nuclei), often producing free neutrons and photons (in the form of gamma rays) and releasing relatively large amounts of energy.

flavour

fluid

fluid mechanics

fluid physics

fluid statics

fluorescence

flux

flux density

focal length

focus

force (F)
A push or pull. Any interaction that, when unopposed, will change the motion of a physical body. A force has both magnitude and direction, making it a vector quantity. The SI unit used to measure force is the newton.

force carrier

Force field (physics)

frame of reference

Fraunhofer lines

free body diagram

frequency

frequency modulation

free fall
Any motion of a body where its own weight is the only force acting upon it.

freezing point
The temperature at which a substance changes state from liquid to solid.

friction

function

fundamental forces
Also called fundamental interactions.

fundamental frequency

fundamental theorem of calculus

fusion
A nuclear reaction in which two or more atomic nuclei join together, or ""fuse"", to form a single heavier nucleus.


== G ==

gamma ray
A form of electromagnetic radiation of very high frequency and therefore very high energy.

gas

general relativity

geophysics

gluon

Graham's law of diffusion

gravitation
Also called gravity.
A natural phenomenon by which physical bodies attract each other with a force proportional to their masses.

gravitational constant (G)
Also called the universal gravitational constant and Newton's constant.
A physical constant involved in the calculation of gravitational force between two bodies.

gravitational energy
The potential energy associated with the gravitational field.

gravitational field
A model used to explain the influence that a massive body extends into the space around itself, producing a force (gravity) on another massive body. Thus, a gravitational field is used to explain and represent gravitational phenomena. It is measured in newtons per kilogram (N/kg).

gravitational potential
The gravitational potential at a location is equal to the work (energy transferred) per unit mass that is done by the force of gravity to move an object to a fixed reference location.

gravitational wave
A ripple in the curvature of spacetime that propagates as a wave and is generated in certain gravitational interactions, travelling outward from their source.

graviton

gravity
See gravitation.

ground

ground reaction force

ground state

group velocity


== H ==

hadron
A composite particle made from three quarks or three antiquarks baryon, or one quark and one antiquark meson.

half-life
The time required for a quantity to fall to half its value as measured at the beginning of the time period. In physics, half-life typically refers to a property of radioactive decay, but may refer to any quantity which follows an exponential decay.

Hamilton's principle

Hamiltonian mechanics

harmonic mean

heat
A form of energy transferred from one body to another by thermal interaction.

heat transfer

Helmholtz free energy

hertz
The SI unit of frequency, defined as the number of cycles per second of a periodic phenomenon.

Higgs boson

homeokinetics
The physics of complex, self-organizing systems.

horsepower (hp)

Huygens–Fresnel principle

hydrostatics


== I ==

ice point
A physical process that results in the phase transition of a substance from a liquid to a solid.

impedance
The measure of the opposition that a circuit presents to a current when a voltage is applied.

Implosion

impulse
The change in momentum, which is equal to the average net external force multiplied by the time this force acts.

indefinite integral

inductance

infrasound

inertia
The resistance of any physical object to a change in its state of motion or rest, or the tendency of an object to resist any change in its motion.

inductive reactance

integral

integral transform

International System of Units (SI)
The modern form of the metric system, comprising a system of units of measurement devised around seven base units and the convenience of the number ten.

invariant mass

ion
An atom or molecule in which the total number of electrons is not equal to the total number of protons, giving the atom a net positive or negative electric charge.

ionic bond
A type of chemical bond formed through an electrostatic attraction between two oppositely charged ions.

ionization
The process of converting an atom or molecule into an ion by adding or removing charged particles such as electrons or other ions.

ionization chamber

ionizing radiation

isotope
A variant of a particular chemical element. While all isotopes of a given element share the same number of protons, each isotope differs from the others in its number of neutrons.


== J ==

Josephson effect

joule
A derived unit of energy, work, or amount of heat in the International System of Units.


== K ==

Kelvin
A scale and unit of measurement of temperature. The Kelvin scale is an absolute thermodynamic temperature scale which uses absolute zero as its null point.

kinematics
The branch of classical mechanics that describes the motion of points, bodies (objects), and systems of bodies (groups of objects) without consideration of the causes of motion. The study of kinematics is often referred to as the ""geometry of motion"".

kinetic energy
The energy that a physical body possesses due to its motion, defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. The body continues to maintain this kinetic energy unless its velocity changes. Contrast potential energy.

Kirchhoff's circuit laws
Also called Kirchhoff's rules or simply Kirchhoff's laws.
Two approximate equalities that deal with the current and voltage in electrical circuits. See Kirchhoff's laws for other meanings of the term.

Kirchhoff's equations
In fluid dynamics, a set of equations which describe the motion of a rigid body in an ideal fluid.


== L ==

Lagrangian mechanics

laminar flow
Also called streamline flow. 
Occurs when a fluid flows in parallel layers with no disruption between the layers.

Laplace transform

Laplace–Runge–Lenz vector
Also abbreviated LRL vector.
A vector used chiefly to describe the shape and orientation of the orbit of one astronomical body around another, such as a planet revolving around a star. For two bodies interacting by Newtonian gravity, the LRL vector is a constant of motion, meaning that it is the same no matter where it is calculated on the orbit; equivalently, the LRL vector is said to be conserved.

laser
A device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The word ""laser"" is an acronym for ""light amplification by stimulated emission of radiation""

law of universal gravitation

LC circuit
A circuit consisting of an inductor (with inductance L) and a capacitor (with capacitance C).

Lenz's law

lepton
An elementary particle which does not undergo strong interactions but is subject to the Pauli exclusion principle. Two main classes of leptons exist: charged leptons (also known as the electron-like leptons) and neutral leptons (better known as neutrinos).

lever
A type of machine consisting of a beam or rigid rod pivoted at a fixed hinge or fulcrum; one of six classical simple machines.

levitation (physics)

light
A form of electromagnetic radiation that occupies a certain range of wavelengths within the electromagnetic spectrum. In physics, the term sometimes refers collectively to electromagnetic radiation of any wavelength, in which case light includes gamma rays, X-rays, microwaves, and radio waves, but in common usage ""light"" more often refers specifically to visible light.

linear actuator
A form of motor that generates a linear movement directly.

linear algebra
The branch of mathematics concerning vector spaces, often finite or countably infinite dimensional, as well as linear mappings between such spaces.

line of force

linear elasticity
The mathematical study of how solid objects deform and become internally stressed due to prescribed loading conditions. Linear elasticity is a simplification of the more general nonlinear theory of elasticity and is a branch of continuum mechanics.

Liouville's theorem
Phase space volume is conserved.

liquid
One of four classical states of matter having a definite volume but no fixed shape.

liquid crystal (LC)
A state of matter which has properties between those of a conventional liquid and those of a solid crystal. For instance, an LC may flow like a liquid, but its molecules may be oriented in a crystal-like way.

longitudinal wave


== M ==

M-theory
An extension of string theory that attempts to unify seemingly contradictory mathematical formulations and which identifies 11 dimensions.

Mach number
A dimensionless quantity representing the ratio of the speed of an object moving through a fluid to the local speed of sound.

Mach's principle
The proposition that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter.

machine
Any powered tool consisting of one or more parts that is constructed to achieve a particular goal. Machines are usually powered by mechanical, chemical, thermal or electrical means, and are frequently motorised.

machine element
An elementary component of a machine. There are three basic types: structural components, mechanisms, and control components.

Maclaurin series
A representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.

magnetic field
A mathematical description of the magnetic influence of electric currents and magnetic materials. The magnetic field at any given point is specified by both a direction and a magnitude (or strength); as such it is a vector field.

magnetism
A property of materials that respond to an applied magnetic field.

magnetostatics

mass

mass balance
Also called material balance.
An application of the law of conservation of mass to the analysis of physical systems.

mass density
See density.

mass flux
The rate of mass flow per unit area. The common symbols are j, J, φ, or Φ, sometimes with subscript m to indicate mass is the flowing quantity. Its SI units are kg s−1 m−2.

mass moment of inertia
A property of a distribution of mass in space that measures its resistance to rotational acceleration about an axis.

mass number
Also called atomic mass number or nucleon number.
The total number of protons and neutrons (together known as nucleons) in an atomic nucleus.

mass spectrometry

material properties

materials science
An interdisciplinary field incorporating elements of physics, chemistry, and engineering that is concerned with the design and discovery of new materials, particularly solids.

mathematical physics
The application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories.

mathematics
The abstract study of topics encompassing quantity, structure, space, change, and other properties.

matrix
A rectangular array of numbers, symbols, or expressions arranged in rows and columns. The individual items in a matrix are called its elements or entries.

matter
Any substance (often a particle) that has rest mass and (usually) also volume.

Maxwell's equations
A set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.

measure of central tendency
A term which relates to the way in which quantitative data tend to cluster around some value. A measure of central tendency is any of a number of ways of specifying this ""central value"".

mechanical energy

mechanical filter

mechanical equilibrium

mechanical wave

mechanics
The branch of science concerned with the behaviour of physical bodies when subjected to forces or displacements and the subsequent effects of the bodies on their environment.

melting
Also called fusion. 
A physical process that results in the phase transition of a substance from a solid to a liquid.

meson
A type of hadronic subatomic particle composed of one quark and one antiquark bound together by the strong interaction. All mesons are unstable, with the longest-lived lasting for only a few hundredths of a microsecond.

modulus of elasticity
The mathematical description of an object's or substance's tendency to be deformed elastically (i.e. non-permanently) when a force is applied to it. The elastic modulus of an object is defined as the slope of its stress–strain curve in the elastic deformation region. As such, a stiffer material will have a higher elastic modulus.

molar concentration

molar mass
A physical property of matter defined as the mass of a given substance divided by the amount of substance and expressed in grams per mole.

molecule
An electrically neutral group of two or more atoms held together by covalent chemical bonds. Molecules are distinguished from ions by having a net electric charge equal to zero.

molecular physics
A branch of physics that studies the physical properties of molecules and the chemical bonds between atoms as well as their molecular dynamics. It is closely related to atomic physics and overlaps greatly with theoretical chemistry, physical chemistry and chemical physics.

moment

moment of inertia
A property of a distribution of mass in space that measures its resistance to rotational acceleration about an axis.

momentum
A vector quantity consisting of the product of the mass and velocity of an object.

monochromatic light

motion
Any change in the position of an object over time. Motion can be mathematically described in terms of displacement, distance, velocity, speed, acceleration, and momentum, and is observed by attaching a frame of reference to an observer and measuring the change in an object's position relative to that frame. An object's motion cannot change unless it is acted upon by a force.

muon
An elementary particle, technically classified as a lepton, that is similar to the electron, with unitary negative electric charge (−1) and a spin of 1⁄2. Muons are not believed to have any sub-structure.


== N ==

nanoengineering
The practice of engineering on the nanoscale. Nanoengineering is largely a synonym for nanotechnology, but emphasizes the applied rather the field.

nanotechnology
Also abbreviated as nanotech.
The manipulation of matter on an atomic and molecular scale; a more generalized description by the National Nanotechnology Initiative is ""the manipulation of matter with at least one dimension sized from 1 to 100 nanometres"".

Navier–Stokes equations

neurophysics

neutrino
A type of electrically neutral subatomic particle denoted by the Greek letter ν (nu). All evidence suggests that neutrinos have mass but that their mass is tiny even by the standards of subatomic particles. Their mass has never been measured accurately.

neutron
Subatomic particle with no charge

prompt neutron
Immediate emission of neutrons after a nuclear fission event

delayed neutron
Delayed emission of neutrons after a nuclear fission event, by one of the fission products (actually, a fission product daughter after beta decay)

neutron cross-section

newton (N)

Newton's laws of motion
A set of three physical laws which describe the relationship between the forces acting on a body and its motion due to those forces. Together they form the basis for classical or Newtonian mechanics.

Newton's law of universal gravitation

Newtonian fluid

Newtonian mechanics

normal force

nuclear force

nuclear physics
The branch of physics that studies the constituents and interactions of atomic nuclei.

nuclear reaction

nuclear transmutation

nucleon
Either a proton or a neutron in its role as a component of an atomic nucleus.

nucleus

nuclide
Also spelled nucleide.
An atomic species characterized by the specific composition of its nucleus, i.e. by its number of protons, its number of neutrons, and its nuclear energy state.


== O ==

Ohm
The SI derived unit of electrical resistance.

Ohm's law
The electric current through a conductor between two points is directly proportional to the potential difference across the two points.

optical tweezers
An optomechanical device used for the capture, analysis, and manipulation of dielectric objects or particles, which operates via the application of force by the electric field of light.

optically detected magnetic resonance
An optical technique for the initialisation and readout of quantum spin in some crystal defects.

optics
The branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it. Optics usually describes the behaviour of visible, ultraviolet, and infrared light; however, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.


== P ==

paraffin

parallel circuit

parity
1.  (mathematics)
2.  (physics)

particle

particle accelerator

particle displacement

particle physics
A branch of physics that studies the nature of particles, which are the constituents of what is usually referred to as matter and radiation.

Pascal's law
A principle in fluid mechanics which states that pressure exerted anywhere in a confined incompressible fluid is transmitted equally in all directions throughout the fluid such that the initial pressure variations remain the same.

Pauli exclusion principle

pendulum

periodic table of the elements
Also simply called the periodic table.
A tabular display of the chemical elements organised on the basis of their atomic numbers, electron configurations, and recurring chemical properties. Elements are presented in order of increasing atomic number (number of protons).

phase (matter)

phase (waves)

phase equilibrium

phenomenology

phosphorescence

photoelectric effect

photon
An elementary particle, the quantum of light and all other forms of electromagnetic radiation, and the force carrier for the electromagnetic force.

photonics

physical chemistry
The study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of laws and concepts of physics.

physical constant

physical quantity

physics
The natural science that involves the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

piezoelectricity

pion

Planck constant (
  
    
      
        h
      
    
    {\displaystyle h}
  
)
Also called Planck's constant. 
A fundamental universal physical constant that is the quantum of action in quantum mechanics.

Planck units

Planck's law

plasma

plasma physics

plasticity

pneumatics
The study and control of mechanical force and movement generated by the application of compressed gas.

positron

potential energy

power

pressure
The ratio of force to the area over which that force is distributed.

principle of relativity

probability
A measure of the expectation that an event will occur or that a statement is true. Probabilities are given a value between 0 (will not occur) and 1 (will occur). The higher the probability of an event, the more certain one can be that the event will occur.

probability distribution

probability theory

proton

psi particle

pulley
A wheel on an axle that is designed to support movement of a cable or belt along its circumference; one of six classical simple machines. Pulleys are used in a variety of ways to lift loads, apply forces, and transmit power.

pulse

pulse wave


== Q ==

quantization

quantum

quantum chromodynamics

quantum electrodynamics (QED)
The relativistic quantum field theory of electrodynamics. In essence, it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism, giving a complete account of matter and light interaction.

quantum field theory
A theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics.

quantum gravity

quantum mechanics
A branch of physics dealing with physical phenomena at microscopic scales, where the action is on the order of the Planck constant. Quantum mechanics departs from classical mechanics at atomic and subatomic length scales, and provides a mathematical description of much of the dual particle-like and wave-like behavior and interactions of energy and matter that occur at this scale.

quantum number

quantum physics

quantum state

quark
An elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei.

quasiparticle


== R ==

radiant energy

radiation

radioactive decay

radionuclide
Also called a radioactive nuclide, radioisotope, or radioactive isotope.
Any nuclide possessing excess nuclear energy to the point that it is unstable. Such excess energy is emitted through any of several processes of radioactive decay, resulting in a stable nuclide or sometimes another unstable radionuclide which can then undergo further decay. Certain radionuclides occur naturally; many others can be produced artificially in nuclear reactors, cyclotrons, particle accelerators, or radionuclide generators.

radius of curvature

redshift
A phenomenon which occurs when light seen coming from an object that is moving away from the observer is proportionally increased in wavelength or ""shifted"" to the red end of the visible light spectrum.

refraction
The change in direction of a wave as it passes from one transmission medium to another or as a result of a gradual change in the medium. Though most commonly used in the context of refraction of light, other waves such as sound waves and fluid waves also experience refraction.

refractive index

relative atomic mass

relativistic mechanics

relativity

rest frame

rigid body
An idealization of a solid body in which deformation is neglected. In other words, the distance between any two given points of a rigid body remains constant in time regardless of the external forces exerted on it. Even though such an object cannot physically exist due to relativity, objects can normally be assumed to be perfectly rigid if they are not moving near the speed of light.

rotational energy
Also called angular kinetic energy.
The kinetic energy due to the rotation of an object, which forms part of its total kinetic energy.

rotational speed
Also called speed of revolution.
The number of complete rotations or revolutions a rotating body makes per unit time.

Rydberg formula
A formula used in atomic physics to describe the wavelengths of spectral lines of many chemical elements.


== S ==

scalar
Any simple physical quantity that can be described by a single number (as opposed to vectors, tensors, etc., which are described by several numbers such as magnitude and direction) and is unchanged by coordinate system rotations or translations (in Newtonian mechanics) or by Lorentz transformations or central-time translations (in relativity).

scattering
The general physical process by which some forms of radiation, such as light, sound, or moving particles, are forced to deviate from a straight trajectory by one or more localised non-uniformities in the medium through which they pass.

science
A systematic enterprise that builds and organises knowledge in the form of testable explanations and predictions about the universe.

screw
A mechanism that converts rotational motion to linear motion, and a torque (rotational force) to a linear force; one of six classical simple machines.

second law of thermodynamics

Seebeck effect

series circuit

shadow matter

shear modulus
Also called modulus of rigidity.

shear strength

shear stress

shortwave radiation (SW)
Radiant energy of the electromagnetic spectrum with wavelengths in the visible, near-ultraviolet, and near-infrared spectra, the broadest definition of which includes all radiation with a wavelength between 0.1 μm and 5.0 μm.

Schrödinger equation
A mathematical equation which describes the time evolution of wave functions in quantum mechanics.

simple harmonic motion

simple machine
A mechanical device that changes the direction or magnitude of a force. In general, a set of six classical simple machines identified by Renaissance scientists drawing from Greek texts on technology are collectively defined as the simplest mechanisms that can provide mechanical advantage (also called leverage).

siphon
A tube in an inverted U shape that causes a liquid to flow uphill without pumps, powered by the fall of the liquid as it flows down the tube under the pull of gravity. The term may also more generally refer to a wide variety of devices involving the flow of liquids through tubes.

Snell's law

solar cell

solid

solid mechanics

solid-state physics

solubility
The tendency of a solid, liquid, or gaseous chemical substance (called a solute) to dissolve in another solid, liquid, or gaseous substance (called a solvent) to form a homogeneous solution of the solute in the solvent. The solubility of a solute fundamentally depends on the specific solvent as well as on temperature and pressure.

Sonoluminescence

sound
A mechanical wave that is an oscillation of pressure transmitted through a solid, liquid, or gas and composed of frequencies within the range of human hearing.

special relativity

specific activity

speed

speed of light (
  
    
      
        c
      
    
    {\displaystyle c}
  
)
A fundamental universal physical constant defined as exactly 299,792,458 metres per second, a figure that is exact because the length of the metre is defined from this constant and the international standard for time. When not otherwise qualified, the term ""speed of light"" usually refers to the speed of light in vacuum, as opposed to the speed of light through some physical medium.

speed of sound

spherical aberration

spin quantum number

stable isotope ratio
The relative abundances of the atomically stable isotopes of a given element as they occur in nature or in a particular experimental context.

stable nuclide
Any nuclide that is not radioactive and does not spontaneously undergo radioactive decay, as opposed to a radionuclide. When such nuclides are referred to in relation to specific elements, they are usually termed stable isotopes.

standard atomic weight

Standard Model
The theory of particle physics which describes three of the four known fundamental forces (the electromagnetic force, the weak force, and the strong force, but not the gravitational force) and classifies all known elementary particles.

standing wave

state of matter

statics
The branch of mechanics concerned with the analysis of loads (force and torque, or ""moment"") on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity.

statistical mechanics

stiffness
The rigidity of an object, i.e. the extent to which it resists deformation in response to an applied force.

strain
The transformation of a body from a reference configuration to a current configuration. A configuration is a set containing the positions of all particles of the body.

strain hardening

strength of materials

stress
1.  An applied force or system of forces that tends to strain or deform a physical body.
2.  A measure of the internal forces acting within a deformable body.
3.  A quantitative measure of the average force per unit area of a surface within a body on which internal forces act.

stress–strain curve

string duality

string theory

structural load

subatomic particle
Any particle that is smaller than an atom.

sublimation
The physical process by which matter is transformed directly from the solid phase to the gas phase without passing through an intermediate liquid phase. Sublimation is an endothermic phase transition that occurs at temperatures and pressures below a substance's triple point in its phase diagram.

superconductivity

superconductor
A phenomenon of exactly zero electrical resistance and expulsion of magnetic fields occurring in certain materials when cooled below a characteristic critical temperature.

superhard material

superposition principle

supersymmetry (SUSY)

surface tension


== T ==

temperature
A physical property of matter that quantitatively expresses the common notions of hot and cold.

tensile modulus

tensile strength

tesla (T)

test particle

theoretical physics
A branch of physics that employs mathematical models and abstractions of physical objects and systems in order to rationalize, explain, and predict natural phenomena, as opposed to experimental physics, which relies on data generated by experimental observations.

theory of everything (ToE)

theory of relativity

thermal conduction

thermal equilibrium
A state in which there is no net flow of thermal energy between two physical systems when the systems are connected by a path permeable to heat. A system may also be said to be in thermal equilibrium with itself if the temperature within the system is spatially and temporally uniform. Systems in thermodynamic equilibrium are always in thermal equilibrium, but the converse is not always true.

thermal radiation

thermionic emission

thermodynamic equilibrium

thermodynamic free energy

thermodynamics

thermometer
An instrument used to measure temperature.

third law of thermodynamics

threshold frequency

torque
Also called moment or moment of force.
The tendency of a force to rotate an object about an axis, fulcrum, or pivot. Just as a force is a push or a pull, a torque can be thought of as a twist to an object.

total internal reflection

toughness
The ability of a material to absorb energy and plastically deform without fracturing. Material toughness is defined as the amount of energy per unit volume that a material can absorb before rupturing. It is also defined as the resistance to fracture of a material when stressed.

trajectory
The path that a moving object follows through space as a function of time.

transducer

transmission medium

transverse wave

trigonometry
A branch of mathematics that studies triangles and the relationships between their sides and the angles between these sides.

trimean

triple point
The temperature and pressure at which the three phases (gas, liquid, and solid) of a given substance coexist in thermodynamic equilibrium.

truncated mean


== U ==

Unbalanced forces
When there is unbalanced force(s); and as such, the object changes its state of motion. The object is not at equilibrium and subsequently accelerates.

uncertainty principle
Any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, such as position x and momentum p, cannot be known simultaneously.

unified atomic mass unit
One dalton: one-twelfth the mass of an isolated neutral atom of the isotope 126C in its ground state.

uniform motion

uniform circular motion

unit vector

utility frequency
The frequency of the oscillations of alternating current (AC) in an electric power grid transmitted from a power plant to the end-user.


== V ==

vacuum
An area of space which contains no matter.

valence electron
An electron that is associated with an atom and can participate in the formation of a chemical bond.

valence shell
The outermost electron shell of an atom.

valley of stability

Van de Graaff generator

variable capacitor

variable resistor

vector
Any quantity that has both magnitude and direction.

vector space
A mathematical structure formed by a collection of elements called vectors, which may be added together and multiplied (""scaled"") by numbers called scalars.

velocity (
  
    
      
        v
      
    
    {\displaystyle v}
  
)
A vector quantity defined as the rate of change of the position of an object with respect to a given frame of reference. Velocity specifies both an object's speed and direction of motion (e.g. 60 kilometres per hour to the north).

virtual image

virtual particle

viscoelasticity

viscosity

visible light
A form of electromagnetic radiation generally defined as the range of wavelengths visible to the average human eye.

volt (V)
The SI derived unit for electric potential, electric potential difference, and electromotive force, defined as the difference in electric potential between two points of a conducting wire when an electric current of one ampere dissipates one watt of power between those two points.

Volta potential

voltage

voltmeter
An instrument used for measuring the difference in electric potential between two points in an electric circuit. Analog voltmeters move a pointer across a scale in proportion to the voltage of the circuit.

volt per metre

volume


== W ==

W and Z bosons

watt (W)
A derived unit of power in the International System of Units (SI) defined as one joule per second. The watt measures the rate of energy conversion or transfer.

wave
A disturbance or oscillation that travels through spacetime accompanied by a transfer of energy.

wave equation

wave function

wave function collapse

wave–particle duality

wavelength
A measure of the distance traversed by a single spatial period of a sinusoidal wave, i.e. the distance over which the wave's shape repeats.

weak interaction
Also called the weak force or weak nuclear force.
One of the four fundamental forces of nature, along with the strong nuclear force, electromagnetism, and gravitation. It is responsible for the radioactive decay of subatomic particles and initiates the process known as hydrogen fusion in stars.

weber (Wb)

wedge
A triangular round tool in the form of a compound and portable inclined plane; one of six classical simple machines.

weight

wheel and axle
A wheel attached to an axle in such a way that the two parts rotate together and transfer forces between them; one of six classical simple machines.

white body
A hypothetical idealized physical body that reflects all incident electromagnetic radiation completely and uniformly in all directions; the opposite of a black body.

wind
The flow of gases on a large scale.

work

work function


== X ==

X-ray
A high-energy photon (between 100 eV and 100 keV) with a wavelength shorter than that of ultraviolet radiation and longer than that of gamma radiation.


== Y ==

Young’s modulus
A measure of the stiffness of a solid material which defines the relationship between mechanical stress and strain.


== Z ==

Zeeman effect
The effect of splitting a spectral line into several components in the presence of a static magnetic field by the lifting of degeneracy in electronic states.


== See also ==
Outline of physics
Index of physics articles
Glossary of areas of mathematics
Glossary of astronomy
Glossary of biology
Glossary of calculus
Glossary of chemistry terms
Glossary of engineering
Glossary of probability and statistics


== References ==",1225363672,79004,https://en.wikipedia.org/wiki/Glossary_of_physics,"['Category:Articles with short description', 'Category:Glossaries of science', 'Category:Short description is different from Wikidata', 'Category:Use dmy dates from July 2018', 'Category:Wikipedia glossaries using description lists']","['Ab initio methods (nuclear physics)', 'Abbe number', 'Absolute electrode potential', 'Absolute humidity', 'Absolute motion', 'Absolute pressure', 'Absolute scale', 'Absolute zero', 'Absorbance', 'Absorption spectroscopy', 'Accelerating expansion of the universe', 'Acceleration', 'Accelerometer', 'Acoustics', 'Additive inverse', 'Adhesion', 'Adiabatic cooling', 'Adiabatic heating', 'Adiabatic process', 'Aerodynamics', 'Afocal system', 'Air', 'Air mass', 'Air mass (solar energy)', 'Airplane', 'Albedo', 'Alloy', 'Alpha (letter)', 'Alpha decay', 'Alpha particle', 'Alternating current', 'Ammeter', 'Amorphous solid', 'Amount of substance', 'Ampere', 'Amplifier', 'Amplitude', 'Analytical mechanics', 'Angle of incidence (optics)', 'Angle of reflection', 'Angular acceleration', 'Angular displacement', 'Angular frequency', 'Angular momentum', 'Angular velocity', 'Anion', 'Annihilation', 'Anode', 'Anti-gravity', 'Antielectron', 'Antimatter', 'Antineutron', 'Antiparticle', 'Antiproton', 'Antiquark', 'Applied physics', 'Arc length', ""Archimedes' principle"", 'Area moment of inertia', 'Astronomy', 'Astronomy (magazine)', 'Astrophysics', 'Atmospheric physics', 'Atmospheric pressure', 'Atom', 'Atomic, molecular, and optical physics', 'Atomic line filter', 'Atomic mass', 'Atomic mass unit', 'Atomic nucleus', 'Atomic number', 'Atomic orbital', 'Atomic packing factor', 'Atomic physics', 'Atomic structure', 'Atomic weight', 'Attenuation coefficient', 'Audio frequency', ""Avogadro's law"", 'Avogadro constant', 'Avogadro number', 'Axion', 'Azimuthal quantum number', ""Babinet's principle"", 'Background radiation', 'Balanced Forces', 'Ballistics', 'Balmer series', 'Barometer', 'Baryon', 'Baryon number', 'Basic research', 'Battery (electricity)', 'Beam (structure)', 'Bending', 'Bending moment', ""Bernoulli's equation"", ""Bernoulli's principle"", 'Bessel function', 'Beta decay', 'Beta particle', 'Big Bang', 'Binding energy', 'Binomial random variable', 'Biocatalysis', 'Biology', 'Biophysics', 'Black-body radiation', 'Black body', 'Block and tackle', 'Bohr model', 'Boiling point', 'Boiling point elevation', 'Boltzmann constant', 'Bose–Einstein condensate', 'Bose–Einstein statistics', 'Boson', ""Boyle's law"", ""Bragg's law"", 'Branches of physics', 'Bra–ket notation', 'Bremsstrahlung', ""Brewster's angle"", 'British thermal unit', 'Brittleness', 'Brownian motion', 'Bubble (physics)', 'Bulk modulus', 'Buoyancy', 'CGh physics', 'Calculus', 'Cambridge University Press', 'Capacitance', 'Capacitive reactance', 'Capacitor', 'Carbon', 'Carnot cycle', 'Cartesian coordinate system', 'Cathode', 'Cathode ray', 'Cation', 'Celestial body', 'Celestial mechanics', 'Celsius scale', 'Center of curvature', 'Center of gravity', 'Center of mass', 'Center of pressure (fluid mechanics)', 'Centigrade (temperature)', 'Centrifugal force', 'Centripetal force', 'Chain reaction', 'Change of base rule', 'Charge carrier', 'Charged particles', 'Chemical bond', 'Chemical element', 'Chemical physics', 'Chemistry', 'Chromatic aberration', 'Circular motion', 'Classical central-force problem', 'Classical electromagnetism', 'Classical mechanics', 'Classical optics', 'Classical physics', 'Coherence (physics)', 'Cohesion (chemistry)', 'Cold fusion', 'Complex harmonic motion', 'Complex number', 'Composite particle', 'Compton scattering', 'Computational physics', 'Concave lens', 'Condensation point', 'Condensed matter physics', 'Conservation law', 'Conservation of energy', 'Conservation of momentum', 'Constantin Carathéodory', 'Constructive interference', 'Continuous spectrum', 'Continuum mechanics', 'Convection', 'Convex lens', 'Coordinate system', 'Cosmic background radiation', 'Coulomb', ""Coulomb's law"", 'Creep (deformation)', 'Crest (physics)', 'Crest factor', 'Critical angle (optics)', 'Critical mass', 'Curie temperature', 'Current density', 'Curvilinear motion', 'Cyclotron', 'Cylindrical coordinates', 'Cylindrical harmonics', 'DC motor', ""Dalton's law"", 'Dalton (unit)', 'Damped vibration', 'Damping ratio', 'Darcy–Weisbach equation', 'Dark energy', 'Dark matter', 'Decibel', 'Definite integral', 'Deflection (engineering)', 'Deformation (mechanics)', 'Degree (angle)', 'Delayed neutron', 'Dennis Overbye', 'Density', 'Derivative', 'Destructive interference', 'Diamagnetism', 'Dielectric', 'Differential calculus', 'Differential equation', 'Diffraction', 'Direct current', 'Dispersion (optics)', 'Displacement (fluid)', 'Distance', 'Doi (identifier)', 'Doppler effect', 'Down antiquark', 'Drag (physics)', 'Drift velocity', 'Ductility', 'Dynamics (mechanics)', 'Dyne', ""Earth's atmosphere"", 'Earth & Sky', 'Ebullioscope', 'Econophysics', 'Effective focal length', 'Elastic collision', 'Elastic energy', 'Elastic instability', 'Elastic modulus', 'Elasticity (physics)', 'Electric charge', 'Electric circuit', 'Electric current', 'Electric displacement field', 'Electric field', 'Electric field gradient', 'Electric generator', 'Electric intensity', 'Electric motor', 'Electric potential', 'Electric power', 'Electrical and electronics engineering', 'Electrical conductor', 'Electrical impedance', 'Electrical insulator', 'Electrical network', 'Electrical potential energy', 'Electrical reactance', 'Electrical resistance', 'Electricity', 'Electro-optic effect', 'Electrochemical cell', 'Electrochemistry', 'Electrodynamics', 'Electrolytic cell', 'Electromagnet', 'Electromagnetic field', 'Electromagnetic induction', 'Electromagnetic radiation', 'Electromagnetic spectrum', 'Electromagnetic wave equation', 'Electromagnetism', 'Electromechanics', 'Electromotive force', 'Electron', 'Electron capture', 'Electron pair', 'Electron paramagnetic resonance', 'Electronegativity', 'Electronics', 'Electronvolt', 'Electrostatics', 'Electrostriction', 'Electroweak interaction', 'Elementary charge', 'Elementary particle', 'Emission spectrum', 'Emissivity', 'Endothermic', 'Energy', 'Energy level', 'Engine', 'Engineering', 'Engineering physics', 'Enthalpy', 'Entropy', 'Equilibrant force', 'Equipartition', 'Escape velocity', 'Excited state', 'Exothermic', 'Experimental physics', 'Falling bodies', 'Farad', 'Faraday (unit)', 'Faraday constant', ""Fermat's principle"", 'Fermi surface', 'Fermion', 'Fermi–Dirac statistics', 'Ferrimagnetism', 'Ferromagnetism', 'Field line', 'Fifth force', 'First law of thermodynamics', 'Flavour (particle physics)', 'Fluid', 'Fluid dynamics', 'Fluid mechanics', 'Fluid physics', 'Fluid statics', 'Fluorescence', 'Flux', 'Focal length', 'Focus (optics)', 'Force', 'Force carrier', 'Force field (physics)', 'Frame of reference', 'Fraunhofer lines', 'Free body diagram', 'Free fall', 'Freezing point', 'Frequency', 'Frequency modulation', 'Friction', 'Friedrich Bessel', 'Function (mathematics)', 'Fundamental forces', 'Fundamental frequency', 'Fundamental interaction', 'Fundamental theorem of calculus', 'Galvanic cell', 'Gamma ray', 'Gas', 'Gas dynamics', 'General relativity', 'Geometric optics', 'Geometrical optics', 'Geophysics', 'George Batchelor', 'Glossary of Arabic toponyms', 'Glossary of Hebrew toponyms', 'Glossary of aerospace engineering', 'Glossary of agriculture', 'Glossary of archaeology', 'Glossary of architecture', 'Glossary of areas of mathematics', 'Glossary of artificial intelligence', 'Glossary of astronomy', 'Glossary of biology', 'Glossary of bird terms', 'Glossary of botanical terms', 'Glossary of calculus', 'Glossary of cell biology', 'Glossary of cellular and molecular biology (0–L)', 'Glossary of cellular and molecular biology (M–Z)', 'Glossary of chemistry terms', 'Glossary of civil engineering', 'Glossary of clinical research', 'Glossary of computer hardware terms', 'Glossary of computer science', 'Glossary of developmental biology', 'Glossary of ecology', 'Glossary of economics', 'Glossary of electrical and electronics engineering', 'Glossary of engineering', 'Glossary of engineering: A–L', 'Glossary of engineering: M–Z', 'Glossary of entomology terms', 'Glossary of environmental science', 'Glossary of genetics and evolutionary biology', 'Glossary of geography terms (A–M)', 'Glossary of geography terms (N–Z)', 'Glossary of geology', 'Glossary of ichthyology', 'Glossary of machine vision', 'Glossary of mechanical engineering', 'Glossary of medicine', 'Glossary of meteorology', 'Glossary of mycology', 'Glossary of nanotechnology', 'Glossary of probability and statistics', 'Glossary of psychiatry', 'Glossary of quantum computing', 'Glossary of robotics', 'Glossary of scientific naming', 'Glossary of structural engineering', 'Glossary of virology', 'Gluon', ""Graham's law of diffusion"", 'Gravitation', 'Gravitational acceleration', 'Gravitational constant', 'Gravitational energy', 'Gravitational field', 'Gravitational potential', 'Gravitational wave', 'Graviton', 'Gravity', 'Ground (electricity)', 'Ground reaction force', 'Ground state', 'Group velocity', 'Hadron', 'Half-integer', 'Half-life', ""Hamilton's principle"", 'Hamiltonian mechanics', 'Harmonic mean', 'Heat', 'Heat transfer', 'Helium-4', 'Helmholtz equation', 'Helmholtz free energy', 'Hertz', 'Higgs boson', 'History of physics', 'Homeokinetics', 'Horsepower', 'Huygens–Fresnel principle', 'Hydrogen atom', 'Hydrogen spectral series', 'Hydrostatics', 'ISBN (identifier)', 'Ice point', 'Ideal gas', 'Implosion (mechanical process)', 'Impulse (physics)', 'Incident ray', 'Indefinite integral', 'Index of physics articles', 'Inductance', 'Inductor', 'Inductors', 'Inertia', 'Infrasound', 'Integer', 'Integral', 'Integral calculus', 'Integral transform', 'International System of Units', 'Invariant mass', 'Inversely proportional', 'Ion', 'Ionic bond', 'Ionization', 'Ionization chamber', 'Ionizing radiation', 'Isotope', 'Johann Balmer', 'Josephson effect', 'Joule', 'Kelvin', 'Kinematics', 'Kinetic energy', ""Kirchhoff's circuit laws"", ""Kirchhoff's equations"", ""Kirchhoff's law of thermal radiation"", 'LC circuit', 'Lagrangian mechanics', 'Laminar flow', ""Laplace's equation"", 'Laplace transform', 'Laplace–Runge–Lenz vector', 'Laser', 'Laurence Joseph Clancy', 'Lawrence Berkeley National Laboratory', 'Lens (optics)', ""Lenz's law"", 'Lepton', 'Lever', 'Levitation (physics)', 'Light', 'Line of force', 'Linear', 'Linear actuator', 'Linear algebra', 'Linear elasticity', ""Liouville's theorem (conformal mappings)"", 'Liquid', 'Liquid crystal', 'List of life sciences', 'List of physicists']"
34189212,Glossary of areas of mathematics,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.","Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.


== A ==

Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus. Instead it is built using sheaf theory and sheaf cohomology.
Abstract harmonic analysis
 A modern branch of harmonic analysis that extends upon the generalized Fourier transforms that can be defined on locally compact groups.
Abstract homotopy theory
 A part of topology that deals with homotopic functions, i.e. functions from one topological space to another which are homotopic (the functions can be deformed into one another).
Actuarial science
 The discipline that applies mathematical and statistical methods to assess risk in insurance, finance and other industries and professions. More generally, actuaries apply rigorous mathematics to model matters of uncertainty.
Additive combinatorics
 The part of arithmetic combinatorics devoted to the operations of addition and subtraction.
Additive number theory
 A part of number theory that studies subsets of integers and their behaviour under addition.
Affine geometry
 A branch of geometry that deals with properties that are independent from distances and angles, such as alignment and parallelism.
Affine geometry of curves
 The study of curve properties that are invariant under affine transformations.
Affine differential geometry
 A type of differential geometry dedicated to differential invariants under volume-preserving affine transformations.
Ahlfors theory
 A part of complex analysis being the geometric counterpart of Nevanlinna theory. It was invented by Lars Ahlfors.
Algebra
 One of the major areas of mathematics. Roughly speaking, it is the art of manipulating and computing with operations acting on symbols called variables that represent indeterminate numbers or other mathematical objects, such as vectors, matrices, or elements of algebraic structures.
Algebraic analysis
 motivated by systems of linear partial differential equations, it is a branch of algebraic geometry and algebraic topology that uses methods from sheaf theory and complex analysis, to study the properties and generalizations of functions. It was started by Mikio Sato.
Algebraic combinatorics
 an area that employs methods of abstract algebra to problems of combinatorics. It also refers to the application of methods from combinatorics to problems in abstract algebra.
Algebraic computation
 An older name of computer algebra.
Algebraic geometry
 a branch that combines techniques from abstract algebra with the language and problems of geometry. Fundamentally, it studies algebraic varieties.
Algebraic graph theory
 a branch of graph theory in which methods are taken from algebra and employed to problems about graphs. The methods are commonly taken from group theory and linear algebra.
 Algebraic K-theory
 an important part of homological algebra concerned with defining and applying a certain sequence of functors from rings to abelian groups.
 Algebraic number theory
 The part of number theory devoted to the use of algebraic methods, mainly those of commutative algebra, for the study of number fields and their rings of integers.
Algebraic statistics
 the use of algebra to advance statistics, although the term is sometimes restricted to label the use of algebraic geometry and commutative algebra in statistics.
Algebraic topology
a branch that uses tools from abstract algebra for topology to study topological spaces.
Algorithmic number theory
 also known as computational number theory, it is the study of algorithms for performing number theoretic computations.
Anabelian geometry
 an area of study based on the theory proposed by Alexander Grothendieck in the 1980s that describes the way a geometric object of an algebraic variety (such as an algebraic fundamental group) can be mapped into another object, without it being an abelian group.
Analysis
 A wide area of mathematics centered on the study of continuous functions and including such topics as differentiation, integration, limits, and series.
Analytic combinatorics
 part of enumerative combinatorics where methods of complex analysis are applied to generating functions.
Analytic geometry
1.  Also known as Cartesian geometry, the study of Euclidean geometry using  Cartesian coordinates.
2.  Analogue to differential geometry, where differentiable functions are replaced with analytic functions. It is a subarea of both complex analysis and algebraic geometry.
 Analytic number theory
 An area of number theory that applies methods from mathematical analysis to solve problems about integers.
 Analytic theory of L-functions
Applied mathematics
 a combination of various parts of mathematics that concern a variety of mathematical methods that can be applied to practical and theoretical problems. Typically the methods used are for science, engineering, finance, economics and logistics.
Approximation theory
 part of analysis that studies how well functions can be approximated by simpler ones (such as polynomials or trigonometric polynomials)
Arakelov geometry
 also known as Arakelov theory
 Arakelov theory
 an approach to Diophantine geometry used to study Diophantine equations in higher dimensions (using techniques from algebraic geometry). It is named after Suren Arakelov.
 Arithmetic
1.   Also known as elementary arithmetic, the methods and rules for computing with addition, subtraction, multiplication and division of numbers.
2.   Also known as higher arithmetic, another name for number theory.
 Arithmetic algebraic geometry
 See arithmetic geometry.
Arithmetic combinatorics
 the study of the estimates from combinatorics that are associated with arithmetic operations such as addition, subtraction, multiplication and division.
Arithmetic dynamics
 Arithmetic dynamics is the study of the number-theoretic properties of integer, rational, p-adic, and/or algebraic points under repeated application of a polynomial or rational function. A fundamental goal is to describe arithmetic properties in terms of underlying geometric structures.
 Arithmetic geometry
 The use of algebraic geometry and more specially scheme theory for solving problems of number theory.
 Arithmetic topology
 a combination of algebraic number theory and topology studying analogies between prime ideals and knots
 Arithmetical algebraic geometry
 Another name for arithmetic algebraic geometry
 Asymptotic combinatorics
 It uses the internal structure of the objects to derive formulas for their generating functions and then complex analysis techniques to get asymptotics.
 Asymptotic theory 
 the study of asymptotic expansions
 Auslander–Reiten theory
 the study of the representation theory of Artinian rings
 Axiomatic geometry
 also known as synthetic geometry: it is a branch of geometry that uses axioms and logical arguments to draw conclusions as opposed to analytic and algebraic methods.
 Axiomatic set theory
 the study of systems of axioms in a context relevant to set theory and mathematical logic.


== B ==

 Bifurcation theory
 the study of changes in the qualitative or topological structure of a given family. It is a part of dynamical systems theory
 Biostatistics
 the development and application of statistical methods to a wide range of topics in biology.
 Birational geometry
a part of algebraic geometry that deals with the geometry (of an algebraic variety) that is dependent only on its function field.
 Bolyai–Lobachevskian geometry
 see hyperbolic geometry


== C ==

 C*-algebra theory
 a complex algebra A of continuous linear operators on a complex Hilbert space with two additional properties-(i) A is a topologically closed set in the norm topology of operators.(ii)A is closed under the operation of taking adjoints of operators.
 Cartesian geometry
 see analytic geometry
 Calculus
 An area of mathematics connected by the fundamental theorem of calculus.
 Calculus of infinitesimals
Also called infinitesimal calculus
 A foundation of calculus, first developed in the 17th century, that makes use of infinitesimal numbers.
 Calculus of moving surfaces
 an extension of the theory of tensor calculus to include deforming manifolds.
 Calculus of variations
 the field dedicated to maximizing or minimizing functionals. It used to be called functional calculus.
 Catastrophe theory
 a branch of bifurcation theory from dynamical systems theory, and also a special case of the more general singularity theory from geometry. It analyses the germs of the catastrophe geometries.
 Categorical logic
 a branch of category theory adjacent to the mathematical logic. It is based on type theory for intuitionistic logics.
 Category theory
 the study of the properties of particular mathematical concepts by formalising them as collections of objects and arrows.
 Chaos theory
 the study of the behaviour of dynamical systems that are highly sensitive to their initial conditions.
 Character theory
 a branch of group theory that studies the characters of group representations or modular representations.
 Class field theory
 a branch of algebraic number theory that studies abelian extensions of number fields.
 Classical differential geometry
 also known as Euclidean differential geometry. see Euclidean differential geometry.
 Classical algebraic topology
 see algebraic topology
 Classical analysis
 usually refers to the more traditional topics of analysis such as real analysis and complex analysis. It includes any work that does not use techniques from functional analysis and is sometimes called hard analysis. However it may also refer to mathematical analysis done according to the principles of classical mathematics.
 Classical analytic number theory
 Classical differential calculus
 Classical Diophantine geometry
 Classical Euclidean geometry 
 see Euclidean geometry
 Classical geometry
 may refer to solid geometry or classical Euclidean geometry. See geometry
 Classical invariant theory
 the form of invariant theory that deals with describing polynomial functions that are invariant under transformations from a given linear group.
 Classical mathematics
 the standard approach to mathematics based on classical logic and ZFC set theory.
 Classical projective geometry
 Classical tensor calculus
 Clifford algebra
 Clifford analysis
the study of Dirac operators and Dirac type operators from geometry and analysis using clifford algebras.
 Clifford theory
 is a branch of representation theory spawned from Cliffords theorem.
 Cobordism theory
 Coding theory
 the study of the properties of codes and their respective fitness for specific applications.
 Cohomology theory
 Combinatorial analysis
 Combinatorial commutative algebra
 a discipline viewed as the intersection between commutative algebra and combinatorics. It frequently employs methods from one to address problems arising in the other. Polyhedral geometry also plays a significant role.
Combinatorial design theory
 a part of combinatorial mathematics that deals with the existence and construction of systems of finite sets whose intersections have certain properties.
 Combinatorial game theory
 Combinatorial geometry
 see discrete geometry
Combinatorial group theory
 the theory of free groups and the presentation of a group. It is closely related to geometric group theory and is applied in geometric topology.
Combinatorial mathematics
 an area primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.
Combinatorial number theory
Combinatorial optimization
Combinatorial set theory
 also known as Infinitary combinatorics. see infinitary combinatorics
Combinatorial theory
Combinatorial topology
 an old name for algebraic topology, when topological invariants of spaces were regarded as derived from combinatorial decompositions.
Combinatorics
 a branch of discrete mathematics concerned with countable structures. Branches of it include enumerative combinatorics, combinatorial design theory, matroid theory, extremal combinatorics and algebraic combinatorics, as well as many more.
Commutative algebra
 a branch of abstract algebra studying commutative rings.
Complex algebraic geometry
 the mainstream of algebraic geometry devoted to the study of the complex points of algebraic varieties.
Complex analysis
 a part of analysis that deals with functions of a complex variable.
Complex analytic dynamics
 a subdivision of complex dynamics being the study of the dynamic systems defined by analytic functions.
Complex analytic geometry
 the application of complex numbers to plane geometry.
Complex differential geometry
 a branch of differential geometry that studies complex manifolds.
Complex dynamics
 the study of dynamical systems defined by iterated functions on complex number spaces.
Complex geometry 
 the study of complex manifolds and functions of complex variables. It includes complex algebraic geometry and complex analytic geometry.
Complexity theory
 the study of complex systems with the inclusion of the theory of complex systems.
Computable analysis
 the study of which parts of real analysis and functional analysis can be carried out in a computable manner. It is closely related to constructive analysis.
Computable model theory
 a branch of model theory dealing with the relevant questions computability.
Computability theory
 a branch of mathematical logic originating in the 1930s with the study of computable functions and Turing degrees, but now includes the study of generalized computability and definability. It overlaps with proof theory and effective descriptive set theory.
Computational algebraic geometry
Computational complexity theory
 a branch of mathematics and theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other.
Computational geometry
 a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. 
Computational group theory
 the study of groups by means of computers.
Computational mathematics
 the mathematical research in areas of science where computing plays an essential role.
Computational number theory
 also known as algorithmic number theory, it is the study of algorithms for performing number theoretic computations.
Computational statistics
Computational synthetic geometry
Computational topology
Computer algebra
 see symbolic computation
Conformal geometry
 the study of conformal transformations on a space.
Constructive analysis
 mathematical analysis done according to the principles of constructive mathematics. This differs from classical analysis.
Constructive function theory
 a branch of analysis that is closely related to approximation theory, studying the connection between the smoothness of a function and its degree of approximation
Constructive mathematics
 mathematics which tends to use intuitionistic logic. Essentially that is classical logic but without the assumption that the law of the excluded middle is an axiom.
Constructive quantum field theory
 a branch of mathematical physics that is devoted to showing that quantum theory is mathematically compatible with special relativity.
Constructive set theory
 an approach to mathematical constructivism following the program of axiomatic set theory, using the usual first-order language of classical set theory.
Contact geometry
 a branch of differential geometry and topology, closely related to and considered the odd-dimensional counterpart of symplectic geometry. It is the study of a geometric structure called a contact structure on a differentiable manifold.
Convex analysis
 the study of properties of convex functions and convex sets.
Convex geometry
 part of geometry devoted to the study of convex sets.
Coordinate geometry
 see analytic geometry
CR geometry
 a branch of differential geometry, being the study of CR manifolds.
Cryptography


== D ==

Decision analysis
Decision theory
Derived noncommutative algebraic geometry
Descriptive set theory
a part of mathematical logic, more specifically a part of set theory dedicated to the study of Polish spaces.
Differential algebraic geometry
the adaption of methods and concepts from algebraic geometry to systems of algebraic differential equations.
Differential calculus
A branch of calculus that's contrasted to integral calculus, and concerned with derivatives.
Differential Galois theory
the study of the Galois groups of differential fields.
Differential geometry
a form of geometry that uses techniques from integral and differential calculus as well as linear and multilinear algebra to study problems in geometry. Classically, these were problems of Euclidean geometry, although now it has been expanded. It is generally concerned with geometric structures on differentiable manifolds. It is closely related to differential topology.
Differential geometry of curves
the study of smooth curves in Euclidean space by using techniques from differential geometry
Differential geometry of surfaces
the study of smooth surfaces with various additional structures using the techniques of differential geometry.
Differential topology
a branch of topology that deals with differentiable functions on differentiable manifolds.
Diffiety theory
Diophantine geometry
in general the study of algebraic varieties over fields that are finitely generated over their prime fields.
Discrepancy theory
Discrete differential geometry
Discrete exterior calculus
Discrete geometry
a branch of geometry that studies combinatorial properties and constructive methods of discrete geometric objects.
Discrete mathematics
the study of mathematical structures that are fundamentally discrete rather than continuous.
Discrete Morse theory
a combinatorial adaption of Morse theory.
Distance geometry
Domain theory
a branch that studies special kinds of partially ordered sets (posets) commonly called domains.
Donaldson theory
the study of smooth 4-manifolds using gauge theory.
Dyadic algebra
Dynamical systems theory
an area used to describe the behavior of the complex dynamical systems, usually by employing differential equations or difference equations.


== E ==

Econometrics
the application of mathematical and statistical methods to economic data.
Effective descriptive set theory
a branch of descriptive set theory dealing with set of real numbers that have lightface definitions. It uses aspects of computability theory.
Elementary algebra
a fundamental form of algebra extending on elementary arithmetic to include the concept of variables.
Elementary arithmetic
the simplified portion of arithmetic considered necessary for primary education. It includes the usage addition, subtraction, multiplication and division of the natural numbers. It also includes the concept of fractions and negative numbers.
Elementary mathematics
parts of mathematics frequently taught at the primary and secondary school levels. This includes elementary arithmetic, geometry, probability and statistics, elementary algebra and trigonometry. (calculus is not usually considered a part)
Elementary group theory
the study of the basics of group theory
Elimination theory
the classical name for algorithmic approaches to eliminating between polynomials of several variables. It is a part of commutative algebra and algebraic geometry.
Elliptic geometry
a type of non-Euclidean geometry (it violates Euclid's parallel postulate) and is based on spherical geometry. It is constructed in elliptic space.
Enumerative combinatorics
an area of combinatorics that deals with the number of ways that certain patterns can be formed.
Enumerative geometry
a branch of algebraic geometry concerned with counting the number of solutions to geometric questions. This is usually done by means of intersection theory.
Epidemiology
Equivariant noncommutative algebraic geometry
Ergodic Ramsey theory
a branch where problems are motivated by additive combinatorics and solved using ergodic theory.
Ergodic theory
the study of dynamical systems with an invariant measure, and related problems.
Euclidean geometry
 An area of geometry based on the axiom system and synthetic methods of the ancient Greek mathematician Euclid.
Euclidean differential geometry
also known as classical differential geometry. See differential geometry.
Euler calculus
a methodology from applied algebraic topology and integral geometry that integrates constructible functions and more recently definable functions by integrating with respect to the Euler characteristic as a finitely-additive measure.
Experimental mathematics
an approach to mathematics in which computation is used to investigate mathematical objects and identify properties and patterns.
 Exterior algebra
 Exterior calculus
Extraordinary cohomology theory
Extremal combinatorics
a branch of combinatorics, it is the study of the possible sizes of a collection of finite objects given certain restrictions.
Extremal graph theory
a branch of mathematics that studies how global properties of a graph influence local substructure.


== F ==

Field theory
 The branch of algebra dedicated to fields, a type of algebraic structure.
Finite geometry
Finite model theory
a restriction of model theory to interpretations on finite structures, which have a finite universe.
Finsler geometry
a branch of differential geometry whose main object of study is Finsler manifolds, a generalisation of a Riemannian manifolds.
First order arithmetic
Fourier analysis
the study of the way general functions may be represented or approximated by sums of trigonometric functions.
Fractal geometry
Fractional calculus
a branch of analysis that studies the possibility of taking real or complex powers of the differentiation operator.
Fractional dynamics
investigates the behaviour of objects and systems that are described by differentiation and integration of fractional orders using methods of fractional calculus.
Fredholm theory
part of spectral theory studying integral equations.
Function theory
an ambiguous term that generally refers to mathematical analysis.
Functional analysis
a branch of mathematical analysis, the core of which is formed by the study of function spaces, which are some sort of topological vector spaces.
Functional calculus
historically the term was used synonymously with calculus of variations, but now refers to a branch of functional analysis connected with spectral theory
Fuzzy mathematics
a branch of mathematics based on fuzzy set theory and fuzzy logic.
Fuzzy measure theory
Fuzzy set theory
a form of set theory that studies fuzzy sets, that is sets that have degrees of membership.


== G ==

Galois cohomology
an application of homological algebra, it is the study of group cohomology of Galois modules.
Galois theory
named after Évariste Galois, it is a branch of abstract algebra providing a connection between field theory and group theory.
Galois geometry
a branch of finite geometry concerned with algebraic and analytic geometry over a Galois field.
Game theory
the study of mathematical models of strategic interaction among rational decision-makers.
Gauge theory
General topology
also known as point-set topology, it is a branch of topology studying the properties of topological spaces and structures defined on them. It differs from other branches of topology as the topological spaces do not have to be similar to manifolds.
Generalized trigonometry
developments of trigonometric methods from the application to real numbers of Euclidean geometry to any geometry or space. This includes spherical trigonometry, hyperbolic trigonometry, gyrotrigonometry, and universal hyperbolic trigonometry.
Geometric algebra
an alternative approach to classical, computational and relativistic geometry. It shows a natural correspondence between geometric entities and elements of algebra.
Geometric analysis
a discipline that uses methods from differential geometry to study partial differential equations as well as the applications to geometry.
Geometric calculus
extends the geometric algebra to include differentiation and integration.
Geometric combinatorics
a branch of combinatorics.  It includes a number of subareas such as polyhedral combinatorics (the study of faces of convex polyhedra), convex geometry (the study of convex sets, in particular combinatorics of their intersections), and discrete geometry, which in turn has many applications to computational geometry.
Geometric function theory
the study of geometric properties of analytic functions.
Geometric invariant theory
a method for constructing quotients by group actions in algebraic geometry, used to construct moduli spaces.
Geometric graph theory
a large and amorphous subfield of graph theory, concerned with graphs defined by geometric means.
Geometric group theory
the study of finitely generated groups via exploring the connections between algebraic properties of such groups and topological and geometric properties of spaces on which these groups act (that is, when the groups in question are realized as geometric symmetries or continuous transformations of some spaces).
Geometric measure theory
the study of geometric properties of sets (typically in Euclidean space) through measure theory.
Geometric number theory
Geometric topology
a branch of topology studying manifolds and mappings between them; in particular the embedding of one manifold into another.
Geometry
a branch of mathematics concerned with shape and the properties of space. Classically it arose as what is now known as solid geometry; this was concerning practical knowledge of length, area and volume. It was then put into an axiomatic form by Euclid, giving rise to what is now known as classical Euclidean geometry. The use of coordinates by René Descartes gave rise to Cartesian geometry enabling a more analytical approach to geometric entities. Since then many other branches have appeared including projective geometry, differential geometry, non-Euclidean geometry, Fractal geometry and algebraic geometry. Geometry also gave rise to the modern discipline of topology.
Geometry of numbers
initiated by Hermann Minkowski, it is a branch of number theory studying convex bodies and integer vectors.
Global analysis
the study of differential equations on manifolds and the relationship between differential equations and topology.
Global arithmetic dynamics
Graph theory
a branch of discrete mathematics devoted to the study of graphs. It has many applications in physical, biological and social systems.
Group-character theory
the part of character theory dedicated to the study of characters of group representations.
Group representation theory
Group theory
the study of algebraic structures known as groups.
Gyrotrigonometry
a form of trigonometry used in gyrovector space for hyperbolic geometry. (An analogy of the vector space in Euclidean geometry.)


== H ==

Hard analysis
see classical analysis
Harmonic analysis
part of analysis concerned with the representations of functions in terms of waves. It generalizes the notions of Fourier series and Fourier transforms from the Fourier analysis.
Higher arithmetic
Higher category theory
the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities.
Higher-dimensional algebra
the study of categorified structures.
Hodge theory
a method for studying the cohomology groups of a smooth manifold M using partial differential equations.
Hodge-Arakelov theory
Holomorphic functional calculus
a branch of functional calculus starting with holomorphic functions.
Homological algebra
the study of homology in general algebraic settings.
Homology theory
Homotopy theory
Hyperbolic geometry
also known as Lobachevskian geometry or Bolyai-Lobachevskian geometry. It is a non-Euclidean geometry looking at hyperbolic space.
hyperbolic trigonometry
the study of hyperbolic triangles in hyperbolic geometry, or hyperbolic functions in Euclidean geometry. Other forms include gyrotrigonometry and universal hyperbolic trigonometry.
Hypercomplex analysis
the extension of real analysis and complex analysis to the study of functions where the argument is a hypercomplex number.
Hyperfunction theory


== I ==

Ideal theory
once the precursor name for what is now known as commutative algebra; it is the theory of ideals in commutative rings.
Idempotent analysis
the study of idempotent semirings, such as the tropical semiring.
Incidence geometry
the study of relations of incidence between various geometric objects, like curves and lines.
Inconsistent mathematics
see paraconsistent mathematics.
Infinitary combinatorics
an expansion of ideas in combinatorics to account for infinite sets.
Infinitesimal analysis
once a synonym for infinitesimal calculus
Infinitesimal calculus
 See calculus of infinitesimals 
Information geometry
an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.
Integral calculus
Integral geometry
the theory of measures on a geometrical space invariant under the symmetry group of that space.
Intersection theory
a branch of algebraic geometry and algebraic topology
Intuitionistic type theory
a type theory and an alternative foundation of mathematics.
Invariant theory
studies how group actions on algebraic varieties affect functions.
Inventory theory
Inversive geometry
the study of invariants preserved by a type of transformation known as inversion
Inversive plane geometry
inversive geometry that is limited to two dimensions
Inversive ring geometry
Itô calculus
extends the methods of calculus to stochastic processes such as Brownian motion (see Wiener process). It has important applications in mathematical finance and stochastic differential equations.
Iwasawa theory
the study of objects of arithmetic interest over infinite towers of number fields.
Iwasawa-Tate theory


== J ==

 Job shop scheduling


== K ==

K-theory
originated as the study of a ring generated by vector bundles over a topological space or scheme. In algebraic topology it is an extraordinary cohomology theory known as topological K-theory. In algebra and algebraic geometry it is referred to as algebraic K-theory. In physics, K-theory has appeared in type II string theory. (In particular twisted K-theory.)
K-homology
a homology theory on the category of locally compact Hausdorff spaces. 
Kähler geometry
a branch of differential geometry, more specifically a union of Riemannian geometry, complex differential geometry and symplectic geometry. It is the study of Kähler manifolds. (named after Erich Kähler)
KK-theory
a common generalization both of K-homology and K-theory as an additive bivariant functor on separable C*-algebras.
Klein geometry
More specifically, it is a homogeneous space X together with a transitive action on X by a Lie group G, which acts as the symmetry group of the geometry.
Knot theory
part of topology dealing with knots
Kummer theory
provides a description of certain types of field extensions involving the adjunction of nth roots of elements of the base field


== L ==

L-theory
 the K-theory of quadratic forms.
Large deviations theory
 part of probability theory studying events of small probability (tail events).
Large sample theory
 also known as asymptotic theory
Lattice theory
 the study of lattices, being important in order theory and universal algebra
Lie algebra theory
Lie group theory
Lie sphere geometry
geometrical theory of planar or spatial geometry in which the fundamental concept is the circle or sphere.
Lie theory
Line geometry
Linear algebra
a branch of algebra studying linear spaces and linear maps. It has applications in fields such as abstract algebra and functional analysis; it can be represented in analytic geometry and it is generalized in operator theory and in module theory. Sometimes matrix theory is considered a branch, although linear algebra is restricted to only finite dimensions. Extensions of the methods used belong to multilinear algebra.
Linear functional analysis
Linear programming
a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships.
List of graphical methods
Included are diagram techniques, chart techniques, plot techniques, and other forms of visualization.
Local algebra
a term sometimes applied to the theory of local rings.
Local class field theory
the study of abelian extensions of local fields.
Low-dimensional topology
the branch of topology that studies manifolds, or more generally topological spaces, of four or fewer dimensions.


== M ==

Malliavin calculus
a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from  deterministic functions to stochastic processes.
Mathematical biology
the mathematical modeling of biological phenomena.
Mathematical chemistry
the mathematical modeling of chemical phenomena.
Mathematical economics
the application of mathematical methods to represent theories and analyze problems in economics.
Mathematical finance
a field of applied mathematics, concerned with mathematical modeling of financial markets. 
Mathematical logic
a subfield of mathematics exploring the applications of formal logic to mathematics.
Mathematical optimization
Mathematical physics
 The development of mathematical methods suitable for application to problems in physics.
Mathematical psychology
an approach to psychological research that is based on mathematical modeling of perceptual, thought, cognitive and motor processes, and on the establishment of law-like rules that relate quantifiable stimulus characteristics with quantifiable behavior.
Mathematical sciences
refers to academic disciplines that are mathematical in nature, but are not considered proper subfields of mathematics. Examples include statistics, cryptography, game theory and actuarial science.
Mathematical sociology
the area of sociology that uses mathematics to construct social theories.
Mathematical statistics
the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data.
Mathematical system theory
Matrix algebra
Matrix calculus
Matrix theory
Matroid theory
Measure theory
Metric geometry
Microlocal analysis
Model theory
the study of classes of mathematical structures (e.g. groups, fields, graphs, universes of set theory) from the perspective of mathematical logic.
Modern algebra
 Occasionally used for abstract algebra. The term was coined by van der Waerden as the title of his book Moderne Algebra, which was renamed Algebra in the latest editions.
Modern algebraic geometry
the form of algebraic geometry given by Alexander Grothendieck and Jean-Pierre Serre drawing on sheaf theory.
Modern invariant theory
the form of invariant theory that analyses the decomposition of representations into irreducibles.
Modular representation theory
a part of representation theory that studies linear representations of finite groups over a field K of positive characteristic p, necessarily a prime number.
Module theory
Molecular geometry
Morse theory
a part of differential topology, it analyzes the topological space of a manifold by studying differentiable functions on that manifold.
Motivic cohomology
Multilinear algebra
an extension of linear algebra building upon concepts of p-vectors and multivectors with Grassmann algebra.
Multiplicative number theory
a subfield of analytic number theory that deals with prime numbers, factorization and divisors.
Multivariable calculus
the extension of calculus in one variable to calculus with functions of several variables: the differentiation and integration of functions involving several variables, rather than just one.
Multiple-scale analysis


== N ==

Neutral geometry
See absolute geometry. 
Nevanlinna theory
part of complex analysis studying the value distribution of meromorphic functions. It is named after Rolf Nevanlinna
Nielsen theory
an area of mathematical research with its origins in fixed point topology, developed by Jakob Nielsen
Non-abelian class field theory
Non-classical analysis
Non-Euclidean geometry
Non-standard analysis
Non-standard calculus
Nonarchimedean dynamics
also known as p-adic analysis or local arithmetic dynamics
Noncommutative algebra
Noncommutative algebraic geometry
a direction in noncommutative geometry studying the geometric properties of formal duals of non-commutative algebraic objects.
Noncommutative geometry
Noncommutative harmonic analysis
see representation theory
Noncommutative topology
Nonlinear analysis
Nonlinear functional analysis
Number theory
a branch of pure mathematics primarily devoted to the study of the integers. Originally it was known as arithmetic or higher arithmetic.
Numerical analysis
Numerical linear algebra


== O ==

 Operad theory 
 a type of abstract algebra concerned with prototypical algebras.
Operation research
Operator K-theory
Operator theory
part of functional analysis studying operators.
Optimal control theory
a generalization of the calculus of variations.
Optimal maintenance
Orbifold theory
Order theory
a branch that investigates the intuitive notion of order using binary relations.
Ordered geometry
a form of geometry  omitting the notion of measurement but featuring the concept of intermediacy. It is a fundamental geometry forming a common framework for affine geometry, Euclidean geometry, absolute geometry and hyperbolic geometry.
Oscillation theory


== P ==

p-adic analysis
a branch of number theory that deals with the analysis of functions of p-adic numbers.
p-adic dynamics
an application of p-adic analysis looking at p-adic differential equations.
p-adic Hodge theory
Parabolic geometry
Paraconsistent mathematics
sometimes called inconsistent mathematics, it is an attempt to develop the classical infrastructure of mathematics based on a foundation of paraconsistent logic instead of classical logic.
Partition theory
Perturbation theory
Picard–Vessiot theory
Plane geometry
Point-set topology
see general topology
Pointless topology
Poisson geometry
Polyhedral combinatorics
a branch within combinatorics and discrete geometry that studies the problems of  describing convex polytopes.
Possibility theory
Potential theory
Precalculus
Predicative mathematics
Probability theory
Probabilistic combinatorics
Probabilistic graph theory
Probabilistic number theory
Projective geometry
a form of geometry that studies geometric properties that are invariant under a projective transformation.
Projective differential geometry
Proof theory
Pseudo-Riemannian geometry
generalizes Riemannian geometry to the study of pseudo-Riemannian manifolds.
Pure mathematics
the part of mathematics that studies entirely abstract concepts.


== Q ==

Quantum calculus
a form of calculus without the notion of limits.
Quantum geometry
the generalization of concepts of geometry used to describe the physical phenomena of quantum physics
Quaternionic analysis


== R ==

Ramsey theory
the study of the conditions in which order must appear. It is named after Frank P. Ramsey.
Rational geometry
Real algebra
the study of the part of algebra relevant to real algebraic geometry.
Real algebraic geometry
the part of algebraic geometry that studies real points of the algebraic varieties.
Real analysis
a branch of mathematical analysis; in particular hard analysis, that is the study of real numbers and functions of Real values. It provides a rigorous formulation of the calculus of real numbers in terms of continuity and smoothness, whilst the theory is extended to the complex numbers in complex analysis.
 Real Clifford algebra
Real K-theory
Recreational mathematics
the area dedicated to mathematical puzzles and mathematical games.
Recursion theory
see computability theory
Representation theory
a subfield of abstract algebra; it studies algebraic structures by representing their elements as linear transformations of vector spaces. It also studies modules over these algebraic structures, providing a way of reducing problems in abstract algebra to problems in linear algebra.
Representation theory of groups
Representation theory of the Galilean group
Representation theory of the Lorentz group
Representation theory of the Poincaré group
Representation theory of the symmetric group
Ribbon theory
a branch of topology studying ribbons.
Ricci calculus
 Also called absolute differential calculus.
A foundation of tensor calculus, developed by Gregorio Ricci-Curbastro in 1887–1896, and later developed for its applications to  general relativity and differential geometry.
Ring theory
Riemannian geometry
a branch of differential geometry that is more specifically, the study of Riemannian manifolds. It is named after Bernhard Riemann and it features many generalizations of concepts from Euclidean geometry, analysis and calculus.
Rough set theory
the a form of set theory based on rough sets.


== S ==

Sampling theory
Scheme theory
the study of schemes introduced by Alexander Grothendieck. It allows the use of sheaf theory to study algebraic varieties and is considered the central part of modern algebraic geometry.
Secondary calculus
Semialgebraic geometry
a part of algebraic geometry; more specifically a branch of real algebraic geometry that studies semialgebraic sets.
Set-theoretic topology
Set theory
Sheaf theory
 The study of sheaves, which connect local and global properties of geometric objects.
Sheaf cohomology
Sieve theory
Single operator theory
deals with the properties and classifications of single operators.
Singularity theory
a branch, notably of geometry; that studies the failure of manifold structure.
Smooth infinitesimal analysis
a rigorous reformation of infinitesimal calculus employing methods of category theory. As a theory, it is a subset of synthetic differential geometry.
Solid geometry
Spatial geometry
Spectral geometry
a field that concerns the relationships between geometric structures of manifolds and spectra of canonically defined differential operators.
Spectral graph theory
the study of properties of a graph using methods from matrix theory.
Spectral theory
part of operator theory extending the concepts of eigenvalues and eigenvectors from linear algebra and matrix theory.
Spectral theory of ordinary differential equations
part of spectral theory concerned with the spectrum and eigenfunction expansion associated with linear ordinary differential equations.
Spectrum continuation analysis
generalizes the concept of a Fourier series to non-periodic functions.
Spherical geometry
a branch of non-Euclidean geometry, studying the 2-dimensional surface of a sphere.
Spherical trigonometry
a branch of spherical geometry that studies polygons on the surface of a sphere. Usually the polygons are triangles.
Statistical mechanics
Statistical modelling
Statistical theory
Statistics
although the term may refer to the more general study of statistics, the term is used in mathematics to refer to the mathematical study of statistics and related fields. This includes probability theory.
Steganography
Stochastic calculus
Stochastic calculus of variations
Stochastic geometry
the study of random patterns of points
Stochastic process
Stratified Morse theory
Super linear algebra
Surgery theory
a part of geometric topology referring to methods used to produce one manifold from another (in a controlled way.)
Survey sampling
Survey methodology
Symbolic computation
also known as algebraic computation and computer algebra. It refers to the techniques used to manipulate mathematical expressions and equations in symbolic form as opposed to manipulating them by the numerical quantities represented by them.
Symbolic dynamics
Symplectic geometry
a branch of differential geometry and topology whose main object of study is the symplectic manifold.
Symplectic topology
Synthetic differential geometry
a reformulation of differential geometry in the language of topos theory and in the context of an intuitionistic logic.
Synthetic geometry
also known as axiomatic geometry, it is a branch of geometry that uses axioms and logical arguments to draw conclusions as opposed to analytic and algebraic methods.
Systolic geometry
a branch of differential geometry studying systolic invariants of manifolds and polyhedra.
Systolic hyperbolic geometry
the study of systoles in hyperbolic geometry.


== T ==

Tensor algebra, Tensor analysis, Tensor calculus, Tensor theory
the study and use of tensors, which are generalizations of vectors. A tensor algebra is also an algebraic structure that is used in the formal definition of tensors.
Tessellation
when periodic tiling has a repeating pattern.
Theoretical physics
a branch primarily of the science physics that uses mathematical models and abstraction of physics to rationalize and predict phenomena.
Theory of computation
Time-scale calculus
Topology
Topological combinatorics
the application of methods from algebraic topology to solve problems in combinatorics.
Topological degree theory
Topological graph theory
Topological K-theory
Topos theory
Toric geometry
Transcendental number theory
a branch of number theory that revolves around the transcendental numbers.
Transformation geometry
Trigonometry
the study of triangles and the relationships between the length of their sides, and the angles between them. It is essential to many parts of applied mathematics.
Tropical analysis
see idempotent analysis
Tropical geometry
Twisted K-theory
a variation on K-theory, spanning abstract algebra, algebraic topology and operator theory.
Type theory


== U ==

Umbral calculus
the study of Sheffer sequences
Uncertainty theory
a new branch of mathematics based on normality, monotonicity, self-duality, countable subadditivity, and product measure axioms.
Universal algebra
a field studying the formalization of algebraic structures itself.
Universal hyperbolic trigonometry
an approach to hyperbolic trigonometry based on rational geometry.


== V ==

Valuation theory
Variational analysis
Vector algebra
a part of linear algebra concerned with the operations of vector addition and scalar multiplication, although it may also refer to vector operations of vector calculus, including the dot and cross product. In this case it can be contrasted with geometric algebra which generalizes into higher dimensions.
Vector analysis
also known as vector calculus, see vector calculus.
Vector calculus
a branch of multivariable calculus concerned with differentiation and integration of vector fields. Primarily it is concerned with 3-dimensional Euclidean space.


== W ==

Wavelets


== See also ==
Lists of mathematics topics
Outline of mathematics
Category:Glossaries of mathematics


== References ==",1163649741,48114,https://en.wikipedia.org/wiki/Glossary_of_areas_of_mathematics,"['Category:All articles that may contain original research', 'Category:All lists having no precise inclusion criteria', 'Category:Articles that may contain original research from May 2022', 'Category:Articles with short description', 'Category:CS1 French-language sources (fr)', 'Category:CS1 German-language sources (de)', 'Category:CS1 maint: archived copy as title', 'Category:Fields of mathematics', 'Category:Glossaries of mathematics', 'Category:Glossaries of science', 'Category:Lists having no precise inclusion criteria from July 2022', 'Category:Short description is different from Wikidata', 'Category:Wikipedia glossaries using description lists']","['4-manifold', 'Abelian extension', 'Abelian group', 'Absolute geometry', 'Abstract algebra', 'Abstract analytic number theory', 'Abstract differential geometry', 'Abstract harmonic analysis', 'Abstraction', 'Academic discipline', 'Actuarial science', 'Addition', 'Additive combinatorics', 'Additive number theory', 'Adjoint of an operator', 'Adjunction (field theory)', 'Affine differential geometry', 'Affine geometry', 'Affine geometry of curves', 'Affine transformation', 'Ahlfors theory', 'Alexander Grothendieck', 'Algebra', 'Algebra over a field', 'Algebraic K-theory', 'Algebraic analysis', 'Algebraic combinatorics', 'Algebraic computation', 'Algebraic differential equation', 'Algebraic fundamental group', 'Algebraic geometry', 'Algebraic graph theory', 'Algebraic number theory', 'Algebraic statistics', 'Algebraic structure', 'Algebraic topology', 'Algebraic varieties', 'Algebraic variety', 'Algorithm', 'Algorithmic number theory', 'Amazon Standard Identification Number', 'Anabelian geometry', 'Analysis', 'Analytic combinatorics', 'Analytic function', 'Analytic geometry', 'Analytic number theory', 'Angle', 'Applied mathematics', 'Approximation theory', 'Arakelov geometry', 'Arakelov theory', 'Area', 'Areas of mathematics', 'Argument of a function', 'Arithmetic', 'Arithmetic algebraic geometry', 'Arithmetic combinatorics', 'Arithmetic dynamics', 'Arithmetic geometry', 'Arithmetic operation', 'Arithmetic topology', 'Arithmetical algebraic geometry', 'Artinian ring', 'Asymptotic combinatorics', 'Asymptotic expansion', 'Asymptotic theory', 'Auslander–Reiten theory', 'Axiom', 'Axiom system', 'Axiomatic set theory', 'Axiomatic system', 'Bernhard Riemann', 'Bifurcation theory', 'Binary relation', 'Biological science', 'Biology', 'Biostatistics', 'Birational geometry', 'Brownian motion', 'C*-algebra', 'CR geometry', 'CR manifold', 'Calculus', 'Calculus of moving surfaces', 'Calculus of variation', 'Calculus of variations', 'Cambridge University Press', 'Carl Benjamin Boyer', 'Cartesian coordinates', 'Cartesian geometry', 'Catastrophe theory', 'Categorical logic', 'Categorification', 'Category (mathematics)', 'Category theory', 'Chaos theory', 'Character theory', 'Characteristic (algebra)', 'Circle', 'Class field theory', 'Classical analysis', 'Classical logic', 'Classical mathematics', 'Clifford algebra', 'Clifford analysis', 'Clifford theory', 'Closed set', 'Cobordism theory', 'Code', 'Coding theory', 'Cohomology group', 'Cohomology theory', 'Collinearity', 'Combinatorial', 'Combinatorial analysis', 'Combinatorial commutative algebra', 'Combinatorial design theory', 'Combinatorial game theory', 'Combinatorial geometry', 'Combinatorial group theory', 'Combinatorial mathematics', 'Combinatorial number theory', 'Combinatorial optimization', 'Combinatorial set theory', 'Combinatorial theory', 'Combinatorial topology', 'Combinatorics', 'Commutative algebra', 'Commutative ring', 'Compact space', 'Complex algebraic geometry', 'Complex analysis', 'Complex analytic dynamics', 'Complex analytic geometry', 'Complex dynamics', 'Complex geometry', 'Complex manifold', 'Complex number', 'Complex numbers', 'Complex system', 'Complex systems', 'Complexity class', 'Complexity theory (disambiguation)', 'Computability', 'Computability theory', 'Computable analysis', 'Computable function', 'Computable model theory', 'Computation', 'Computational algebraic geometry', 'Computational complexity theory', 'Computational geometry', 'Computational group theory', 'Computational mathematics', 'Computational number theory', 'Computational problem', 'Computational statistics', 'Computational synthetic geometry', 'Computational topology', 'Computer algebra', 'Computer science', 'Conformal geometry', 'Conformal map', 'Constructible function', 'Constructive analysis', 'Constructive function theory', 'Constructive mathematics', 'Constructive quantum field theory', 'Constructive set theory', 'Constructivism (mathematics)', 'Contact geometry', 'Continuous function', 'Continuous linear operator', 'Convex analysis', 'Convex bodies', 'Convex function', 'Convex geometry', 'Convex polyhedron', 'Convex polytope', 'Convex set', 'Coordinate', 'Coordinate geometry', 'Countable set', 'Cross product', 'Cryptography', 'Curve', 'Curve (mathematics)', 'Data', 'Decision analysis', 'Decision theory', 'Definable function', 'Derivative', 'Descriptive set theory', 'Difference equation', 'Differentiable function', 'Differentiable manifold', 'Differential Galois theory', 'Differential algebraic geometry', 'Differential calculus', 'Differential equation', 'Differential field', 'Differential geometry', 'Differential geometry of curves', 'Differential geometry of surfaces', 'Differential operator', 'Differential topology', 'Differentiation (mathematics)', 'Differentiation operator', 'Diffiety', 'Dimension', 'Diophantine equation', 'Diophantine geometry', 'Dirac operator', 'Discrepancy theory', 'Discrete Morse theory', 'Discrete differential geometry', 'Discrete exterior calculus', 'Discrete geometry', 'Discrete mathematics', 'Discrete space', 'Distance geometry', 'Division (mathematics)', 'Divisor', 'Doi (identifier)', 'Domain theory', 'Donaldson theory', 'Dot product', 'Dyadics', 'Dynamic system', 'Dynamical system', 'Dynamical systems theory', 'Econometrics', 'Economics', 'Effective descriptive set theory', 'Eigenfunction', 'Eigenvalue', 'Eigenvector', 'Elementary algebra', 'Elementary arithmetic', 'Elementary group theory', 'Elementary mathematics', 'Elimination theory', 'Elliptic geometry', 'Elliptic space', 'Embedding', 'Engineering', 'Enumerative combinatorics', 'Enumerative geometry', 'Epidemiology', 'Equation', 'Ergodic Ramsey theory', 'Ergodic theory', 'Erich Kähler', 'Euclid', 'Euclidean geometry', 'Euclidean space', 'Euclidean vector', 'Euler calculus', 'Euler characteristic', 'Event (probability theory)', 'Experimental mathematics', 'Expression (mathematics)', 'Extraordinary cohomology theory', 'Extremal combinatorics', 'Extremal graph theory', 'Face (geometry)', 'Factorization', 'Field (mathematics)', 'Field extension', 'Field theory (mathematics)', 'Finance', 'Financial market', 'Finite geometry', 'Finite group', 'Finite model theory', 'Finite set', 'Finitely generated group', 'Finsler geometry', 'Finsler manifold', 'First-order logic', 'Fixed point topology', 'Foundations of mathematics', 'Fourier analysis', 'Fourier series', 'Fourier transform', 'Fractal geometry', 'Fraction (mathematics)', 'Fractional calculus', 'Fractional dynamics', 'Frank P. Ramsey', 'Fredholm theory', 'Free group', 'Function (mathematics)', 'Function field of an algebraic variety', 'Function of several variables', 'Function space', 'Function theory (disambiguation)', 'Functional (mathematics)', 'Functional analysis', 'Functional calculus', 'Functor', 'Fundamental theorem of calculus', 'Fuzzy logic', 'Fuzzy mathematics', 'Fuzzy measure theory', 'Fuzzy set', 'Fuzzy set theory', 'Galois cohomology', 'Galois field', 'Galois geometry', 'Galois group', 'Galois module', 'Galois theory', 'Game theory', 'Gauge theory', 'General relativity', 'General topology', 'Generalized trigonometry', 'Generating function', 'Geometric algebra', 'Geometric analysis', 'Geometric calculus', 'Geometric combinatorics', 'Geometric function theory', 'Geometric graph theory', 'Geometric group theory', 'Geometric invariant theory', 'Geometric measure theory', 'Geometric number theory', 'Geometric topology', 'Geometry', 'Geometry of number', 'Germ (mathematics)', 'Global analysis', 'Glossary of Arabic toponyms', 'Glossary of Hebrew toponyms', 'Glossary of aerospace engineering', 'Glossary of agriculture', 'Glossary of archaeology', 'Glossary of architecture', 'Glossary of artificial intelligence', 'Glossary of astronomy', 'Glossary of biology', 'Glossary of bird terms', 'Glossary of botanical terms', 'Glossary of calculus', 'Glossary of cell biology', 'Glossary of cellular and molecular biology (0–L)', 'Glossary of cellular and molecular biology (M–Z)', 'Glossary of chemistry terms', 'Glossary of civil engineering', 'Glossary of clinical research', 'Glossary of computer hardware terms', 'Glossary of computer science', 'Glossary of developmental biology', 'Glossary of ecology', 'Glossary of economics', 'Glossary of electrical and electronics engineering', 'Glossary of engineering: A–L', 'Glossary of engineering: M–Z', 'Glossary of entomology terms', 'Glossary of environmental science', 'Glossary of genetics and evolutionary biology', 'Glossary of geography terms (A–M)', 'Glossary of geography terms (N–Z)', 'Glossary of geology', 'Glossary of ichthyology', 'Glossary of machine vision', 'Glossary of mechanical engineering', 'Glossary of medicine', 'Glossary of meteorology', 'Glossary of mycology', 'Glossary of nanotechnology', 'Glossary of physics', 'Glossary of probability and statistics', 'Glossary of psychiatry', 'Glossary of quantum computing', 'Glossary of robotics', 'Glossary of scientific naming', 'Glossary of structural engineering', 'Glossary of virology', 'Graph (discrete mathematics)', 'Graph theory', 'Grassmann algebra', 'Gregorio Ricci-Curbastro', 'Group (mathematics)', 'Group action (mathematics)', 'Group cohomology', 'Group representation', 'Group representation theory', 'Group theory', 'Gyrotrigonometry', 'Gyrovector space', 'Hard analysis', 'Harmonic analysis', 'Hausdorff space', 'Hermann Minkowski', 'Higher-dimensional algebra', 'Higher category theory', 'Hilbert space', 'Hodge-Arakelov theory', 'Hodge theory', 'Holomorphic function', 'Holomorphic functional calculus', 'Homogeneous space', 'Homological algebra', 'Homology (mathematics)', 'Homology theory', 'Homotopy theory', 'Hyperbolic function', 'Hyperbolic geometry', 'Hyperbolic space', 'Hyperbolic triangle', 'Hypercomplex analysis', 'Hypercomplex number', 'Hyperfunction', 'ISBN (identifier)', 'Ideal (ring theory)', 'Ideal theory', 'Idempotent analysis', 'Idempotent semiring', 'Incidence (geometry)', 'Incidence geometry', 'Inconsistent mathematics', 'Infinitary combinatorics', 'Infinite set', 'Infinitesimal', 'Infinitesimal calculus', 'Information geometry', 'Insurance', 'Integer', 'Integer point', 'Integral', 'Integral calculus', 'Integral equation', 'Integral geometry', 'Integration (mathematics)', 'Intermediacy', 'Interpretation (logic)', 'Intersection theory', 'Intuitionistic logic', 'Intuitionistic type theory', 'Invariant (mathematics)', 'Invariant measure', 'Invariant theory', 'Inventory theory', 'Inversive geometry', 'Inversive ring geometry', 'Iterated function', 'Itô calculus', 'Iwasawa-Tate theory', 'Iwasawa theory', 'Jakob Nielsen (mathematician)', 'Jean-Pierre Serre', 'Job shop scheduling', 'K-homology', 'K-theory', 'K-theory (physics)', 'KK-theory', 'Klein geometry', 'Knot (mathematics)', 'Knot theory', 'Kummer theory', 'Kähler manifold', 'L-functions', 'L-theory', 'Large deviations theory', 'Large sample theory', 'Lars Ahlfors', 'Lattice (order)', 'Lattice theory', 'Law of the excluded middle', 'Length', 'Lie algebra', 'Lie group', 'Lie group theory', 'Lie sphere geometry', 'Lie theory', 'Lightface', 'Limit (mathematics)', 'Line (mathematics)', 'Line geometry', 'Linear algebra', 'Linear differential equation', 'Linear function', 'Linear group', 'Linear map', 'Linear programming', 'Linear representation', 'Linear space', 'Linear transformation', 'List of graphical method', 'Lists of mathematics topics', 'Local algebra', 'Local class field theory', 'Local field', 'Local ring', 'Locally compact group', 'Logic']"
53252845,Glossary of calculus,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.","Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.


== A ==

Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        
          
            ∑
            
              n
              =
              0
            
            
              ∞
            
          
          
            a
            
              n
            
          
        
      
    
    {\displaystyle \textstyle \sum _{n=0}^{\infty }a_{n}}
  
 is said to converge absolutely if 
  
    
      
        
          
            ∑
            
              n
              =
              0
            
            
              ∞
            
          
          
            |
            
              a
              
                n
              
            
            |
          
          =
          L
        
      
    
    {\displaystyle \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L}
  
 for some real number 
  
    
      
        
          L
        
      
    
    {\displaystyle \textstyle L}
  
. Similarly, an improper integral of a function, 
  
    
      
        
          
            ∫
            
              0
            
            
              ∞
            
          
          f
          (
          x
          )
          
          d
          x
        
      
    
    {\displaystyle \textstyle \int _{0}^{\infty }f(x)\,dx}
  
, is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        
          
            ∫
            
              0
            
            
              ∞
            
          
          
            |
            
              f
              (
              x
              )
            
            |
          
          d
          x
          =
          L
          .
        
      
    
    {\displaystyle \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.}
  

absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3. The absolute value of a number may be thought of as its distance from zero.
alternating series
An infinite series whose terms alternate between positive and negative.
alternating series test
Is the method used to prove that an alternating series with terms that decrease in absolute value is a convergent series. The test was used by Gottfried Leibniz and is sometimes known as Leibniz's test, Leibniz's rule, or the Leibniz criterion.
annulus
A ring-shaped object, a region bounded by two concentric circles.
antiderivative
An antiderivative, primitive function, primitive integral or indefinite integral of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as 
  
    
      
        
          F
          ′
        
        =
        f
      
    
    {\displaystyle F'=f}
  
. The process of solving for antiderivatives is called antidifferentiation (or indefinite integration) and its opposite operation is called differentiation, which is the process of finding a derivative.
arcsin

area under a curve

asymptote
In analytic geometry, an asymptote of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity. Some sources include the requirement that the curve may not cross the line infinitely often, but this is unusual for modern authors. In projective geometry and related contexts, an asymptote of a curve is a line which is tangent to the curve at a point at infinity.
automatic differentiation
In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.

average rate of change


== B ==

binomial coefficient
Any of the positive integers that occurs as a coefficient in the binomial theorem is a binomial coefficient. Commonly, a binomial coefficient is indexed by a pair of integers n ≥ k ≥ 0 and is written 
  
    
      
        
          
            
              
                (
              
              
                n
                k
              
              
                )
              
            
          
        
        .
      
    
    {\displaystyle {\tbinom {n}{k}}.}
  
 It is the coefficient of the xk term in the polynomial expansion of the binomial power (1 + x)n, and it is given by the formula

  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        =
        
          
            
              n
              !
            
            
              k
              !
              (
              n
              −
              k
              )
              !
            
          
        
        .
      
    
    {\displaystyle {\binom {n}{k}}={\frac {n!}{k!(n-k)!}}.}
  

binomial theorem (or binomial expansion)
 Describes the algebraic expansion of powers of a binomial.
bounded function
A function f defined on some set X with real or complex values is called bounded, if the set of its values is bounded. In other words, there exists a real number M such that 

  
    
      
        
          |
        
        f
        (
        x
        )
        
          |
        
        ≤
        M
      
    
    {\displaystyle |f(x)|\leq M}
  

for all x in X. A function that is not bounded is said to be unbounded.

Sometimes, if f(x) ≤ A for all x in X, then the function is said to be bounded above by A. On the other hand, if f(x) ≥ B for all x in X, then the function is said to be bounded below by B.
bounded sequence
 .


== C ==

calculus
(From Latin calculus, literally 'small pebble', used for counting and calculations, as on an abacus) is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
Cavalieri's principle
 Cavalieri's principle, a modern implementation of the method of indivisibles, named after Bonaventura Cavalieri, is as follows:
2-dimensional case: Suppose two regions in a plane are included between two parallel lines in that plane. If every line parallel to these two lines intersects both regions in line segments of equal length, then the two regions have equal areas.
3-dimensional case: Suppose two regions in three-space (solids) are included between two parallel planes. If every plane parallel to these two planes intersects both regions in cross-sections of equal area, then the two regions have equal volumes.
chain rule
The chain rule is a formula for computing the derivative of the composition of two or more functions. That is, if f and g are functions, then the chain rule expresses the derivative of their composition f ∘ g (the function which maps x to f(g(x)) ) in terms of the derivatives of f and g and the product of functions as follows:

  
    
      
        (
        f
        ∘
        g
        
          )
          ′
        
        =
        (
        
          f
          ′
        
        ∘
        g
        )
        ⋅
        
          g
          ′
        
        .
      
    
    {\displaystyle (f\circ g)'=(f'\circ g)\cdot g'.}
  

This may equivalently be expressed in terms of the variable. Let F = f ∘ g, or equivalently, F(x) = f(g(x)) for all x. Then one can also write

  
    
      
        
          F
          ′
        
        (
        x
        )
        =
        
          f
          ′
        
        (
        g
        (
        x
        )
        )
        
          g
          ′
        
        (
        x
        )
        .
      
    
    {\displaystyle F'(x)=f'(g(x))g'(x).}
  

The chain rule may be written in Leibniz's notation in the following way. If a variable z depends on the variable y, which itself depends on the variable x, so that y and z are therefore dependent variables, then z, via the intermediate variable of y, depends on x as well. The chain rule then states,

  
    
      
        
          
            
              d
              z
            
            
              d
              x
            
          
        
        =
        
          
            
              d
              z
            
            
              d
              y
            
          
        
        ⋅
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        .
      
    
    {\displaystyle {\frac {dz}{dx}}={\frac {dz}{dy}}\cdot {\frac {dy}{dx}}.}
  

The two versions of the chain rule are related; if 
  
    
      
        z
        =
        f
        (
        y
        )
      
    
    {\displaystyle z=f(y)}
  
 and 
  
    
      
        y
        =
        g
        (
        x
        )
      
    
    {\displaystyle y=g(x)}
  
, then

  
    
      
        
          
            
              d
              z
            
            
              d
              x
            
          
        
        =
        
          
            
              d
              z
            
            
              d
              y
            
          
        
        ⋅
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        =
        
          f
          ′
        
        (
        y
        )
        
          g
          ′
        
        (
        x
        )
        =
        
          f
          ′
        
        (
        g
        (
        x
        )
        )
        
          g
          ′
        
        (
        x
        )
        .
      
    
    {\displaystyle {\frac {dz}{dx}}={\frac {dz}{dy}}\cdot {\frac {dy}{dx}}=f'(y)g'(x)=f'(g(x))g'(x).}
  

In integration, the counterpart to the chain rule is the substitution rule.
change of variables
 Is a basic technique used to simplify problems in which the original variables are replaced with functions of other variables. The intent is that when expressed in new variables, the problem may become simpler, or equivalent to a better understood problem.
cofunction
A function f is cofunction of a function g if f(A) = g(B) whenever A and B are complementary angles. This definition typically applies to trigonometric functions. The prefix ""co-"" can be found already in Edmund Gunter's Canon triangulorum (1620).
concave function
 Is the negative of a convex function. A concave function is also synonymously called concave downwards, concave down, convex upwards, convex cap or upper convex.
constant of integration
The indefinite integral of a given function (i.e., the set of all antiderivatives of the function) on a connected domain is only defined up to an additive constant, the constant of integration. This constant expresses an ambiguity inherent in the construction of antiderivatives. If a function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 is defined on an interval and 
  
    
      
        F
        (
        x
        )
      
    
    {\displaystyle F(x)}
  
 is an antiderivative of 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
, then the set of all antiderivatives of 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 is given by the functions 
  
    
      
        F
        (
        x
        )
        +
        C
      
    
    {\displaystyle F(x)+C}
  
, where C is an arbitrary constant (meaning that any value for C makes 
  
    
      
        F
        (
        x
        )
        +
        C
      
    
    {\displaystyle F(x)+C}
  
 a valid antiderivative). The constant of integration is sometimes omitted in lists of integrals for simplicity.
continuous function
Is a function for which sufficiently small changes in the input result in arbitrarily small changes in the output. Otherwise, a function is said to be a discontinuous function. A continuous function with a continuous inverse function is called a homeomorphism.
continuously differentiable
A function f is said to be continuously differentiable if the derivative f′(x) exists and is itself a continuous function.
contour integration
In the mathematical field of complex analysis, contour integration is a method of evaluating certain integrals along paths in the complex plane.
convergence tests
Are methods of testing for the convergence, conditional convergence, absolute convergence, interval of convergence or divergence of an infinite series 
  
    
      
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          a
          
            n
          
        
      
    
    {\displaystyle \sum _{n=1}^{\infty }a_{n}}
  
.
convergent series
In mathematics, a series is the sum of the terms of an infinite sequence of numbers.

Given an infinite sequence 
  
    
      
        
          (
          
            
              a
              
                1
              
            
            ,
             
            
              a
              
                2
              
            
            ,
             
            
              a
              
                3
              
            
            ,
            …
          
          )
        
      
    
    {\displaystyle \left(a_{1},\ a_{2},\ a_{3},\dots \right)}
  
, the nth partial sum 
  
    
      
        
          S
          
            n
          
        
      
    
    {\displaystyle S_{n}}
  
 is the sum of the first n terms of the sequence, that is,

  
    
      
        
          S
          
            n
          
        
        =
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        
          a
          
            k
          
        
        .
      
    
    {\displaystyle S_{n}=\sum _{k=1}^{n}a_{k}.}
  

A series is convergent if the sequence of its partial sums 
  
    
      
        
          {
          
            
              S
              
                1
              
            
            ,
             
            
              S
              
                2
              
            
            ,
             
            
              S
              
                3
              
            
            ,
            …
          
          }
        
      
    
    {\displaystyle \left\{S_{1},\ S_{2},\ S_{3},\dots \right\}}
  
 tends to a limit; that means that the partial sums become closer and closer to a given number when the number of their terms increases. More precisely, a series converges, if there exists a number 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
  
 such that for any arbitrarily small positive number 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, there is a (sufficiently large) integer 
  
    
      
        N
      
    
    {\displaystyle N}
  
 such that for all 
  
    
      
        n
        ≥
         
        N
      
    
    {\displaystyle n\geq \ N}
  
,

  
    
      
        
          |
          
            
              S
              
                n
              
            
            −
            ℓ
          
          |
        
        ≤
         
        ε
        .
      
    
    {\displaystyle \left|S_{n}-\ell \right\vert \leq \ \varepsilon .}
  

If the series is convergent, the number 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
  
 (necessarily unique) is called the sum of the series.

Any series that is not convergent is said to be divergent.
convex function
In mathematics, a real-valued function defined on an n-dimensional interval is called convex (or convex downward or concave upward) if the line segment between any two points on the graph of the function lies above or on the graph, in a Euclidean space (or more generally a vector space) of at least two dimensions. Equivalently, a function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. For a twice differentiable function of a single variable, if the second derivative is always greater than or equal to zero for its entire domain then the function is convex. Well-known examples of convex functions include the quadratic function 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x^{2}}
  
 and the exponential function 
  
    
      
        
          e
          
            x
          
        
      
    
    {\displaystyle e^{x}}
  
.
Cramer's rule
 In linear algebra, Cramer's rule is an explicit formula for the solution of a system of linear equations with as many equations as unknowns, valid whenever the system has a unique solution. It expresses the solution in terms of the determinants of the (square) coefficient matrix and of matrices obtained from it by replacing one column by the column vector of right-hand-sides of the equations. It is named after Gabriel Cramer (1704–1752), who published the rule for an arbitrary number of unknowns in 1750, although Colin Maclaurin also published special cases of the rule in 1748 (and possibly knew of it as early as 1729).
critical point
A critical point or stationary point of a differentiable function of a real or complex variable is any value in its domain where its derivative is 0.
curve
A curve (also called a curved line in older texts) is, generally speaking, an object similar to a line but that need not be straight. 
curve sketching
In geometry, curve sketching (or curve tracing) includes techniques that can be used to produce a rough idea of overall shape of a plane curve given its equation without computing the large numbers of points required for a detailed plot. It is an application of the theory of curves to find their main features. Here input is an equation.
 In digital geometry it is a method of drawing a curve pixel by pixel. Here input is an array (digital image).


== D ==

damped sine wave
Is a sinusoidal function whose amplitude approaches zero as time increases.
degree of a polynomial
Is the highest degree of its monomials (individual terms) with non-zero coefficients. The degree of a term is the sum of the exponents of the variables that appear in it, and thus is a non-negative integer.
derivative
The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.
derivative test
A derivative test uses the derivatives of a function to locate the critical points of a function and determine whether each point is a local maximum, a local minimum, or a saddle point. Derivative tests can also give information about the concavity of a function.
differentiable function
A differentiable function of one real variable is a function whose derivative exists at each point in its domain. As a result, the graph of a differentiable function must have a (non-vertical) tangent line at each point in its domain, be relatively smooth, and cannot contain any breaks, bends, or cusps.
differential (infinitesimal)
The term differential is used in calculus to refer to an infinitesimal (infinitely small) change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is extremely useful intuitively, and there are a number of ways to make the notion mathematically precise.

Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

  
    
      
        d
        y
        =
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
        d
        x
        ,
      
    
    {\displaystyle dy={\frac {dy}{dx}}\,dx,}
  

where dy/dx denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.
differential calculus
Is a subfield of calculus concerned with the study of the rates at which quantities change. It is one of the two traditional divisions of calculus, the other being integral calculus, the study of the area beneath a curve.
differential equation
Is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two.
differential operator
.
differential of a function
In calculus, the differential represents the principal part of the change in a function y = f(x) with respect to changes in the independent variable. The differential dy is defined by

  
    
      
        d
        y
        =
        
          f
          ′
        
        (
        x
        )
        
        d
        x
        ,
      
    
    {\displaystyle dy=f'(x)\,dx,}
  

where 
  
    
      
        
          f
          ′
        
        (
        x
        )
      
    
    {\displaystyle f'(x)}
  
 is the derivative of f with respect to x, and dx is an additional real variable (so that dy is a function of x and dx). The notation is such that the equation

  
    
      
        d
        y
        =
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
        d
        x
      
    
    {\displaystyle dy={\frac {dy}{dx}}\,dx}
  

holds, where the derivative is represented in the Leibniz notation dy/dx, and this is consistent with regarding the derivative as the quotient of the differentials. One also writes

  
    
      
        d
        f
        (
        x
        )
        =
        
          f
          ′
        
        (
        x
        )
        
        d
        x
        .
      
    
    {\displaystyle df(x)=f'(x)\,dx.}
  

The precise meaning of the variables dy and dx depends on the context of the application and the required level of mathematical rigor. The domain of these variables may take on a particular geometrical significance if the differential is regarded as a particular differential form, or analytical significance if the differential is regarded as a linear approximation to the increment of a function. Traditionally, the variables dx and dy are considered to be very small (infinitesimal), and this interpretation is made rigorous in non-standard analysis.
differentiation rules
.
direct comparison test
A convergence test in which an infinite series or an improper integral is compared to one with known convergence properties.
Dirichlet's test
Is a method of testing for the convergence of a series. It is named after its author Peter Gustav Lejeune Dirichlet, and was published posthumously in the Journal de Mathématiques Pures et Appliquées in 1862. The test states that if 
  
    
      
        {
        
          a
          
            n
          
        
        }
      
    
    {\displaystyle \{a_{n}\}}
  
 is a sequence of real numbers and 
  
    
      
        {
        
          b
          
            n
          
        
        }
      
    
    {\displaystyle \{b_{n}\}}
  
 a sequence of complex numbers satisfying

  
    
      
        
          a
          
            n
            +
            1
          
        
        ≤
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n+1}\leq a_{n}}
  

  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          a
          
            n
          
        
        =
        0
      
    
    {\displaystyle \lim _{n\rightarrow \infty }a_{n}=0}
  

  
    
      
        
          |
          
            
              ∑
              
                n
                =
                1
              
              
                N
              
            
            
              b
              
                n
              
            
          
          |
        
        ≤
        M
      
    
    {\displaystyle \left|\sum _{n=1}^{N}b_{n}\right|\leq M}
  
 for every positive integer N

where M is some constant, then the series

  
    
      
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          a
          
            n
          
        
        
          b
          
            n
          
        
      
    
    {\displaystyle \sum _{n=1}^{\infty }a_{n}b_{n}}
  

converges.
disc integration
Also known in integral calculus as the disc method, is a means of calculating the volume of a solid of revolution of a solid-state material when integrating along an axis ""parallel"" to the axis of revolution.
divergent series
Is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit.
discontinuity
Continuous functions are of utmost importance in mathematics, functions and applications. However, not all functions are continuous. If a function is not continuous at a point in its domain, one says that it has a discontinuity there. The set of all points of discontinuity of a function may be a discrete set, a dense set, or even the entire domain of the function.
dot product
In mathematics, the dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used and often called ""the"" inner product (or rarely projection product) of Euclidean space even though it is not the only inner product that can be defined on Euclidean space; see also inner product space.
double integral
The multiple integral is a definite integral of a function of more than one real variable, for example, f(x, y) or f(x, y, z). Integrals of a function of two variables over a region in R2 are called double integrals, and integrals of a function of three variables over a region of R3 are called triple integrals.


== E ==

e (mathematical constant)
The number e is a mathematical constant that is the base of the natural logarithm: the unique number whose natural logarithm is equal to one. It is approximately equal to 2.71828, and is the limit of (1 + 1/n)n as n approaches infinity, an expression that arises in the study of compound interest. It can also be calculated as the sum of the infinite series

  
    
      
        e
        =
        
          
            ∑
            
              n
              =
              0
            
            
              ∞
            
          
          
            
              
                1
                
                  n
                  !
                
              
            
          
          =
          
            
              1
              1
            
          
          +
          
            
              1
              1
            
          
          +
          
            
              1
              
                1
                ⋅
                2
              
            
          
          +
          
            
              1
              
                1
                ⋅
                2
                ⋅
                3
              
            
          
          +
          ⋯
        
      
    
    {\displaystyle e=\displaystyle \sum \limits _{n=0}^{\infty }{\dfrac {1}{n!}}={\frac {1}{1}}+{\frac {1}{1}}+{\frac {1}{1\cdot 2}}+{\frac {1}{1\cdot 2\cdot 3}}+\cdots }
  

elliptic integral
In integral calculus, elliptic integrals originally arose in connection with the problem of giving the arc length of an ellipse. They were first studied by Giulio Fagnano and Leonhard Euler (c. 1750). Modern mathematics defines an ""elliptic integral"" as any function f which can be expressed in the form

  
    
      
        f
        (
        x
        )
        =
        
          ∫
          
            c
          
          
            x
          
        
        R
        
          (
          
            t
            ,
            
              
                P
                (
                t
                )
              
            
          
          )
        
        
        d
        t
        ,
      
    
    {\displaystyle f(x)=\int _{c}^{x}R\left(t,{\sqrt {P(t)}}\right)\,dt,}
  

where R is a rational function of its two arguments, P is a polynomial of degree 3 or 4 with no repeated roots, and c is a constant..
essential discontinuity
For an essential discontinuity, only one of the two one-sided limits needs not exist or be infinite.
Consider the function 

  
    
      
        f
        (
        x
        )
        =
        
          
            {
            
              
                
                  sin
                  ⁡
                  
                    
                      5
                      
                        x
                        −
                        1
                      
                    
                  
                
                
                  
                    
                       for 
                    
                  
                  x
                  <
                  1
                
              
              
                
                  0
                
                
                  
                    
                       for 
                    
                  
                  x
                  =
                  1
                
              
              
                
                  
                    
                      1
                      
                        x
                        −
                        1
                      
                    
                  
                
                
                  
                    
                       for 
                    
                  
                  x
                  >
                  1
                
              
            
            
          
        
      
    
    {\displaystyle f(x)={\begin{cases}\sin {\frac {5}{x-1}}&{\mbox{ for }}x<1\\0&{\mbox{ for }}x=1\\{\frac {1}{x-1}}&{\mbox{ for }}x>1\end{cases}}}
  

Then, the point 
  
    
      
        
          
            x
            
              0
            
          
          
          =
          
          1
        
      
    
    {\displaystyle \scriptstyle x_{0}\;=\;1}
  
 is an essential discontinuity.

In this case, 
  
    
      
        
          
            L
            
              −
            
          
        
      
    
    {\displaystyle \scriptstyle L^{-}}
  
 doesn't exist and 
  
    
      
        
          
            L
            
              +
            
          
        
      
    
    {\displaystyle \scriptstyle L^{+}}
  
 is infinite – thus satisfying twice the conditions of essential discontinuity. So x0 is an essential discontinuity, infinite discontinuity, or discontinuity of the second kind. (This is distinct from the term essential singularity which is often used when studying functions of complex variables.
Euler method
Euler's method is a numerical method to solve first order first degree differential equation with a given initial value. It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–1870).
exponential function
In mathematics, an exponential function is a function of the form

where b is a positive real number, and in which the argument x occurs as an exponent. For real numbers c and d, a function of the form 
  
    
      
        f
        (
        x
        )
        =
        a
        
          b
          
            c
            x
            +
            d
          
        
      
    
    {\displaystyle f(x)=ab^{cx+d}}
  
 is also an exponential function, as it can be rewritten as 

  
    
      
        a
        
          b
          
            c
            x
            +
            d
          
        
        =
        
          (
          
            a
            
              b
              
                d
              
            
          
          )
        
        
          
            (
            
              b
              
                c
              
            
            )
          
          
            x
          
        
        .
      
    
    {\displaystyle ab^{cx+d}=\left(ab^{d}\right)\left(b^{c}\right)^{x}.}
  

extreme value theorem
States that if a real-valued function f is continuous on the closed interval [a,b], then f must attain a maximum and a minimum, each at least once. That is, there exist numbers c and d in [a,b] such that:

  
    
      
        f
        (
        c
        )
        ≥
        f
        (
        x
        )
        ≥
        f
        (
        d
        )
        
        
          for all 
        
        x
        ∈
        [
        a
        ,
        b
        ]
        .
      
    
    {\displaystyle f(c)\geq f(x)\geq f(d)\quad {\text{for all }}x\in [a,b].}
  

A related theorem is the boundedness theorem which states that a continuous function f in the closed interval [a,b] is bounded on that interval. That is, there exist real numbers m and M such that:

  
    
      
        m
        <
        f
        (
        x
        )
        <
        M
        
        
          for all 
        
        x
        ∈
        [
        a
        ,
        b
        ]
        .
      
    
    {\displaystyle m<f(x)<M\quad {\text{for all }}x\in [a,b].}
  

The extreme value theorem enriches the boundedness theorem by saying that not only is the function bounded, but it also attains its least upper bound as its maximum and its greatest lower bound as its minimum.
extremum
In mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.

As defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.


== F ==

Faà di Bruno's formula
Is an identity in mathematics generalizing the chain rule to higher derivatives, named after Francesco Faà di Bruno (1855, 1857), though he was not the first to state or prove the formula. In 1800, more than 50 years before Faà di Bruno, the French mathematician Louis François Antoine Arbogast stated the formula in a calculus textbook, considered the first published reference on the subject.

Perhaps the most well-known form of Faà di Bruno's formula says that

  
    
      
        
          
            
              d
              
                n
              
            
            
              d
              
                x
                
                  n
                
              
            
          
        
        f
        (
        g
        (
        x
        )
        )
        =
        ∑
        
          
            
              n
              !
            
            
              
                m
                
                  1
                
              
              !
              
              1
              
                !
                
                  
                    m
                    
                      1
                    
                  
                
              
              
              
                m
                
                  2
                
              
              !
              
              2
              
                !
                
                  
                    m
                    
                      2
                    
                  
                
              
              
              ⋯
              
              
                m
                
                  n
                
              
              !
              
              n
              
                !
                
                  
                    m
                    
                      n
                    
                  
                
              
            
          
        
        ⋅
        
          f
          
            (
            
              m
              
                1
              
            
            +
            ⋯
            +
            
              m
              
                n
              
            
            )
          
        
        (
        g
        (
        x
        )
        )
        ⋅
        
          ∏
          
            j
            =
            1
          
          
            n
          
        
        
          
            (
            
              
                g
                
                  (
                  j
                  )
                
              
              (
              x
              )
            
            )
          
          
            
              m
              
                j
              
            
          
        
        ,
      
    
    {\displaystyle {d^{n} \over dx^{n}}f(g(x))=\sum {\frac {n!}{m_{1}!\,1!^{m_{1}}\,m_{2}!\,2!^{m_{2}}\,\cdots \,m_{n}!\,n!^{m_{n}}}}\cdot f^{(m_{1}+\cdots +m_{n})}(g(x))\cdot \prod _{j=1}^{n}\left(g^{(j)}(x)\right)^{m_{j}},}
  

where the sum is over all n-tuples of nonnegative integers (m1, …, mn) satisfying the constraint

  
    
      
        1
        ⋅
        
          m
          
            1
          
        
        +
        2
        ⋅
        
          m
          
            2
          
        
        +
        3
        ⋅
        
          m
          
            3
          
        
        +
        ⋯
        +
        n
        ⋅
        
          m
          
            n
          
        
        =
        n
        .
      
    
    {\displaystyle 1\cdot m_{1}+2\cdot m_{2}+3\cdot m_{3}+\cdots +n\cdot m_{n}=n.}
  

Sometimes, to give it a memorable pattern, it is written in a way in which the coefficients that have the combinatorial interpretation discussed below are less explicit:

  
    
      
        
          
            
              d
              
                n
              
            
            
              d
              
                x
                
                  n
                
              
            
          
        
        f
        (
        g
        (
        x
        )
        )
        =
        ∑
        
          
            
              n
              !
            
            
              
                m
                
                  1
                
              
              !
              
              
                m
                
                  2
                
              
              !
              
              ⋯
              
              
                m
                
                  n
                
              
              !
            
          
        
        ⋅
        
          f
          
            (
            
              m
              
                1
              
            
            +
            ⋯
            +
            
              m
              
                n
              
            
            )
          
        
        (
        g
        (
        x
        )
        )
        ⋅
        
          ∏
          
            j
            =
            1
          
          
            n
          
        
        
          
            (
            
              
                
                  
                    g
                    
                      (
                      j
                      )
                    
                  
                  (
                  x
                  )
                
                
                  j
                  !
                
              
            
            )
          
          
            
              m
              
                j
              
            
          
        
        .
      
    
    {\displaystyle {d^{n} \over dx^{n}}f(g(x))=\sum {\frac {n!}{m_{1}!\,m_{2}!\,\cdots \,m_{n}!}}\cdot f^{(m_{1}+\cdots +m_{n})}(g(x))\cdot \prod _{j=1}^{n}\left({\frac {g^{(j)}(x)}{j!}}\right)^{m_{j}}.}
  

Combining the terms with the same value of m1 + m2 + ... + mn = k and noticing that m j has to be zero for j > n − k + 1 leads to a somewhat simpler formula expressed in terms of Bell polynomials Bn,k(x1,...,xn−k+1):

  
    
      
        
          
            
              d
              
                n
              
            
            
              d
              
                x
                
                  n
                
              
            
          
        
        f
        (
        g
        (
        x
        )
        )
        =
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        
          f
          
            (
            k
            )
          
        
        (
        g
        (
        x
        )
        )
        ⋅
        
          B
          
            n
            ,
            k
          
        
        
          (
          
            
              g
              ′
            
            (
            x
            )
            ,
            
              g
              ″
            
            (
            x
            )
            ,
            …
            ,
            
              g
              
                (
                n
                −
                k
                +
                1
                )
              
            
            (
            x
            )
          
          )
        
        .
      
    
    {\displaystyle {d^{n} \over dx^{n}}f(g(x))=\sum _{k=1}^{n}f^{(k)}(g(x))\cdot B_{n,k}\left(g'(x),g''(x),\dots ,g^{(n-k+1)}(x)\right).}
  

first-degree polynomial

first derivative test
The first derivative test examines a function's monotonic properties (where the function is increasing or decreasing) focusing on a particular point in its domain. If the function ""switches"" from increasing to decreasing at the point, then the function will achieve a highest value at that point. Similarly, if the function ""switches"" from decreasing to increasing at the point, then it will achieve a least value at that point. If the function fails to ""switch"", and remains increasing or remains decreasing, then no highest or least value is achieved.
Fractional calculus
Is a branch of mathematical analysis that studies the several different possibilities of defining real number powers or complex number powers of the differentiation operator D

  
    
      
        D
        f
        (
        x
        )
        =
        
          
            
              d
              
                d
                x
              
            
          
        
        f
        (
        x
        )
      
    
    {\displaystyle Df(x)={\dfrac {d}{dx}}f(x)}
  
,

and of the integration operator J

  
    
      
        J
        f
        (
        x
        )
        =
        
          ∫
          
            0
          
          
            x
          
        
        
        
        
        
        f
        (
        s
        )
        
          d
          s
        
      
    
    {\displaystyle Jf(x)=\int _{0}^{x}\!\!\!\!f(s){ds}}
  
,

and developing a calculus for such operators generalizing the classical one.

In this context, the term powers refers to iterative application of a linear operator to a function, in some analogy to function composition acting on a variable, i.e. f ∘2(x) = f ∘ f (x) = f ( f (x) ).
frustum
In geometry, a frustum (plural: frusta or frustums) is the portion of a solid (normally a cone or pyramid) that lies between one or two parallel planes cutting it. A right frustum is a parallel truncation of a right pyramid or right cone.
function
Is a process or a relation that associates each element x of a set X,  the domain of the function, to a single element y of another set Y (possibly the same set), the codomain of the function. If the function is called f, this relation is denoted y = f (x) (read f of x), the element x is the argument or input of the function, and y is the value of the function, the output, or the image of x by f. The symbol that is used for representing the input is the variable of the function (one often says that f is a function of the variable x).
function composition
Is an operation that takes two functions f and g and produces a function h such that h(x) = g(f(x)). In this operation, the function g is applied to the result of applying the function f to x. That is, the functions f : X → Y and g : Y → Z are composed to yield a function that maps x in X to g(f(x)) in Z.
fundamental theorem of calculus
The fundamental theorem of calculus is a theorem that links the concept of differentiating a function with the concept of integrating a function. The first part of the theorem, sometimes called the first fundamental theorem of calculus, states that one of the antiderivatives (also called indefinite integral), say F, of some function f may be obtained as the integral of f with a variable bound of integration. This implies the existence of antiderivatives for continuous functions.  Conversely, the second part of the theorem, sometimes called the second fundamental theorem of calculus, states that the integral of a function f over some interval can be computed by using any one, say F, of its infinitely many antiderivatives. This part of the theorem has key practical applications, because explicitly finding the antiderivative of a function by symbolic integration avoids numerical integration to compute integrals. This provides generally a better numerical accuracy.


== G ==

general Leibniz rule
The general Leibniz rule, named after Gottfried Wilhelm Leibniz, generalizes the product rule (which is also known as ""Leibniz's rule"").  It states that if 
  
    
      
        f
      
    
    {\displaystyle f}
  
 and 
  
    
      
        g
      
    
    {\displaystyle g}
  
 are 
  
    
      
        n
      
    
    {\displaystyle n}
  
-times differentiable functions, then the product 
  
    
      
        f
        g
      
    
    {\displaystyle fg}
  
 is also 
  
    
      
        n
      
    
    {\displaystyle n}
  
-times differentiable and its 
  
    
      
        n
      
    
    {\displaystyle n}
  
th derivative is given by

  
    
      
        (
        f
        g
        
          )
          
            (
            n
            )
          
        
        =
        
          ∑
          
            k
            =
            0
          
          
            n
          
        
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        
          f
          
            (
            n
            −
            k
            )
          
        
        
          g
          
            (
            k
            )
          
        
        ,
      
    
    {\displaystyle (fg)^{(n)}=\sum _{k=0}^{n}{n \choose k}f^{(n-k)}g^{(k)},}
  

where 
  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        =
        
          
            
              n
              !
            
            
              k
              !
              (
              n
              −
              k
              )
              !
            
          
        
      
    
    {\displaystyle {n \choose k}={n! \over k!(n-k)!}}
  
 is the binomial coefficient and 
  
    
      
        
          f
          
            (
            0
            )
          
        
        ≡
        f
        .
      
    
    {\displaystyle f^{(0)}\equiv f.}
  

This can be proved by using the product rule and mathematical induction.
global maximum
In mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.

As defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.
global minimum
In mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.

As defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.
golden spiral
In geometry, a golden spiral is a logarithmic spiral whose growth factor is φ, the golden ratio. That is, a golden spiral gets wider (or further from its origin) by a factor of φ for every quarter turn it makes.
gradient
Is a multi-variable generalization of the derivative.  While a derivative can be defined on functions of a single variable, for functions of several variables, the gradient takes its place.  The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.


== H ==

harmonic progression
In mathematics, a harmonic progression (or harmonic sequence) is a progression formed by taking the reciprocals of an arithmetic progression. It is a sequence of the form

  
    
      
        
          
            1
            a
          
        
        ,
         
        
          
            1
            
              a
              +
              d
            
          
        
         
        ,
        
          
            1
            
              a
              +
              2
              d
            
          
        
         
        ,
        
          
            1
            
              a
              +
              3
              d
            
          
        
         
        ,
        ⋯
        ,
        
          
            1
            
              a
              +
              k
              d
            
          
        
        ,
      
    
    {\displaystyle {\frac {1}{a}},\ {\frac {1}{a+d}}\ ,{\frac {1}{a+2d}}\ ,{\frac {1}{a+3d}}\ ,\cdots ,{\frac {1}{a+kd}},}
  

where −a/d is not a natural number and k is a natural number.

Equivalently, a sequence is a harmonic progression when each term is the harmonic mean of the neighboring terms.

It is not possible for a harmonic progression (other than the trivial case where a = 1 and k = 0)  to sum to an integer. The reason is that, necessarily, at least one denominator of the progression will be divisible by a prime number that does not divide any other denominator.

higher derivative
Let f be a differentiable function, and let f ′ be its derivative. The derivative of f ′ (if it has one) is written f ′′ and is called the second derivative of f.  Similarly, the derivative of the second derivative, if it exists, is written f ′′′ and is called the third derivative of f. Continuing this process, one can define, if it exists, the nth derivative as the derivative of the (n-1)th derivative. These repeated derivatives are called higher-order derivatives. The nth derivative is also called the derivative of order n.
homogeneous linear differential equation
A differential equation can be homogeneous in either of two respects.

A first order differential equation is said to be homogeneous if it may be written

  
    
      
        f
        (
        x
        ,
        y
        )
        d
        y
        =
        g
        (
        x
        ,
        y
        )
        d
        x
        ,
      
    
    {\displaystyle f(x,y)dy=g(x,y)dx,}
  

where f and g are homogeneous functions of the same degree of x and y. In this case, the change of variable y = ux leads to an equation of the form 

  
    
      
        
          
            
              d
              x
            
            x
          
        
        =
        h
        (
        u
        )
        d
        u
        ,
      
    
    {\displaystyle {\frac {dx}{x}}=h(u)du,}
  

which is easy to solve by integration of the two members.

Otherwise, a differential equation is homogeneous if it is a homogeneous function of the unknown function and its derivatives. In the case of linear differential equations, this means that there are no constant terms. The solutions of any linear ordinary differential equation of any order may be deduced by integration from the solution of the homogeneous equation obtained by removing the constant term.
hyperbolic function
Hyperbolic functions are analogs of the ordinary trigonometric, or circular, functions.


== I ==

identity function
Also called an identity relation or identity map or identity transformation, is a function that always returns the same value that was used as its argument. In equations, the function is given by f(x) = x.
imaginary number
Is a complex number that can be written as a real number multiplied by the imaginary unit i, which is defined by its property i2 = −1. The square of an imaginary number bi is −b2. For example, 5i is an imaginary number, and its square is −25. Zero is considered to be both real and imaginary.
implicit function
In mathematics, an implicit equation is a relation  of the form 
  
    
      
        R
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        =
        0
      
    
    {\displaystyle R(x_{1},\ldots ,x_{n})=0}
  
, where 
  
    
      
        R
      
    
    {\displaystyle R}
  
 is a function of several variables (often a polynomial). For example, the implicit equation of the unit circle is 
  
    
      
        
          x
          
            2
          
        
        +
        
          y
          
            2
          
        
        −
        1
        =
        0
      
    
    {\displaystyle x^{2}+y^{2}-1=0}
  
.

An implicit function is a function that is defined implicitly by an implicit equation, by associating one of the variables (the value) with the others (the arguments).: 204–206  Thus, an implicit function for 
  
    
      
        y
      
    
    {\displaystyle y}
  
 in the context of the unit circle is defined implicitly by 
  
    
      
        
          x
          
            2
          
        
        +
        f
        (
        x
        
          )
          
            2
          
        
        −
        1
        =
        0
      
    
    {\displaystyle x^{2}+f(x)^{2}-1=0}
  
. This implicit equation defines 
  
    
      
        f
      
    
    {\displaystyle f}
  
 as a function of 
  
    
      
        x
      
    
    {\displaystyle x}
  
 only if 
  
    
      
        −
        1
        ≤
        x
        ≤
        1
      
    
    {\displaystyle -1\leq x\leq 1}
  
 and one considers only non-negative (or non-positive) values for the values of the function.

The implicit function theorem provides conditions under which some kinds of relations define an implicit function, namely relations defined as the indicator function of the zero set of some continuously differentiable multivariate function.
improper fraction
Common fractions can be classified as either proper or improper. When the numerator and the denominator are both positive, the fraction is called proper if the numerator is less than the denominator, and improper otherwise. In general, a common fraction is said to be a proper fraction if the absolute value of the fraction is strictly less than one—that is, if the fraction is greater than −1 and less than 1.
It is said to be an improper fraction, or sometimes top-heavy fraction, if the absolute value of the fraction is greater than or equal to 1. Examples of proper fractions are 2/3, –3/4, and 4/9; examples of improper fractions are 9/4, –4/3, and 3/3.
improper integral
In mathematical analysis, an improper integral is the limit of a definite integral as an endpoint of the interval(s) of integration approaches either a specified real number, 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
  
, 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
, or in some instances as both endpoints approach limits. Such an integral is often written symbolically just like a standard definite integral, in some cases with infinity as a limit of integration.

Specifically, an improper integral is a limit of the form:

  
    
      
        
          lim
          
            b
            →
            ∞
          
        
        
          ∫
          
            a
          
          
            b
          
        
        f
        (
        x
        )
        
        d
        x
        ,
        
        
          lim
          
            a
            →
            −
            ∞
          
        
        
          ∫
          
            a
          
          
            b
          
        
        f
        (
        x
        )
        
        d
        x
        ,
      
    
    {\displaystyle \lim _{b\to \infty }\int _{a}^{b}f(x)\,dx,\qquad \lim _{a\to -\infty }\int _{a}^{b}f(x)\,dx,}
  

or

  
    
      
        
          lim
          
            c
            →
            
              b
              
                −
              
            
          
        
        
          ∫
          
            a
          
          
            c
          
        
        f
        (
        x
        )
        
        d
        x
        ,
        
        
          lim
          
            c
            →
            
              a
              
                +
              
            
          
        
        
          ∫
          
            c
          
          
            b
          
        
        f
        (
        x
        )
        
        d
        x
        ,
      
    
    {\displaystyle \lim _{c\to b^{-}}\int _{a}^{c}f(x)\,dx,\quad \lim _{c\to a^{+}}\int _{c}^{b}f(x)\,dx,}
  

in which one takes a limit in one or the other (or sometimes both) endpoints (Apostol 1967, §10.23).
inflection point 
In differential calculus, an inflection point, point of inflection, flex, or inflection (British English: inflexion) is a point on a continuous plane curve at which the curve changes from being concave (concave downward) to convex (concave upward), or vice versa.
instantaneous rate of change
The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.  For this reason, the derivative is often described as the ""instantaneous rate of change"", the ratio of the instantaneous change in the dependent variable to that of the independent variable. .
instantaneous velocity
If we consider v as velocity and x as the displacement (change in position) vector, then we can express the (instantaneous) velocity of a particle or object, at any particular time t, as the derivative of the position with respect to time:

  
    
      
        
          v
        
        =
        
          lim
          
            
              Δ
              t
            
            →
            0
          
        
        
          
            
              Δ
              
                x
              
            
            
              Δ
              t
            
          
        
        =
        
          
            
              d
              
                x
              
            
            
              d
              
                
                  t
                
              
            
          
        
        .
      
    
    {\displaystyle {\boldsymbol {v}}=\lim _{{\Delta t}\to 0}{\frac {\Delta {\boldsymbol {x}}}{\Delta t}}={\frac {d{\boldsymbol {x}}}{d{\mathit {t}}}}.}
  

From this derivative equation, in the one-dimensional case it can be seen that the area under a velocity vs. time (v vs. t graph) is the displacement, x. In calculus terms, the integral of the velocity function v(t) is the displacement function x(t). In the figure, this corresponds to the yellow area under the curve labeled s (s being an alternative notation for displacement).

  
    
      
        
          x
        
        =
        ∫
        
          v
        
         
        d
        
          
            t
          
        
        .
      
    
    {\displaystyle {\boldsymbol {x}}=\int {\boldsymbol {v}}\ d{\mathit {t}}.}
  

Since the derivative of the position with respect to time gives the change in position (in metres) divided by the change in time (in seconds), velocity is measured in metres per second (m/s). Although the concept of an instantaneous velocity might at first seem counter-intuitive, it may be thought of as the velocity that the object would continue to travel at if it stopped accelerating at that moment. .
integral
An integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse operation, differentiation, being the other. .
integral symbol
The integral symbol:
∫ (Unicode), 
  
    
      
        
          ∫
        
      
    
    {\displaystyle \displaystyle \int }
  
 (LaTeX)
is used to denote integrals and antiderivatives in mathematics. .
integrand 
The function to be integrated in an integral.
integration by parts
In calculus, and more generally in mathematical analysis, integration by parts or partial integration is a process that finds the integral of a product of functions in terms of the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be readily derived by integrating the product rule of differentiation.

If u = u(x) and du = u′(x) dx, while v = v(x) and dv = v′(x) dx, then integration by parts states that:

  
    
      
        
          
            
              
                
                  ∫
                  
                    a
                  
                  
                    b
                  
                
                u
                (
                x
                )
                
                  v
                  ′
                
                (
                x
                )
                
                d
                x
              
              
                
                =
                
                  
                    [
                  
                
                u
                (
                x
                )
                v
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    a
                  
                  
                    b
                  
                
                −
                
                  ∫
                  
                    a
                  
                  
                    b
                  
                
                
                  u
                  ′
                
                (
                x
                )
                v
                (
                x
                )
                
                d
                x
              
            
            
              
              
                
                =
                u
                (
                b
                )
                v
                (
                b
                )
                −
                u
                (
                a
                )
                v
                (
                a
                )
                −
                
                  ∫
                  
                    a
                  
                  
                    b
                  
                
                
                  u
                  ′
                
                (
                x
                )
                v
                (
                x
                )
                
                d
                x
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\int _{a}^{b}u(x)v'(x)\,dx&={\Big [}u(x)v(x){\Big ]}_{a}^{b}-\int _{a}^{b}u'(x)v(x)\,dx\\&=u(b)v(b)-u(a)v(a)-\int _{a}^{b}u'(x)v(x)\,dx\end{aligned}}}
  

or more compactly:

  
    
      
        ∫
        u
        
        d
        v
        =
        u
        v
        −
        ∫
        v
        
        d
        u
        .
      
    
    {\displaystyle \int u\,dv=uv-\int v\,du.}
  

Mathematician Brook Taylor discovered integration by parts, first publishing the idea in 1715. More general formulations of integration by parts exist for the Riemann–Stieltjes and Lebesgue–Stieltjes integrals. The discrete analogue for sequences is called summation by parts.
 .
integration by substitution
Also known as u-substitution, is a method for solving integrals. Using the fundamental theorem of calculus often requires finding an antiderivative. For this and other reasons, integration by substitution is an important tool in mathematics. It is the counterpart to the chain rule for differentiation. .
intermediate value theorem
In mathematical analysis, the intermediate value theorem states that if a continuous function, f, with an interval, [a, b], as its domain, takes values f(a) and f(b) at each end of the interval, then it also takes any value between f(a) and f(b) at some point within the interval.

This has two important corollaries:

If a continuous function has values of opposite sign inside an interval, then it has a root in that interval (Bolzano's theorem).
The image of a continuous function over an interval is itself an interval. .
inverse trigonometric functions
(Also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios.


== J ==

jump discontinuity
Consider the function  

  
    
      
        f
        (
        x
        )
        =
        
          
            {
            
              
                
                  
                    x
                    
                      2
                    
                  
                
                
                  
                    
                       for 
                    
                  
                  x
                  <
                  1
                
              
              
                
                  0
                
                
                  
                    
                       for 
                    
                  
                  x
                  =
                  1
                
              
              
                
                  2
                  −
                  (
                  x
                  −
                  1
                  
                    )
                    
                      2
                    
                  
                
                
                  
                    
                       for 
                    
                  
                  x
                  >
                  1
                
              
            
            
          
        
      
    
    {\displaystyle f(x)={\begin{cases}x^{2}&{\mbox{ for }}x<1\\0&{\mbox{ for }}x=1\\2-(x-1)^{2}&{\mbox{ for }}x>1\end{cases}}}
  

Then, the point x0 = 1 is a jump discontinuity.

In this case, a single limit does not exist because the one-sided limits, L− and L+, exist and are finite, but are not equal: since, L− ≠ L+, the limit L does not exist. Then, x0 is called a jump discontinuity, step discontinuity, or discontinuity of the first kind. For this type of discontinuity, the function f may have any value at x0.


== L ==

Lebesgue integration
In mathematics, the integral of a non-negative function of a single variable can be regarded, in the simplest case, as the area between the graph of that function and the x-axis. The Lebesgue integral extends the integral to a larger class of functions. It also extends the domains on which these functions can be defined.
L'Hôpital's rule
L'Hôpital's rule or L'Hospital's rule  uses derivatives to help evaluate limits involving indeterminate forms.  Application (or repeated application) of the rule often converts an indeterminate form to an expression that can be evaluated by substitution, allowing easier evaluation of the limit. The rule is named after the 17th-century French mathematician Guillaume de l'Hôpital. Although the contribution of the rule is often attributed to L'Hôpital, the theorem was first introduced to L'Hôpital in 1694 by the Swiss mathematician Johann Bernoulli.

L'Hôpital's rule states that for functions f and g which are differentiable on an open interval I except possibly at a point c contained in I, if

  
    
      
        
          lim
          
            x
            →
            c
          
        
        f
        (
        x
        )
        =
        
          lim
          
            x
            →
            c
          
        
        g
        (
        x
        )
        =
        0
        
           or 
        
        ±
        ∞
        ,
      
    
    {\displaystyle \lim _{x\to c}f(x)=\lim _{x\to c}g(x)=0{\text{ or }}\pm \infty ,}
  
 
  
    
      
        
          g
          ′
        
        (
        x
        )
        ≠
        0
      
    
    {\displaystyle g'(x)\neq 0}
  
 for all x in I with x ≠ c, and 
  
    
      
        
          lim
          
            x
            →
            c
          
        
        
          
            
              
                f
                ′
              
              (
              x
              )
            
            
              
                g
                ′
              
              (
              x
              )
            
          
        
      
    
    {\displaystyle \lim _{x\to c}{\frac {f'(x)}{g'(x)}}}
  
 exists, then

  
    
      
        
          lim
          
            x
            →
            c
          
        
        
          
            
              f
              (
              x
              )
            
            
              g
              (
              x
              )
            
          
        
        =
        
          lim
          
            x
            →
            c
          
        
        
          
            
              
                f
                ′
              
              (
              x
              )
            
            
              
                g
                ′
              
              (
              x
              )
            
          
        
        .
      
    
    {\displaystyle \lim _{x\to c}{\frac {f(x)}{g(x)}}=\lim _{x\to c}{\frac {f'(x)}{g'(x)}}.}
  

The differentiation of the numerator and denominator often simplifies the quotient or converts it to a limit that can be evaluated directly.
limit comparison test
The limit comparison test allows one to determine the convergence of one series based on the convergence of another.
limit of a function
.
limits of integration
.
linear combination
In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form ax + by, where a and b are constants). The concept of linear combinations is central to linear algebra and related fields of mathematics.
linear equation
A linear equation is an equation relating two or more variables to each other in the form of 
  
    
      
        
          a
          
            1
          
        
        
          x
          
            1
          
        
        +
        ⋯
        +
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        +
        b
        =
        0
        ,
      
    
    {\displaystyle a_{1}x_{1}+\cdots +a_{n}x_{n}+b=0,}
  
 with the highest power of each variable being 1.
linear system
.
list of integrals
.
logarithm
.
logarithmic differentiation
.
lower bound
.


== M ==

mean value theorem
.
monotonic function
.
multiple integral
.
Multiplicative calculus
.
multivariable calculus
.


== N ==

natural logarithm
The natural logarithm of a number is its logarithm to the base of the mathematical constant e, where e is an irrational and transcendental number approximately equal to 2.718281828459. The natural logarithm of x is generally written as ln x, loge x,  or sometimes, if the base e is implicit, simply log x. Parentheses are sometimes added for clarity, giving ln(x), loge(x) or log(x). This is done in particular when the argument to the logarithm is not a single symbol, to prevent ambiguity.
non-Newtonian calculus
.
nonstandard calculus
.
notation for differentiation
.
numerical integration
.


== O ==

one-sided limit
.
ordinary differential equation
.


== P ==

Pappus's centroid theorem
(Also known as  the Guldinus theorem, Pappus–Guldinus theorem or Pappus's theorem) is either of two related theorems dealing with the surface areas and volumes of surfaces and solids of revolution.
parabola
Is a plane curve that is mirror-symmetrical and is approximately U-shaped. It fits several superficially different other mathematical descriptions, which can all be proved to define exactly the same curves.
paraboloid
.
partial derivative
.
partial differential equation
.
partial fraction decomposition
.
particular solution
.
piecewise-defined function
A function defined by multiple sub-functions that apply to certain intervals of the function's domain.
position vector
.
power rule
.
product integral
.
product rule
.
proper fraction
.
proper rational function
.
Pythagorean theorem
.
Pythagorean trigonometric identity
.


== Q ==

quadratic function
In algebra, a quadratic function, a quadratic polynomial, a polynomial of degree 2, or simply a quadratic, is a polynomial function with one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables x, y, and z contains exclusively terms x2, y2, z2, xy, xz, yz, x, y, z, and a constant:

  
    
      
        f
        (
        x
        ,
        y
        ,
        z
        )
        =
        a
        
          x
          
            2
          
        
        +
        b
        
          y
          
            2
          
        
        +
        c
        
          z
          
            2
          
        
        +
        d
        x
        y
        +
        e
        x
        z
        +
        f
        y
        z
        +
        g
        x
        +
        h
        y
        +
        i
        z
        +
        j
        ,
      
    
    {\displaystyle f(x,y,z)=ax^{2}+by^{2}+cz^{2}+dxy+exz+fyz+gx+hy+iz+j,}
  

with at least one of the coefficients a, b, c, d, e, or f of the second-degree terms being non-zero.

A univariate (single-variable) quadratic function has the form

  
    
      
        f
        (
        x
        )
        =
        a
        
          x
          
            2
          
        
        +
        b
        x
        +
        c
        ,
        
        a
        ≠
        0
      
    
    {\displaystyle f(x)=ax^{2}+bx+c,\quad a\neq 0}
  

in the single variable x. The graph of a univariate quadratic function is a parabola whose axis of symmetry is parallel to the y-axis, as shown at right.

If the quadratic function is set equal to zero, then the result is a quadratic equation. The solutions to the univariate equation are called the roots of the univariate function.

The bivariate case in terms of variables x and y has the form

  
    
      
        f
        (
        x
        ,
        y
        )
        =
        a
        
          x
          
            2
          
        
        +
        b
        
          y
          
            2
          
        
        +
        c
        x
        y
        +
        d
        x
        +
        e
        y
        +
        f
        
        
      
    
    {\displaystyle f(x,y)=ax^{2}+by^{2}+cxy+dx+ey+f\,\!}
  

with at least one of a, b, c not equal to zero, and an equation setting this function equal to zero gives rise to a conic section (a circle or other ellipse, a parabola, or a hyperbola).

In general there can be an arbitrarily large number of variables, in which case the resulting surface is called a quadric, but the highest degree term must be of degree 2, such as x2, xy, yz, etc.
quadratic polynomial
.
quotient rule
A formula for finding the derivative of a function that is the ratio of two functions.


== R ==

radian
Is the SI unit for measuring angles, and is the standard unit of angular measure used in many areas of mathematics.  The length of an arc of a unit circle is numerically equal to the measurement in radians of the angle that it subtends; one radian is just under 57.3 degrees (expansion at OEIS: A072097). The unit was formerly an SI supplementary unit, but this category was abolished in 1995 and the radian is now considered an SI derived unit.  Separately, the SI unit of solid angle measurement is the steradian .
ratio test
.
reciprocal function
.
reciprocal rule
.
Riemann integral
.
related rates
.
removable discontinuity
.
Rolle's theorem
.
root test
.


== S ==

scalar
.
secant line
.
second-degree polynomial
.
second derivative
.
second derivative test
.
second-order differential equation
.
series
.
shell integration
.
Simpson's rule
.
sine
.
sine wave
.
slope field
.
squeeze theorem
.
sum rule in differentiation
.
sum rule in integration
.
summation
.
supplementary angle
.
surface area
.
system of linear equations
.


== T ==

table of integrals
.
Taylor series
.
Taylor's theorem
.
tangent
.
third-degree polynomial
.
third derivative
.
toroid
.
total differential
.
trigonometric functions
.
trigonometric identities
.
trigonometric integral
.
trigonometric substitution
.
trigonometry
.
triple integral
.


== U ==

upper bound
.


== V ==

variable
.
vector
.
vector calculus
.


== W ==

washer
.
washer method
.


== See also ==
Outline of calculus
Glossary of areas of mathematics
Glossary of astronomy
Glossary of biology
Glossary of botany
Glossary of chemistry
Glossary of ecology
Glossary of engineering
Glossary of physics
Glossary of probability and statistics


== References ==


=== Works cited ===
Apostol, T (1967), Calculus, Vol. 1 (2nd ed.), Jon Wiley & Sons.
Arbogast, L. F. A. (1800), Du calcul des derivations [On the calculus of derivatives] (in French), Strasbourg: Levrault, pp. xxiii+404.
Butcher, John C. (2003). Numerical Methods for Ordinary Differential Equations. New York: John Wiley & Sons. ISBN 978-0-471-96758-3.
Craik, Alex D. D. (February 2005), ""Prehistory of Faà di Bruno's Formula"", American Mathematical Monthly, 112 (2): 217–234, doi:10.2307/30037410, JSTOR 30037410, MR 2121322, Zbl 1088.01008.
Hairer, Ernst; Nørsett, Syvert Paul; Wanner, Gerhard (1993). Solving ordinary differential equations I: Nonstiff problems. Berlin, New York: Springer-Verlag. ISBN 978-3-540-56670-0.
Johnson, Warren P. (March 2002), ""The Curious History of Faà di Bruno's Formula"" (PDF), American Mathematical Monthly, 109 (3): 217–234, CiteSeerX 10.1.1.109.4135, doi:10.2307/2695352, JSTOR 2695352, MR 1903577, Zbl 1024.01010.


== Notes ==",1219716372,85276,https://en.wikipedia.org/wiki/Glossary_of_calculus,"['Category:All articles with unsourced statements', 'Category:Articles with short description', 'Category:Articles with unsourced statements from June 2016', 'Category:Articles with unsourced statements from March 2017', 'Category:CS1: long volume value', 'Category:CS1 French-language sources (fr)', 'Category:CS1 Hungarian-language sources (hu)', 'Category:CS1 location test', 'Category:Fields of mathematics', 'Category:Glossaries of mathematics', 'Category:Pages using sidebar with the child parameter', 'Category:Short description is different from Wikidata', 'Category:Webarchive template wayback links', 'Category:Wikipedia glossaries using description lists']","['(ε, δ)-definition of limit', '1715 in science', 'Abacus', ""Abel's test"", 'Abraham Robinson', 'Absolute convergence', 'Absolute value', 'Academic Press', 'Addison-Wesley', 'Addison–Wesley', 'Additive inverse', 'Adequality', 'Algebra', 'Algebraic operation', 'Alpha Chiang', 'Alternating series', 'Alternating series test', 'American Mathematical Monthly', ""Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes"", 'Analytic geometry', 'Angle', 'Annulus (mathematics)', 'Antiderivative', 'Antiderivatives', 'Arc length', 'Area', 'Area under a curve', 'Argument of a function', 'Arithmetic operations', 'Arithmetic progression', 'Arithmetico-geometric sequence', 'Arithmetico–geometric sequence', 'Asymptote', 'Augustin-Louis Cauchy', 'Automatic differentiation', 'Axis of revolution', 'Base (exponentiation)', 'Basel problem', 'Bell polynomial', 'Bernoulli number', 'Binomial (polynomial)', 'Binomial coefficient', 'Binomial expansion', 'Binomial series', 'Binomial theorem', 'Bochner integral', 'Bonaventura Cavalieri', 'Bose–Einstein integral', 'Bounded function', 'Bounded interval', 'Bounded set', 'Brook Taylor', 'Brooks/Cole', 'Brooks Cole', 'Bureau International des Poids et Mesures', 'Burkill integral', 'Calculus', 'Calculus of variations', 'Calculus on Euclidean space', 'Carl Benjamin Boyer', 'Cartesian coordinates', 'Cauchy condensation test', ""Cavalieri's principle"", 'Cengage Learning', 'Chain rule', 'Change of variables', 'Circle', 'CiteSeerX (identifier)', 'Classification of discontinuities', 'Coefficient', 'Cofunction', 'Colin Maclaurin', 'Common integrals in quantum field theory', 'Complementary angles', 'Complete Fermi–Dirac integral', 'Complex analysis', 'Complex number', 'Complex variable', 'Compound interest', 'Computer algebra', 'Concave function', 'Concentric circles', 'Conditional convergence', 'Cone (geometry)', 'Conic section', 'Connected set', 'Constant of integration', 'Constructive nonstandard analysis', 'Continuous function', 'Continuously differentiable', 'Contour integral', 'Contour integration', 'Convergence test', 'Convergence tests', 'Convergent series', 'Convex function', 'Convex set', 'Coordinate vector', 'Corollary', 'Cosecant', 'Cosine', 'Cotangent', ""Cours d'Analyse"", ""Cramer's rule"", 'Critical point (mathematics)', 'Criticism of nonstandard analysis', 'Cross section (geometry)', 'Curl (mathematics)', 'Curvature', 'Curve', 'Curve sketching', 'Cusp (singularity)', 'Damped sine wave', 'Daniell integral', 'Darboux integral', 'Definite integral', 'Degree (angle)', 'Degree of a monomial', 'Degree of a polynomial', 'Delta (Greek)', 'Dense set', 'Dependent variable', 'Derivative', 'Derivative test', 'Determinant', 'Differentiable function', 'Differential (infinitesimal)', 'Differential (mathematics)', 'Differential calculus', 'Differential equation', 'Differential form', 'Differential geometry of curves', 'Differential geometry of surfaces', 'Differential of a function', 'Differential operator', 'Differentiation rules', 'Digital geometry', 'Direct comparison test', 'Directional derivative', ""Dirichlet's test"", 'Dirichlet integral', 'Disc integration', 'Discontinuity (mathematics)', 'Discrete set', 'Distance', 'Divergence', 'Divergence theorem', 'Divergent series', 'Doi (identifier)', 'Domain of a function', 'Dot product', 'Double integral', 'Double integrals', 'Dual number', 'E (mathematical constant)', 'Edmund Gunter', 'Elementary Calculus: An Infinitesimal Approach', 'Ellipse', 'Elliptic integral', 'Encyclopedic Dictionary of Mathematics', 'Engineering', 'Epigraph (mathematics)', 'Equation', 'Eric W. Weisstein', 'Essential singularity', 'Euclidean geometry', 'Euclidean space', 'Euler method', 'Euler substitution', 'Euler–Maclaurin formula', 'Explicit and implicit methods', 'Exponential function', 'Exponentiation', 'Expression (mathematics)', 'Exterior derivative', 'Extreme value theorem', 'Factorial', ""Faà di Bruno's formula"", 'Finite difference', 'First derivative test', 'First order differential equation', 'Fluxion', 'For all', 'Formula', 'Fourier series', 'Fractional calculus', 'France', 'Francesco Faà di Bruno', 'Free variables and bound variables', 'Frullani integral', 'Frustum', 'Function (mathematics)', 'Function application', 'Function composition', 'Function of a real variable', 'Function of several variables', 'Fundamental Theorem of Line Integrals', 'Fundamental theorem of calculus', ""Gabriel's horn"", 'Gabriel Cramer', 'Garrett Birkhoff', 'Gaussian integral', 'General Leibniz rule', 'Generality of algebra', 'Generalizations of the derivative', ""Generalized Stokes' theorem"", 'Generalized Stokes theorem', 'Generalized function', 'Geometric calculus', 'Geometric series', 'Geometry', 'George B. Thomas', 'Gilbert Strang', ""Giulio Carlo de' Toschi di Fagnano"", 'Glossary of Arabic toponyms', 'Glossary of Hebrew toponyms', 'Glossary of aerospace engineering', 'Glossary of agriculture', 'Glossary of archaeology', 'Glossary of architecture', 'Glossary of areas of mathematics', 'Glossary of artificial intelligence', 'Glossary of astronomy', 'Glossary of biology', 'Glossary of bird terms', 'Glossary of botanical terms', 'Glossary of cell biology', 'Glossary of cellular and molecular biology (0–L)', 'Glossary of cellular and molecular biology (M–Z)', 'Glossary of chemistry terms', 'Glossary of civil engineering', 'Glossary of clinical research', 'Glossary of computer hardware terms', 'Glossary of computer science', 'Glossary of developmental biology', 'Glossary of ecology', 'Glossary of economics', 'Glossary of electrical and electronics engineering', 'Glossary of engineering', 'Glossary of engineering: A–L', 'Glossary of engineering: M–Z', 'Glossary of entomology terms', 'Glossary of environmental science', 'Glossary of genetics and evolutionary biology', 'Glossary of geography terms (A–M)', 'Glossary of geography terms (N–Z)', 'Glossary of geology', 'Glossary of ichthyology', 'Glossary of machine vision', 'Glossary of mechanical engineering', 'Glossary of medicine', 'Glossary of meteorology', 'Glossary of mycology', 'Glossary of nanotechnology', 'Glossary of physics', 'Glossary of probability and statistics', 'Glossary of psychiatry', 'Glossary of quantum computing', 'Glossary of robotics', 'Glossary of scientific naming', 'Glossary of structural engineering', 'Glossary of virology', 'Glyph', 'Golden ratio', 'Golden spiral', 'Gottfried Leibniz', 'Gottfried Wilhelm Leibniz', 'Gradient', 'Gradient theorem', 'Graph of a function', 'Greatest and least elements', ""Green's theorem"", ""Guillaume de l'Hôpital"", 'Haar measure', 'Harmonic mean', 'Harmonic progression (mathematics)', 'Harmonic series (mathematics)', 'Hellinger integral', 'Helmholtz decomposition', 'Henry Holt and Company', 'Henstock–Kurzweil integral', 'Hessian matrix', 'History of calculus', 'Homeomorphism', 'Homogeneous differential equation', 'Homogeneous function', 'Hyperbola', 'Hyperbolic function', 'Hyperfinite set', 'Hyperinteger', 'Hyperreal number', 'ISBN (identifier)', 'Identity (mathematics)', 'Identity function', 'Image (mathematics)', 'Imaginary number', 'Imaginary unit', 'Implicit differentiation', 'Implicit function', 'Implicit function theorem', 'Improper fraction', 'Improper integral', 'Incomplete Fermi–Dirac integral', 'Increment theorem', 'Indefinite integral', 'Indeterminate form', 'Indicator function', 'Infinite sequence', 'Infinite series', 'Infinitesimal', 'Infinitesimal calculus', 'Infinitesimal strain theory', 'Infinity', 'Inflection point', 'Inner product space', 'Institutionum calculi integralis', 'Integer', 'Integral', 'Integral (mathematics)', 'Integral calculus', 'Integral equation', 'Integral of inverse functions', 'Integral of secant cubed', 'Integral of the secant function', 'Integral symbol', 'Integral test for convergence', 'Integral transform', 'Integrand', 'Integration Bee', 'Integration by partial fractions', 'Integration by parts', 'Integration by reduction formulae', 'Integration by substitution', ""Integration using Euler's formula"", 'Integration using parametric derivatives', 'Integro-differential equation', 'Intermediate value theorem', 'Internal set', 'Internal set theory', 'International System of Units', 'Interval (mathematics)', 'Interval of convergence', 'Inverse function', 'Inverse function rule', 'Inverse function theorem', 'Inverse functions and differentiation', 'Inverse trigonometric functions', 'Irrational number', 'Isaac Newton', 'Itô calculus', 'JSTOR (identifier)', 'Jacobian matrix and determinant', 'James Stewart (mathematician)', 'Joel Hass', 'Johann Bernoulli', 'John C. Butcher', 'John Wiley & Sons', 'Journal de Mathématiques Pures et Appliquées', 'Khinchin integral', 'Kolmogorov integral', ""L'Hôpital's rule"", 'LaTeX', 'Lagrange multiplier', ""Laplace's method"", 'Laplace operator', 'Laplace transform', 'Latin', 'Law of Continuity', 'Law of continuity', 'Lebesgue integration', 'Lebesgue–Stieltjes integral', 'Lebesgue–Stieltjes integration', ""Leibniz's notation"", 'Leibniz integral rule', 'Leibniz notation', 'Leonhard Euler', 'Levi-Civita field', 'Limit (mathematics)', 'Limit comparison test', 'Limit of a function', 'Limit of a sequence', 'Limit of distributions', 'Limits of integration', 'Line (geometry)', 'Line integral', 'Line segment', 'Linear algebra', 'Linear approximation', 'Linear combination', 'Linear differential equation', 'Linear equation', 'Linear function', 'Linear system', 'Linearity', 'Linearity of differentiation', 'List of calculus topics', 'List of integrals', 'List of integrals of exponential functions', 'List of integrals of hyperbolic functions', 'List of integrals of inverse hyperbolic functions', 'List of integrals of inverse trigonometric functions', 'List of integrals of irrational functions', 'List of integrals of logarithmic functions', 'List of integrals of rational functions', 'List of integrals of trigonometric functions', 'List of limits', 'List of trigonometric identities', 'Lists of integrals', 'Local maximum', 'Local minimum', 'Logarithm', 'Logarithmic derivative', 'Logarithmic differentiation', 'Logarithmic spiral', 'Louis François Antoine Arbogast', 'MR (identifier)', 'Maclaurin series', 'Malliavin calculus', 'Manifold', 'MathWorld', 'Mathematical analysis', 'Mathematical constant', 'Mathematical induction', 'Mathematician', 'Mathematics', 'Matrix (mathematics)', 'Matrix calculus', 'Maxima and minima', 'Maximum', 'Maximum and minimum', 'Mean value theorem', 'Method of Fluxions', 'Methods of contour integration', 'Metre', 'Metre per second', 'Microcontinuity', 'Minimum', 'Monad (nonstandard analysis)', 'Monomial', 'Monotonic function', 'Multiple integral', 'Multivariable calculus', 'Natural logarithm', 'Natural number', 'Negative number', ""Newton's method"", ""Newton's notation for differentiation"", 'Non-Newtonian calculus', 'Non-negative', 'Non-standard analysis', 'Nonstandard analysis', 'Nonstandard calculus', 'Notation for differentiation', 'Numerical integration', 'Numerical ordinary differential equations', 'Oikonyms in Western and South Asia', 'On-Line Encyclopedia of Integer Sequences', 'One-sided limit', 'Order of approximation', 'Order of integration (calculus)', 'Ordinary differential equation', 'Outline of calculus', 'Overspill', 'Oxford English Dictionary', ""Pappus's centroid theorem"", 'Parabola', 'Paraboloid', 'Parallel planes', 'Parentheses', 'Partial derivative', 'Partial differential equation', 'Partial fraction decomposition', 'Partial fractions in integration', 'Partial sum', 'Particular solution', 'Paul Erdős', 'Peter Gustav Lejeune Dirichlet', 'Pettis integral', 'Pfeffer integral', 'Phi', 'Piecewise', 'Pierre de Fermat', 'Plane curve', 'Point at infinity', 'Pointwise product', 'Polyhedron', 'Polynomial', 'Polynomial expansion', 'Polynomial function', 'Position (vector)', 'Positive number', 'Power rule', 'Power series', 'Precalculus', 'Prime number', 'Principal part', 'Product (mathematics)', 'Product integral', 'Product rule', 'Projective geometry']"
57143357,Glossary of computer science,"This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.","This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.


== A ==

abstract data type (ADT)
A mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.

abstract method
One with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.

abstraction
1.  In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.
2.  The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.

agent architecture
A blueprint for software agents and intelligent control systems depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.

agent-based model (ABM)
A class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.

aggregate function
In database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a sum, count, or max.

agile software development
An approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.

algorithm
An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. They are ubiquitous in computing technologies.

algorithm design
A method or mathematical process for problem-solving and for engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

American Standard Code for Information Interchange (ASCII)
A character encoding standard for electronic communications. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.

application software
Also simply application or app.
Computer software designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. Common examples of applications include word processors, spreadsheets, accounting applications, web browsers, media players, aeronautical flight simulators, console games, and photo editors. This contrasts with system software, which is mainly involved with managing the computer's most basic running operations, often without direct input from the user. The collective noun application software refers to all applications collectively.

array data structure
Also simply array.
A data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called a one-dimensional array.

artifact
One of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g. use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements, and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.

artificial intelligence (AI)
Also machine intelligence.
Intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of ""intelligent agents"": devices capable of perceiving their environment and taking actions that maximize the chance of successfully achieving their goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".

ASCII
See American Standard Code for Information Interchange.

assertion
In computer programming, a statement that a predicate (Boolean-valued function, i.e. a true–false expression) is always true at that point in code execution. It can help a programmer read the code, help a compiler compile it, or help the program detect its own defects. For the latter, some programs check assertions by actually evaluating the predicate as they run and if it is not in fact true – an assertion failure – the program considers itself to be broken and typically deliberately crashes or throws an assertion failure exception.

associative array
An associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.

Operations associated with this data type allow:

the addition of a pair to the collection
the removal of a pair from the collection
the modification of an existing pair
the lookup of a value associated with a particular key

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.


== B ==

bandwidth
The maximum rate of data transfer across a given path. Bandwidth may be characterized as network bandwidth, data bandwidth, or digital bandwidth.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

benchmark
The act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.

best, worst and average case
Expressions of what the resource usage is at least, at most, and on average, respectively, for a given algorithm. Usually the resource being considered is running time, i.e. time complexity, but it could also be memory or some other resource. Best case is the function which performs the minimum number of steps on input data of n elements; worst case is the function which performs the maximum number of steps on input data of size n; average case is the function which performs an average number of steps on input data of n elements.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary number
In mathematics and digital electronics, a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically 0 (zero) and 1 (one).

binary search algorithm
Also simply binary search, half-interval search, logarithmic search, or binary chop.
A search algorithm that finds the position of a target value within a sorted array.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well.

bioinformatics
An interdisciplinary field that combines biology, computer science, information engineering, mathematics, and statistics to develop methods and software tools for analyzing and interpreting biological data. Bioinformatics is widely used for in silico analyses of biological queries using mathematical and statistical techniques.

bit
A basic unit of information used in computing and digital communications; a portmanteau of binary digit. A binary digit can have one of two possible values, and may be physically represented with a two-state device. These state values are most commonly represented as either a 0or1.

bit rate (R)

Also bitrate.
In telecommunications and computing, the number of bits that are conveyed or processed per unit of time.

blacklist
Also block list.
In computing, a basic access control mechanism that allows through all elements (email addresses, users, passwords, URLs, IP addresses, domain names, file hashes, etc.), except those explicitly mentioned in a list of prohibited elements. Those items on the list are denied access. The opposite is a whitelist, which means only items on the list are allowed through whatever gate is being used while all other elements are blocked. A greylist contains items that are temporarily blocked (or temporarily allowed) until an additional step is performed.

BMP file format
Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap.
A raster graphics image file format used to store bitmap digital images independently of the display device (such as a graphics adapter), used especially on Microsoft Windows and OS/2 operating systems.

Boolean data type
A data type that has one of two possible values (usually denoted true and false), intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid-19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type (see probabilistic logic)—i.e. logic need not always be Boolean.

Boolean expression
An expression used in a programming language that returns a Boolean value when evaluated, that is one of true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.

Boolean algebra
In mathematics and mathematical logic, the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. Contrary to elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and (denoted as ∧), the disjunction or (denoted as ∨), and the negation not (denoted as ¬). It is thus a formalism for describing logical relations in the same way that elementary algebra describes numeric relations.

byte
A unit of digital information that most commonly consists of eight bits, representing a binary number. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.

booting
The procedures implemented in starting up a computer or computer appliance until it can be used. It can be initiated by hardware such as a button press or by a software command. After the power is switched on, the computer is relatively dumb and can read only part of its storage called read-only memory. There, a small program is stored called firmware. It does power-on self-tests and, most importantly, allows access to other types of memory like a hard disk and main memory. The firmware loads bigger programs into the computer's main memory and runs it.


== C ==

callback
Also a call-after function.
Any executable code that is passed as an argument to other code that is expected to ""call back"" (execute) the argument at a given time. This execution may be immediate, as in a synchronous callback, or it might happen at a later time, as in an asynchronous callback.

central processing unit (CPU)
The electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions. The computer industry has used the term ""central processing unit"" at least since the early 1960s. Traditionally, the term ""CPU"" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.

character
A unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.

cipher
Also cypher.
In cryptography, an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure.

class
In object-oriented programming, an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.

class-based programming
Also class-orientation.
A style of object-oriented programming (OOP) in which inheritance occurs via defining ""classes"" of objects, instead of via the objects alone (compare prototype-based programming).

client
A piece of computer hardware or software that accesses a service made available by a server. The server is often (but not always) on another computer system, in which case the client accesses the service by way of a network. The term applies to the role that programs or devices play in the client–server model.

cleanroom software engineering
A software development process intended to produce software with a certifiable level of reliability. The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM. The focus of the cleanroom process is on defect prevention, rather than defect removal.

closure
Also lexical closure or function closure.
A technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment.

cloud computing
Shared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.  Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.

code library
A collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.

coding
Computer programming is the process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

coding theory
The study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

cognitive science
The interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology.

collection
A collection or container is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.  Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type. A collection is a concept applicable to abstract data types, and does not prescribe a specific implementation as a concrete data structure, though often there is a conventional choice (see Container for type theory discussion).

comma-separated values (CSV)
A delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.  Each line of the file is a data record.  Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format.

compiler
A computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower-level language (e.g. assembly language, object code, or machine code) to create an executable program.

computability theory
also known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.

computation
Any type of calculation that includes both arithmetical and non-arithmetical steps and follows a well-defined model, e.g. an algorithm. The study of computation is paramount to the discipline of computer science.

computational biology
Involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, and evolution.  Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids.

computational complexity theory
A subfield of computational science which focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational model
A mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience.
A branch of neuroscience which employs mathematical models, theoretical analysis, and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational physics
Is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.

computational science
Also scientific computing and scientific computation (SC).
An interdisciplinary field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core it involves the development of computer models and simulations to understand complex natural systems.

computational steering
Is the practice of manually intervening with an otherwise autonomous computational process, to change its outcome.

computer
A device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks.

computer architecture
A set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.

computer data storage
Also simply storage or memory.
A technology consisting of computer components and recording media that are used to retain digital data. Data storage is a core function and fundamental component of all modern computer systems.: 15–16 

computer ethics
A part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.

computer graphics
Pictures and films created using computers. Usually, the term refers to computer-generated image data created with the help of specialized graphical hardware and software. It is a vast and recently developed area of computer science.

computer network
Also data network.
A digital telecommunications network which allows nodes to share resources. In computer networks, computing devices exchange data with each other using connections (data links) between nodes. These data links are established over cable media such as wires or optic cables, or wireless media such as Wi-Fi.

computer program
Is a collection of instructions that can be executed by a computer to perform a specific task. 

computer programming
The process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.

computer scientist
A person who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.

computer security
Also cybersecurity or information technology security (IT security).
The protection of computer systems from theft or damage to their hardware, software, or electronic data, as well as from disruption or misdirection of the services they provide.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

computing
Is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes study of algorithmic processes and development of both hardware and software. It has scientific, engineering, mathematical, technological and social aspects. Major computing fields include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.

concatenation
In formal language theory and computer programming, string concatenation  is the operation of joining character strings end-to-end.  For example, the concatenation of ""snow"" and ""ball"" is ""snowball"". In certain formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.

Concurrency
The ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units.

conditional
Also conditional statement, conditional expression, and conditional construct.
A feature of a programming language which performs different computations or actions depending on whether a programmer-specified Boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.

container
Is a class, a data structure, or an abstract data type (ADT) whose instances are collections of other objects. In other words, they store objects in an organized way that follows specific access rules. The size of the container depends on the number of objects (elements) it contains. Underlying (inherited) implementations of various container types may vary in size and complexity, and provide flexibility in choosing the right implementation for any given scenario.

continuation-passing style (CPS)
A style of functional programming in which control is passed explicitly in the form of a continuation. This is contrasted with direct style, which is the usual style of programming. Gerald Jay Sussman and Guy L. Steele, Jr. coined the phrase in AI Memo 349 (1975), which sets out the first version of the Scheme programming language.

control flow
Also flow of control.
The order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.

Creative Commons (CC)
An American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public.

cryptography
Or cryptology,  is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.

CSV
See comma-separated values.

cyberbullying
Also cyberharassment or online bullying.
A form of bullying or harassment using electronic means.

cyberspace
Widespread, interconnected digital technology.


== D ==

daemon
In multitasking computer operating systems, a daemon ( or ) is a computer program that runs as a background process, rather than being under the direct control of an interactive user. Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program. For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

Data

data center
Also data centre.
A dedicated space used to house computer systems and associated components, such as telecommunications and data storage systems. It generally includes redundant or backup components and infrastructure for power supply, data communications connections, environmental controls (e.g. air conditioning and fire suppression) and various security devices.

database
An organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex, they are often developed using formal design and modeling techniques.

data mining
Is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. 

data science
An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a ""concept to unify statistics, data analysis, machine learning and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data structure
A data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.

data type
Also simply type.
An attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support common data types of real, integer, and Boolean. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A type of value from which an expression may take its value.

debugging
The process of finding and resolving defects or problems within a computer program that prevent correct operation of computer software or the system as a whole. Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling.

declaration
In computer programming, a language construct that specifies properties of an identifier: it declares what a word (identifier) ""means"". Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions. Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types, to be specified with a declaration before use, and is used in forward declaration. The term ""declaration"" is frequently contrasted with the term ""definition"", but meaning and usage varies significantly between languages.

digital data
In information theory and information systems, the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.

digital signal processing (DSP)
The use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.  The signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency.

discrete event simulation (DES)
A model of the operation of a system as a discrete sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system. Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.

disk storage
(Also sometimes called drive storage) is a general category of storage mechanisms where data is recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks. A disk drive is a device implementing such a storage mechanism. Notable types are the hard disk drive (HDD) containing a non-removable disk, the  floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives (ODD) and associated optical disc media.

distributed computing
A field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.

divide and conquer algorithm
An algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

DNS
See Domain Name System.

documentation
Written text or illustration that accompanies computer software or is embedded in the source code. It either explains how it operates or how to use it, and may mean different things to people in different roles.

domain
Is the targeted subject area of a computer program. It is a term used in software engineering. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.

Domain Name System (DNS)
A hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or to a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.

double-precision floating-point format
A computer number format. It represents a wide dynamic range of numerical values by using a floating radix point.

download
In computer networks, to receive data from a remote system, typically a server such as a web server, an FTP server, an email server, or other similar systems. This contrasts with uploading, where data is sent to a remote server. A download is a file offered for downloading or that has been downloaded, or the process of receiving such a file.


== E ==

edge device
A device which provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.

encryption
In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often utilized in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes utilize the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.

event
An action or occurrence recognized by software, often originating asynchronously from the external environment, that may be handled by the software. Because an event is an entity which encapsulates the action and the contextual variables triggering the action, the acrostic mnemonic ""Execution Variable Encapsulating Named Trigger"" is often used to clarify the concept.

event-driven programming
A programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input. This is also true of programming for device drivers (e.g. P in USB device driver stacks).

evolutionary computing
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial-and-error problem-solvers with a metaheuristic or stochastic optimization character.

executable
Also executable code, executable file, executable program, or simply executable.
Causes a computer ""to perform indicated tasks according to encoded instructions,"" as opposed to a data file that must be parsed by a program to be meaningful. The exact interpretation depends upon the use - while ""instructions"" is traditionally taken to mean machine code instructions for a physical CPU, in some contexts a file containing bytecode or scripting language instructions may also be considered executable.

executable module

execution
In computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program. Each instruction of a program is a description of a particular 
action which to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed. 

exception handling
The process of responding to the occurrence, during computation, of exceptions – anomalous or exceptional conditions requiring special processing – often disrupting the normal flow of program execution. It is provided by specialized programming language constructs, computer hardware mechanisms like interrupts, or operating system IPC facilities like signals.

Existence detection
An existence check before reading a file can catch and/or prevent a fatal error.

expression
In a programming language, a combination of one or more constants, variables, operators, and functions that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (""to return"", in a stateful environment) another value. This process, as for mathematical expressions, is called evaluation.

external library


== F ==

fault-tolerant computer system
A system designed around the concept of fault tolerance. In essence, they must be able to continue working to a level of satisfaction in the presence of errors or breakdowns.

feasibility study
An investigation which aims to objectively and rationally uncover the strengths and weaknesses of an existing business or proposed venture, opportunities and threats present in the natural environment, the resources required to carry through, and ultimately the prospects for success. In its simplest terms, the two criteria to judge feasibility are cost required and value to be attained.

field
Data that has several parts, known as a record, can be divided into fields. Relational databases arrange data as sets of database records, so called rows. Each record consists of several fields; the fields of all records form the columns.
Examples of fields: name, gender, hair colour. 

filename extension
An identifier specified as a suffix to the name of a computer file. The extension indicates a characteristic of the file contents or its intended use.

filter (software)
A computer program or subroutine to process a stream, producing another stream. While a single filter can be used individually, they are frequently strung together to form a pipeline.

floating point arithmetic
In computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision. For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times. A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:

  
    
      
        
          significand
        
        ×
        
          
            base
          
          
            exponent
          
        
        ,
      
    
    {\displaystyle {\text{significand}}\times {\text{base}}^{\text{exponent}},}
  

where significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.
For example:

  
    
      
        1.2345
        =
        
          
            
              12345
              ⏟
            
          
          
            significand
          
        
        ×
        
          
            
              10
              ⏟
            
          
          
            base
          
        
        
        
        
        
        
        
          
          
            
              
                
                  
                    −
                    4
                  
                  ⏞
                
              
              
                exponent
              
            
          
        
        .
      
    
    {\displaystyle 1.2345=\underbrace {12345} _{\text{significand}}\times \underbrace {10} _{\text{base}}\!\!\!\!\!\!^{\overbrace {-4} ^{\text{exponent}}}.}
  

for loop
Also for-loop. 
A control flow statement for specifying iteration, which allows code to be executed repeatedly. Various keywords are used to specify this statement: descendants of ALGOL use ""for"", while descendants of Fortran use ""do"". There are also other possibilities, e.g. COBOL uses ""PERFORM VARYING"".

formal methods
A set of mathematically based techniques for the specification, development, and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.

formal verification
The act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.

functional programming
A programming paradigm—a style of building the structure and elements of computer programs–that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm in that programming is done with expressions or declarations instead of statements.


== G ==

game theory
The study of mathematical models of strategic interaction between rational decision-makers. It has applications in all fields of social science, as well as in logic and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.

garbage in, garbage out (GIGO)
A term used to describe the concept that flawed or nonsense input data produces nonsense output or ""garbage"". It can also refer to the unforgiving nature of programming, in which a poorly written program might produce nonsensical behavior.

Graphics Interchange Format

gigabyte
A multiple of the unit byte for digital information. The prefix giga means 109 in the International System of Units (SI). Therefore, one gigabyte is 1000000000bytes.  The unit symbol for the gigabyte is GB.

global variable
In computer programming, a variable with global scope, meaning that it is visible (hence accessible) throughout the program, unless shadowed. The set of all global variables is known as the global environment or global state. In compiled languages, global variables are generally static variables, whose extent (lifetime) is the entire runtime of the program, though in interpreted languages (including command-line interpreters), global variables are generally dynamically allocated when declared, since they are not known ahead of time.

graph theory
In mathematics, the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically.


== H ==

handle
In computer programming, a handle is an abstract reference to a resource that is used when application software references blocks of memory or objects that are managed by another system like a database or an operating system.

hard problem
Computational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

hash function
Any function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file.

hash table
In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.

heap
A specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: if P is a parent node of C, then the key (the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. The node at the ""top"" of the heap (with no parents) is called the root node.

heapsort
A comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.

human-computer interaction (HCI)
Researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study.


== I ==

identifier
In computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines,  and packages.

IDE
Integrated development environment.

image processing

imperative programming
A programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.

incremental build model
A method of software development where the product is designed, implemented and tested incrementally (a little more is added each time) until the product is finished. It involves both development and maintenance. The product is defined as finished when it satisfies all of its requirements. This model combines the elements of the waterfall model with the iterative philosophy of prototyping.

information space analysis
A deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.

information visualization

inheritance
In object-oriented programming, the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones (super class or base class) and forming them into a hierarchy of classes.

input/output (I/O)
Also informally io or IO. 
The communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system. Inputs are the signals or data received by the system and outputs are the signals or data sent from it. The term can also be used as part of an action; to ""perform I/O"" is to perform an input or output operation.

insertion sort
A simple sorting algorithm that builds the final sorted array (or list) one item at a time.

instruction cycle
Also fetch–decode–execute cycle or simply fetch-execute cycle.
The cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage.

integer
A datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.

integrated development environment (IDE)
A software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools, and a debugger.

integration testing
(sometimes called integration and testing, abbreviated I&T) is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified functional requirements. It occurs after unit testing and before validation testing. Integration testing takes as its input modules that have been unit tested, groups them in larger aggregates, applies tests defined in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.

intellectual property (IP)
A category of legal property that includes intangible creations of the human intellect. There are many types of intellectual property, and some countries recognize more than others. The most well-known types are copyrights, patents, trademarks, and trade secrets.

intelligent agent
In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.

interface
A shared boundary across which two or more separate components of a computer system exchange information. The exchange can be between software, computer hardware, peripheral devices, humans, and combinations of these. Some computer hardware devices, such as a touchscreen, can both send and receive data through the interface, while others such as a mouse or microphone may only provide an interface to send data to a given system.

internal documentation
Computer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments.  It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code. This contrasts with external documentation, where programmers keep their notes and explanations in a separate document.

internet
The global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies.

internet bot
Also web robot, robot, or simply bot.
A software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone. The largest use of bots is in web spidering (web crawler), in which an automated script fetches, analyzes and files information from web servers at many times the speed of a human.

interpreter
A computer program that directly executes instructions written in a programming or scripting language, without requiring them to have been previously compiled into a machine language program.

invariant
One can encounter invariants that can be relied upon to be true during the execution of a program, or during some portion of it. It is a logical assertion that is always held to be true during a certain phase of execution. For example, a loop invariant is a condition that is true at the beginning and the end of every execution of a loop.

iteration
Is the repetition of a process in order to generate an outcome. The sequence will approach some end point or end value. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration.  In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.


== J ==

Java
A general-purpose programming language that is class-based, object-oriented(although not a pure OO language), and designed to have as few implementation dependencies as possible. It is intended to let application developers ""write once, run anywhere"" (WORA), meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.


== K ==

kernel
The first section of an operating system to load into memory. As the center of the operating system, the kernel needs to be small, efficient, and loaded into a protected area in the memory so that it cannot be overwritten. It may be responsible for such essential tasks as disk drive management, file management, memory management, process management, etc.


== L ==

library (computing)
A collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values, or type specifications.

linear search
Also sequential search.
A method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched.

linked list
A linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence.

linker
 or link editor, is a computer utility program that takes one or more object files generated by a compiler or an assembler and combines them into a single executable file, library file, or another 'object' file.  A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.

list
An abstract data type that represents a countable number of ordered values, where the same value may occur more than once. An instance of a list is a computer representation of the mathematical concept of a finite sequence; the (potentially) infinite analog of a list is a stream.: §3.5  Lists are a basic example of containers, as they contain other values. If the same value occurs multiple times, each occurrence is considered a distinct item.

loader
The part of an operating system that is responsible for loading programs and libraries. It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution. Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running. Once loading is complete, the operating system starts the program by passing control to the loaded program code.

logic error
In computer programming, a bug in a program that causes it to operate incorrectly, but not to terminate abnormally (or crash). A logic error produces unintended or undesired output or other behaviour, although it may not immediately be recognized as such.

logic programming
A type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.


== M ==

machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to perform the task.

machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

mathematical logic
A subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.

matrix
In mathematics, a matrix, (plural matrices), is a rectangular array (see irregular matrix) of numbers, symbols, or expressions, arranged in rows and columns.

memory
Computer data storage, often called storage, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.: 15–16 

merge sort
Also mergesort.
An efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.

method
In object-oriented programming (OOP), a procedure associated with a message and an object. An object consists of data and behavior. The data and behavior comprise an interface, which specifies how the object may be utilized by any of various consumers of the object.

methodology
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management. It is also known as a software development life cycle (SDLC). The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.

modem
Portmanteau of modulator-demodulator.
A hardware device that converts data into a format suitable for a transmission medium so that it can be transmitted from one computer to another (historically along telephone wires). A modem modulates one or more carrier wave signals to encode digital information for transmission and demodulates signals to decode the transmitted information. The goal is to produce a signal that can be transmitted easily and decoded reliably to reproduce the original digital data. Modems can be used with almost any means of transmitting analog signals from light-emitting diodes to radio. A common type of modem is one that turns the digital data of a computer into modulated electrical signal for transmission over telephone lines and demodulated by another modem at the receiver side to recover the digital data.


== N ==

natural language processing (NLP)
A subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.  Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

node
Is a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.

number theory
A branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions.

numerical analysis
The study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).

numerical method
In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm.


== O ==

object
An object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.  In the class-based object-oriented programming paradigm, object refers to a particular instance of a class, where the object can be a combination of variables, functions, and data structures.  In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).

object code
Also object module.
The product of a compiler. In a general sense object code is a sequence of statements or instructions in a computer language, usually a machine code language (i.e., binary) or an intermediate language such as register transfer language (RTL). The term indicates that the code is the goal or result of the compiling process, with some early sources referring to source code as a ""subject program.""

object-oriented analysis and design (OOAD)
A technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.

object-oriented programming (OOP)
A programming paradigm based on the concept of ""objects"", which can contain data, in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of ""this"" or ""self""). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.

operating system (OS)
System software that manages computer hardware, software resources, and provides common services for computer programs.

optical fiber
A flexible, transparent fiber made by drawing glass (silica) or plastic to a diameter slightly thicker than that of a human hair. Optical fibers are used most often as a means to transmit light between the two ends of the fiber and find wide usage in fiber-optic communications, where they permit transmission over longer distances and at higher bandwidths (data rates) than electrical cables. Fibers are used instead of metal wires because signals travel along them with less loss; in addition, fibers are immune to electromagnetic interference, a problem from which metal wires suffer.


== P ==

pair programming
An agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in. The two programmers switch roles frequently.

parallel computing
A type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism.

parameter
Also formal argument.
In computer programming, a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine. These pieces of data are the values of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.

peripheral
Any auxiliary or ancillary device connected to or integrated within a computer system and used to send information to or retrieve information from the computer. An input device sends data or instructions to the computer; an output device provides output from the computer to the user; and an input/output device performs both functions.

pointer
Is an object in many programming languages that stores a memory address. This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page. The actual format and content of a pointer variable is dependent on the underlying computer architecture.

postcondition
In computer programming, a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. Often, postconditions are simply included in the documentation of the affected section of code.

precondition
In computer programming, a condition or predicate that must always be true just prior to the execution of some section of code or before an operation in a formal specification.  If a precondition is violated, the effect of the section of code becomes undefined and thus may or may not carry out its intended work.  Security problems can arise due to incorrect preconditions.

primary storage
(Also known as main memory, internal memory or prime memory), often referred to simply as memory, is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.

primitive data type

priority queue
An abstract data type which is like a regular queue or stack data structure, but where additionally each element has a ""priority"" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.

procedural programming

procedure
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.

program lifecycle phase
Program lifecycle phases are the stages a computer program undergoes, from initial creation to deployment and execution. The phases are edit time, compile time, link time, distribution time, installation time, load time, and run time.

programming language
A formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.

programming language implementation
Is a system for executing computer programs. There are two general approaches to programming language implementation: interpretation and compilation.

programming language theory
(PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.  It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.  It has become a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications.

Prolog
Is a logic programming language associated with artificial intelligence and computational linguistics.  Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.

Python
Is an interpreted, high-level and general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.


== Q ==

quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically.: I-5 

queue
A collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as enqueue, and removal of entities from the front terminal position, known as dequeue.

quicksort
Also partition-exchange sort.
An efficient sorting algorithm which serves as a systematic method for placing the elements of a random access file or an array in order.


== R ==

R programming language
R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radix
Also base.
In digital numeral systems, the number of unique digits, including the digit zero, used to represent numbers in a positional numeral system. For example, in the decimal/denary system (the most common system in use today) the radix (base number) is ten, because it uses the ten digits from 0 through 9, and all other numbers are uniquely specified by positional combinations of these ten base digits; in the binary system that is the standard in computing, the radix is two, because it uses only two digits, 0 and 1, to uniquely specify each number.

record
A record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called ""rows"".

recursion
Occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

reference
Is a value that enables a program to indirectly access a particular datum, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference.

reference counting
A programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others. In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.

relational database
Is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.
A software system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.

reliability engineering
A sub-discipline of systems engineering that emphasizes dependability in the lifecycle management of a product. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.

regression testing
(rarely non-regression testing) is re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change. If not, that would be called a regression. Changes that may require regression testing include bug fixes, software enhancements, configuration changes, and even substitution of electronic components. As regression test suites tend to grow with each found defect, test automation is frequently involved. Sometimes a change impact analysis is performed to determine an appropriate subset of tests (non-regression analysis).

requirements analysis
In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.

robotics
An interdisciplinary branch of engineering and science that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics involves design, construction, operation, and use of robots, as well as computer systems for their perception, control, sensory feedback, and information processing. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe.

round-off error
Also rounding error.
The difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of quantization error. When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors. Computation errors, also called numerical errors, include both truncation errors and roundoff errors.

router
A networking device that forwards data packets between computer networks. Routers perform the traffic directing functions on the Internet.  Data sent through the internet, such as a web page or email, is in the form of data packets.   A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.

routing table
In computer networking a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes. The routing table contains information about the topology of the network immediately around it.

run time
Runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.

run time error
A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language.


== S ==

search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

secondary storage
Also known as external memory or auxiliary storage, differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.

selection sort
Is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sequence
In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order does matter.  Like a set, it contains members (also called elements, or terms).  The number of elements (possibly infinite) is called the length of the sequence.  Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order does matter.  Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n).

The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention.  When a symbol is used to denote a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence F is generally denoted Fn.

For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.  This sequence differs from (A, R, M, Y).  Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.  Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).  In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.  The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.

serializability
In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

serialization
Is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment). When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.

This process of serializing an object is also called marshalling an object in some situations.[1][2] The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).

service level agreement
(SLA), is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user. The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.

set
Is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.

singleton variable
A variable that is referenced only once. May be used as a dummy argument in a function call, or when its address is assigned to another variable which subsequently accesses its allocated storage. Singleton variables sometimes occur because a mistake has been made – such as assigning a value to a variable and forgetting to use it later, or mistyping one instance of the variable name. Some compilers and lint-like tools flag occurrences of singleton variables.

soft computing

software
Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.

software agent
Is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such ""action on behalf of"" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot
executing on a phone (e.g. Siri)  or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).

software construction
Is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.

software deployment
Is all of the activities that make a software system available for use.

software design
Is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.""

software development
Is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.

software development process
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.  Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.

software engineering
Is the systematic application of engineering approaches to the development of software. Software engineering is a computing discipline.

software maintenance
In software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.

software prototyping
Is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.  A prototype typically simulates only a few aspects of, and may be completely different from, the final product. 

software requirements specification
(SRS), is a description of a software system to be  developed. The software requirements specification lays out functional and non-functional requirements, and it may include a set of use cases that describe user interactions that the software must provide to the user for perfect interaction.

software testing
Is an investigation conducted to provide stakeholders with information about the quality of the software product or service under test. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include the process of executing a program or application with the intent of finding software bugs (errors or other defects), and verifying that the software product is fit for use. 

sorting algorithm
Is an algorithm that puts elements of a list in a certain order. The most frequently used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output. More formally, the output of any sorting algorithm must satisfy two conditions:

The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (a reordering, yet retaining all of the original elements) of the input.

Further, the input data is often stored in an array, which allows random access, rather than a list, which only allows sequential access; though many algorithms can be applied to either type of data after suitable modification.

source code
In computing, source code is any collection of code, with or without comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code that can be executed by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.

spiral model
Is a risk-driven software development process model. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.

stack
Is an abstract data type that serves as a collection of elements, with two main principal operations:
push, which adds an element to the collection, and
pop, which removes the most recently added element that was not yet removed.
The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack. The name ""stack"" for this type of structure comes from the analogy to a set of physical items stacked on top of each other. This structure makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.

state
In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statement
In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out. A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).

storage
Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.: 15–16 

stream
Is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.

string
In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.

structured storage
A NoSQL (originally referring to ""non-SQL"" or ""non-relational"") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ""NoSQL"" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications.  NoSQL systems are also sometimes called ""Not only SQL"" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.

subroutine
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.

symbolic computation
In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.

syntax
The syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language. This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.

syntax error
Is an error in the syntax of a sequence of characters or tokens that is intended to be written in compile-time. A program will not compile until all syntax errors are corrected. For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds. There is some disagreement as to just what errors are ""syntax errors"". For example, some would say that the use of an uninitialized variable's value in Java code is a syntax error, but many others would disagree and would classify this as a (static) semantic error.

system console
The system console, computer console, root console, operator's console, or simply console is the text entry and display device for system administration messages, particularly those from the BIOS or boot loader, the kernel, from the init system and from the system logger. It is a physical device consisting of a keyboard and a screen, and traditionally is a text terminal, but may also be a graphical terminal. System consoles are generalized to computer terminals, which are abstracted respectively by virtual consoles and terminal emulators. Today communication with system consoles is generally done abstractly, via the standard streams (stdin, stdout, and stderr), but there may be system-specific interfaces, for example those used by the system kernel.


== T ==

technical documentation
In engineering, any type of documentation that describes handling, functionality, and architecture of a technical product or a product under development or use. The intended recipient for product technical documentation is both the (proficient) end user as well as the administrator/service or maintenance technician. In contrast to a mere ""cookbook"" manual, technical documentation aims at providing enough information for a user to understand inner and outer dependencies of the product at hand.

third-generation programming language
A third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations. Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.

top-down and bottom-up design

tree
A widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.

type theory
In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every ""term"" has a ""type"" and operations are restricted to terms of a certain type.


== U ==

upload
In computer networks, to send data to a remote system such as a server or another client so that the remote system can store a copy. Contrast download.

Uniform Resource Locator (URL)
Colloquially web address.
A reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it. A URL is a specific type of Uniform Resource Identifier (URI), although many people use the two terms interchangeably. URLs occur most commonly to reference web pages (http), but are also used for file transfer (ftp), email (mailto), database access (JDBC), and many other applications.

user
Is a person who utilizes a computer or network service. Users of computer systems and software products generally lack the technical expertise required to fully understand how they work. Power users use advanced features of programs, though they are not necessarily capable of computer programming and system administration.

user agent
Software (a software agent) that acts on behalf of a user, such as a web browser that ""retrieves, renders and facilitates end user interaction with Web content"". An email reader is a mail user agent.

user interface (UI)
The space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.

user interface design
Also user interface engineering.
The design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing usability and the user experience. The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).


== V ==

variable
In computer programming, a variable, or scalar, is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may therefore change during the course of program execution.

virtual machine (VM)
An emulation of a computer system. Virtual machines are based on computer architectures and attempt to provide the same functionality as a physical computer. Their implementations may involve specialized hardware, software, or a combination of both.

V-Model
A software development process that may be considered an extension of the waterfall model, and is an example of the more general V-model. Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape. The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing. The horizontal and vertical axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain abstraction uppermost), respectively.


== W ==

waterfall model
A breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks.  The approach is typical for certain areas of engineering design. In software development, it tends to be among the less iterative and flexible approaches, as progress flows in largely one direction (""downwards"" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.

Waveform Audio File Format
Also WAVE or WAV due to its filename extension.
An audio file format standard, developed by Microsoft and IBM, for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in ""chunks"", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Microsoft Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.

web crawler
Also spider, spiderbot, or simply crawler.
An Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).

Wi-Fi
A family of wireless networking technologies, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access. Wi‑Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing.


== X ==

XHTML
Abbreviaton of eXtensible HyperText Markup Language.
Part of the family of XML markup languages. It mirrors or extends versions of the widely used HyperText Markup Language (HTML), the language in which web pages are formulated.


== See also ==
Outline of computer science


== References ==


=== Works cited ===


== Notes ==",1227624846,116380,https://en.wikipedia.org/wiki/Glossary_of_computer_science,"['Category:All articles lacking reliable references', 'Category:All articles with dead external links', 'Category:All articles with unsourced statements', 'Category:Articles lacking reliable references from June 2018', 'Category:Articles with dead external links from July 2023', 'Category:Articles with dead external links from March 2022', 'Category:Articles with permanently dead external links', 'Category:Articles with short description', 'Category:Articles with unsourced statements from February 2018', 'Category:CS1 German-language sources (de)', 'Category:Computer science', 'Category:Computers', 'Category:Glossaries of computers', 'Category:Glossaries of science', 'Category:Short description is different from Wikidata', 'Category:Webarchive template archiveis links', 'Category:Webarchive template wayback links', 'Category:Wikipedia glossaries using description lists']","['0 (number)', '1 (number)', '8SVX', 'ACM Computing Classification System', 'AI Memo', 'ALGOL', 'ASCII', 'Abstract and concrete', 'Abstract data type', 'Abstract machine', 'Abstract method', 'Abstraction (computer science)', 'Abstraction (software engineering)', 'Academic journal', 'Access control', 'Accounting software', 'Addison-Wesley Publishing Company, Inc.', 'Address space', 'Adversary (cryptography)', 'Agency (philosophy)', 'Agent-based model', 'Agent architecture', 'Aggregate function', 'Agile software development', 'Alan Mackworth', 'Alfred Aho', 'Algebra', 'Algorithm', 'Algorithm design', 'Algorithm design paradigm', 'Algorithm efficiency', 'Algorithmic efficiency', 'Alphabet', 'American Standard Code for Information Interchange', 'Amiga', 'Amsterdam', 'Analysis', 'Analysis of algorithms', 'Answer set programming', 'Apple Macintosh', 'Application developer', 'Application monitoring', 'Application programming interface', 'Application security', 'Application software', 'Applied mathematics', 'Approximation', 'ArXiv (identifier)', 'Archive.today', 'Arithmetic function', 'Arithmetic underflow', 'Array data structure', 'Array data type', 'Arthur Samuel (computer scientist)', 'Artifact (software development)', 'Artificial Intelligence: A Modern Approach', 'Artificial intelligence', 'Ashlee Vance', 'Asimo', 'Aspect-oriented programming', 'Assembler (computing)', 'Assembly language', 'Assertion (computing)', 'Assertion (software development)', 'Association for Computing Machinery', 'Associative array', 'Asymptotic analysis', 'Asynchronous I/O', 'Attenuation', 'Attribute–value pair', 'Audio Interchange File Format', 'Audio file format', 'Authentication', 'Authority', 'Automata theory', 'Automated planning and scheduling', 'Automated reasoning', 'Automaton', 'Autonomous', 'Auxiliary memory', 'Availability', 'BASIC', 'BIOS', 'BMP file format', 'Background process', 'Backward compatibility', 'Ballistic Research Laboratory', 'Bandwidth (computing)', 'Bayesian programming', 'Behavioral sciences', 'Benchmark (computing)', 'Best, worst and average case', 'Bibcode (identifier)', 'Big O notation', 'Big data', 'Binary file', 'Binary number', 'Binary search algorithm', 'Binary tree', 'Biochemistry', 'Bioengineering', 'Bioinformatics', 'Biological computing', 'Biological evolution', 'Biology', 'Biophysics', 'Bit', 'Bit-level parallelism', 'Bit rate', 'Bite', 'Bitmap', 'Bitstream format', 'Blacklist (computing)', 'Block (data storage)', 'Blueprint', 'Boolean-valued function', 'Boolean algebra', 'Boolean data type', 'Boolean expression', 'Boolean value', 'Boot loader', 'Booting', 'Bot (disambiguation)', 'Botnet', 'Bounds checking', 'Branch of science', 'Branch predication', 'Build automation', 'Bull Gamma 60', 'Bullying', 'Byte', 'Bytecode', 'C. Michael Sperberg-McQueen', 'CI/CD', 'COBOL', 'C (programming language)', 'Calculation', 'Callback (computer programming)', 'Cambridge University Press', 'Canonicalization', 'Capability Maturity Model Integration', 'Carrier wave', 'Catena (unit)', 'Cem Kaner', 'Central processing unit', 'Change impact analysis', 'Character (computing)', 'Character encoding', 'Character string (computer science)', 'Charles E. Leiserson', 'Chatbots', 'Chemistry', 'Child node', 'Cipher', 'Ciphertext', 'CiteSeerX (identifier)', 'Class-based programming', 'Class (computer programming)', 'Class (computer science)', 'Class diagram', 'Cleanroom software engineering', 'Client (computing)', 'Client–server model', 'Clifford Stein', 'Clock synchronization', 'Closure (computer programming)', 'Cloud computing', 'Code', 'Code generation (compiler)', 'Code readability', 'Code reuse', 'Code review', 'Coding theory', 'Cognition', 'Cognitive architecture', 'Cognitive neuroscience', 'Cognitive science', 'Collection (abstract data type)', 'Column (database)', 'Comma', 'Comma-separated values', 'Command-line interpreter', 'Comment (computer programming)', 'Communication protocol', 'Communication protocols', 'Communication science', 'Communications of the ACM', 'Communications protocol', 'Comparison sort', 'Compatibility layer', 'Compatibility mode', 'Compile time', 'Compiler', 'Compiler construction', 'Compilers: Principles, Techniques, and Tools', 'Complex systems', 'Complexity', 'Component-based software engineering', 'Computability theory', 'Computable function', 'Computation', 'Computational biology', 'Computational chemistry', 'Computational complexity', 'Computational complexity theory', 'Computational engineering', 'Computational geometry', 'Computational linguistics', 'Computational mathematics', 'Computational model', 'Computational neuroscience', 'Computational physics', 'Computational problem', 'Computational process', 'Computational resource', 'Computational science', 'Computational social science', 'Computational sociology', 'Computational steering', 'Computer', 'Computer Weekly', 'Computer accessibility', 'Computer animation', 'Computer appliance', 'Computer architecture', 'Computer compatibility', 'Computer data storage', 'Computer engineering', 'Computer ethics', 'Computer file', 'Computer graphics', 'Computer hardware', 'Computer language', 'Computer memory', 'Computer mouse', 'Computer multitasking', 'Computer network', 'Computer networking', 'Computer number format', 'Computer operator', 'Computer platform', 'Computer program', 'Computer programmer', 'Computer programming', 'Computer programs', 'Computer science', 'Computer scientist', 'Computer security', 'Computer simulation', 'Computer system', 'Computer systems', 'Computer terminal', 'Computer vision', 'Computing', 'Computing device', 'Computing platform', 'Concatenation', 'Concatenation theory', 'Concept', 'Concurrency (computer science)', 'Concurrency control', 'Concurrent computing', 'Conditional (computer programming)', 'Confidentiality', 'Configuration file', 'Console game', 'Constant (computer programming)', 'Constraint (mathematics)', 'Constructor (object-oriented programming)', 'Container (abstract data type)', 'Container (type theory)', 'Continual improvement process', 'Continuation', 'Continuation-passing style', 'Continuous or discrete variable', 'Continuous variable', 'Control flow', 'Control theory', 'Control variable (programming)', 'Conveyor belt', 'Copyright', 'Crash (computing)', 'Creative Commons', 'Creative Commons license', 'Credit card chip', 'Cross-functional team', 'Cross-validation (statistics)', 'Cryptography', 'Customer', 'Cyberbullying', 'Cybersecurity', 'Cyberspace', 'Cyberwarfare', 'Daemon (computing)', 'Daniel Connolly (computer scientist)', 'Data', 'Data (computer science)', 'Data (computing)', 'Data analysis', 'Data buffer', 'Data center', 'Data compression', 'Data element', 'Data integrity', 'Data link', 'Data management', 'Data mining', 'Data model', 'Data modeling', 'Data packet', 'Data parallelism', 'Data pre-processing', 'Data processing', 'Data retrieval', 'Data science', 'Data set', 'Data storage', 'Data storage device', 'Data structure', 'Data table', 'Data transmission', 'Data type', 'Data visualization', 'Database', 'Database management', 'Database record', 'Database system', 'Database transaction', 'Database transaction schedule', 'Datalog', 'Datum', 'David Gries', 'David Poole (researcher)', 'Debugger', 'Debugging', 'Decision-making', 'Decision support system', 'Declaration (computer programming)', 'Declarative programming', 'Decryption', 'Deductive reasoning', 'Deliverable', 'Demodulation', 'Denial-of-service attack', 'Dependability', 'Dereference operator', 'Design', 'DevOps', 'Developmental neuroscience', 'Device driver', 'Dickinson College', 'Dictionary of Algorithms and Data Structures', 'Digital art', 'Digital currencies', 'Digital data', 'Digital electronics', 'Digital image', 'Digital information', 'Digital library', 'Digital marketing', 'Digital media', 'Digital processing', 'Digital signal', 'Digital signal processing', 'Digital signal processor', 'Direct style', 'Directory service', 'Discrete event simulation', 'Discrete mathematics', 'Discrete time', 'Disk storage', 'Display device', 'Distributed artificial intelligence', 'Distributed computing', 'Divide and conquer algorithm', 'Division by zero', 'Document management system', 'Documentation', 'Doi (identifier)', 'Domain-specific language', 'Domain (software engineering)', 'Domain Name System', 'Domain name', 'Donald Knuth', 'Double-precision floating-point format', 'Douglas Comer', 'Download', 'Drawing (manufacturing)', 'Dynamic programming', 'E-commerce', 'E. F. Codd', 'Economies of scale', 'Edge device', 'Edmund Landau', 'Educational technology', 'Edward N. Zalta', 'Effective descriptive set theory', 'Election Assistance Commission', 'Electrical engineering', 'Electrical signal', 'Electromagnetic interference', 'Electronic commerce', 'Electronic component', 'Electronic design automation', 'Electronic engineering', 'Electronic publishing', 'Electronic voting', 'Element (mathematics)', 'Elementary algebra', 'Email', 'Embedded system', 'Emergence', 'Empirical software engineering', 'Empty set', 'Encryption', 'Encyclopædia Britannica', 'End user', 'Engineering', 'Engineering design', 'Enterprise architecture', 'Enterprise information system', 'Enterprise software', 'Enterprise unified process', 'Entity–relationship model', 'Environment (biophysical)', 'Equation', 'Ergonomics', 'Eric S. Raymond', 'Error analysis (mathematics)', 'Error detection and correction', 'Essential systems analysis', 'Even and odd numbers', 'Event-driven programming', 'Event (computing)', 'Event handler', 'Evolution', 'Evolutionary computing', 'Evolutionary programming', 'Exception handling', 'Executable', 'Executable UML', 'Executable code', 'Execution (computers)', 'Execution (computing)', 'Existence detection', 'Experimental software engineering', 'Exponentiation', 'Expression (computer science)', 'Expression (mathematics)', 'Extreme programming', 'FIFO (computing and electronics)', 'False discovery rate', 'Fault-tolerant computer system', 'Fault tolerance', 'Feasibility study', 'Feasible region', 'Fiber', 'Fiber-optic communication', 'Fibonacci sequence', 'Field (computer science)', 'Fifth-generation programming language', 'File Transfer Protocol', 'File format', 'Filename', 'Filename extension', 'Filter (software)', 'Finitary relation', 'Finite set', 'Firmware', 'First-class function', 'First-generation programming language', 'First-order logic', 'Flight simulator', 'Floating point arithmetic', 'Floating point number', 'Floppy disk', 'For loop', 'Formal language', 'Formal logic', 'Formal methods', 'Formal semantics of programming languages', 'Formal specification', 'Formal system', 'Formal verification', 'Fortran', 'Forward compatibility', 'Forward declaration', 'Foundations of mathematics', 'Fourth-generation programming language', 'Fragile base class', 'Fred B. Schneider', 'Frederick Phillips Brooks, Jr.', 'Free On-line Dictionary of Computing', 'Free software', 'Function (computer science)', 'Function (mathematics)', 'Function call', 'Function model', 'Function pointer', 'Function signature']"
50336055,Glossary of artificial intelligence,"This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.","This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.


== A ==

abductive logic programming (ALP)
A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates.

abductive reasoning
Also abduction.
A form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it. abductive inference, or retroduction

abstract data type
A mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations.

abstraction
The process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest

accelerating change
A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.

action language
A language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world. Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning.

action model learning
An area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.

action selection
A way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, ""the action selection problem"" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment.

activation function
In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.

adaptive algorithm
An algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion.

adaptive neuro fuzzy inference system (ANFIS)
Also adaptive network-based fuzzy inference system.
A kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm.

admissible heuristic
In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.

affective computing
Also artificial emotional intelligence or emotion AI.
The study and development of systems and devices that can recognize, interpret, process, and simulate human affects. Affective computing is an interdisciplinary field spanning computer science, psychology, and cognitive science.

agent architecture
A blueprint for software agents and intelligent control systems, depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.

AI accelerator
A class of microprocessor or computer system designed as hardware acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning.

AI-complete
In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm.

algorithm
An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

algorithmic probability
In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s.

AlphaGo
A computer program that plays the board game Go. It was developed by Alphabet Inc.'s Google DeepMind in London. AlphaGo has several versions including AlphaGo Zero, AlphaGo Master, AlphaGo Lee, etc. In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player without handicaps on a full-sized 19×19 board.

ambient intelligence (AmI)
Electronic environments that are sensitive and responsive to the presence of people.

analysis of algorithms
The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity).

analytics
The discovery, interpretation, and communication of meaningful patterns in data.

answer set programming (ASP)
A form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming. In ASP, search problems are reduced to computing stable models, and answer set solvers—programs for generating stable models—are used to perform search.

anytime algorithm
An algorithm that can return a valid solution to a problem even if it is interrupted before it ends.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer. An API may be for a web-based system, operating system, database system, computer hardware, or software library.

approximate string matching
Also fuzzy string searching.
The technique of finding strings that match a pattern approximately (rather than exactly). The problem of approximate string matching is typically divided into two sub-problems: finding approximate substring matches inside a given string and finding dictionary strings that match the pattern approximately.

approximation error
The discrepancy between an exact value and some approximation to it.

argumentation framework
Also argumentation system.
A way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.  There exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.

artificial general intelligence (AGI)

artificial immune system (AIS)
A class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.

artificial intelligence (AI)
Also machine intelligence.
Any intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".

Artificial Intelligence Markup Language
An XML dialect for creating natural language software agents.

artificial neural network (ANN)
Also connectionist system.
Artificial neural networks (ANNs), also shortened to neural networks (NNs) or neural nets, are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.

Association for the Advancement of Artificial Intelligence (AAAI)
An international, nonprofit, scientific society devoted to promote research in, and responsible use of, artificial intelligence. AAAI also aims to increase public understanding of artificial intelligence (AI), improve the teaching and training of AI practitioners, and provide guidance for research planners and funders concerning the importance and potential of current AI developments and future directions.

asymptotic computational complexity
In computational complexity theory, asymptotic computational complexity is the usage of asymptotic analysis for the estimation of computational complexity of algorithms and computational problems, commonly associated with the usage of the big O notation.

attention mechanism
Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates ""soft"" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recursive neural networks). ""Soft"" weights can change during each runtime, in contrast to ""hard"" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards. Multiple attention heads are used in transformer-based large language models.

attributional calculus
A logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, an inductive learning process whose results are in forms natural to people.

augmented reality (AR)

An interactive experience of a real-world environment where the objects that reside in the real-world are ""augmented"" by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory, and olfactory.

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated machine learning (AutoML)
 A field of machine learning which aims to automatically configure a machine learning system to maximize its performance (e.g, classification accuracy).

automated planning and scheduling
Also simply AI planning.
A branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.

autonomic computing (AC)
The self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth.

autonomous car
Also self-driving car, robot car, and driverless car.
A vehicle that is capable of sensing its environment and moving with little or no human input.

autonomous robot
A robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering.


== B ==

backpropagation
A method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. Backpropagation is shorthand for ""the backward propagation of errors"", since an error is computed at the output and distributed backwards throughout the network's layers. It is commonly used to train deep neural networks, a term referring to neural networks with more than one hidden layer.

backpropagation through time (BPTT)
A gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.

backward chaining
Also backward reasoning.
An inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.

bag-of-words model
A simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.

bag-of-words model in computer vision
In computer vision, the bag-of-words model (BoW model) can be applied to image classification, by treating image features as words.  In document classification, a bag of words is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary. In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.

batch normalization
A technique for improving the performance and stability of artificial neural networks. It is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance. Batch normalization was introduced in a 2015 paper. It is used to normalize the input layer by adjusting and scaling the activations.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

bees algorithm
A population-based search algorithm which was developed by Pham, Ghanbarzadeh and et al. in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.

behavior informatics (BI)
The informatics of behaviors so as to obtain behavior intelligence and behavior insights.

behavior tree (BT)
A mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error-prone and very popular in the game developer community. BTs have shown to generalize several other control architectures.

belief–desire–intention software model (BDI)
A software model developed for programming intelligent agents. Superficially characterized by the implementation of an agent's beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming. In essence, it provides a mechanism for separating the activity of selecting a plan (from a plan library or an external planner application) from the execution of currently active plans. Consequently, BDI agents are able to balance the time spent on deliberating about plans (choosing what to do) and executing those plans (doing it). A third activity, creating the plans in the first place (planning), is not within the scope of the model, and is left to the system designer and programmer.

bias–variance tradeoff
In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

Big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.  It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well.

blackboard system
An artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the ""blackboard"", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution.  Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state.  In this way, the specialists work together to solve the problem.

Boltzmann machine
Also stochastic Hopfield network with hidden units.
A type of stochastic recurrent neural network and Markov random field.  Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield networks.

Boolean satisfiability problem
Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT.
The problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. On the other hand, if no such assignment exists, the function expressed by the formula is FALSE for all possible variable assignments and the formula is unsatisfiable. For example, the formula ""a AND NOT b"" is satisfiable because one can find the values a = TRUE and b = FALSE, which make (a AND NOT b) = TRUE. In contrast, ""a AND NOT a"" is unsatisfiable.

brain technology
Also self-learning know-how system.
A technology that employs the latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the ROBOY project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as ""know-how maps"".

branching factor
In computing, tree data structures, and game theory, the number of children at each node, the outdegree. If this value is not uniform, an average branching factor can be calculated.

brute-force search
Also exhaustive search or generate and test.
A very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.


== C ==

capsule neural network (CapsNet)
A machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization.

case-based reasoning (CBR)
Broadly construed, the process of solving new problems based on the solutions of similar past problems.

chatbot
Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity.
A computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.

cloud robotics
A field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent ""brain"" in the cloud. The ""brain"" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support, etc.

cluster analysis
Also clustering.
The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.

Cobweb
An incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University. COBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.

cognitive architecture
The Institute of Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together – in conjunction with knowledge and skills embodied within the architecture – to yield intelligent behavior in a diversity of complex environments.""

cognitive computing
In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus.

cognitive science
The interdisciplinary scientific study of the mind and its processes.

combinatorial optimization
In Operations Research, applied mathematics and theoretical computer science, combinatorial optimization  is a topic that consists of finding an optimal object from a finite set of objects.

committee machine
A type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response.  The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare ensembles of classifiers.

commonsense knowledge
In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as ""Lemons are sour"", that all humans are expected to know.  The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy.

commonsense reasoning
A branch of artificial intelligence concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems.

computational complexity theory
Focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational creativity
Also artificial creativity, mechanical creativity, creative computing, or creative computation.
A multidisciplinary endeavour that includes the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.

computational cybernetics
The integration of cybernetics and computational intelligence techniques.

computational humor
A branch of computational linguistics and artificial intelligence which uses computers in humor research.

computational intelligence (CI)
Usually refers to the ability of a computer to learn a specific task from data or experimental observation.

computational learning theory
In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.

computational linguistics
An interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.

computational mathematics
The mathematical research in areas of science where computing plays an essential role.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience. 
A branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational number theory
Also algorithmic number theory.
The study of algorithms for performing number theoretic computations.

computational problem
In theoretical computer science, a computational problem is a mathematical object representing a collection of questions that computers might be able to solve.

computational statistics
Also statistical computing.
The interface between statistics and computer science.

computer-automated design (CAutoD)
Design automation usually refers to electronic design automation, or Design Automation which is a Product Configurator. Extending Computer-Aided Design (CAD), automated design and computer-automated design are concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification and optimization, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems. More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically inspired machine learning, including heuristic search techniques such as evolutionary computation, and swarm intelligence algorithms.

computer audition (CA)
See machine listening.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

concept drift
In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.

connectionism
An approach in the fields of cognitive science, that hopes to explain mental phenomena using artificial neural networks.

consistent heuristic
In the study of path-finding problems in artificial intelligence, a heuristic function is said to be consistent, or monotone, if its estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor.

constrained conditional model (CCM)
A machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints.

constraint logic programming
A form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction. A constraint logic program is a logic program that contains constraints in the body of clauses. An example of a clause including a constraint is A(X,Y) :- X+Y>0, B(X), C(Y). In this clause, X+Y>0 is a constraint; A(X,Y), B(X), and C(Y) are literals as in regular logic programming. This clause states one condition under which the statement A(X,Y) holds: X+Y is greater than zero and both B(X) and C(Y) are true.

constraint programming
A programming paradigm wherein relations between variables are stated in the form of constraints. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found.

constructed language
Also conlang. 
A language whose phonology, grammar, and vocabulary are consciously devised, instead of having developed naturally. Constructed languages may also be referred to as artificial, planned, or invented languages.

control theory
In control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability.

convolutional neural network
In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.

crossover
Also recombination.
In genetic algorithms and evolutionary computation, a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and analogous to the crossover that happens during sexual reproduction in biological organisms. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population.


== D ==

Darkforest
A computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.

Dartmouth workshop
The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many (though not all) to be the seminal event for artificial intelligence as a field.

data augmentation
Data augmentation in data analysis are techniques used to increase the amount of data. It helps reduce overfitting when training a machine learning.

data fusion
The process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.

data integration
The process of combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes. It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

data mining
The process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.

data science
An interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a ""concept to unify statistics, data analysis, machine learning and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data set
Also dataset.
A collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set.  Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows.

data warehouse (DW or DWH)
Also enterprise data warehouse (EDW).
A system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place

Datalog
A declarative logic programming language that syntactically is a subset of Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.

decision boundary
In the case of backpropagation-based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has. If it has no hidden layers, then it can only learn linear problems. If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary.

decision support system (DSS)
Aan information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.

decision theory
Also theory of choice.
The study of the reasoning underlying an agent's choices. Decision theory can be broken into two branches: normative decision theory, which gives advice on how to make the best decisions given a set of uncertain beliefs and a set of values, and descriptive decision theory which analyzes how existing, possibly irrational agents actually make decisions.

decision tree learning
Uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning.

declarative programming
A programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.

deductive classifier
A type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values.

Deep Blue
was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls.

deep learning
Also deep structured learning or hierarchical learning.
Part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised, or unsupervised.

DeepMind Technologies
A British artificial intelligence company founded in September 2010, currently owned by Alphabet Inc. The company is based in London, with research centres in Canada, France, and the United States. Acquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. The company made headlines in 2016 after its AlphaGo program beat human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing Go, chess, and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.

default logic
A non-monotonic logic proposed by Raymond Reiter to formalize reasoning with default assumptions.

description logic (DL)
A family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors.

developmental robotics (DevRob)
Also epigenetic robotics.
A scientific field which aims at studying the developmental mechanisms, architectures, and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines.

diagnosis
Concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on observations, which provide information on the current behaviour.

dialogue system
Also conversational agent (CA).
A computer system intended to converse with a human with a coherent structure. Dialogue systems have employed text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.

diffusion model
In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable models. They are Markov chains trained using variational inference. The goal of diffusion models is to learn the latent structure of a dataset by modeling the way in which data points diffuse through the latent space. In computer vision, this means that a neural network is trained to denoise images blurred with Gaussian noise by learning to reverse the diffusion process. It mainly consists of three major components: the forward process, the reverse process, and the sampling procedure. Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.

dimensionality reduction
Also dimension reduction. 
The process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.

discrete system
Any system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models. A computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.

distributed artificial intelligence (DAI)
Also decentralized artificial intelligence.
A subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems.

dynamic epistemic logic (DEL)
A logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur.


== E ==

eager learning
A learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system.

Ebert test
A test which gauges whether a computer-based synthesized voice can tell a joke with sufficient skill to cause people to laugh. It was proposed by film critic Roger Ebert at the 2011 TED conference as a challenge to software developers to have a computerized voice master the inflections, delivery, timing, and intonations of a speaking human. The test is similar to the Turing test proposed by Alan Turing in 1950 as a way to gauge a computer's ability to exhibit intelligent behavior by generating performance indistinguishable from a human being.

echo state network (ESN)
A recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.

embodied agent
Also interface agent.
An intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment.

embodied cognitive science
An interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.

error-driven learning
A sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning.

ensemble averaging
In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model.

epoch (machine learning)
In machine learning, particularly in the creation of artificial neural networks, an epoch is training the model for one cycle through the full training dataset. Small models are typically trained for as many epochs as it takes to reach the best performance on the validation dataset. The largest models may train for only one epoch.

ethics of artificial intelligence
The part of the ethics of technology specific to artificial intelligence.

evolutionary algorithm (EA)
A subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.

evolutionary computation
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.

evolving classification function (ECF)
Evolving classifier functions or evolving classifiers are used for classifying and clustering in the field of machine learning and artificial intelligence, typically employed for data stream mining tasks in dynamic and changing environments.

existential risk
The hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe.

expert system
A computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.


== F ==

fast-and-frugal trees
A type of classification tree. Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category.

feature extraction
In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations.

feature learning
In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.

feature selection
In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.

federated learning
A type of machine learning that allows for training on multiple devices with decentralized data, thus helping preserve the privacy of individual users and their data.

first-order logic
Also known as first-order predicate calculus and predicate logic.
A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form ""there exists X such that X is Socrates and X is a man"" and there exists is a quantifier while X is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations.

fluent
A condition that can change over time. In logical approaches to reasoning about actions, fluents can be represented in first-order logic by predicates having an argument that depends on time.

formal language
A set of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.

forward chaining
Also forward reasoning. 
One of the two main methods of reasoning when using an inference engine and can be described logically as repeated application of modus ponens. Forward chaining is a popular implementation strategy for expert systems, businesses and production rule systems. The opposite of forward chaining is backward chaining.  Forward chaining starts with the available data and uses inference rules to extract more data (from an end user, for example) until a goal is reached. An inference engine using forward chaining searches the inference rules until it finds one where the antecedent (If clause) is known to be true. When such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data.

frame
An artificial intelligence data structure used to divide knowledge into substructures by representing ""stereotyped situations"". Frames are the primary data structure used in artificial intelligence frame language.

frame language
A technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

frame problem
The problem of finding adequate collections of axioms for a viable description of a robot environment.

friendly artificial intelligence
Also friendly AI or FAI.
A hypothetical artificial general intelligence (AGI) that would have a positive effect on humanity. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.

futures studies
The study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them.

fuzzy control system
A control system based on fuzzy logic—a mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively).

fuzzy logic
A simple form for the many-valued logic, in which the truth values of variables may have any degree of ""Truthfulness"" that can be represented by any real number in the range between 0 (as in Completely False) and 1 (as in Completely True) inclusive. Consequently, It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. In contrast to Boolean logic, where the truth values of variables may have the integer values 0 or 1 only.

fuzzy rule
A rule used within fuzzy logic systems to infer an output based on input variables.

fuzzy set
In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition — an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1]. Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1. In fuzzy set theory, classical bivalent sets are usually called crisp sets. The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics.


== G ==

game theory
The study of mathematical models of strategic interaction between rational decision-makers.

general game playing (GGP)
General game playing is the design of artificial intelligence programs to be able to run and play more than one game successfully.

generative adversarial network (GAN)
A class of machine learning systems. Two neural networks contest with each other in a zero-sum game framework.

generative artificial intelligence
Generative artificial intelligence is artificial intelligence capable of generating text, images, or other media in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics, typically using Transformer-based deep neural networks.

genetic algorithm (GA)
A metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.

genetic operator
An operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful.

generative pretrained transformer (GPT)
A large language model based on the transformer architecture that generates text. It is first pretrained to predict the next token in texts (a token is typically a word, subword, or punctuation). After their pretraining, GPT models can generate human-like text by repeatedly predicting the token that they would expect to follow. GPT models are usually also fine-tuned, for example with RLHF to reduce hallucinations or harmful behaviour, or to format the ouput in a conversationnal format.

glowworm swarm optimization
A swarm intelligence optimization algorithm based on the behaviour of glowworms (also known as fireflies or lightning bugs).

graph (abstract data type)
In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from mathematics; specifically, the field of graph theory.

graph (discrete mathematics)
In mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense ""related"". The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called an arc or line).

graph database (GDB)
A database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data. A key concept of the system is the graph (or edge or relationship), which directly relates data items in the store a collection of nodes of data and edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly, and in many cases retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships within a graph database is fast because they are perpetually stored within the database itself. Relationships can be intuitively visualized using graph databases, making it useful for heavily inter-connected data.

graph theory
The study of graphs, which are mathematical structures used to model pairwise relations between objects.

graph traversal
Also graph search.
The process of visiting (checking and/or updating) each vertex in a graph. Such traversals are classified by the order in which the vertices are visited. Tree traversal is a special case of graph traversal.


== H ==

halting problem

heuristic
A technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.  A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.

hidden layer
An internal layer of neurons in an artificial neural network, not dedicated to input or output.

hidden unit
A neuron in a hidden layer in an artificial neural network.

hyper-heuristic
A heuristic search method that seeks to automate the process of selecting, combining, generating, or adapting several simpler heuristics (or components of such heuristics) to efficiently solve computational search problems, often by the incorporation of machine learning techniques. One of the motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just one problem.


== I ==

IEEE Computational Intelligence Society
A professional society of the Institute of Electrical and Electronics Engineers (IEEE) focussing on ""the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained"".

incremental learning
A method of machine learning, in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.

inference engine
A component of the system that applies logical rules to the knowledge base to deduce new information.

information integration (II)
The merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in data mining and consolidation of data from unstructured or semi-structured resources. Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content. Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.

Information Processing Language (IPL)
A programming language that includes features intended to help with programs that perform simple problem solving actions such as lists, dynamic memory allocation, data types, recursion, functions as arguments, generators, and cooperative multitasking.  IPL invented the concept of list processing, albeit in an assembly-language style.

intelligence amplification (IA)
Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence.
The effective use of information technology in augmenting human intelligence.

intelligence explosion
A possible outcome of humanity building artificial general intelligence (AGI). AGI would be capable of recursive self-improvement leading to rapid emergence of ASI (artificial superintelligence), the limits of which are unknown, at the time of the technological singularity.

intelligent agent (IA)
An autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex.

intelligent control
A class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms.

intelligent personal assistant
Also virtual assistant or personal digital assistant.
A software agent that can perform tasks or services for an individual based on verbal commands. Sometimes the term ""chatbot"" is used to refer to virtual assistants generally or specifically accessed by online chat (or in some cases online chat programs that are exclusively for entertainment purposes).  Some virtual assistants are able to interpret human speech and respond via synthesized voices. Users can ask their assistants questions, control home automation devices and media playback via voice, and manage other basic tasks such as email, to-do lists, and calendars with verbal commands.

interpretation
An assignment of meaning to the symbols of a formal language. Many formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called formal semantics.

intrinsic motivation
An intelligent agent is intrinsically motivated to act if the information content alone, of the experience resulting from the action, is the motivating factor. Information content in this context is measured in the information theory sense as quantifying uncertainty. A typical intrinsic motivation is to search for unusual (surprising) situations, in contrast to a typical extrinsic motivation such as the search for food. Intrinsically motivated artificial agents display behaviours akin to exploration and curiosity.

issue tree
Also logic tree.
A graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.: 47   Issue trees are useful in problem solving to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.


== J ==

junction tree algorithm
Also Clique Tree.
A method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The graph is called a tree because it branches into different sections of data; nodes of variables are the branches.


== K ==

kernel method
In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets.

KL-ONE
A well-known knowledge representation system in the tradition of semantic networks and frames; that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.

knowledge acquisition
The process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies.

knowledge-based system (KBS)
A computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and refers to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge. Thus, a knowledge-based system has two distinguishing features: a knowledge base and an inference engine.

knowledge engineering (KE)
All technical, scientific, and social aspects involved in building, maintaining, and using knowledge-based systems.

knowledge extraction
The creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.

knowledge Interchange Format (KIF)
A computer language designed to enable systems to share and re-use information from knowledge-based systems. KIF is similar to frame languages such as KL-ONE and LOOM but unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems. The designers of KIF likened it to PostScript. PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents. In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc.

knowledge representation and reasoning (KR² or KR&R)
The field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets. Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.


== L ==

language model
A probabilistic model that manipulates natural language.

large language model (LLM)
A language model with a large number of parameters (typically at least a billion) that are adjusted during training. Due to its size, it requires a lot of data and computing capability to train. Large language models are usually based on the transformer architecture.

lazy learning
In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.

Lisp (programming language) (LISP)
A family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.

logic programming
A type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.

long short-term memory (LSTM)
An artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a ""general purpose computer"" (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video).


== M ==

machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry.  Machine vision is a term encompassing a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science.  It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

Markov chain
A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

Markov decision process (MDP)
A discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning.

mathematical optimization
Also mathematical programming.
In mathematics, computer science, and operations research, the selection of a best element (with regard to some criterion) from some set of available alternatives.

machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead.

machine listening
Also computer audition (CA).
A general field of study of algorithms and systems for audio understanding by machine.

machine perception
The capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.

mechanism design
A field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally. Because it starts at the end of the game, then goes backwards, it is also called reverse game theory. It has broad applications, from economics and politics (markets, auctions, voting procedures) to networked-systems (internet interdomain routing, sponsored search auctions).

mechatronics
Also mechatronic engineering.
A multidisciplinary branch of engineering that focuses on the engineering of both electrical and mechanical systems, and also includes a combination of robotics, electronics, computer, telecommunications, systems, control, and product engineering.

metabolic network reconstruction and simulation
Allows for an in-depth insight into the molecular mechanisms of a particular organism. In particular, these models correlate the genome with molecular physiology.

metaheuristic
In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a set of solutions which is too large to be completely sampled.

model checking
In computer science, model checking or property checking is, for a given model of a system, exhaustively and automatically checking whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of finite-state systems.

modus ponens
In propositional logic, modus ponens is a rule of inference. It can be summarized as ""P implies Q and P is asserted to be true, therefore Q must be true.""

modus tollens
In propositional logic, modus tollens is a valid argument form and a rule of inference.  It is an application of the general truth that if a statement is true, then so is its contrapositive.  The inference rule modus tollens asserts that the inference from P implies Q to the negation of Q implies the negation of P is valid.

Monte Carlo tree search
In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes.

multi-agent system (MAS)
Also self-organized system.
A computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.

multi-swarm optimization
A variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm. The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms. The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist.

mutation
A genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to a better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.

Mycin
An early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patient's body weight – the name derived from the antibiotics themselves, as many antibiotics have the suffix ""-mycin"". The MYCIN system was also used for the diagnosis of blood clotting diseases.


== N ==

naive Bayes classifier
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

naive semantics
An approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications. In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas.

name binding
In programming languages, name binding is the association of entities (data and/or code) with identifiers. An identifier bound to an object is said to reference that object. Machine languages have no built-in notion of identifiers, but name-object bindings as a service and notation for the programmer is implemented by programming languages. Binding is intimately connected with scoping, as scope determines which names bind to which objects – at which locations in the program code (lexically) and in which one of the possible execution paths (temporally).  Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence. In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to; such occurrences are called applied occurrences.

named-entity recognition (NER)
Also entity identification, entity chunking, and entity extraction.
A subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.

named graph
A key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata.  Named graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large.

natural language generation (NLG)
A software process that transforms structured data into plain-English content.  It can be used to produce long-form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out loud by a text-to-speech system.

natural language processing (NLP)
A subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

natural language programming
An ontology-assisted way of programming in terms of natural-language sentences, e.g. English.

network motif
All networks, including biological networks, social networks, technological networks (e.g., computer networks and electrical circuits) and more, can be represented as graphs, which include a wide variety of subgraphs. One important local property of networks are so-called network motifs, which are defined as recurrent and statistically significant sub-graphs or patterns.

neural machine translation (NMT)
An approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

neural network
Also artificial neural network.
A neural network can refer to either a neural circuit of biological neurons (sometimes also called a biological neural network), or a network of artificial neurons or nodes in the case of an artificial neural network. Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.

neural Turing machine (NTM)
A recurrent neural network model. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.

neuro-fuzzy
Combinations of artificial neural networks and fuzzy logic.

neurocybernetics
Also brain–computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain–machine interface (BMI).
A direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.

neuromorphic engineering
Also neuromorphic computing.
A concept describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, and transistors.

node
A basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.

nondeterministic algorithm
An algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm.

nouvelle AI
Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the ""real world"", instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.

NP
In computational complexity theory, NP (nondeterministic polynomial time) is a complexity class used to classify decision problems.  NP is the set of decision problems for which the problem instances, where the answer is ""yes"", have proofs verifiable in polynomial time.

NP-completeness
In computational complexity theory, a problem is NP-complete when it can be solved by a restricted class of brute force search algorithms and it can be used to simulate any other problem with a similar algorithm. More precisely, each input to the problem should be associated with a set of solutions of polynomial length, whose validity can be tested quickly (in polynomial time), such that the output for any input is ""yes"" if the solution set is non-empty and ""no"" if it is empty.

NP-hardness
Also non-deterministic polynomial-time hardness.
In computational complexity theory, the defining property of a class of problems that are, informally, ""at least as hard as the hardest problems in NP"". A simple example of an NP-hard problem is the subset sum problem.


== O ==

Occam's razor
Also Ockham's razor or Ocham's razor.
The problem-solving principle that states that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions; the principle is not meant to filter out hypotheses that make different predictions. The idea is attributed to the English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.

offline learning

online machine learning
A method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time.

ontology learning
Also ontology extraction, ontology generation, or ontology acquisition.
The automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval.

OpenAI
The for-profit corporation OpenAI LP, whose parent organization is the non-profit organization OpenAI Inc that conducts research in the field of artificial intelligence (AI) with the stated aim to promote and develop friendly AI in such a way as to benefit humanity as a whole.

OpenCog
A project that aims to build an open-source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system.

Open Mind Common Sense
An artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web.

open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.


== P ==

Parameter#Artificial_Intelligence

partial order reduction
A technique for reducing the size of the state-space to be searched by a model checking or automated planning and scheduling algorithm. It exploits the commutativity of concurrently executed transitions, which result in the same state when executed in different orders.

partially observable Markov decision process (POMDP)
A generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.

particle swarm optimization (PSO)
A computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.

pathfinding
Also pathing.
The plotting, by a computer application, of the shortest route between two points. It is a more practical variant on solving mazes. This field of research is based heavily on Dijkstra's algorithm for finding a shortest path on a weighted graph.

pattern recognition
Concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

predicate logic
Also first-order logic, predicate logic, and first-order predicate calculus.
A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form ""there exists x such that x is Socrates and x is a man"" and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.

predictive analytics
A variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.

principal component analysis (PCA)
A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.

principle of rationality
Also rationality principle.
A principle coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. According to Popper's rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis.

probabilistic programming (PP)
A programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general-purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty.  Programming languages used for probabilistic programming are referred to as ""Probabilistic programming languages"" (PPLs).

production system

programming language
A formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.

Prolog
A logic programming language associated with artificial intelligence and computational linguistics.  Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.

propositional calculus
Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic.
A branch of logic which deals with propositions (which can be true or false) and argument flow. Compound propositions are formed by connecting propositions by logical connectives. The propositions without logical connectives are called atomic propositions. Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. However, all the machinery of propositional logic is included in first-order logic and higher-order logics. In this sense, propositional logic is the foundation of first-order logic and higher-order logic.

Python
An interpreted, high-level, general-purpose programming language created by Guido van Rossum and first released in 1991. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.


== Q ==

qualification problem
In philosophy and artificial intelligence (especially knowledge-based systems), the qualification problem is concerned with the impossibility of listing all of the preconditions required for a real-world action to have its intended effect. It might be posed as how to deal with the things that prevent me from achieving my intended result. It is strongly connected to, and opposite the ramification side of, the frame problem.

quantifier
In logic, quantification specifies the quantity of specimens in the domain of discourse that satisfy an open formula. The two most common quantifiers mean ""for all"" and ""there exists"". For example, in arithmetic, quantifiers allow one to say that the natural numbers go on forever, by writing that for all n (where n is a natural number), there is another number (say, the successor of n) which is one bigger than n.

quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically.: I-5 

query language
Query languages or data query languages (DQLs) are computer languages used to make queries in databases and information systems.  Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.


== R ==

R programming language
A programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radial basis function network
In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.

random forest
Also random decision forest.
An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.

reasoning system
In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.

recurrent neural network (RNN)
A class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.

region connection calculus

reinforcement learning (RL)
An area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.  It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).

reinforcement learning from human feedback (RLHF)
A technique that involve training a ""reward model"" to predict how humans rate the quality of generated content, and then training a generative AI model to satisfy this reward model via reinforcement learning. It can be used for example to make the generative AI model more truthful or less harmful.

reservoir computing
A framework for computation that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir and the dynamics of the reservoir map the input to a higher dimension. Then a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The main benefit is that training is performed only at the readout stage and the reservoir is fixed. Liquid-state machines and echo state networks are two major types of reservoir computing.

Resource Description Framework (RDF)
A family of World Wide Web Consortium (W3C) specifications originally designed as a metadata data model. It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications.

restricted Boltzmann machine (RBM)
A generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.

Rete algorithm
A pattern matching algorithm for implementing rule-based systems. The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base. It is used to determine which of the system's rules should fire based on its data store, its facts.

robotics
An interdisciplinary branch of science and engineering that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.

rule-based system
In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research.  Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets.  Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type.


== S ==

satisfiability
In mathematical logic, satisfiability and validity are elementary concepts of semantics. A formula is satisfiable if it is possible to find an interpretation (model) that makes the formula true.  A formula is valid if all interpretations make the formula true. The opposites of these concepts are unsatisfiability and invalidity, that is, a formula is unsatisfiable if none of the interpretations make the formula true, and invalid if some such interpretation makes the formula false. These four concepts are related to each other in a manner exactly analogous to Aristotle's square of opposition.

search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

selection
The stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).

self-management
The process by which computer systems manage their own operation without human intervention.

semantic network
Also frame network.
A knowledge base that represents semantic relations between concepts in a network. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts, mapping or connecting semantic fields.

semantic reasoner
Also reasoning engine, rules engine, or simply reasoner.
A piece of software able to infer logical consequences from a set of asserted facts or axioms. The notion of a semantic reasoner generalizes that of an inference engine, by providing a richer set of mechanisms to work with. The inference rules are commonly specified by means of an ontology language, and often a description logic language. Many reasoners use first-order predicate logic to perform reasoning; inference commonly proceeds by forward chaining and backward chaining.

semantic query
Allows for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide-open questions through pattern matching and digital reasoning.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sensor fusion
The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually.

separation logic
An extension of Hoare logic, a way of reasoning about programs. The assertion language of separation logic is a special case of the logic of bunched implications (BI).

similarity learning
An area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.

simulated annealing (SA)
A probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem.

situated approach
In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI ""from the bottom-up"" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.

situation calculus
A logic formalism designed for representing and reasoning about dynamical domains.

Selective Linear Definite clause resolution
Also simply SLD resolution.
The basic inference rule used in logic programming. It is a refinement of resolution, which is both sound and refutation complete for Horn clauses.

software
A collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media.

software engineering
The application of engineering to the development of software in a systematic method.

spatial-temporal reasoning
An area of artificial intelligence which draws from the fields of computer science, cognitive science, and cognitive psychology. The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind. The applied goal—on the computing side—involves developing high-level control systems of automata for navigating and understanding time and space.

SPARQL
An RDF query language—that is, a semantic query language for databases—able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.

speech recognition
An interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.

spiking neural network (SNN)
An artificial neural network that more closely mimics a natural neural network. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their Operating Model.

state
In information technology and computer science, a program is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statistical classification
In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of pattern recognition.

statistical relational learning (SRL)
A subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming.

stochastic optimization (SO)
Any optimization method that generates and uses random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.

stochastic semantic analysis
An approach used in computer science as a semantic component of natural language understanding. Stochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach.

Stanford Research Institute Problem Solver (STRIPS)

subject-matter expert

superintelligence
A hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. Superintelligence may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act within the physical world. A superintelligence may or may not be created by an intelligence explosion and be associated with a technological singularity.

supervised learning
The machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).

support-vector machines
In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.

swarm intelligence (SI)
The collective behavior of decentralized, self-organized systems, either natural or artificial. The expression was introduced in the context of cellular robotic systems.

symbolic artificial intelligence
The term for the collection of all methods in artificial intelligence research that are based on high-level ""symbolic"" (human-readable) representations of problems, logic, and search.

synthetic intelligence (SI)
An alternative term for artificial intelligence which emphasizes that the intelligence of machines need not be an imitation or in any way artificial; it can be a genuine form of intelligence.

systems neuroscience
A subdiscipline of neuroscience and systems biology that studies the structure and function of neural circuits and systems.  It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks.


== T ==

technological singularity
Also simply the singularity.
A hypothetical point in the future when technological growth becomes uncontrollable and irreversible, resulting in unfathomable changes to human civilization.

temporal difference learning
A class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.

tensor network theory
A theory of brain function (particularly that of the cerebellum) that provides a mathematical model of the transformation of sensory space-time coordinates into motor coordinates and vice versa by cerebellar neuronal networks. The theory was developed as a geometrization of brain function (especially of the central nervous system) using tensors.

TensorFlow
A free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.

theoretical computer science (TCS)
A subset of general computer science and mathematics that focuses on more mathematical topics of computing and includes the theory of computation.

theory of computation
In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory, which are linked by the question: ""What are the fundamental capabilities and limitations of computers?"".

Thompson sampling
A heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.

time complexity
The computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.

transformer
A type of deep learning architecture that exploits a multi-head attention mechanism. Transformers address some of the limitations of LSTMs, and became widely used in natural language processing, although it can also process other types of data such as images in the case of vision transformers.

transhumanism
Abbreviated H+ or h+.
An international philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology.

transition system
In theoretical computer science, a transition system is a concept used in the study of computation. It is used to describe the potential behavior of discrete systems. It consists of states and transitions between states, which may be labeled with labels chosen from a set; the same label may appear on more than one transition. If the label set is a singleton, the system is essentially unlabeled, and a simpler definition that omits the labels is possible.

tree traversal
Also tree search.
A form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited.

true quantified Boolean formula
In computational complexity theory, the language TQBF is a formal language consisting of the true quantified Boolean formulas.  A (fully) quantified Boolean formula is a formula in quantified propositional logic where every variable is quantified (or bound), using either existential or universal quantifiers, at the beginning of the sentence. Such a formula is equivalent to either true or false (since there are no free variables). If such a formula evaluates to true, then that formula is in the language TQBF. It is also known as QSAT (Quantified SAT).

Turing machine

Turing test
A test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human, developed by Alan Turing in 1950. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test results do not depend on the machine's ability to give correct answers to questions, only how closely its answers resemble those a human would give.

type system
In programming languages, a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions, or modules. These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. ""string"", ""array of float"", ""function returning boolean""). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.


== U ==

unsupervised learning
A type of self-organized Hebbian learning that helps find previously unknown patterns in data set without pre-existing labels. It is also known as self-organization and allows modeling probability densities of given inputs. It is one of the main three categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning has also been described and is a hybridization of supervised and unsupervised techniques.


== V ==

vision processing unit (VPU)
A type of microprocessor designed to accelerate machine vision tasks.Value-alignment complete – Analogous to an AI-complete problem, a value-alignment complete problem is a problem where the AI control problem needs to be fully solved to solve it.


== W ==

Watson
A question-answering computer system capable of answering questions posed in natural language, developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's first CEO, industrialist Thomas J. Watson.

weak AI
Also narrow AI.
Artificial intelligence that is focused on one narrow task.

World Wide Web Consortium (W3C)
The main international standards organization for the World Wide Web (abbreviated WWW or W3).


== See also ==
Artificial intelligence


== References ==


=== Works cited ===


== Notes ==",1228928983,123747,https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence,"['Category:All articles with dead external links', 'Category:All articles with unsourced statements', 'Category:Articles with dead external links from July 2022', 'Category:Articles with dead external links from September 2023', 'Category:Articles with permanently dead external links', 'Category:Articles with short description', 'Category:Articles with unsourced statements from January 2019', 'Category:CS1: long volume value', 'Category:CS1 German-language sources (de)', 'Category:CS1 errors: periodical ignored', 'Category:Glossaries of science', 'Category:Glossaries of technology', 'Category:Machine learning', 'Category:Short description is different from Wikidata', 'Category:Use dmy dates from September 2017', 'Category:Webarchive template wayback links', 'Category:Wikipedia glossaries using description lists']","['3D optical data storage', 'ACM Computing Classification System', 'AI-complete', 'AI accelerator (computer hardware)', 'AI boom', 'AI control problem', 'AI safety', 'AI takeover', 'AI winter', 'Abductive logic programming', 'Abductive reasoning', 'Abstract data type', 'Abstract machine', 'Abstraction (computer science)', 'Abstraction (software engineering)', 'Academic journal', 'Accelerating change', 'Accuracy and precision', 'Action language', 'Action model learning', 'Action selection', 'Activation function', 'Adaptable robotics', 'Adaptive algorithm', 'Adaptive neuro fuzzy inference system', 'Admissible heuristic', 'Adversarial machine learning', 'Advice Taker', 'Aerobot', 'Affect (psychology)', 'Affective computing', 'Agent (economics)', 'Agent architecture', 'Agile software development', 'Agricultural robot', 'Alan Cobham', 'Alan Mackworth', 'Alan Turing', 'AlexNet', 'Alex Graves (computer scientist)', 'Algebraic data type', 'Algorithm', 'Algorithm design', 'Algorithmic efficiency', 'Algorithmic information theory', 'Algorithmic paradigm', 'Algorithmic probability', 'AlphaFold', 'AlphaGo', 'AlphaGo Lee', 'AlphaGo Master', 'AlphaGo Zero', 'AlphaGo versus Lee Sedol', 'AlphaZero', 'Alphabet (computer science)', 'Alphabet Inc.', 'Amazon Robotics', 'Ambient intelligence', 'Amplitude', 'Analog circuit', 'Analog signal', 'Analysis of algorithms', 'Analytics', 'Andrew Ng', 'Andrew W. Moore', 'Android (robot)', 'Animatronics', 'Answer set programming', 'Ant colony optimization', 'Antal van den Bosch', 'Antecedent (logic)', 'Anthropic', 'Anthropomorphic', 'Antibiotic', 'Anybots', 'Anytime algorithm', 'Application programming interface', 'Application security', 'Application software', 'Applications of artificial intelligence', 'Applied mathematics', 'Approximate string matching', 'Approximation error', 'ArXiv', 'ArXiv (identifier)', 'Argument of a function', 'Argumentation framework', 'Aristotle', 'Articulated robot', 'Artificial Intelligence: A Modern Approach', 'Artificial Intelligence Markup Language', 'Artificial development', 'Artificial general intelligence', 'Artificial immune system', 'Artificial intelligence', 'Artificial intelligence, situated approach', 'Artificial intelligence (video games)', 'Artificial intelligence art', 'Artificial intelligence in government', 'Artificial intelligence in healthcare', 'Artificial intelligence in industry', 'Artificial intelligence in mental health', 'Artificial intelligence systems integration', 'Artificial life', 'Artificial neural network', 'Artificial neural networks', 'Artificial neuron', 'Asexual reproduction', 'Ashlee Vance', 'Aspect-oriented programming', 'Assembly language', 'Association for the Advancement of Artificial Intelligence', 'Asymptotic analysis', 'Asymptotic computational complexity', 'Atomtronics', 'Attention', 'Attention (machine learning)', 'Attention mechanism', 'Attribute (computing)', 'Attributional calculus', 'Audio-Animatronics', 'Augmented reality', 'Auto-GPT', 'Autoencoder', 'Automata theory', 'Automated machine learning', 'Automated planning', 'Automated planning and scheduling', 'Automated reasoning', 'Automated restaurant', 'Automated retail', 'Automated theorem prover', 'Automated theorem proving', 'Automatic differentiation', 'Automation', 'Automaton', 'Automotive engineering', 'Autonomic computing', 'Autonomous', 'Autonomous car', 'Autonomous robot', 'Autonomous underwater vehicle', 'Autonomy', 'Autoregressive model', 'Auxiliary memory', 'Axiom', 'BBC News', 'BEAM robotics', 'BERT (language model)', 'BLOOM (language model)', 'Backpropagation', 'Backpropagation through time', 'Backward chaining', 'Backward compatibility', 'Bacteremia', 'Bacterial Colony Optimization', 'Bag-of-words model', 'Bag-of-words model in computer vision', 'Bag of words', 'Barbara Hayes-Roth', 'Bard (chatbot)', 'Barrett Technology', 'Batch normalization', ""Bayes' theorem"", 'Bayesian network', 'Bayesian probability', 'Bayesian programming', 'Bees algorithm', 'Behavior-based robotics', 'Behavior informatics', 'Behavior tree (artificial intelligence, robotics and control)', 'Belief propagation', 'Belief–desire–intention software model', 'Ben Taskar', 'Benjamin C. Pierce', 'Benjamin Schrauwen', 'Bias of an estimator', 'Bias–variance tradeoff', 'Bibcode (identifier)', 'Big O notation', 'Big data', 'Binary relation', 'Binary tree', 'Bioethics', 'Bioinformatics', 'Biological evolution', 'Biological neural network', 'Biorobotics', 'Blackboard design pattern', 'Blackboard system', 'Blueprint', 'Board game', 'Boltzmann machine', 'Boolean algebra', 'Boolean logic', 'Boolean satisfiability problem', 'Bootstrapping (statistics)', 'Boston Dynamics', 'Bound variable', 'Brain', 'Brain technology', 'Branches of science', 'Branching factor', 'Brute-force search', 'Brute force search', 'Bug (computer programming)', 'Bulletin of Symbolic Logic', 'Business rules engine', 'CI/CD', 'CMA-ES', 'Canada', 'Candidate solution', 'Capability Maturity Model Integration', 'Capsule neural network', 'Carbon nanotube field-effect transistor', 'Case-based reasoning', 'Categorical data', 'Catholic theology', 'Cellular evolutionary algorithm', 'Central nervous system', 'Cerebellum', 'ChatGPT', 'Chatbot', 'Chemistry', 'Chess', 'Child node', 'Chinchilla AI', 'Chinese room', 'Chipless RFID', 'Chromosomal crossover', 'Chromosome (genetic algorithm)', 'CiteSeerX (identifier)', 'Civil engineering', 'Class hierarchy', 'Classification in machine learning', 'Classification tree', 'Classifier (mathematics)', 'Claytronics', 'Climber (BEAM)', 'Cloning', 'Cloud computing', 'Cloud robotics', 'Cloud storage', 'Cluster analysis', 'Cobweb (clustering)', 'Code readability', 'Cognitive architecture', 'Cognitive computing', 'Cognitive neuroscience', 'Cognitive psychology', 'Cognitive science', 'Collective behavior', 'Collingridge dilemma', 'Column (database)', 'Combinatorial optimization', 'Committee machine', 'Commonsense knowledge (artificial intelligence)', 'Commonsense reasoning', 'Communication protocol', 'Communications of the ACM', 'Compact space', 'Companion robot', 'Compatibility layer', 'Compatibility mode', 'Competitions and prizes in artificial intelligence', 'Compile time', 'Compiler construction', 'Completeness (logic)', 'Complex network', 'Complex systems', 'Complexity', 'Complexity class', 'Component-based software engineering', 'Composite material', 'Computability theory', 'Computation', 'Computational biology', 'Computational chemistry', 'Computational complexity', 'Computational complexity theory', 'Computational creativity', 'Computational cybernetics', 'Computational engineering', 'Computational geometry', 'Computational humor', 'Computational intelligence', 'Computational learning theory', 'Computational linguistics', 'Computational mathematics', 'Computational neuroscience', 'Computational number theory', 'Computational physics', 'Computational problem', 'Computational resource', 'Computational science', 'Computational social science', 'Computational statistics', 'Computational theory', 'Computer', 'Computer-Aided Design', 'Computer-aided diagnosis', 'Computer-automated design', 'Computer Go', 'Computer accessibility', 'Computer animation', 'Computer architecture', 'Computer chess', 'Computer compatibility', 'Computer data storage', 'Computer engineering', 'Computer go', 'Computer graphics', 'Computer hardware', 'Computer language', 'Computer network', 'Computer platform', 'Computer program', 'Computer programming', 'Computer science', 'Computer scientist', 'Computer security', 'Computer simulation', 'Computer software', 'Computer system', 'Computer vision', 'Computing', 'Computing platform', 'Concept', 'Concept drift', 'Conceptual clustering', 'Conceptualization (information science)', 'Concurrency (computer science)', 'Concurrent computing', 'Conditional (programming)', 'Connectionism', 'Consequent', 'Consistent heuristic', 'Constant factor', 'Constrained conditional model', 'Constraint (mathematics)', 'Constraint logic programming', 'Constraint programming', 'Constraint satisfaction', 'Constructed language', 'Context window', 'Contextualization (computer science)', 'Continuous function', 'Continuous optimization', 'Continuous or discrete variable', 'Continuous track', 'Continuum robot', 'Contradiction', 'Contrapositive', 'Control engineering', 'Control flow', 'Control system', 'Control theory', 'Control variable (programming)', 'Conversation', 'Convolution', 'Convolutional neural network', 'Cooperative multitasking', 'Copyright', 'Correlation', 'Correlation and dependence', 'Crash (computing)', 'Crisp set', 'Critique of work', 'Cross-validation (statistics)', 'Crossover (genetic algorithm)', 'Cryptography', 'Cuckoo search', 'Cultural algorithm', 'Curiosity', 'Cyberethics', 'Cybermethodology', 'Cybernetics', 'Cyberwarfare', 'Cyborg', 'DALL-E', 'Darkforest', 'Dartmouth workshop', 'Data', 'Data (computing)', 'Data analysis', 'Data augmentation', 'Data center', 'Data compression', 'Data fusion', 'Data integration', 'Data matrix (multivariate statistics)', 'Data mining', 'Data model', 'Data modeling', 'Data pre-processing', 'Data processing', 'Data science', 'Data serialization', 'Data set', 'Data stream mining', 'Data structure', 'Data type', 'Data warehouse', 'Database', 'Database schema', 'Database system', 'Dataflow programming', 'Datalog', 'David Ferrucci', 'David Poole (researcher)', 'David Silver (computer scientist)', 'David Silver (programmer)', 'David Verstraeten', 'Deadlock', 'Decentralization', 'Decision-making', 'Decision boundary', 'Decision making', 'Decision problem', 'Decision process', 'Decision support system', 'Decision theory', 'Decision tree', 'Decision tree learning', 'Declarative programming', 'Deductive classifier', 'Deductive database', 'Deductive reasoning', 'DeepMind Technologies', 'Deep Blue (chess computer)', 'Deep learning', 'Deep learning speech synthesis', 'Deep neural network', 'Deepfake', 'Default logic', 'Demis Hassabis', 'Denoise', 'Dependability', 'Description logic', 'Design Automation', 'Deterministic algorithm', 'Deterministic system (mathematics)', 'DevOps', 'Developmental neuroscience', 'Developmental robotics', 'Diagnosis (artificial intelligence)', 'Dialogue system', 'Differentiable function', 'Differentiable neural computer', 'Differentiable programming', 'Differential evolution', 'Differential technological development', 'Diffusion model', 'Diffusion process', 'Digital art', 'Digital data', 'Digital image', 'Digital library', 'Digital marketing', 'Digital media', 'Digital organism', ""Dijkstra's algorithm"", 'Dimensionality reduction', 'Directed graph', 'Disability robot', 'Discrete mathematics', 'Discrete system', 'Discrete time', 'Disruptive innovation', 'Distributed artificial intelligence', 'Distributed computing', 'Divide and rule', 'Document classification', 'Document management system', 'Doi (identifier)', 'Domain-specific language', 'Domain (software engineering)', 'Domain model', 'Domain of discourse', 'Domestic robot', 'Douglas H. Fisher', 'Dynamic epistemic logic', 'Dynamic memory allocation', 'Dynamic programming', 'Dynamical system', 'E-commerce', 'EWeek', 'Eager learning', 'Ebert test', 'Echo state network', 'Economic incentive', 'Economics', 'Edge (graph theory)', 'Edmund Landau', 'Educational robotics', 'Educational technology', 'Electric unicycle', 'Electrical engineering', 'Electrical synapse', 'Electrochemical RAM']"
233488,Machine learning,"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.
ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.
The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. 
From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.","Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.
ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.
The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. 
From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.


== History ==

The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.
Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.
By the early 1960s an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognize patterns and equipped with a ""goof"" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.
Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".
Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.


== Relationships to other fields ==


=== Artificial intelligence ===

As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.: 488 
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.: 25 
Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.


=== Data compression ===


=== Data mining ===
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).


=== Generalization ===
The difference between optimization and machine learning arises from the goal of generalization: While optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.


=== Statistics ===
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.
Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.
Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein ""algorithmic model"" means more or less the machine learning algorithms like Random Forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.


=== Statistical physics ===
Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.


== Theory ==

A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.


== Approaches ==

Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the ""signal"" or ""feedback"" available to the learning system:

Supervised learning: The computer is presented with example inputs and their desired outputs, given by a ""teacher"", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.
Although each algorithm has advantages and limitations, no single algorithm works for all problems.


=== Supervised learning ===

Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.
Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.


=== Unsupervised learning ===

Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.  

Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.


=== Semi-supervised learning ===

Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.


=== Reinforcement learning ===

Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.


=== Dimensionality reduction ===
Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the ""number of features"". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.


=== Other types ===
Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.


==== Self-learning ====
Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 

in situation s perform action a
receive a consequence situation s'
compute emotion of being in the consequence situation v(s')
update crossbar memory  w'(a,s) = w(a,s) + v(s')
It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.


==== Feature learning ====

Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.


==== Sparse dictionary learning ====

Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.


==== Anomaly detection ====

In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.
Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.


==== Robot learning ====
Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).


==== Association rules ====

Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".
Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
  
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.
Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.


== Models ==
A machine learning model is a type of mathematical model that, after being ""trained"" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions. By extension, the term ""model"" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.
Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.


=== Artificial neural networks ===

Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.


=== Decision trees ===

Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.


=== Support-vector machines ===

Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.


=== Regression analysis ===

Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.


=== Bayesian networks ===

A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.


=== Gaussian processes ===

A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.
Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.


=== Genetic algorithms ===

A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.


=== Belief functions ===

The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.


=== Training models ===
Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.


==== Federated learning ====

Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.


== Applications ==
There are many applications for machine learning, including:

In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.
Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.
Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires.


== Limitations ==
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.
The ""black box theory"" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an “intelligence system” that could have a “substantial impact on an individual’s life” would not be considered acceptable unless it provided “a full and satisfactory explanation for the decisions” it makes.
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.
Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.


=== Bias ===

Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.
Language models learned from data have been shown to contain human-like biases. In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged ""black defendants high risk twice as often as white defendants."" In 2015, Google Photos would often tag black people as gorillas, and in 2018, this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.
Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that ""[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.""


=== Explainability ===

Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the ""black box"" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.


=== Overfitting ===

Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.


=== Other limitations and vulnerabilities ===
Learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in ""adversarial"" images that the system misclassifies.
Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.
Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories ""spam"" and well-visible ""not spam"" of posts) machine learning models that are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.


== Model assessments ==
Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.
In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).


== Ethics ==

Machine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in “disproportionately high levels of over-policing in low-income and minority communities” after being trained with historical crime data.
While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, “female faculty merely make up 16.1%” of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of “new U.S. resident AI PhD graduates,” 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.
AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.
Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.


== Hardware ==
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.


=== Neuromorphic/Physical Neural Networks ===
A physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. ""Physical"" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.


=== Embedded Machine Learning ===
Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more. Pruning, Quantization, Knowledge Distillation, Low-Rank Factorization, Network Architecture Search (NAS) & Parameter Sharing are few of the techniques used for optimization of machine learning models.


== Software ==
Software suites containing a variety of machine learning algorithms include the following:


=== Free and open-source software ===


=== Proprietary software with free and open-source editions ===
KNIME
RapidMiner


=== Proprietary software ===


== Journals ==
Journal of Machine Learning Research
Machine Learning
Nature Machine Intelligence
Neural Computation
IEEE Transactions on Pattern Analysis and Machine Intelligence


== Conferences ==
AAAI Conference on Artificial Intelligence
Association for Computational Linguistics (ACL)
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)
International Conference on Machine Learning (ICML)
International Conference on Learning Representations (ICLR)
International Conference on Intelligent Robots and Systems (IROS)
Conference on Knowledge Discovery and Data Mining (KDD)
Conference on Neural Information Processing Systems (NeurIPS)


== See also ==
Automated machine learning – Process of automating the application of machine learning
Big data – Extremely large or complex datasets
Differentiable programming – Programming paradigm
Force control
List of important publications in machine learning
List of datasets for machine-learning research


== References ==


== Sources ==
Domingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.
Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.


== Further reading ==


== External links ==

 Quotations related to Machine learning at Wikiquote
International Machine Learning Society
mloss is an academic database of open-source machine learning software.",1230438059,57098,https://en.wikipedia.org/wiki/Machine_learning,"['Category:Articles with GND identifiers', 'Category:Articles with J9U identifiers', 'Category:Articles with LCCN identifiers', 'Category:Articles with NDL identifiers', 'Category:Articles with NKC identifiers', 'Category:Articles with excerpts', 'Category:Articles with short description', 'Category:Commons category link from Wikidata', 'Category:Cybernetics', 'Category:Learning', 'Category:Machine learning', 'Category:Short description is different from Wikidata', 'Category:Webarchive template wayback links', 'Category:Wikipedia articles needing clarification from January 2024']","['AAAI Conference on Artificial Intelligence', 'ACM Computing Classification System', 'ACM Computing Surveys', 'ADALINE', 'AIXI', 'AI boom', 'AI control problem', 'AI safety', 'AI takeover', 'AI winter', 'AT&T Labs', 'Action selection', 'Activation function', 'Active learning (machine learning)', 'Adaptive website', 'Adversarial machine learning', 'Affective computing', 'Agriculture', 'Alan Mackworth', 'Alan Turing', 'AlexNet', 'Alex Graves (computer scientist)', 'Algorithm', 'Algorithm design', 'Algorithmic bias', 'Algorithmic efficiency', 'Algorithmic transparency', 'AlphaFold', 'AlphaGo', 'AlphaZero', 'Amazon Machine Learning', 'Analysis of algorithms', 'Andrew Ng', 'Angoss', 'Anomaly detection', 'Anthropic', 'Apache Mahout', 'Apache Spark', 'Apache SystemML', 'Application security', 'Applications of artificial intelligence', 'Apprenticeship learning', 'Approximate computing', 'ArXiv', 'ArXiv (identifier)', 'Arithmetic coding', 'Array data structure', 'Arthur Samuel (computer scientist)', 'Artificial Intelligence: A Modern Approach', 'Artificial general intelligence', 'Artificial immune system', 'Artificial intelligence', 'Artificial intelligence art', 'Artificial intelligence in government', 'Artificial intelligence in healthcare', 'Artificial intelligence in industry', 'Artificial intelligence in mental health', 'Artificial intelligence systems integration', 'Artificial neural network', 'Artificial neuron', 'Association for Computational Linguistics', 'Association rule learning', 'Astroinformatics', 'Attention (machine learning)', 'Auto-GPT', 'Autoencoder', 'Automata theory', 'Automated decision-making', 'Automated machine learning', 'Automated medical diagnosis', 'Automated planning and scheduling', 'Automated theorem proving', 'Automatic differentiation', 'Autonomous car', 'Autoregressive model', 'Azure Machine Learning', 'BERT (language model)', 'BIRCH', 'BLOOM (language model)', 'Backdoor (computing)', 'Backpropagation', 'Bank fraud', 'Banking', 'Bard (chatbot)', 'Basic Books', 'Basis function', 'Batch learning', 'Batch normalization', 'Bayesian inference', 'Bayesian network', 'Bayesian optimization', 'Behaviorism', 'Bias–variance decomposition', 'Bias–variance tradeoff', 'Bibcode (identifier)', 'Big data', 'Binary classifier', 'Bing Chat', 'Bioinformatics', 'Biological neural network', 'Biology', 'Black box', 'Boosting (machine learning)', 'Bootstrap aggregating', 'Bootstrapping (statistics)', 'Brain', 'Brain–computer interface', 'CURE algorithm', 'Caffe (software)', 'Canadians', 'Canonical correlation', 'Carla Brodley', 'Centroid', 'Chartered Financial Analyst (CFA)', 'ChatGPT', 'Chatbot', 'Chemical synapse', 'Cheminformatics', 'Chinchilla AI', 'Chinese room', 'Christopher Bishop', 'Christopher M. Bishop', 'Chromosome (genetic algorithm)', 'CiteSeerX (identifier)', 'Citizen Science', 'Climate Science', 'Cluster analysis', 'Coefficient of determination', 'Cognitive computing', 'Commission for Racial Equality', 'Compiler construction', 'Computability theory', 'Computational anatomy', 'Computational biology', 'Computational chemistry', 'Computational complexity', 'Computational complexity theory', 'Computational economics', 'Computational engineering', 'Computational geometry', 'Computational learning theory', 'Computational linguistics', 'Computational mathematics', 'Computational physics', 'Computational science', 'Computational social science', 'Computational statistics', 'Computer accessibility', 'Computer animation', 'Computer architecture', 'Computer data storage', 'Computer gaming', 'Computer graphics', 'Computer hardware', 'Computer network', 'Computer program', 'Computer science', 'Computer security', 'Computer vision', 'Computing Machinery and Intelligence', 'Computing platform', 'Concurrency (computer science)', 'Concurrent computing', 'Conditional independence', 'Conditional random field', 'Conference on Knowledge Discovery and Data Mining', 'Conference on Neural Information Processing Systems', 'Confusion matrix', 'Connectionism', 'Continuous production', 'Control theory', 'Control variable (programming)', 'Convolution', 'Convolutional neural network', 'Corinna Cortes', 'Corpus of text', 'Covariance function', 'Credit-card fraud', 'Cross-validation (statistics)', 'Crossover (genetic algorithm)', 'Crowdsourcing', 'Cryptography', 'Curriculum learning', 'Cyberwarfare', 'DALL-E', 'DBSCAN', 'DNA sequence', 'Dartmouth workshop', 'Data', 'Data augmentation', 'Data cleaning', 'Data collection', 'Data compression', 'Data mining', 'Data quality', 'Data science', 'Database', 'David J. C. MacKay', 'David Rumelhart', 'David Silver (computer scientist)', 'Decision boundary', 'Decision making', 'Decision support system', 'Decision tree', 'Decision tree learning', 'DeepDream', 'DeepMind', 'DeepSpeed', 'Deep learning', 'Deep learning speech synthesis', 'Deep neural network', 'Deepfake', 'Deeplearning4j', 'Demis Hassabis', 'Dempster–Shafer theory', 'Density estimation', 'Dependability', 'Dictionary learning', 'Differentiable function', 'Differentiable neural computer', 'Differentiable programming', 'Diffusion model', 'Diffusion process', 'Digital art', 'Digital library', 'Digital marketing', 'Dimensionality reduction', 'Directed acyclic graph', 'Discipline (academia)', 'Discovery (observation)', 'Discrete mathematics', 'Distributed artificial intelligence', 'Distributed computing', 'Document management system', 'Doi (identifier)', 'Domain-specific language', 'Donald O. Hebb', 'Dynamic Bayesian network', 'Dynamic programming', 'E-commerce', 'ECML PKDD', 'ELKI', 'Echo state network', 'Edge device', 'Educational technology', 'Ehud Shapiro', 'Electrocardiography', 'Electrochemical RAM', 'Electronic design automation', 'Electronic publishing', 'Electronic voting', 'EleutherAI', 'Email filtering', 'Embedded system', 'Embedded systems', 'Empirical risk minimization', 'Ensemble Averaging', 'Ensemble learning', 'Ensemble methods', 'Entailment', 'Enterprise information system', 'Enterprise software', 'Errors and residuals', 'Ethics of artificial intelligence', 'European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases', 'Evolutionary algorithm', 'Existential risk from artificial general intelligence', 'Expectation–maximization algorithm', 'Expert system', 'Explainable artificial intelligence', 'Exploratory data analysis', 'Facial recognition system', 'Factor analysis', 'Fairness (machine learning)', 'False negative rate', 'False positive rate', 'Feature (machine learning)', 'Feature engineering', 'Feature extraction', 'Feature learning', 'Feature space vector', 'Feature vector', 'Federated learning', 'Feedforward neural network', 'Fei-Fei Li', 'Field of study', 'Financial market', 'Flux (machine-learning framework)', 'Force control', 'Formal language', 'Formal methods', 'Free Lossless Audio Codec', 'Friendly artificial intelligence', 'Functional programming', 'Fuzzy clustering', 'Fuzzy logic', 'GPT-1', 'GPT-2', 'GPT-3', 'GPT-4', 'GPT-J', 'GPU', 'Game theory', 'Gated recurrent unit', 'Gaussian processes', 'Gboard', 'Gemini (language model)', 'General game playing', 'Generalization (learning)', 'Generalize', 'Generalized linear model', 'Generative adversarial network', 'Generative artificial intelligence', 'Generative audio', 'Generative model', 'Genetic algorithm', 'Geoff Hinton', 'Geoffrey Hinton', 'Geographic information system', 'Geolitica', 'Glossary of artificial intelligence', 'Goof', 'Google', 'Google APIs', 'Google Cloud Platform', 'Google DeepMind', 'Google JAX', 'Gordon Plotkin', 'Gradient descent', 'Grammar induction', 'Graph neural network', 'Graphcore', 'Graphical model', 'Graphics processing unit', 'Green computing', 'Hallucination (artificial intelligence)', 'Handwriting recognition', 'Haplotype', 'Hardware acceleration', 'Hardware security', 'Harvard University', 'Hdl (identifier)', 'Health informatics', 'Hebbian theory', 'Heuristic', 'Heuristic (computer science)', 'Hidden Markov model', 'Hierarchical clustering', 'History of artificial intelligence', 'Huawei', 'Huawei PanGu', 'Hugging Face', 'Human-in-the-loop', 'Human brain', 'Human image synthesis', 'Human–computer interaction', 'Hutter Prize', 'Hybrid intelligent system', 'Hyperparameter optimization', 'IBM', 'IBM Granite', 'IBM Watson', 'IBM Watson Studio', 'IBM Watsonx', 'IEEE Spectrum', 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'ISBN (identifier)', 'ISSN (identifier)', 'I (newspaper)', 'Ian Goodfellow', 'Ilya Sutskever', 'Image compression', 'Image de-noising', 'Image processing', 'Imprecise probability', 'Indel', 'Independent component analysis', 'Inductive bias', 'Inductive logic programming', 'Inductive programming', 'Inductive reasoning', 'Infer.NET', 'Influence diagram', 'Information geometry', 'Information retrieval', 'Information security', 'Information system', 'Information theory', 'Insurance', 'Integrated circuit', 'Integrated development environment', 'Intelligent Agent', 'Interaction design', 'International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics', 'International Conference on Intelligent Robots and Systems', 'International Conference on Learning Representations', 'International Conference on Machine Learning', 'International Joint Conference on Artificial Intelligence', 'Internet fraud', 'Interpreter (computing)', 'Intrusion detection', 'Intrusion detection system', 'Investigative journalism', 'Isolation forest', 'JSTOR (identifier)', 'Jerome H. Friedman', 'John Hopfield', 'Journal of Machine Learning Research', 'Jürgen Schmidhuber', 'K-SVD', 'K-means clustering', 'K-nearest neighbors algorithm', 'KNIME', 'KXEN Inc.', 'Keras', 'Kernel machines', 'Kernel regression', 'Kernel trick', 'Knowledge discovery', 'Knowledge distillation', 'Knowledge graph embedding', 'Knowledge representation and reasoning', 'Kubeflow', 'LIONsolver', 'LLaMA', 'LaMDA', 'Language model', 'Large language model', 'Leaf node', 'Learning classifier system', 'Learning curve (machine learning)', 'Learning to rank', 'Leo Breiman', 'Library (computing)', 'LightGBM', 'Linear classifier', 'Linear discriminant analysis', 'Linear regression', 'List of artificial intelligence projects', 'List of datasets for machine-learning research', 'List of datasets in computer vision and image processing', 'List of important publications in computer science', 'Local outlier factor', 'Logic in computer science', 'Logic programming', 'Logical conjunction', 'Logistic regression', 'Long short-term memory', 'Loss function', 'Loss functions for classification', 'MATLAB', 'MIT Computer Science and Artificial Intelligence Laboratory', 'ML.NET', 'MOA (Massive Online Analysis)', 'MXNet', 'Machine Learning (journal)', 'Machine code', 'Machine ethics', 'Machine learning control', 'Machine learning in bioinformatics', 'Machine learning in earth sciences', 'Machine learning in physics', 'Machine perception', 'Machine translation', 'Mallet (software project)', 'Mamba (deep learning)', 'Mamba (deep learning architecture)', 'Manifold', 'Manifold hypothesis', 'Manifold learning', 'Manifold regularization', 'Map (mathematics)', 'Market basket analysis', 'Marketing', 'Markov decision process', 'Mathematica', 'Mathematical analysis', 'Mathematical induction', 'Mathematical model', 'Mathematical optimization', 'Mathematical software', 'Matrix (mathematics)', 'Matrix decomposition', 'Mean shift', 'Medical diagnosis', 'Medical diagnostics', 'Mehryar Mohri', 'Memristor', 'Memtransistor', 'Meta-learning (computer science)', 'Meta AI', 'Michael I. Jordan', 'Michal Aharon', 'Microcontrollers', 'Microsoft Cognitive Toolkit', 'Middleware', 'Midjourney', 'Mila (research institute)', 'MindSpore', 'Mixed reality']"
32472154,Deep learning,"Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective ""deep"" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.
Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.","Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective ""deep"" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.
Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.


== Overview ==
Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.
Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.
Importantly, a deep learning process can learn which features to optimally place in which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate upon. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.
The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.


== Interpretations ==
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.


== History ==
There were two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.
Charles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today, referring to Rosenblatt's 1962 book which introduced multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).: section 16  In addition, term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.
The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples, but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.  Instead, subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
In 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. 
The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.
Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision.
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.
In 1988, Wei Zhang et al. applied the backpropagation algorithm 
to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. 
In 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.
In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.
In 1992, Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.
The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". 
It combines this with a softmax operator and a projection matrix.
Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.
In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion.
Sepp Hochreiter's diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor, but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.
In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.
Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.
Speech recognition was taken over by LSTM. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision.
Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the ""big bang"" of deep learning, ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.


=== Deep learning revolution ===

In the late 2000s, deep learning started to outperform other methods in machine learning competitions.
In 2009, a long short-term memory trained by connectionist temporal classification (Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, 2006) was the first RNN to win pattern recognition contests, winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.
Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, the DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012, DanNet also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012, the similar AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. 
The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and
won the ImageNet 2014 competition, following a similar trend in large-scale speech recognition.
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.
In 2012, a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Sepp Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the ""Tox21 Data Challenge"" of NIH, FDA and NCATS.
In 2016, Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.


== Neural networks ==

Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing ""Go"").


=== Deep neural networks ===
A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.
For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name ""deep"" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.
Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).


==== Challenges ====
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell _{2}}
  
-regularization) or sparsity (
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
  
-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.


== Hardware ==
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.
Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).
Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).
In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.


== Applications ==


=== Automatic speech recognition ===

Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.

The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:

Scale-up/out and accelerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.


=== Image recognition ===

A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.
Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.
Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.


=== Visual art processing ===

Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of

identifying the style period of a given painting
Neural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video
generating striking imagery based on random visual input fields.


=== Natural language processing ===

Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.
Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time, rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs.


=== Drug discovery and toxicology ===

A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.
AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.
In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.


=== Customer relationship management ===

Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.


=== Recommendation systems ===

Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.


=== Bioinformatics ===

An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.
In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.
Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.


=== Deep Neural Network Estimations ===
Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.


=== Medical image analysis ===
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.


=== Mobile advertising ===
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.


=== Image restoration ===
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration"" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.


=== Financial fraud detection ===
Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.


=== Materials science ===
In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.


=== Military ===
The United States Department of Defense applied deep learning to train robots in new tasks through observation.


=== Partial differential equations ===
Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.


=== Image reconstruction ===
Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.


=== Weather prediction ===
Traditional weather prediction systems solve a very complex system of patrial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems. 


=== Epigenetic clock ===

An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.


== Relation to human cognitive and brain development ==
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature"".
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.


== Commercial activity ==
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.
In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.
As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as ""good job"" and ""bad job"".


== Criticism and comment ==
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.


=== Theory ===

A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:

Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.

In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.


=== Errors ===
Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).


=== Cyber threat ===
As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".
In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.
In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could ""serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)"".
In ""data poisoning"", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.


=== Data collection ethics ===

Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.
Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose, Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether or not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate ""a constant stream of verification data"" to further train the network in real-time. As Mühlhoff argues, the involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as ""human-aided artificial intelligence"".


== See also ==
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Differentiable programming
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine-learning research
Reservoir computing
Scale space and deep learning
Sparse coding
Stochastic parrot
Topological deep learning


== References ==


== Further reading ==",1229112619,57342,https://en.wikipedia.org/wiki/Deep_learning,"['Category:All articles needing additional references', 'Category:All articles with dead external links', 'Category:All articles with unsourced statements', 'Category:Articles needing additional references from April 2021', 'Category:Articles prone to spam from June 2015', 'Category:Articles with dead external links from April 2024', 'Category:Articles with permanently dead external links', 'Category:Articles with short description', 'Category:Articles with unsourced statements from July 2016', 'Category:Articles with unsourced statements from June 2024', 'Category:Articles with unsourced statements from March 2022', 'Category:Articles with unsourced statements from November 2020', 'Category:Artificial neural networks', 'Category:CS1: long volume value', 'Category:CS1 Finnish-language sources (fi)', 'Category:CS1 German-language sources (de)', 'Category:CS1 maint: multiple names: authors list', 'Category:CS1 maint: postscript', 'Category:Deep learning', 'Category:Pages using multiple image with auto scaled images', 'Category:Short description matches Wikidata', 'Category:Webarchive template wayback links']","['AI boom', 'AI control problem', 'AI safety', 'AI takeover', 'AI winter', 'Acoustic model', 'Action selection', 'Activation function', 'Activity tracker', 'Adversarial machine learning', 'Aja Huang', 'AlexNet', 'Alex Graves (computer scientist)', 'Alex Krizhevsky', 'Alexey Ivakhnenko', 'Algorithm', 'AlphaFold', 'AlphaGo', 'AlphaZero', 'Alphabet', 'Amazon Alexa', 'Amazon Mechanical Turk', 'American English', 'Andrew Ng', 'Andrew Zisserman', 'Anthropic', 'Applications of artificial intelligence', 'ArXiv (identifier)', 'Artificial general intelligence', 'Artificial intelligence', 'Artificial intelligence art', 'Artificial intelligence in government', 'Artificial intelligence in healthcare', 'Artificial intelligence in industry', 'Artificial intelligence in mental health', 'Artificial intelligence systems integration', 'Artificial neural network', 'Artificial neuron', 'Atari', 'Attention (machine learning)', 'Auto-GPT', 'Autoencoder', 'Automated planning and scheduling', 'Automatic differentiation', 'Automatic image annotation', 'Automatic speech recognition', 'Autoregressive model', 'BERT (language model)', 'BLOOM (language model)', 'Backpropagation', 'Baidu', 'Bard (chatbot)', 'Batch learning', 'Batch normalization', 'Bayesian inference', 'Bayesian network', 'Ben Goertzel', 'Bernard Widrow', 'Bhuvana Ramabhadran', 'Bibcode (identifier)', 'Bigram', 'Bioinformatics', 'Biological neural network', 'Biological system', 'Biomarkers of aging', 'Biomolecular target', 'Black box', 'Board game', 'Boltzmann machine', 'Boolean network', 'Brain', 'Brain development', 'Brendan Frey', 'CAPTCHA', 'Causality', 'Cerebellar model articulation controller', 'Cerebras', 'Chain rule', 'ChatGPT', 'Chinchilla AI', 'Chinese room', 'Christopher Bishop', 'CiteSeerX (identifier)', 'Classifier (machine learning)', 'Clickworkers', 'Climatology', 'Cloud computing', 'Cluster analysis', 'Cognitive neuroscientist', 'Commonsense reasoning', 'Comparison of deep learning software', 'Compressed sensing', 'Computational fluid dynamics', 'Computational learning theory', 'Computational science', 'Computer hardware', 'Computer vision', 'Connectionism', 'Connectionist temporal classification', 'Consistent estimator', 'Continuous functions', 'Control theory', 'Convolution', 'Convolutional neural network', 'Convolutional neural networks', 'Cortana (software)', 'CpG site', 'Crystal structure', 'Cumulative distribution function', 'Customer lifetime value', 'Customer relationship management', 'DALL-E', 'DARPA', 'Data augmentation', 'David E. Rumelhart', 'David Silver (computer scientist)', 'David Silver (programmer)', 'Deception', 'Decision tree', 'Deductive reasoning', 'DeepFace', 'DeepMind Technologies', 'Deep Image Prior', 'Deep Learning (South Park)', 'Deep belief network', 'Deep learning processor', 'Deep learning speech synthesis', 'Deep neural network', 'Deep reinforcement learning', 'Deepfake', 'Demis Hassabis', 'Denoising', 'Dialect', 'Differentiable function', 'Differentiable neural computer', 'Differentiable programming', 'Diffusion process', 'Direct marketing', 'Doi (identifier)', 'Domain knowledge', 'Dropout (neural networks)', 'Drug design', 'Drug discovery', 'Ebola virus', 'Echo state network', 'Electronic circuit', 'Electronic health record', 'EleutherAI', 'Epigenetic clock', 'Ernst Ising', 'Ethics of artificial intelligence', 'Evolutionary algorithm', 'Example-based machine translation', 'Existential risk from artificial general intelligence', 'Explainable artificial intelligence', 'FDA', 'Facebook', 'Facial recognition system', 'False positive', 'Feature (computer vision)', 'Feature engineering', 'Feedforward neural network', 'Feedforward neural networks', 'Fei-Fei Li', 'Felix Gers', 'Field-effect transistor', 'Film colorization', 'Filter bank', 'Fine-tuning (deep learning)', 'Floating-gate', 'Flux (machine-learning framework)', 'Frank Rosenblatt', 'Fraud detection', 'Frequency comb', 'Friendly artificial intelligence', 'Frontotemporal dementia', 'GPT-1', 'GPT-2', 'GPT-3', 'GPT-4', 'GPT-J', 'GPU', 'Gabor filter', 'Gamification', 'Gary Marcus', 'Gated recurrent unit', 'Gemini (language model)', 'Gene Ontology', 'General game playing', 'Generalization', 'Generative adversarial network', 'Generative artificial intelligence', 'Generative audio', 'Generative model', 'Genetic algorithm', 'Geoff Hinton', 'Geoffrey Hinton', 'George Cybenko', 'Glossary of artificial intelligence', 'Go (game)', 'Google', 'Google Cloud Platform', 'Google DeepMind', 'Google JAX', 'Google Neural Machine Translation', 'Google Now', 'Google Translate', 'Google Voice Search', 'Gottfried Wilhelm Leibniz', 'Gradient descent', 'Grammar induction', 'Graph neural network', 'Graphcore', 'Graphics processing unit', 'Greedy algorithm', 'Group method of data handling', 'Hallucination (artificial intelligence)', 'Handwriting recognition', 'Hardware accelerator', 'Hdl (identifier)', 'Henry J. Kelley', 'Herbert Robbins', 'Hidden Markov model', 'Highway network', 'History of artificial intelligence', 'Huawei', 'Huawei PanGu', 'Hugging Face', 'Human brain', 'Human image synthesis', 'Hybrid intelligent system', 'Hyperparameter optimization', 'IBM Granite', 'IBM Watson', 'IBM Watsonx', 'IFlytek', 'ISBN (identifier)', 'ISSN (identifier)', 'Ian Goodfellow', 'Ilya Sutskever', 'Image', 'ImageNet competition', 'Image classification', 'Image recognition', 'Inductive bias', 'Inference', 'Inflammatory bowel disease', 'Information geometry', 'Information mining', 'Inpainting', 'Insilico Medicine', 'Intelligent Agent', 'Inverse problems', 'Ising model', 'John Hopfield', 'Journal of Medical Internet Research', 'Jürgen Schmidhuber', 'Karen Simonyan', 'Keras', 'Knowledge distillation', 'Knowledge representation', 'Knowledge representation and reasoning', 'Kumpati S. Narendra', 'Kunihiko Fukushima', 'LLaMA', 'LSTM', 'LaMDA', 'Labeled data', 'Language model', 'Large language model', 'Larry Heck', 'Lawrence Berkeley National Laboratory', 'Learning rate', 'Lebesgue integration', 'Liquid state machine', 'List of artificial intelligence projects', 'List of datasets for machine-learning research', 'Long short-term memory', 'Loss functions for classification', 'Luca Maria Gambardella', 'MIT Computer Science and Artificial Intelligence Laboratory', 'MIT Technology Review', 'MNIST database', 'Machine learning', 'Machine learning in bioinformatics', 'Machine learning in earth sciences', 'Machine learning in physics', 'Machine translation', 'Malware', 'Mamba (deep learning)', 'Materials Project', 'Materials science', 'Mathematics of Control, Signals, and Systems', 'Matrix (mathematics)', 'Max pooling', 'Media studies', 'Medical image analysis', 'Mel-frequency cepstrum', 'Memristor', 'Meta AI', 'Microwork', 'Midjourney', 'Mila (research institute)', 'MindSpore', 'Mixture model', 'Mobile advertising', 'MuZero', 'Multi-task learning', 'Multilayer perceptron', 'Multiple sclerosis', 'Multiplexing', 'Multivariate polynomial', 'Music and artificial intelligence', 'NIH', 'Named-entity recognition', 'National Center for Advancing Translational Sciences', 'National Institute of Standards and Technology', 'National Security Agency', 'Natural language processing', 'Nature (journal)', 'Navier–Stokes equations', 'Neocognitron', 'Neocortex', 'Nerve growth factor', 'Neural Computation (journal)', 'Neural Style Transfer', 'Neural Turing machine', 'Neural machine translation', 'Neural network (machine learning)', 'Neuromorphic engineering', 'Neuron', 'Nuance Communications', 'Nvidia', 'OSTI (identifier)', 'Obesity', 'Off-target', 'OpenAI', 'OpenAI Five', 'Optical character recognition', 'Optimization', 'Outer product', 'Ovarian cancer', 'Overfitting', 'PMC (identifier)', 'PMID (identifier)', 'PaLM', 'Partial differential equation', 'Pattern recognition', 'Paul Werbos', 'Paywall', 'Perceptron', 'Peter Dayan', 'Philosophy of artificial intelligence', 'Phone (phonetics)', 'Photonic', 'Photonic integrated circuit', 'Photonics', 'Pixel', 'Pixels', 'Predictive coding', 'Primitive data type', 'Probabilistic', 'Probabilistic context free grammar', 'Probability distribution', 'Production (computer science)', 'Progress in artificial intelligence', 'Project Debater', 'Prompt engineering', 'Propositional formula', 'Protein structure prediction', 'Psychedelic art', 'PyTorch', 'Q-learning', 'Quantified self', 'RFM (customer value)', 'Rainer Mühlhoff', 'Random variable', 'Random variables', 'ReLU', 'Real numbers', 'Recommender system', 'Rectified linear unit', 'Rectifier (neural networks)', 'Recurrent neural network', 'Recurrent neural networks', 'Recursive self-improvement', 'Regression analysis', 'Regularization (mathematics)', 'Regulation of artificial intelligence', 'Representation learning', 'Reservoir computing', 'Residual neural network', 'Restricted Boltzmann machine', 'Rina Dechter', 'Robot control', 'Robotics', 'Rule-based programming', 'Russ Salakhutdinov', 'S2CID (identifier)', 'SRI International', 'Scale space', 'Sea urchin', 'Search engine results page', 'Seashell', 'Self-driving car', 'Self-organization', 'Self-supervised learning', 'Semi-supervised learning', 'Semiconductors', 'Sentence embedding', 'Sentiment analysis', 'Sepp Hochreiter', 'Seppo Linnainmaa', 'Seq2seq', ""Shun'ichi Amari"", 'Sigmoid function', 'Siri', 'Situated approach (artificial intelligence)', 'Skype Translator', 'Smartphone', 'Social network', 'Softmax', 'Softmax function', 'Sora (text-to-video model)', 'Sparse coding', 'Sparse matrix', 'Speaker recognition', 'Speech recognition', 'SpiNNaker', 'Stable Diffusion', 'Starfish', 'State–action–reward–state–action', 'Statistical manifold', 'Stephen Grossberg', 'Stochastic gradient descent', 'Stochastic parrot', 'Stochastic process', 'Stop sign', 'Strong artificial intelligence', 'StyleGAN', 'Super-resolution', 'Supervised learning', 'Support vector machine', 'Symbolic artificial intelligence', 'Synapse', 'TIMIT', 'Tag (Facebook)', 'Tara Sainath', 'TensorFlow', 'Tensor (machine learning)', 'Tensor Processing Unit', 'Tensor calculus', 'Tensor processing unit', 'Test (assessment)', 'Text-to-image model', 'Text-to-video model', 'The Economist', 'The Globe and Mail', 'The Guardian', 'The Scream', 'Theano (software)', 'Timeline of artificial intelligence', 'TinEye', 'Tony Robinson (speech recognition)', 'Topological deep learning', 'Toxicity', 'Toxicology', 'Training', 'Training, validation, and test sets', 'Transducer', 'Transfer learning', 'Transformer (machine learning model)', 'Transformer (neural network)', 'Turing Award', 'Turing test', 'Types of artificial neural networks', 'U.S. Army Research Laboratory', 'Universal approximation theorem', 'University of Texas at Austin', 'Unsupervised learning', 'Vanishing gradient problem', 'Variational autoencoder', 'Vector (mathematics and physics)', 'Vector space', 'Venture Beat', 'VideoPoet', 'Vision processing unit', 'Vivienne Sze', 'Wake-sleep algorithm', 'Watson (computer)', 'WaveNet', 'Waveform', 'Wavelength', 'Wayback Machine', 'Weight decay', 'Whisper (speech recognition system)', 'Wilhelm Lenz', 'Wired (magazine)', 'Word2vec', 'Word embedding']"
21652,Natural language processing,"Natural language processing (NLP) is an interdisciplinary subfield of computer science - specifically Artificial Intelligence - and linguistics. It is primarily concerned with providing computers the ability to process data encoded in natural language, typically collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.
Major tasks in Natural Language Processing are speech recognition, text classification, natural-language understanding, and natural-language generation.","Natural language processing (NLP) is an interdisciplinary subfield of computer science - specifically Artificial Intelligence - and linguistics. It is primarily concerned with providing computers the ability to process data encoded in natural language, typically collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.
Major tasks in Natural Language Processing are speech recognition, text classification, natural-language understanding, and natural-language generation.


== History ==

Natural language processing has its roots in the 1940s. Already in 1940, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.


=== Symbolic NLP (1950s – early 1990s) ===
The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.

1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.
1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted ""blocks worlds"" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the ""patient"" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to ""My head hurts"" with ""Why do you say your head hurts?"". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.
1970s: During the 1970s, many programmers began to write ""conceptual ontologies"", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).
1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.


=== Statistical NLP (1990s–2010s) ===
Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 

1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.


=== Neural NLP (present) ===
In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. 
In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.


== Approaches: Symbolic, statistical, neural networks ==
Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.
Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 

both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.
language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.
the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.
Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. 
Before that they were commonly used:

when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,
for preprocessing in NLP pipelines, e.g., tokenization, or
for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.


=== Statistical approach ===
In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.
The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.
Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.


=== Neural networks ===

A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, the statistical approach was replaced by the neural networks approach, using word embeddings to capture semantic properties of words. 
Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.


== Common NLP tasks ==
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.


=== Text and speech processing ===
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Speech recognition
Given a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed ""AI-complete"" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Speech segmentation
Given a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.
Text-to-speech
Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.
Word segmentation (Tokenization)
Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.
For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.


=== Morphological analysis ===
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.
Morphological segmentation
Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., ""open, opens, opened, opening"") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Part-of-speech tagging
Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, ""book"" can be a noun (""the book on the table"") or verb (""to book a flight""); ""set"" can be a noun, verb or adjective; and ""out"" can be any of at least five different parts of speech.
Stemming
The process of reducing inflected (or sometimes derived) words to a base form (e.g., ""close"" will be the root for ""closed"", ""closing"", ""close"", ""closer"" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.


=== Syntactic analysis ===

Grammar induction
Generate a formal grammar that describes a language's syntax.
Sentence breaking (also known as ""sentence boundary disambiguation"")
Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).
Parsing
Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).


=== Lexical semantics (of individual words in context) ===
Lexical semantics
What is the computational meaning of individual words in context?
Distributional semantics
How can we learn semantic representations from data?
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Another name for this task is token classification.
Sentiment analysis (see also Multimodal sentiment analysis)
Sentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.
Word-sense disambiguation (WSD)
Many words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.
Entity linking
Many words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.


=== Relational semantics (semantics of individual sentences) ===
Relationship extraction
Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Semantic parsing
Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).
Semantic role labelling (see also implicit semantic role labelling below)
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).


=== Discourse (semantics beyond individual sentences) ===
Coreference resolution
Given a sentence or larger chunk of text, determine which words (""mentions"") refer to the same objects (""entities""). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called ""bridging relationships"" involving referring expressions. For example, in a sentence such as ""He entered John's house through the front door"", ""the front door"" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis
This rubric includes several related tasks.  One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.).
Implicit semantic role labelling
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.
Recognizing textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Argument mining
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.


=== Higher-level NLP applications ===
Automatic summarization (text summarization)
Produce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.
Grammatical error correction
Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.
Logic translation
Translate a text from a natural language into formal logic.
Machine translation (MT)
Automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed ""AI-complete"", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.
Natural-language understanding (NLU)
Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Natural-language generation (NLG):
Convert information from computer databases or semantic intents into readable human language.
Book generation
Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.
Document AI
A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.
Dialogue management
Computer systems intended to converse with a human.
Question answering
Given a human-language question, determine its answer. Typical questions have a specific right answer (such as ""What is the capital of Canada?""), but sometimes open-ended questions are also considered (such as ""What is the meaning of life?"").
Text-to-image generation
Given a description of an image, generate an image that matches the description.
Text-to-scene generation
Given a description of a scene, generate a 3D model of the scene.
Text-to-video
Given a description of a video, generate a video that matches the description.


== General tendencies and (possible) future directions ==
Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:

Interest on increasingly abstract, ""cognitive"" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).
Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)
Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)


=== Cognition ===
Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
Cognition refers to ""the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses."" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:

Apply the theory of conceptual metaphor, explained by Lakoff as ""the understanding of one idea, in terms of another"" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (""That is a big tree""), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience.  When used metaphorically (""Tomorrow is a big day""), the author's intent to imply importance.  The intent behind other usages, like in ""She is a big person"", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.
Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in  US Patent 9269353:

  
    
      
        
          R
          M
          M
          (
          t
          o
          k
          e
          
            n
            
              N
            
          
          )
        
        =
        
          P
          M
          M
          (
          t
          o
          k
          e
          
            n
            
              N
            
          
          )
        
        ×
        
          
            1
            
              2
              d
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                −
                d
              
              
                d
              
            
            
              (
              (
              P
              M
              M
              (
              t
              o
              k
              e
              
                n
                
                  N
                
              
              )
            
            ×
            
              P
              F
              (
              t
              o
              k
              e
              
                n
                
                  N
                  −
                  i
                
              
              ,
              t
              o
              k
              e
              
                n
                
                  N
                
              
              ,
              t
              o
              k
              e
              
                n
                
                  N
                  +
                  i
                
              
              )
              
                )
                
                  i
                
              
            
          
          )
        
      
    
    {\displaystyle {RMM(token_{N})}={PMM(token_{N})}\times {\frac {1}{2d}}\left(\sum _{i=-d}^{d}{((PMM(token_{N})}\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\right)}
  

Where
RMM is the relative measure of meaning
token is any block of text, sentence, phrase or word
N is the number of tokens being analyzed
PMM is the probable measure of meaning based on a corpora
d is the non zero location of the token along the sequence of N tokens
PF is the probability function specific to a language
Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of ""cognitive AI"". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.


== See also ==


== References ==


== Further reading ==


== External links ==
 Media related to Natural language processing at Wikimedia Commons",1228670289,32698,https://en.wikipedia.org/wiki/Natural_language_processing,"['Category:Accuracy disputes from December 2013', 'Category:All accuracy disputes', 'Category:All articles needing additional references', 'Category:All articles with unsourced statements', 'Category:Articles needing additional references from May 2024', 'Category:Articles with J9U identifiers', 'Category:Articles with LCCN identifiers', 'Category:Articles with NDL identifiers', 'Category:Articles with NKC identifiers', 'Category:Articles with short description', 'Category:Articles with unsourced statements from May 2024', 'Category:CS1 errors: periodical ignored', 'Category:CS1 maint: location', 'Category:Commons category link from Wikidata', 'Category:Computational fields of study', 'Category:Computational linguistics', 'Category:Harv and Sfn no-target errors', 'Category:Natural language processing', 'Category:Short description is different from Wikidata', 'Category:Speech recognition']","['1 the Road', '3D model', 'ACT-R', 'AI-complete', 'AI winter', 'ALPAC', 'Abbreviation', 'Abstract Meaning Representation', 'Adjective', 'Agglutination', 'Alan Turing', 'Alphabet (formal languages)', 'Ambiguous', 'Analog signal', 'Anaphora resolution', 'Ancient language', 'Apertium', 'ArXiv (identifier)', 'Arabic language', 'Argument mining', 'Argument scheme', 'Artificial Intelligence', 'Artificial general intelligence', 'Artificial intelligence', 'Artificial intelligence detection software', 'Artificial intelligence in healthcare', 'Artificial neural network', 'Association for Computational Linguistics', 'Atomic formula', 'Automata theory', 'Automated essay scoring', 'Automatic identification and data capture', 'Automatic summarization', 'Automatic translation', 'BERT (language model)', 'BabelNet', 'Bag-of-words model', 'Bag of words', 'Bank of English', 'Bibcode (identifier)', 'Bigram', 'Biomedical text mining', 'Blocks world', 'Brno University of Technology', ""Cain's Jawbone"", 'Capitalization', 'Chatbot', 'Chatterbots', 'Chinese language', 'Chinese room', 'CiteSeerX (identifier)', 'Civilization', 'Closed-world assumption', 'Coarticulation', 'Cognition', 'Cognitive linguistics', 'Cognitive science', 'Collocation extraction', 'Compound-term processing', 'Compound term processing', 'Computational linguistics', 'Computer', 'Computer-assisted reviewing', 'Computer-assisted translation', 'Computer science', 'Computing Machinery and Intelligence', 'Concept mining', 'Conceptual metaphor', 'Concordancer', 'Conditional (computer programming)', 'Context (linguistics)', 'Controlled natural language', 'Coreference', 'Corner case', 'Corpus linguistics', 'DBpedia', 'Decision tree', 'Deep learning', 'Deep linguistic processing', 'Dialogue system', 'Dictionary.com', 'Discourse', 'Discourse analysis', 'Discourse representation theory', 'Distant reading', 'Distributional semantics', 'Document-term matrix', 'Document AI', 'Document classification', 'Doi (identifier)', 'ELIZA', 'Electronic health record', 'English language', 'Entity linking', 'European Union', 'Example-based machine translation', 'Explainable artificial intelligence', 'Explicit semantic analysis', 'FastText', 'Feature engineering', 'First-order logic', 'Foreign language reading aid', 'Foreign language writing aid', 'Formal grammar', 'Formal languages', 'Formal methods', 'Formal semantics (natural language)', 'Formal system', 'Formal verification', 'Formation rule', 'FrameNet', 'Frame semantics (linguistics)', 'Free energy principle', 'French language', 'Full stop', 'GPT-2', 'Generative grammar', 'George Lakoff', 'Georgetown-IBM experiment', 'German language', 'GloVe', 'Google Ngram Viewer', 'Grammar', 'Grammar checker', 'Grammar induction', 'Ground expression', 'Hallucination (artificial intelligence)', 'Hdl (identifier)', 'Head-driven phrase structure grammar', 'History of natural language processing', 'IBM alignment models', 'ISBN (identifier)', 'ISSN (identifier)', 'Inflectional morphology', 'Information extraction', 'Information retrieval', 'Interactive fiction', 'Interdisciplinary', 'Intractable problem', 'Jabberwacky', 'Japanese language', 'John Searle', 'Joseph Weizenbaum', 'Karl J. Friston', 'Kenna Hughes-Castleberry', 'Kimmo Koskenniemi', 'Knowledge extraction', 'Language and Communication Technologies', 'Language model', 'Language modeling', 'Language processing in the brain', 'Language resource', 'Language technology', 'Large language model', 'Latent Dirichlet allocation', 'Latent semantic analysis', 'Latent semantic indexing', 'Lemmatisation', 'Lesk algorithm', 'Lexical analysis', 'Lexical resource', 'Lexical semantics', 'Linguistic Linked Open Data', 'Linguistics', 'Logic translation', 'Machine-readable dictionary', 'Machine learning', 'Machine translation', 'Markov model', 'Mathematical notation', 'Meaning (linguistics)', 'Meitei language', ""Moore's law"", 'Morpheme', 'Morphology (linguistics)', 'Multi-agent system', 'Multi-document summarization', 'Multi-layer perceptron', 'Multimodal interaction', 'Multimodal sentiment analysis', 'N-gram', 'NLP (disambiguation)', 'Named-entity recognition', 'Named entity', 'Named entity recognition', 'Native-language identification', 'Natural-language processing', 'Natural-language programming', 'Natural-language understanding', 'Natural-language user interface', 'Natural Language Toolkit', 'Natural language', 'Natural language generation', 'Natural language user interface', 'Natural speech', 'Neural machine translation', 'Noam Chomsky', 'Noun', 'Ontology (information science)', 'Ontology learning', 'Open-world assumption', 'Optical character recognition', 'Outline of natural language processing', 'Oxford University Press', 'PARRY', 'PMC (identifier)', 'PMID (identifier)', 'Pachinko allocation', 'Parallel text', 'Parliament of Canada', 'Parse tree', 'Parsing', 'Part-of-speech tagging', 'Part of speech', 'Pathological (mathematics)', 'Poverty of the stimulus', 'Predicate logic', 'Predictive text', 'Pro-drop language', 'Probabilistic context-free grammar', 'Production (computer science)', 'Programming language theory', 'Pronoun', 'Pronunciation assessment', 'PropBank', 'Propositional calculus', 'Punctuation mark', 'Query expansion', 'Query understanding', 'Question answering', 'Racter', 'Recurrent neural network', 'Referring expression', 'Regular expression', 'Reification (linguistics)', 'Relationship extraction', 'Representation learning', 'Rhetorical structure theory', 'Rogerian psychotherapy', 'Ross Quillian', 'Rule-based machine translation', 'S2CID (identifier)', 'SHRDLU', 'Scientific American', 'Semantic analysis (machine learning)', 'Semantic decomposition (natural language processing)', 'Semantic network', 'Semantic parsing', 'Semantic role labeling', 'Semantic roles', 'Semantic similarity', 'Semantics (computer science)', 'Semantics of logic', 'Semi-supervised learning', 'Sentence boundary disambiguation', 'Sentence breaking', 'Sentence extraction', 'Sentiment analysis', 'Seq2seq', 'Shallow parsing', 'Simple Knowledge Organization System', 'SpaCy', 'Spanish language', 'Speech act', 'Speech corpus', 'Speech processing', 'Speech recognition', 'Speech segmentation', 'Speech synthesis', 'Spell checker', 'Spoken dialogue systems', 'Statistical machine translation', 'Stemming', 'Stochastic grammar', 'Stop word', 'Supervised learning', 'Syntactic parsing (computational linguistics)', 'Syntax (logic)', 'Syntax analysis', 'Syntax guessing', 'Term frequency-inverse document frequency', 'Terminology extraction', 'Text-proofing', 'Text-to-image generation', 'Text-to-speech', 'Text-to-video model', 'Text classification', 'Text corpus', 'Text mining', 'Text processing', 'Text segmentation', 'Text simplification', 'Text to speech', 'Textual entailment', 'Thai language', 'Thesaurus (information retrieval)', 'Thought experiment', 'Time complexity', 'Tokenization (lexical analysis)', 'Tomáš Mikolov', 'Topic model', 'Topic segmentation', 'Training data', 'Transfer-based machine translation', 'Transformational grammar', 'Transformer (machine learning model)', 'Treebank', 'Trigram', 'Truecasing', 'Turing test', 'Turkish language', 'UBY', 'Universal Dependencies', 'University of Helsinki', 'Unsupervised learning', 'Verb', 'Virtual assistant', 'Vocabulary', 'Voice user interface', 'Well-formed formula', 'Wikidata', 'Word', 'Word-sense disambiguation', 'Word-sense induction', 'Word2vec', 'WordNet', 'Word embedding', 'Word n-gram language model', 'Word segmentation', 'World Wide Web', 'Yingli Tian', 'Yoshua Bengio', 'Wikipedia:Citation needed', 'Wikipedia:Contents/Portals', 'Wikipedia:Verifiability', 'Template:Cite book', 'Template:Cite journal', 'Template:Formal languages', 'Template:Natural language processing', 'Template talk:Formal languages', 'Template talk:Natural language processing', 'Help:Authority control', 'Help:CS1 errors', 'Help:Maintenance template removal', 'Help:Referencing for beginners', 'Category:Accuracy disputes from December 2013', 'Category:Articles needing additional references from May 2024', 'Category:Articles with J9U identifiers', 'Category:Articles with LCCN identifiers', 'Category:Articles with NDL identifiers', 'Category:Articles with NKC identifiers', 'Category:Articles with unsourced statements from May 2024', 'Category:CS1 maint: location', 'Category:Formal languages', 'Category:Harv and Sfn template errors', 'Portal:Language']"
