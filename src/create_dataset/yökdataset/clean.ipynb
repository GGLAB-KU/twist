{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:11.620669Z",
     "start_time": "2024-07-02T05:20:05.921422Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "PROJECT_PATH = './'\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'output')\n",
    "SPECIAL_CHARS = {'¨', 'ï', '¸', '?', '$'}\n",
    "TURKISH_LETTERS = set('ıüğüşçö')\n",
    "\n",
    "# Utility functions\n",
    "def starts_with_two_capitals(text):\n",
    "    return text[:2].isupper()\n",
    "\n",
    "def count_periods(text):\n",
    "    return text.count('.')\n",
    "\n",
    "def contains_turkish_letters(text):\n",
    "    return any(letter in text for letter in TURKISH_LETTERS)\n",
    "\n",
    "def contains_any_special_char(text, chars):\n",
    "    return any(char in text for char in chars)\n",
    "\n",
    "def starts_with_capital(text):\n",
    "    return text[0].isupper()\n",
    "\n",
    "def ends_with_dot(text):\n",
    "    return text.endswith('.')\n",
    "\n",
    "def contains_single_letter_word(text):\n",
    "    return any(len(word) == 1 and word.isalpha() for word in text.split())\n",
    "\n",
    "def length_difference_within_limit(text1, text2, limit=0.1):\n",
    "    len1 = len(text1)\n",
    "    len2 = len(text2)\n",
    "    return abs(len1 - len2) / max(len1, len2) <= limit\n",
    "\n",
    "def transform_dashes(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'([a-zA-Z])-([a-zA-Z])', r'\\1\\1', text)\n",
    "        text = re.sub(r'([a-zA-Z]{2})- ([a-zA-Z]{2})', r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "# Merge all CSV files\n",
    "def merge_csv_files(data_path):\n",
    "    csv_files = glob(os.path.join(data_path, \"*.csv\"))\n",
    "    return pd.concat((pd.read_csv(file) for file in csv_files), ignore_index=True)\n",
    "\n",
    "merged_df = merge_csv_files(DATA_PATH)\n",
    "print(merged_df.info())\n",
    "\n",
    "# Ensure 'tr' and 'en' columns are of string type before applying transformations\n",
    "merged_df['tr'] = merged_df['tr'].astype(str)\n",
    "merged_df['en'] = merged_df['en'].astype(str)\n",
    "\n",
    "# Apply transformation to replace patterns like a-a with aa and aa- aa with aa\n",
    "merged_df['tr'] = merged_df['tr'].apply(transform_dashes)\n",
    "merged_df['en'] = merged_df['en'].apply(transform_dashes)\n",
    "\n",
    "# Data cleaning and filtering\n",
    "filtered_data_final = (\n",
    "    merged_df.dropna()\n",
    "    .drop_duplicates(subset=['tr', 'en'])\n",
    "    .loc[\n",
    "        (~merged_df['en'].apply(starts_with_two_capitals)) &\n",
    "        (~merged_df['tr'].apply(starts_with_two_capitals)) &\n",
    "        (merged_df['tr'].apply(count_periods) == merged_df['en'].apply(count_periods)) &\n",
    "        (merged_df['tr'].apply(starts_with_capital)) &\n",
    "        (merged_df['en'].apply(starts_with_capital)) &\n",
    "        (merged_df['tr'].apply(contains_turkish_letters)) &\n",
    "        (~merged_df['tr'].apply(contains_any_special_char, chars=SPECIAL_CHARS)) &\n",
    "        (~merged_df['en'].apply(contains_any_special_char, chars=SPECIAL_CHARS)) &\n",
    "        (merged_df['tr'].apply(ends_with_dot)) &\n",
    "        (merged_df['en'].apply(ends_with_dot)) &\n",
    "        (~merged_df['tr'].apply(contains_single_letter_word)) &\n",
    "        (merged_df.apply(lambda row: length_difference_within_limit(row['tr'], row['en']), axis=1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# add n_characters, n_words, n_sentences columns for en\n",
    "filtered_data_final['n_characters_en'] = filtered_data_final['en'].apply(len)\n",
    "filtered_data_final['n_words_en'] = filtered_data_final['en'].apply(lambda x: len(x.split()))\n",
    "filtered_data_final['n_sentences_en'] = filtered_data_final['en'].apply(lambda x: len(re.split(r'[.!?]', x)))\n",
    "\n",
    "print(filtered_data_final.info())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23676 entries, 0 to 23675\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   university  23676 non-null  object\n",
      " 1   konu        23676 non-null  object\n",
      " 2   tr          23119 non-null  object\n",
      " 3   en          23235 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 740.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3632 entries, 5 to 23669\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   university       3632 non-null   object\n",
      " 1   konu             3632 non-null   object\n",
      " 2   tr               3632 non-null   object\n",
      " 3   en               3632 non-null   object\n",
      " 4   n_characters_en  3632 non-null   int64 \n",
      " 5   n_words_en       3632 non-null   int64 \n",
      " 6   n_sentences_en   3632 non-null   int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 227.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:11.626700Z",
     "start_time": "2024-07-02T05:20:11.621631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use abbreviations for universities\n",
    "filtered_data_final['university'] = filtered_data_final['university'].replace(\n",
    "    {'Orta Doğu Teknik Üniversitesi': 'ODTÜ', \n",
    "     'Boğaziçi Üniversitesi': 'BOUN', \n",
    "     'İstanbul Teknik Üniversitesi': 'İTÜ', \n",
    "     'İhsan Doğramacı Bilkent Üniversitesi': 'BİLKENT', \n",
    "     'Koç Üniversitesi': 'KOÇ', \n",
    "     'Sabancı Üniversitesi': 'SABANCI'})\n",
    "\n",
    "\n",
    "# use abbreviations for departments\n",
    "filtered_data_final['konu'] = filtered_data_final['konu'].replace(\n",
    "    {'Elektrik ve Elektronik Mühendisliği = Electrical and Electronics Engineering': 'ELEC',\n",
    "     'Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control': 'COMP',\n",
    "     'Makine Mühendisliği = Mechanical Engineering': 'MECH',\n",
    "     'Endüstri ve Endüstri Mühendisliği = Industrial and Industrial Engineering': 'INDR',\n",
    "     'Fizik ve Fizik Mühendisliği = Physics and Physics Engineering': 'PHYS',\n",
    "     'Matematik = Mathematics': 'MATH'})\n"
   ],
   "id": "4c8a99e67619ab94",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:11.763297Z",
     "start_time": "2024-07-02T05:20:11.627466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create paper_id column as a unique identifier for each row from 1 to n\n",
    "filtered_data_final['paper_id'] = range(1, len(filtered_data_final) + 1)\n",
    "\n",
    "# make paper_id the first column\n",
    "filtered_data_final = filtered_data_final[['paper_id', 'university', 'konu', 'tr', 'en', 'n_characters_en', 'n_words_en', 'n_sentences_en']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "json_data = []\n",
    "id_counter = 1\n",
    "\n",
    "for index, row in filtered_data_final.iterrows():\n",
    "    json_data.append({\n",
    "        \"id\": id_counter,\n",
    "        \"paper_id\": row[\"paper_id\"],\n",
    "        \"university\": row[\"university\"],\n",
    "        \"konu\": row[\"konu\"],\n",
    "        \"data\": {\n",
    "            \"my_text\": f\"ENGLISH: {row['en']} \\n \\n TURKISH: {row['tr']}\"\n",
    "        }\n",
    "        })\n",
    "    id_counter += 1\n",
    "\n",
    "# Save the JSON structure to a file\n",
    "json_file_path = 'label_studio/yok_all.json'\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(json_data, json_file, ensure_ascii=False, indent=4)"
   ],
   "id": "c06cc86b53954398",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:12.633969Z",
     "start_time": "2024-07-02T05:20:11.764434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the filtered data\n",
    "filtered_data_final.to_csv(os.path.join(PROJECT_PATH+'/cleaned_output/', 'cleaned_data.csv'), index=False)\n",
    "\n",
    "# save as excel\n",
    "filtered_data_final.to_excel(os.path.join(PROJECT_PATH+'/cleaned_output/', 'cleaned_data.xlsx'), index=False, engine='xlsxwriter')"
   ],
   "id": "7bd5b8c614b12c3d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:12.642016Z",
     "start_time": "2024-07-02T05:20:12.634671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample data 43 rows \n",
    "sample_data = filtered_data_final.sample(n=43, random_state=1)\n",
    "\n",
    "# Save the sample data as JSON\n",
    "sample_json_data = []\n",
    "id_counter = 1\n",
    "\n",
    "for index, row in sample_data.iterrows():\n",
    "    sample_json_data.append({\n",
    "        \"id\": id_counter,\n",
    "        \"paper_id\": row[\"paper_id\"],\n",
    "        \"university\": row[\"university\"],\n",
    "        \"konu\": row[\"konu\"],\n",
    "        \"data\": {\n",
    "            \"my_text\": f\"ENGLISH: {row['en']} \\n \\n TURKISH: {row['tr']}\"\n",
    "        }\n",
    "        })\n",
    "    id_counter += 1\n",
    "    \n",
    "sample_json_file_path = 'label_studio/yok_sample.json'\n",
    "with open(sample_json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(sample_json_data, json_file, ensure_ascii=False, indent=4)"
   ],
   "id": "7385b61375fbe697",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T05:20:12.657157Z",
     "start_time": "2024-07-02T05:20:12.642684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the sample data\n",
    "sample_data.to_csv(os.path.join(PROJECT_PATH+'/cleaned_output/', 'sample_data.csv'), index=False)\n",
    "\n",
    "# save as excel\n",
    "sample_data.to_excel(os.path.join(PROJECT_PATH+'/cleaned_output/', 'sample_data.xlsx'), index=False, engine='xlsxwriter')"
   ],
   "id": "bd1594fc32c84830",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
