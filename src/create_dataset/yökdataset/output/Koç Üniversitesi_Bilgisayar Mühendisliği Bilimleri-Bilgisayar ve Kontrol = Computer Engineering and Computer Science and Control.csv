university,konu,tr,en
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bellek bant genişliği, yüksek performanslı uygulamalar için performans artırmada sınırlayıcı bir faktör olmuştur. Bu sınırlamanın üstesinden gelmek için çeşitli heterojen bellek sistemleri ortaya çıkmıştır. Heterojen bir bellek sistemi her biri farklı özelliklere sahip olan birden fazla bellekten oluşur. Bu bellek sistemleri çeşitlilik gösterseler de ortak bir özelliğe sahiptirler. Bu özellik sistemdeki diğer belleklere kıyasla daha yüksek bant genişliğine sahip bir belleğin içerilmesidir. Bu özel bellek ise genelde yüksek bant genişliği belleği (HBM) olarak bilinir. HBM teknolojilerinden bazıları, Micron'un hibrid bellek küpünü (HMC) ve JEDEC'in yüksek bant genişliği bellek standardını içerir. Intel'in en yeni Xeon Phi işlemcisi Intel Knights Landing (KNL), DDR ile birlikte çok kanallı DRAM veya MCDRAM olarak da bilinen bir HBM ile donatılmıştır. DDR'de bant genişliği 88 GB/s iken, MCDRAM'de bu rakam 450 GB/s'dir. Ancak, bant genişliği arttıkça, MCDRAM için gecikme süresi artar. Buna ek olarak, teknolojik kısıtlardan ve bayt başına yüksek fiyattan dolayı HBM, heterojen bellek sistemlerinde geleneksel DDR'ye kıyasla küçük bir kapasitede sunulmaktadır. HBM'nin bu kapasite kısıtının üstesinden gelmek için, heterojen bellek sistemleri daha yüksek kapasiteli bir DDR ile donatılmıştır. Bu tür sistemlerde, programcı her belleğe özel ayırma yapabilir veya donanım HBM'yi önbellek olarak kullanabilir. Akıllı bir nesne yerleştirme şeması, uygulamalarda performans artışı sağlayabilir. Bunun aksine, bellek ve uygulamaların özelliklerini dikkate almadan yerleştirme yapılırsa, uygulamaların genel performansı büyük ölçüde düşebilir. Nesne yerleştirme seçimi ve buna ek olarak sistem konfigürasyonuna karar verme seçimi, programlayıcı için fazladan sorumluluk oluşturur. Bu sorumluluk da artan programlama çabası ve zaman tüketimine neden olur. Bu tez, sistem ve uygulamaya özel maliyet modeline dayanan bir nesne yerleştirme şemasını, genellikle 0/1 Knapsack olarak bilinen bir kombinatorik optimizasyon algoritmasını ve bu ikisini pratikte birleştiren bir aracı sunmaktadır. Maliyet modeli nesne boyutları, bellek erişim sayıları, erişim türü, ve bunun gibi çeşitli uygulama özelliklerini göz önünde bulundurmaktadır. Uygulama karakteristiğine ek olarak, maliyet modeli, veri akış bant genişliği ve veri kopyalama bant genişliği de dahil olmak üzere çeşitli açılardan bellek bant genişliğini de dikkate almaktadır. Bu özellikler, akıllı bir nesne yerleştirme şeması önermeye yarayan maliyet modelimizin içeriğini zenginleştirmektedir. Belirtilen özellikleri kullanarak, maliyet modeli her nesne için bir skor belirler. Bu skorlar, her bir nesnenin Knapsack algoritmasındaki boyutunu ifade eder. Knapsack boyutu olarak ise HBM'nin boyutu kullanılır. Araç, nesne yerleştirmede akıllı bir karar vermek için maliyet modelini kullanır ve iki farklı yerleştirme yapabilir: 1) nesnelerin yerleşiminin en başta yapıldığı statik yerleştirme ve 2) nesnelerin transferine neden olan, uygulamanın fazlarına dayalı olarak HBM'ye aktarma veya HBM'den çıkarma yapılan dinamik yerleştirme. Dinamik yerleştirmede, maliyet modeli, HBM'ye yerleştirilecek nesnelere karar verirken, nesnelerin bir bellekten diğer belleğe olan transferinin maliyetini göz önünde bulundurur. Ayrıca araç, nesneleri eşzamansız transfer etme yeteneğine sahiptir. Eşzamansız transferler, aracın transfer maliyetini uygulamanın fazları arasında gizlemesine olanak sağlar. Yerleştirme şemamızı, NAS Paralel ve Rodinia karşılaştırmalı değerlendirme paketlerinden alınan bir dizi uygulama üzerinde test ettik. Kullanılan uygulamalar, pratikte kullanılan uygulamalarının özelliklerini sergileyen, değişken iş yüklerine ve bellek erişim modellerine sahiptirler. Değerlendirme için Intel'in Knights Landing işlemcisini ve yüksek bant genişliği belleği olan MCDRAM'yi kullanıldık. Geliştirdiğimiz araç tarafından önerilen nesne şeması, 2,5 kata kadar bir hızlanma sağladı. Gecikme süresine duyarlı uygulamaların yüksek bant genişliğine sahip bellek yerleşiminden yararlanamadıklarını da gözlemledik. Bunun nedeni, HBM'nin daha yüksek gecikme süresine neden olmasıdır. Ayrıca, sonuçları Intel KNL'nin otomatik donanım önbelleği ile de karşılaştırdık. Donanım modunda, uygulama, herhangi bir değişiklik yapılmadan yürütüldü ve donanım tarafından otomatik olarak önbelleğe alma işlemi yapıldı. Yerleştirme şemamızın, çoğu zaman otomatik donanım önbelleğinden daha iyi sonuç verdiğini gözlemledik.","Memory bandwidth has long been the limiting scaling factor for high-performance applications. To overcome this limitation, various heterogeneous memory systems have emerged. A heterogeneous memory system is equipped with multiple memories each with distinct characteristics. Among other characteristics, one common characteristic across these systems includes a memory with a significantly higher bandwidth than the others. This particular memory is known as the high bandwidth memory in general. Some of such high bandwidth memory (HBM) technologies include the hybrid memory cube (HMC) by Micron and high bandwidth memory standard by JEDEC. Intel's latest Xeon Phi processor, namely Intel Knights Landing (KNL), is equipped with an HBM known as the multi-channel DRAM, or MCDRAM, along with a DDR. The MCDRAM boasts up to 450 GB/s memory bandwidth as compared to its slower counterpart DDR which boasts only up to 88 GB/s. Unfortunately, as the bandwidth increases, the access latency for MCDRAM increases. Due to technology limitations and the high price per byte rate, HBM is offered in a small capacity as compared to traditional DDR in heterogeneous memory systems. Therefore, to overcome the smaller capacity of HBM, a heterogeneous memory system is also equipped with a higher capacity DDR. In such systems, the programmer is offered a choice to perform explicit allocations to each memory or let hardware handle data caching to the HBM. An intelligent object allocation scheme can yield a performance boost of the application. On the contrary, if an allocation is made without considering memory and application characteristics the overall performance of an application can drastically degrade. The object allocation choice coupled with the choice of deciding a system configuration can overburden the programmer resulting in increased programming effort and time consumption. This thesis presents an object allocation scheme which is based on a system and application-specific cost model, a combinatorics optimization algorithm commonly known as the 0/1 Knapsack and a tool which combines these two components in practice. The cost model considers various characteristics of application data such as the object sizes, memory access counts, type of access, etc. In addition to application characteristics, the cost model also considers memory bandwidth under various conditions, including data streaming bandwidth and data copy bandwidth. These characteristics make our cost model rich allowing it to suggest an intelligent object allocation scheme. Using the aforementioned characteristics, the cost model determines a score for each object. These scores are used as values for each object in the 0/1 Knapsack algorithm to determine objects to be allocated on HBM where Knapsack size is HBM size. The tool uses the cost model to make an intelligent decision for object placement. The tool comes in two flavors: 1) static placement where object placement is decided at the beginning of application execution and 2) dynamic placement where objects are evicted and admitted to high bandwidth memory on the run based on application phases thus incurring the object movement cost. In the latter variant, the cost model considers the movement cost of objects from one memory to another while deciding on objects to be placed on the HBM. The tool is also capable of conducting object transfers asynchronously. The asynchronous transfers allow the tool to hide the transfer cost between phases. We evaluate our allocation scheme using a diverse set of applications from NAS Parallel and Rodinia benchmark suites. The included applications have varying workloads and memory access patterns which exhibit the characteristics of real-world applications. During the evaluation, Intel's Knights Landing and its high bandwidth memory, namely MCDRAM, was used. The object placement suggested by the tool yields a speedup of up to 2.5x. We observe that latency-sensitive applications fail to benefit from high bandwidth memory allocation. This is because of the higher access latency of HBM. We also compared the results with the automatic hardware caching of Intel KNL. In hardware mode, the application is executed on the system without any changes and the caching is done by hardware automatically. We observe that our allocation scheme yields result better than that of hardware caching majority of the time."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dağıtık dizin temelli önbellek tutarlılığına sahip modern çok çekirdekli bilgisayar mimarilerinde her bir bellek adresi, ona atanmış olan bir dağıtık dizin birimi tarafından yönetilir. Bu birime Önbellekleme/Merkez Aracısı (ÖMA) ismi verilir ve birim, önbellek satırını gözlemler. ÖMA ve çekirdeklerin fiziksel konumları programcılar tarafından bilinmez. Bu çalışmada öncelikle, Intel Xeon işlemciler için ÖMA ve çekirdeklerin konumlarını açığa çıkaran farklı yöntemlerin analizi ve kıyaslaması yapılmıştır. Bununla birlikte bellek adreslerinin ÖMA birimlerine haritalamasını yapan yöntemlerin de analiz ve kıyaslaması yapılmıştır. Topoloji ve adres haritalaması bilgisi kullanılarak, birbiriyle haberleşen çekirdekler ve ÖMA'ların arasındaki fiziksel mesafenin uygulama performansı üzerindeki etkileri üzerine araştırma yapılmıştır. Bu araştırmadan yola çıkarak önbellek tutarlılığını sağlayan trafiği azaltmayı hedefleyen ve iş parçacıklarının çekirdeklere atanmasıyla görevli bir iş parçacığı haritalama algoritması geliştirilmiştir. Geliştirdiğimiz algoritmanın, iş parçacıkları arasında paylaşımlı yazılabilir bilginin yüksek oranda mevcut olduğu uygulamalarda performansı geliştirmesini bekliyoruz. Bu algoritma, yüksek oranda yonga trafiğine sebep olan uygulamalar üzerinde test edilmiştir. Ardışık iş parçacığı haritalamasına kıyasla Seyrek matris-vektör çarpımında %5.6'ya, Barnes'ta %8'e, sıvı akışkanlığı simülasyon uygulamasında %25'e, LU ayrıştırmasında %6'ya varan hızlanmalar sağladığı gözlemlenmiştir.","In modern multi-core architectures with distributed directory-based cache coherence, each memory address is overseen by a distributed directory unit, known as a Caching/Home Agent (CHA), that monitors cache line state and location. Neither the CHA nor core locations in a processor are directly exposed to the programmer. In this work, we firstly analyze and compare the methodologies for uncovering both the CHA and core topology of Intel Xeon Scalable processors, as well as the methods to reveal the mapping of memory addresses to CHAs. Leveraging the topology and the address mapping information, we investigate the impact of spatial proximity between communicating cores and CHAs on application performance, and propose a thread mapping heuristic that assigns threads to cores by considering cache coherence traffic. We expect our heuristic to achieve significant performance gains on applications with high amount of on-chip cache coherence traffic due to high percentage of shared written data. We evaluated our heuristic on applications that exhibit high amount of on-chip communication traffic. The heuristic achieves up to 5.6% speedup over compact placement on merge-based SpMV application, up to 8% with an average of around 4.4% on Barnes application, around 25% for Fluidanimate application to simulate 60 frame per second, and lastly approximately 6% for LU across different matrices. We also prove the improved performance is in fact related to reduced on-chip traffic on the mesh."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sanal Gerçeklik (SG), son yıllarda eğlence, eğitim ve eğitim alanlarında popülerleşerek önemli bir ilgi alanı haline gelmiştir. Küresel doğası ve geniş görüş alanları nedeniyle, geleneksel 2D görsel değerlendirme yöntemlerini zorlayan etkileşimli 360 içeriklerdeki bu artış, özelleştirilmiş görsel kalite değerlendirme (KD) yöntemlerine olan gereksinimleri de artırmıştır. Bu tez, bilgisayarlı görüntüde görüntü dönüştürü cülerinin başarısından ilham alarak, ST360IQ ve LGT360IQ adlı iki KD modeller önermektedir. Küresel Görüntü Dönüştürücüsü tabanlı bu modeller, çok yönlü görüntüleri değerlendirmek için özelleştirilmiştir. ST360IQ, teğet görüntü temsili kullanarak, dönüştürücü performansını görsel belirginlik özellikleri ile güçlendirmekte ve küresel içerik özelliklerinin bozulmadan yeniden üretilmesi için geometrik ve kaynak gömme bilgilerini entegre etmektedir. Ek olarak, LG360IQA modeli ile birlikte, kapsamlı bir değerlendirme için yerel ve küresel düzeydeki bilgileri birleştiren, yukarıdan aşağıya ve aşağıdan yukarıya dikkat mekanizmalarıyla iki akışlı bir yapı tanıtılmıştır. Görüntü dönüştürücüleri tabanlı ST360IQ ve LGT360IQ modellerinin her ikisi de, üç veri kümesinde en iyi performansı vererek, 360 görüntü kalitesi değerlendirmesindeki etkinliklerini göstermektedir. Etkileşimli içeriklerde bulunan uzamsal ses, ambisonik ses içeren bir veri kümesine olan ihtiyacı vurgulamaktadır. Bu veri kümesi, önemli ses bilgilerini entegre ederek video kalitesi değerlendirmesi alanındaki araştırmalara katkıda bulunacaktır. Literatürdeki bu veri kümesi boşluğunu doldurmak için, bir öznel kalite değerlendirme deneyi yapılmış, ve YT360-VQA veri kümesi toplanmıştır. Veri kümemizdeki videolar kullanıcılar tarafından oluşturulduğu için, bilinçli olarak içerdikleri gerçekçi bozulmalar ile gerçek dünya senaryolarını simüle ettikleri vurgulanmalıdır. Sonuç olarak, bu araştırma uzamsal sesin etkileşimli küresel videoların kalite değerlendirmesindeki etkisini nesnel olarak analiz etmeyi amaçlamış ve genel kullanıcı deneyimi üzerindeki etkisini göstermiştir.","In recent years, Virtual Reality (VR) has gained significant attention and has become popular across entertainment, education, and training. This surge necessitates a specialized Quality Assessment(QA) framework designed for evaluating immersive 360 content, challenging traditional 2D visual assessment methods due to the spherical nature and wider field of view. Drawing inspiration from Vision Transformers (ViTs)' success in computer vision, this thesis proposes two QA models: ST360IQ and LGT360IQ. These models, based on Spherical Vision Transformers, specifically evaluate omnidirectional image quality. ST360IQ uses tangent image representation, enhancing transformer encoder performance with saliency information, and integrates geometric and source embeddings for replication of spherical content attributes. Additionally, the LG360IQA model introduces a dual-branch structure, combining local-level and global-level information for comprehensive evaluation through top-down and bottom-up attention mechanisms. Both ST360IQ and LGT360IQ ViT-based models achieve state-of-the-art performance across three datasets, showcasing their effectiveness in 360 Image Quality Assessment (IQA). The significance of spatial audio in immersive content underscores the need for a dataset containing ambisonic audio. This dataset would facilitate research in Video Quality Assessment (VQA) by integrating crucial audio information. To bridge this gap, a subjective quality assessment experiment is conducted to collect the YT360-VQA dataset. Remarkably, this user-generated dataset deliberately includes authentic distortions, simulating real-world scenarios. Ultimately, this research aims to objectively analyze ambisonic audio's impact on spherical video quality assessment within immersive content experiences, recognizing its significance in shaping the overall visual quality."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son zamanlarda büyük ölçekli metinden görüntü üretim yöntemlerinin ses getiren başarısıyla koşullu görüntü üretimi alanında bir devrime şahitlik etmekteyiz. Bu başarılar aynı zamanda farklı çok kipli koşullanmış görüntü üretimleri için de yeni fırsatlar sunmakta. Uzamsal kontrolü belirleyen görüntü, derinlik, eskiz gibi kipin araştırma konusu olarak ilgi çekerken ses ve görüntünün insan algısının iki ana bileşeni olmasından ötürü eşit miktarda etkili olabilecek bir diğer kipin ses olduğunu savunmaktayız. Bu sebeple, bu tezde büyük ölçekli yayınım modellerinde sese koşullanmış görüntü üretimini mümkün kılan SonicDiffusion adını verdiğimiz bir yöntem önermekteyiz. Yöntemimiz öncelikle ses kliplerinden elde edilen öznitelikleri metin belirteçlerine benzer şekilde yayınım modeline enjekte edilebilen belirteçlere dönüştürmekte. Ardından yayınım modellerinin orijinal değişkenlerini dondurarak, ekstra ses-görüntü çapraz dikkat katmanları sunmakta ve bu katmanları ince ayarlamakta. Yöntemimiz ses koşullandırılmış görüntü üretiminin yanı sıra, görüntü düzenlemesi için yayınım modelleri tabanlı görüntü düzenleme yöntemleriyle birlikte kullanılabilmekte. Yöntemimizin performansını çeşitli ses ve resim veri kümeleri üzerinde göstermekteyiz. Yakın dönemde öne çıkan diğer yöntemlerle de kapsamlı bir kıyaslama yapmakta ve lehimize sonuçlar göstermekteyiz.","We are witnessing a revolution in conditional image synthesis with the recent success of large scale text-to-image generation methods. This success also opens up new opportunities in controlling the generation and editing process using multi-modal input. While spatial control using cues such as depth, sketch, and other images has attracted a lot of research, we argue that another equally effective modality is audio since sound and sight are two main components of human perception. Hence, in this thesis we propose SonicDiffusion to enable audio-conditioning in large scale image diffusion models. Our method first maps features obtained from audio clips to tokens that can be injected into the diffusion model in a fashion similar to text tokens. We introduce additional audio-image cross attention layers which we finetune while freezing the weights of the original layers of the diffusion model. In addition to audio conditioned image generation, our method can also be utilized in conjuction with diffusion based editing methods to enable audio conditioned image editing. We demonstrate our method on a wide range of audio and image datasets. We perform extensive comparisons with recent methods and show favorable performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kanser moleküler özelliklerinin derinlemesine incelenmesini gerektirir. Hedeflenen tedavi stratejileri için kanser evrelerinin ayırt edilmesi esastır. Bu tez, gen ifade profillerini kullanarak erken ve geç evre kanserlerin ayrımına odaklanmıştır. Hesaplama tekniklerinin tıbbi araştırmalarla bütünleşmesiyle, makine öğrenimi modelleri bu görevde üstün başarı göstermektedir ve biyolojik mekanizmalara dair içgörüler sunmaktadır. Bu içgörüleri kullanabilmek için, seyreklik teşvik eden öncüllerle Bayesian Sinir Ağları (BNN) adında yeni bir yaklaşım öneriyoruz. Önerilen seyrek BNN'ler, kanser evrelerini belirlemede yüksek tahmin performansı sunarken yüksek düzeyde yorumlanabilirlik sağlamak için tasarlanmıştır. Seyrek BNN modellerimizi değerlendirmek için, onları 15 farklı kanser kohortunda üç makine öğrenimi algoritması ile karşılaştırdık. Çalışmamızın sonuçları, seyrek BNN modellerimizin geleneksel referans modellerle karşılaştırılabilir tahmin performanslarına ulaştığını göstermektedir. Ayrıca, sinir ağlarının tıpta kara kutu sorununu ele alıyoruz. Bu sorun, hangi girdi özelliklerinin tahminler için kritik olduğunu belirsiz bırakır ve önemli sonuçları olan karar verme sürecinde ciddi bir sorundur. Bu sorunu ele almak için, ana katkımız veri yorumlanabilirliğini önemli ölçüde artıran yeni bir BNN mimarisi geliştirmek olmuştur. Yaklaşımımızda, üç tür seyreklik teşvik eden öncülleri entegre ettik: Laplace, Student t ve Sivri ve Taban. Her bir öncülün sıfır ortalaması ve düşük varyansı vardır, bu da bağlantıların azaltılmasını teşvik eder ve böylece odaklanmış bir özellik seçimi sürecini mümkün kılar. Bu metodoloji, en etkili gen ifadelerini belirlememize ve üzerine odaklanmamıza olanak tanır. Analizimiz, seyrek BNN'lerin belirli gen setlerine belirgin bir tercih gösterdiğini ortaya koymaktadır. Sonuç olarak, seyrek BNN'lerin geliştirilmesi, kanser araştırmaları alanını anahtar gen yollarını aydınlatarak ve kanser evreleme sürecini önemli ölçüde iyileştirerek biyolojik olarak bilgilendirici ve yorumlanabilir bir araç sunar.","Cancer requires an in-depth exploration of its molecular characteristics. For targeted treatment strategies, distinguishing cancer stages is essential. This thesis has focused on the differentiation of early- and late-stage cancers using gene expression profiles. With the integration of computational techniques into medical research, machine learning models excel in this task, offering insights into biological mechanisms. To harness these insights, we proposed a novel approach, which is Bayesian Neural Networks (BNNs) with sparsity-inducing priors. The proposed sparse BNNs are designed to deliver high predictive performance in identifying cancer stages while maintaining a high level of interpretability. To evaluate our sparse BNN models, we benchmarked them against three machine learning algorithms across 15 different cancer cohorts. The results of our study revealed that our sparse BNN models achieve predictive performances comparable to traditional benchmark models. Additionally, we addressed the black-box issue of neural networks in medicine, which obscures which input features are crucial for predictions, a serious issue in decision-making with significant implications. To address this issue, our primary contribution has been the development of a novel BNN architecture that considerably enhances data interpretability. In our approach, we have integrated three types of sparsity inducing priors, namely, Laplace, Student's t, and Spike-and-Slab. Each prior has a mean of zero and low variance, promoting a reduction in connections and thus enabling a focused feature selection process. This methodology allows us to identify and concentrate on the most influential gene expressions. Our analysis revealed that sparse BNNs show a distinct preference for specific gene sets. In conclusion, the development of sparse BNNs offers a biologically informative and interpretative tool, enhancing the field of cancer research by shedding light on key gene pathways and significantly improving the process of cancer staging."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yaratıcılık, yeni fikirler üretebilme yeteneğidir. Ancak yaratıcı kişiler dahi sıradan fikirlere takılıp kalabilir. Bu takılıp kalma halinden kurtulmak ve yeni fikirler bulmak için görsel tasarım uzmanları akla gelen ilk fikri çizmeyi önerirler. Zira çizimler soyutlama gerektirir ve soyutlanmış temsiller çok anlamlı yorumlanmaya açıktır. Çok anlamlı şekilleri yorumlamak ve yeniden yorumlamak ise klişelerden kurtulmaya yardımcı olur. Üstelik çok anlamlı görsellerin başka anlamlarının keşfedilmesi yaratıcı düşünmeyi tetikler. Bu çalışma tasarım deneyimi olmayan kullanıcılara görsel tasarım konusunda yardımcı olmak ve onları daha yaratıcı çözümler bulma konusunda desteklemek amacıyla hazırlanmıştır. Çizim Temelli Yaratıcılık Asistanı, kullanıcıların çizimlerini farklı şekillerde yorumlayabilen ve takılıp kalma durumunu yok eden akıllı bir bilgisayar sistemidir. Bu sistem, çok anlamlı çizimleri ve onlara atfedilmiş anlamları birbirine bağlayan, kitle kaynak çalışmaları ile oluşturulmuş Sketch Net adlı bir anlamsal ağ üzerine kurulmuştur. Çalışmamızın katkıları şu şekildedir: 1) Çok anlamlı çizimleri uygun anlamlarla ilişkilendiren Sketch Net, 2) çok anlamlı çizim oluşturma yöntemi ve 3) kullanıcının çizimini Sketch Net'teki çok anlamlı görsellerle ilişkilendirerek yeniden yorumlayabilen, bu sayede de yaratıcı süreçleri destekleyen akıllı bir bilgisayar sistemi. Çizim Temelli Yaratıcılık Asistanı'nın yaratıcılığı etkin şekilde arttırıp arttıramadığı bir kullanıcı çalışmasıyla test edilmiştir. Sonuçlar asistanın son üründeki fikirsel değerini kayda değer miktarda geliştirdiğini göstermiştir.","Creativity is the ability to come up with new ideas. Yet, even creative people may find themselves stuck with overused ideas, a phenomenon known as 'design fixation.' To overcome the fixation and discover fresh ideas, experts in visual design suggest sketching out the initial idea. Sketches, being abstract representations, introduce an element of ambiguity. Exploring and reinterpreting these ambiguous shapes helps break away from cliches and foster creative thinking. In our project, we introduce a computerized intelligent system that leverages sketching to assist non-designers in visual design tasks. This system assists users in reinterpreting their sketches creatively. To accomplish this, we've developed a unique semantic network called the 'Sketch Net,' which connects ambiguous sketches to multiple crowdsourced interpretations. Our contributions include 1) Sketch Net, 2) a method for generating ambiguous sketches, and 3) an intelligent system that provides diverse interpretations of the user's sketch to enhance the creative process by associating it with Sketch Net elements. The effectiveness of this assistant in boosting creativity is evaluated through a user study. The results show that the Sketch-Based Creativity Assistant significantly improves the quality of the final product."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsanlar, başta gözlemler olmak üzere deneyimler yoluyla dili dünyaya dayandırmayı öğrenirler. İnsanlara benzer şekilde akıl yürütebilen doğal dil işleme (NLP) yaklaşımları geliştirmek, yapay zeka topluluğunun uzun süredir devam eden bir hedefidir. Son zamanlarda, dönüştürücü modeller çok sayıda NLP görevinde kayda değer performans ortaya koymuştur. Bunu, görüntü altyazılama ve görsel soru yanıtlama gibi, dili görsel dünyaya bağlamayı gerektiren görü-dil (V&L) görevlerindeki atılımlar izledi. Dönüştürücü modellerin bu başarıları, V&L topluluğunu, özellikle zamansal ve sağduyulu akıl yürütme gibi daha zorlu yönleri takip etmeye yönlendirmiştir. Bu tez, zamansal muhakeme, sağduyulu muhakeme ya da her ikisini aynı anda gerektiren V&L problemlerine odaklanmaktadır. Zamansal akıl yürütme, zaman içinde akıl yürütme yeteneğidir. V&L bağlamında bu, durağan görüntülerin ötesine geçmek, yani videoları işlemek anlamına gelmektedir. Sağduyulu muhakeme, bizi çevreleyen dünya hakkındaki örtük genel bilgiyi yakalamayı ve bu bilgiyi belirli bir içerik dahilinde kullanarak doğru bir yargıya varmayı gerektirir. Bu tez, zamansal ve sağduyulu muhakemenin çeşitli yönlerini araştırarak dil ve görüyü birbirine bağlayan dört farklı çalışmadan oluşmaktadır. Bu zorlu yönlere geçmeden önce, (i) ilk olarak konumlandırma aşamasına odaklanılmaktadır: Dil koşullandırmasının aşağıdan yukarıya ve yukarıdan aşağıya görsel işleme dallarını nasıl etkilemesi gerektiğinin sistematik olarak değerlendirilmesini sağlayan bir modelle çalışılmıştır. Aşağıdan yukarıya olan dalın dile koşullanmasının, renkler ve nesne kategorileri gibi görsel kavramları temellendirmek için çok önemli olduğunu gösterilmiştir. (ii) Sonrasında, mevcut video-dil modellerinin karmaşık dinamik sahnelerle ilgili soruları yanıtlamada başarılı olup olmadığı araştırılmıştır. Test ortamı olarak CRAFT veri kümesi tercih edilmiş ve son teknoloji video-dil modellerinin dinamik sahneleri yetkin bir şekilde işleyemeyerek büyük bir farkla insan performansının gerisinde kaldığı gösterilmiştir. (iii) Üçüncü çalışmada, önceden eğitilmiş video-dil modellerinin dil anlama yeteneklerini değerlendirmek için sıfır atış video-dil değerlendirme ölçütü geliştiriyoruz. Yapılan deneyler, mevcut video-dil modellerinin, günlük dinamik eylemlerin işlenmesinde girdi olarak statik görüntüleri işleyen görme-dil modellerinden daha iyi olmadığını ortaya koymaktadır. (iv) Son çalışmada, örtmece algılama adı verilen mecazi bir dil anlama problemi üzerinde çalışılmıştır. Örtmeceler, hassas veya hoş olmayan konularla ilgili ifadeleri yumuşatır. Örtmece terimlerin müphem doğası, sağduyu bilgisinin ve sağduyulu muhakemenin gerekli olduğu bir durumda gerçek anlamlarının tespit edilmesini zorlaştırmaktadır. Düşük kaynaklı ortamlarda ek metinsel ve görsel bilginin dahil edilmesinin örtmece terimlerin tespit edilmesinde faydalı olduğunu gösterilmiştir. Bununla birlikte, bu dört çalışma ile ilgili elde edilen bulgular, mevcut V&L modellerinin yetenekleri ile insan muhakemesi arasında hala ciddi bir uçurum olduğunu göstermektedir.","Humans learn to ground language to the world through experience, primarily visual observations. Devising natural language processing (NLP) approaches that can reason in a similar sense to humans is a long-standing objective of the artificial intelligence community. Recently, transformer models exhibited remarkable performance on numerous NLP tasks. This is followed by breakthroughs in vision-language (V&L) tasks, like image captioning and visual question answering, which require connecting language to the visual world. These successes of transformer models encouraged the V&L community to pursue more challenging directions, most notably temporal and commonsense reasoning. This thesis focuses on V&L problems that require either temporal reasoning, commonsense reasoning, or both simultaneously. Temporal reasoning is the ability to reason over time. In the context of V&L, this means going beyond static images, i.e., processing videos. Commonsense reasoning requires capturing the implicit general knowledge about the world surrounding us and making an accurate judgment using this knowledge within a particular context. This thesis comprises four distinct studies that connect language and vision by exploring various aspects of temporal and commonsense reasoning. Before advancing to these challenging directions, (i) we first focus on the localization stage: We experiment with a model that enables systematic evaluation of how language-conditioning should affect the bottom-up and the top-down visual processing branches. We show that conditioning the bottom-up branch on language is crucial to ground visual concepts like colors and object categories. (ii) Next, we investigate whether the existing video-language models thrive in answering questions about complex dynamic scenes. We choose the CRAFT benchmark as our test bed and show that the state-of-the-art video language models fall behind human performance by a large margin, failing to process dynamic scenes proficiently. (iii) In the third study, we develop a zero-shot video-language evaluation benchmark to evaluate the language understanding abilities of pretrained video-language models. Our experiments reveal that the current video-language models are no better than the vision-language models, processing static images as input in processing daily dynamic actions. (iv) In the last study, we work on a figurative language understanding problem called euphemism detection. Euphemisms tone down expressions about sensitive or unpleasant issues. The ambiguous nature of euphemistic terms makes it challenging to detect their actual meaning within a context where commonsense knowledge and reasoning are necessities. We show that incorporating additional textual and visual knowledge in low-resource settings is beneficial to detect euphemistic terms. Nonetheless, our findings on these four studies still demonstrate a substantial gap between current V&L models' abilities and human cognition."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Öğrencilerin sınıf-içi angajmanlarını (ilgi ve motivasyon) sürdürmeleri eğitimlerinin başarısı için büyük önem taşımaktadır. Programlama eğitimindeki güncel örnekler, fiziksel etkileşim içeren aktiviteleri teşvik etmenin ve programlamanın gerçek hayattaki iyi örneklerini göstermenin, öğrencilerin angajmanını artırmak için etkili yöntemler olduğunu göstermiştir. Ancak, sosyo-ekonomik olarak dezavantajlı bölgelerde bulunan okullar genellikle bu tür etkileşimi destekleyecek yeterli dijital altyapıya sahip değildir. Buna karşın, akıllı telefon gibi mobil cihazların uygun maliyetleri ile yaygınlaşması, programlama eğitiminin erişilebilirliğini artırma potansiyeline sahiptir. Ek olarak, mobil cihazlar üzerinde çalışabilecek yapay zeka modelleri, işbirlikli programlama aktivitelerinde kullanılabilecek fiziksel objelerin tanınmasını ve kullanılmasını sağlayabilir. Bu bağlamda, araştırmamın ilk hedefi, öğrencilerin işbirliği içinde çalışmasını destekleyen mobil cihazlar kullanılarak uygun maliyetli bir fiziksel programlama eğitim ortamı geliştirmektir. Sınıf içi angajmanı destekleyecek bu araçları oluşturmanın yanı sıra, sınıf içindeki angajmanı analiz etmek ve yorumlamak da öğretim sürecinin önemli bir bileşenidir. Sınıf içi angajmanın analizi, duygusal, davranışsal ve bilişsel durumların çok bilesenli bir değerlendirmesini gerektirir. Ancak, çok kipli bir sınıf içi angajman veri seti ve analiz modeli oluşturmak için sınırlı sayıda araştırma yapılmıştır. Bu boşluğu doldurmak için ikinci hedefim, grup etkinliklerinde öğretmenlerin iş yükünü hafifletmek için tek bir kamera kullanarak çok kipli bir angajman değerlendirme aracı oluşturmaktır. Bu tez, İnsan-Bilgisayar Etkileşimi (HCI), Yapay Zeka (AI) ve eğitim teknolojisi araştırma alanlarına altı açıdan katkıda bulunmaktadır: 1. İteratif kullanıcı çalışmalarından elde edilen bilgilerle şekillenen, kullanıcı odaklı, uygun maliyetli bir akıllı fiziksel programlama ortamı geliştirilmiş ve tüm kaynakları açık-kaynak olarak sunulmuştur. 2. Geniş katılımlı kullanıcı deneyimi çalışmaları gerçekleştirilmiş ve geliştiriciler için akıllı fiziksel programlama ortamları geliştirmek için bir dizi tasarım önerisi sunulmuştur. 3. Öğretmenlerin programlama ortamımızı MEB Programlama müfredatı kapsamında kullanabilmeleri için bir dizi aktivite geliştirilmiştir. 4. Öğrencilerin kendi değerlendirme puanları kullanılarak sınıf içi angajman düzeylerini tahmin etmek için yaklaşık sekiz saatlik sesli ve görsel verilerden oluşan yeni bir çok kipli veri seti toplanmış ve açık kaynak olarak sunulmuştur. 5. Angajmanı çok bilesenli olarak ele alan çok kipli makine öğrenme modelleri geliştirilmiştir. Görüntü modelleri, kişiye dayalı ilgi düzeyi tahmininde %84'e kadar test doğruluk oranı elde etmiştir. Akış halindeki videolarda çalışabilen gerçek zamanlı video modeli ise %71 test doğruluk oranına ulaşmıştır. 6. Öğrencilerin angajmanlarını zaman içinde görüntülemelerine ve model çıktılarını yorumlamalarını destekleyecek, öğrenci odaklı etkileşimli bir kontrol paneli geliştirilmiştir.","Ensuring students remain engaged in the classroom is crucial for their success in any given topic. In programming education, promoting hands-on interactions and demonstrating real-world use cases are effective methods to foster engagement. However, schools located in socio-economically disadvantaged areas often lack adequate digital infrastructure, such as computer laboratories, to support such engagement-building tasks. Nevertheless, utilizing mobile devices can support programming education due to their availability and affordability. Additionally, mobile devices can help augment the tangible materials to use in collaborative programming experiences. In this respect, the first goal of my research is to develop an affordable tangible programming education environment using mobile devices that supports the collaborative work of students. On top of building tools to foster engagement, analyzing and interpreting student engagement is also a critical component of the teaching process. Analyzing classroom engagement requires a multi-component evaluation of affective, behavioral, and cognitive states. Yet, limited research has been conducted to create a multimodal classroom engagement dataset and analysis model. To fill this gap, my second goal is to build a multimodal engagement evaluation tool using a single camera to ease teachers' workload in group activities. Overall my thesis contributes to Human-Computer Interaction (HCI), Artificial Intelligence (AI), and educational technology research areas with six research outputs: 1. An open-source, user-centered, AI-powered, affordable tangible programming environment that was informed by iterative user studies. 2. A set of design considerations for developing paper-based intelligent tangible programming environments which are informed by iterative and classroom-wide user experience studies. 3. A set of curricular activities to help teachers adapt our programming environment into Turkish national curricula. 4. An open-source audio-visual dataset comprising eight-hour-long video recordings of thirty-three students to predict classroom engagement levels using their self-evaluation scores. 5. Multimodal machine-learning models that can address the multi-component definition of engagement. The image models achieved up to 84% test accuracy on person-based engagement level prediction. The real-time video model that can run on streaming videos achieved 71% test accuracy. 6. A student-centric interactive dashboard to help students view their engagement over time and interpret the results of engagement model prediction."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otonom sürüş, karmaşık şehir ortamlarındaki etkileşimler nedeniyle belirsizlik altında karar verme gerektiren zorlu bir görevdir. Sürüş etmenine, etmenin gerçekleştirdiği eylemin uygunluğunu ölçen geri bildirim sağlayacak şekilde hedefler tasarlıyoruz. Önceki çalışmalar, olası eylemlerle planlama yapmak üzere otoyol sürüşü için türevlenebilir bir maliyet fonksiyonu tanıtır. Ancak şehir içi sürüş, araç yoğunluğu ve kentsel ortamlardaki kavşaklar gibi sahne yapıları nedeniyle otoyolda sürüşten daha karmaşıktır. Bu amaçla, şehir içi sürüşte sıklıkla meydana gelen birçok olası özelliği hesaba katan yeni bir türevlenebilir maliyet fonksiyonu öneriyoruz. Gelecek tahmini için, etmen ve çevrenin gelecekteki durumunu ayrı ayrı tahmin eden ileri modeller öneriyoruz. Etmen (ego) ve çevre (dünya) için ayrı tahminler aracılığıyla alan bilgisini çerçevemize entegre edebiliriz. Ampirik sonuçlar, ego etmeninin geleceğini güvenilir bir şekilde tahmin edebildiğimizi gösterirken dünya modeli, sola veya sağa dönme gibi daha az sıklıkta gerçekleşen durumlarda sınırlamalar sergiliyor. Türevlenebilir maliyet fonksiyonumuzu, gradyan tabanlı model öngörülü kontrol (MÖK) ve bir politika ağıyla test ediyoruz. Her iki yaklaşım da şehir içi sürüş düzeninde benzersiz sınırlamalara sahip makul bir sürüş performansı sergiliyor. MÖK, farklı durumlarda hiper parametrelere duyarlılığı nedeniyle yerel optimum sorunlarıyla karşı karşıyadır ve politika ağı bir uzmanın denetimine bağlıdır. Sonuç olarak, bu çalışmada şehir içi sürüş için türevlenebilir bir maliyet fonksiyonu tasarladık. Aynı zamanda gelecekteki çalışmaların, çalışmamızı temel alarak çeşitli şehir içi senaryolara genelleme yapmanın sınırlamalarını ve zorluklarını ortaya koyduk.","Self-driving is a challenging task that requires decision-making under uncertainties due to interactions in complex urban environments. We design objectives to provide the driving agent with feedback quantifying the eligibility of the action taken by the agent. For planning with possible actions, previous work introduces a differentiable cost function for highway driving. However, urban driving is more complex than highway driving due to the density of vehicles and scene structures like intersections in urban environments. To this end, we propose a new differentiable cost function that accounts for multiple possible features that frequently occur in urban driving. For future prediction, we propose forward models that predict the future state of the agent and the environment separately. Via separate predictions for the agent (ego) and environment (world), we can integrate domain knowledge into our framework. Empirical results demonstrate that we can reliably predict the future for the ego agent, while the world model exhibits limitations in less frequent cases, such as turning left or right. We further test our differentiable cost function with a gradient-based model predictive control (MPC) and a policy network. Both approaches demonstrate a plausible driving performance in urban driving setup with unique limitations. MPC faces local optima problems due to its sensitivity to the hyper-parameters under different situations, and the policy network depends on supervision from an expert. In conclusion, we design a differentiable cost function for urban driving and show its importance for action as well as limitations for future work to generalize to diverse urban scenarios by building on our work."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizgi romanlar, hikayeleri ve fikirleri sıralı görseller aracılığıyla ileten benzersiz ve çok-kipli bir görsel iletişim aracıdır. Görsellere genellikle diyalog ve anlatı metinleri eşlik eder. Çizgi romanların karmaşık görsel dilinde, farklı yazarlar, kültürler, zaman dönemleri, teknolojiler ve sanatsal tarzlar arasında değişiklikler görülür. Bu nedenle çizgi romanların hesaplamalı analizi, temel bilgisayarlı görü ve doğal dil işleme yöntemlerinin kullanılmasını gerektirir. Bu tezde, çizgi romanların benzersiz çoklu modalitesini kullanarak nöral çizgi roman anlayışını artırmayı amaçlıyorum ve bunu yaparken çizgi romanları karakter merkezli bir yaklaşım ile işlemeyi hedefliyorum. Kamuya açık olan Amerikan Çizgi Romanlarının Altın Çağı dönemine ait çizgi romanlar ile çalışmak hem döneme ait çizgi roman sayısının fazlalığından hem de telif hakları uygunluğundan dolayı seçilmiştir. Ancak, etiketli veri bu alanda oldukça sınırlıdır. Bu nedenle amacıma ulaşmak için, veri kümesi oluşturma, çizgi romanlar için yeni görevler ve mimariler önerme gibi çalışmalar içeren dört temel adımdan oluşan bütünsel bir yaklaşım benimsedim. İlk adım, Optik Karakter Tanıma (OCR) modelleri kullanarak konuşma balonları ve anlatı kutusu görüntülerinden yüksek kaliteli metin verisi çıkarmayı içerir. İkinci adımda, Çok Görevli Öğrenme (MTL) modelini iyileştirme yoluyla çizgi roman sayfalarını bileşenlerine ayırmayı başardım. Bu bileşenler şunlardır: panel, konuşma balonu, anlatı kutusu, karakter yüzü ve vücudunu tespit etmek, konuşma balonu ve panel segmente etmek. Ayrıca MTL modelinin görevlerinden birisi de konuşma balonlarını karakterlerin yüz ve vücutlarıyla ilişkilendirmektir. Üçüncü adımda ise, önceki aşamadan elde edilen karakter yüzlerini ve vücutlarını birleştirerek tekil karakterler bulunur ve karakterleri sıralı paneller arasında yeniden tanımlamanır ve takip edilir. Bu üç adım, çizgi roman panelini bulma, bileşenlerini tanımlama ve karakter kimliklerini diyalog benzeri bir yapıya dönüştürme imkanı sağlamaktadır. Dolayısıyla tezin son adımında ise, önceki yapının faydalarından yararlanmak için multimodal ComicBERT modelini geliştirdim. viComicBERT'in içerik anlama yeteneklerini değerlendirmek için cloze tarzı görevleri kullandım. Ayrıca, Scene-Cloze adını verdiğim yeni bir görev öneriyorum. Sonuç olarak, yaklaşımım, özellikle metin ve visual cloze görevlerinde %69.5 ve %77.1'lik doğruluklar elde ederek insan seviyesine yaklaşıyor. Genel olarak, katkılarım şunlardır: 1. COMICS Text+ Dataset adını taşıyan iki milyondan fazla konuşma balonu ve anlatı kutusunun transkriptini içeren bir veri kümesi oluşturuldu ve paylaşıldı. Ayrıca, metin algılama ve tanıma modellerini açık kaynak olarak eğitimlerinde kullandığım etiketli veri setleriyle birlikte paylaşıldı. 2. Algılama, segmentasyon ve ilişkilendirme görevleri için bir MTL model'ini her yönüyle daha iyi hale getirerek, çizgi roman karakter yüzü ve vücudu ile konuşma balonu ilişkilendirme görevinde SOTA sonuçlar elde edildi. 3. Birleşik ve kimlik uyumlu çizgi roman karakter özellik vektörleri ve kimlik temsilleri üretmek için Çizgi Roman Karakterinin Yeniden Tanımlanması için Kimliğe Duyarlı Yarı Denetimli Öğrenme yapısını öne sürüldü. Ayrıca, öz denetim aşamasında kullanılan veri kümesini, Comic Character Instances Dataset, oluşturdum ve yarı izleme aşamasında kullanılan dörtlü ardışık çizgi roman panellerinin içindeki kimlik etiketlerini içeren Comic Sequence Identity Dataset'i derlendi. 4. Sıralı panelleri ve bileşenlerini işleyebilen bir transformer-kodlayıcı mimarisi olan multimodal Comicsformer tanıtıldı. Çizgi romanlar için yeni, kendi kendini denetleyen bir ön eğitim stratejisi olan Masked Comic Modeling (MCM) görevi için omurga görevi görür ve sonuçta çizgi romanlar için potansiyel bir Foundation model olan ComicBERT ortaya çıkar. ComicBERT, cloze tarzı görevlerde, özellikle metin cloze ve görsel cloze görevlerinde, insan düzeyinde kavramaya yaklaşan SOTA performansına ulaşmıştır.","Comics are a unique and multimodal medium that conveys stories and ideas through sequential imagery often accompanied by text for dialogue and narration. Comics' elaborate visual language exhibits variations from different authors, cultures, periods, technologies, and artistic styles. Consequently, the computational analysis of comic books requires addressing fundamental challenges in computer vision and natural language processing. In this thesis, I aim to enhance neural comic book understanding by making use of comics' unique multimodal nature and processing comics in a character-centric approach. The primary data source for this thesis is the Golden Age of American Comics due to its public accessibility and abundance of comic series. However, the availability of annotated data is limited. Thus, to achieve my goal, I have adopted a holistic approach composed of four main steps ranging from curating datasets to proposing novel tasks and architectures for comics. The first three steps aim to create a machine-readable comics database by locating comic book panels, identifying their components, and transforming character identities into a dialogue-like structure and the final step uses this database to train a transformer- based model. The first step involves extracting high-quality text data from speech bubbles and narrative box images using OCR models. I decompose comic pages into their constituent components in the second step through detection, segmentation, and association tasks with a refined Multi-Task Learning (MTL) model. Detection involves identifying panels, speech bubbles, narrative boxes, character faces, and bodies. Segmentation focuses on isolating speech bubbles and panels, while the association task involves linking speech bubbles with character faces and bodies. In the third step, I utilize the paired character faces and bodies obtained from the previous stage to create character instances and, subsequently, reidentify and track these instances across sequential panels. In the final step of my thesis, I propose a multimodal framework by introducing the ComicBERT model, which exploits the abovementioned structure. Cloze-style tasks were used to evaluate ComicBERT's contextual understanding capabilities. Furthermore, I propose a new task called Scene-Cloze, which predicts the next panel given n previous panels as context. As a result, my approach achieves a new state-of-the-art performance in Text-Cloze and Visual-Cloze tasks with accuracies of 69.5% and 77.1%, respectively, thus getting closer to the human baseline. Overall, the highlights of my contributions are as follows: 1. I curated and shared COMICS Text+ Dataset with over two million transcrip- tions of textboxes from the golden age of comics. In addition, I open-sourced the text detection and recognition models that are fine-tuned for the task and datasets used in their training. 2. I refined a MTL framework for detection, segmentation, and association tasks and achieved SOTA results in comic character face and body-to-speech bubble association tasks. 3. I proposed a novel Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification framework to generate unified and identity-aligned comic character embeddings and identity representations. Furthermore, I generated two new datasets: the Comic Character Instances Dataset, encompassing over a million character instances used in the self-supervision phase, and the Comic Sequence Identity Dataset, containing annotations of identities within sets of four consecutive comic panels used in semi-supervision phase. 4. I introduced the multimodal Comicsformer, a transformer-encoder architecture capable of processing sequential panels and their constituents. It serves as the backbone for the Masked Comic Modeling (MCM) task, a novel self- supervised pre-training strategy for comics, resulting in ComicBERT, a potential foundation model for golden age comics. ComicBERT achieves SOTA performance in cloze-style tasks, particularly in text-cloze and visual-cloze tasks, approaching human-level comprehension."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda GPU'lar, modern yüksek performanslı sistemlerde önde gelen hızlandırıcı haline gelmiştir ve bu nedenle HPC hesaplama gücünün büyük bir kısmı GPU kümelemelerine odaklanmıştır. Çoklu GPU hızlandırma kullanımı, birçok HPC ve Makine Öğrenmesi uygulamasına büyük hesaplama avantajları getirmiştir. Ancak GPU'lar arasında, hem düğümler içinde hem de aralarında iletişim kurma ihtiyacı, uygulama ölçeklendirmesini engelleyen bir darboğaz haline gelebilir. Bunun önemli bir nedeni, geleneksel olarak iletişimin ana bilgisayar üzerinden yönetilmesidir. Tipik bir çoklu GPU uygulamasında ana bilgisayar, çekirdekleri başlatarak, iletişim çağrıları yaparak ve cihazlar için bir senkronizasyon sağlayarak yürütümü yönetir. Bu, yürütümün kritik yolunda CPU'nun dahil olması, gereksiz bir iş yükü oluşturur ve çoklu GPU iletişimi içeren uygulamalarda performansı artırmak için cihazlara tamamen devredilebilir. İlk olarak, tek düğümlü çoklu GPU uygulamaları için tamamen otonom bir yürütüm modeli sunuyoruz, bu da başlangıçta çekirdek başlatma dışında CPU'nun dahil edilmediği anlamına gelir. Önerilen CPU'suz yürütüm modelinde, mevcut teknikleri, kalıcı çekirdekler, iş parça özelleştirme, cihaz tarafından başlatılan bariyerler ve cihaz tarafından başlatılan haberleşme çağrıları gibi teknikleri kullanarak tamamen otonom çoklu GPU kodu yazmak ve iletişim üzerinde önemli ölçüde azaltılmış bir iş yükü sağlamak için kullanıyoruz. Önerilen modelimizi, geniş kullanıma sahip iki farklı türe sahip Conjugate Gradient (CG) çözücüsünün, Standart CG ve Pipelined CG'nin üzerinde gösteriyoruz. CPU tarafından kontrol edilen yöntemlerle karşılaştırıl\-dığında, CPU'suz model, 8 NVIDIA A100 GPU'sunda Standart CG ve Pipelined CG için sırasıyla 1.54x ve 1.63x hızlanma sağlar. Tezin ikinci kısmında, geleneksel çoklu GPU iletişim modellerinin eksikliklerine yanıt olarak önerilen GPU-odaklı iletişimi kapsamlı bir şekilde incelemekteyiz. Genel olarak, bu ilerlemeler, yürütümün kritik yolundaki CPU'nun dahilini azaltmakta, GPU'ya iletişimi başlatma ve senkronize etme konusunda daha fazla özerklik sağlamakta ve çoklu GPU iletişimi ile hesaplama arasındaki anlamsal uyumsuzluğu gidermektedir. Bu tezde GPU-odaklı iletişimi sınıflandırıyor, temel yöntemleri özetliyor ve faydaları ve zorlukları da içeren en önemli özellikleri üzerinde duruyoruz.","In recent years, GPUs have become the leading accelerator in modern high-performance systems such that much of HPC computational capability has concentrated in clusters of GPUs. Using multi-GPU acceleration has brought great computational benefits to many HPC and ML applications. However, the need to communicate between GPUs, both within and across nodes, can quickly become a bottleneck that hinders application scaling. A significant reason for this is that traditionally communication has been mediated through the host. In a typical multi-GPU application, the host orchestrates execution by launching kernels, issuing communication calls, and acting as a synchronizer for devices. This CPU involvement in the critical path of execution causes undue overhead and can be delegated entirely to devices to improve performance in applications that involve multi-GPU communication. We first present a fully autonomous execution model for single-node multi-GPU applications that completely excludes the involvement of the CPU beyond the initial kernel launch. For the proposed CPU-free} execution model, we leverage existing techniques such as persistent kernels, thread block specialization, device-side barriers, and device-initiated communication routines to write fully autonomous multi-GPU code and achieve significantly reduced communication overheads. We demonstrate our proposed model on two variants of the broadly used Conjugate Gradient (CG) solver, Standard CG, and Pipelined CG. Compared to the CPU-controlled baselines, the CPU-free model provides a 1.54x and 1.63x speedup for Standard and Pipelined CG, respectively, on 8 NVIDIA A100 GPUs. In the second part of the thesis, we conduct an extensive survey of GPU-centric communication, communication mechanisms proposed in response to the deficiencies of traditional multi-GPU communication models. At a high level, these advancements reduce the CPU's involvement in the critical path of execution, give the GPU more autonomy in initiating and synchronizing communication and fix the semantic mismatch between multi-GPU communication and computation. We chart out the landscape of GPU-centric communication, summarize the main methods and expound on their most salient features, including associated benefits and challenges."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kanser araştırmalarını ilerletmek ve hasta bakımını iyileştirmek, büyük ölçüde kanser ilerledikçe biyolojik mekanizmaların nasıl değiştiğinin anlaşılmasına bağlıdır. Tümörlerdeki aktif biyolojik süreçlerin belirlenmesi, hedefe yönelik tedavilerin ve erken teşhis ve prognoz için potansiyel biyobelirteçlerin geliştirilmesi açısından çok önemlidir. Bu tezde, gen ekspresyon profillerini kullanarak güçlü ve yorumlanabilir bir model geliştirme amacıyla erken evre ve geç evre kanserleri ayırt etmeye odaklandık. Yapay öğrenme yöntemleri, gizli kalıpları ve kestirimci özellikleri ortaya çıkarmak için geniş genomik ve klinik verilerin analiz edilmesini sağlayarak kanser araştırmalarında umut vaat etmektedir. Çalışmamız, çoklu çekirdek öğrenimi bağ\-lamında seyrek bir çözüm oluşturmak için çok katmanlı bir perseptron modelinin yeteneklerinden yararlanmaya odaklandı. Bu, modelimizin gen ifade profillerine dayalı olarak erken evre ve geç evre kanserler arasında yetkin bir ayrım yapmasını sağladı. Modelimiz sadece yüksek tahmin performansı sunmakla kalmadı, aynı zamanda kanserin ilerlemesini yönlendiren önemli genler ve yolaklar hakkında değerli bilgiler sağladı. Çok katmanlı perseptron modelimizi değerlendirmek için, onu üç köklü yapay öğrenme algoritmasıyla karşılaştırdık: rassal orman, destek vektör makinesi ve çoklu çekirdek öğrenimi. Dikkat çekici bir şekilde, modelimiz 15 kanser kohortunda alıcı işletim karakteristik eğrisi altındaki alan ile ölçüldüğü üzere tutarlı bir şekilde daha iyi veya karşılaştırılabilir kestirim performansı elde etti. Bu bulgular, çok katmanlı perseptron modelimizin, kanserin ilerlemesini yönlendiren kritik genleri ve yolakları etkili bir şekilde tanımladığını ve erken evre ve geç evre kanser sınıflandırmasına ilişkin değerli bilgiler sunduğunu göstermektedir.","Advancing cancer research and improving patient care heavily rely on understanding how biological mechanisms change as cancer progresses. Identifying the active biological processes within tumors is crucial for developing targeted treatments and potential biomarkers for early detection and prognosis. In this thesis, we focused on distinguishing between early-stage and late-stage cancers using gene expression profiles, aiming to develop a powerful and interpretable model. Machine learning methods have shown promise in cancer research by enabling the analysis of vast genomic and clinical data to uncover hidden patterns and predictive features. Our work centered around harnessing the capabilities of a multilayer perceptron (MLP) model to create a sparse solution within the context of multiple kernel learning (MKL). This enabled our model to proficiently differentiate between early-stage and late-stage cancers based on gene expression profiles. Our model not only offered high predictive performance but also provided valuable insights into the crucial genes and pathways driving cancer progression. To evaluate our MLP model, we benchmarked it against three well-established machine learning algorithms: random forest, support vector machine, and MKL. Remarkably, our model consistently achieved better or comparable predictive performance, as measured by the area under the receiver operating characteristic curve, across 15 cancer cohorts. The findings demonstrate that our proposed MLP model effectively identifies critical genes and pathways driving cancer progression, offering valuable insights into early-stage and late-stage cancer classification."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Büyük dil modelleri, birçok doğal dil işleme görevinde başarılıdır. Bu çalışmanın ilk kısmında, uzun hikaye ve romanlardaki karakterlerin bulunduğu mekanların tespit edilmesi görevinde büyük dil modellerinin performansını ölçüyoruz. Görevde, bir düzyazı parçası ve bu düzyazıdaki bir karakterin bulunduğu mekanı soran bir soru bulunuyor, büyük dil modelinin amacı ise bu soruya doğru cevap vermek. Bu ölçümü yapmak için, yazılardaki karakterleri ve onların bulunduğu mekanları işeretle-yerek iki yeni veri kümesi oluşturduk: Andersen ve Persuasion. Makine öğrenmesi kullanmayan basit bir modelin sonuçlarıyla karşılaştırdığımızda, büyük dil modellerinin bu veri kümeleri üzerinde yetersiz performansa sahip olduğunu gösteriyoruz. Sonuçları iyileştirmek için ""bağlamsal öğrenme"" metodunu da deniyoruz ve sonuçları raporluyoruz. Bunlarla birlikte, büyük dil modellerinin sınırlı girdi uzunlukları tarafından kısıtlanmış olduğu problemini de ele alıyoruz. Hipotezimize göre, eğer karakter-mekan ilişkisi bilgilerinin büyük dil modellerinin hangi aktivasyonlarında yer aldığını tespit edebilirsek, bu aktivasyonları kaydedip daha sonra başka girdilerle çalıştırılan bir büyük dil modeline enjekte ederek o bilgiyle ilgili sorunun, girdi içinde o bilgi doğal dille açıkça bahsedilmemiş olmasına rağmen, doğru cevaplanmasını sağlayabiliriz. Bu aktivasyon yerinin tespiti işi için beş farklı teknik geliştiriyoruz: Büyük dil modeli aktivasyonlarının taşınması ve başka girdilere eklenmesi, model aktivasyonlarına gürültü eklenmesi, model aksivasyonları arasındaki kosinüs benzerliğine bakılması, model aktivasyonların değiştirilmesi ve cevap oluştuturulurken dikkat skorlarının görselleştirilmesi. Bu teknikleri kullanarak yaptığımız gözlemlerimizi raporluyoruz.","Large language models (LLMs) are very proficient in NLP tasks. In the first part of this work, we evaluate the performance of LLMs on the task of finding the locations of characters inside a long narrative. The objective of the task is to generate the correct answer when the input is a piece of a narrative followed by a question asking the location of a character. For the evaluation of the task, we generate two new datasets by annotating the characters and their locations in the narratives: Andersen and Persuasion. We show that the LLM performance is not satisfactory on these datasets when compared to the simple baseline we designed that does not use machine learning. We also experiment with in-context learning to improve the performance and report results. Moreover, we address the problem that the LLMs are limited by the bounded context length. We hypothesize that if we localize the character-location relation information among the activations inside an LLM, we can store those activations and inject them into other models that are run with a different prompt so that the LLM can answer the questions about the information that was carried from another prompt, even though the character and location relation is not mentioned explicitly in the current prompt. We develop five different techniques to localize the character-location relation information occurring in the LLMs: Moving and adding LLM activations to other prompts, adding noise to LLM activations, checking cosine similarity between LLM activations, editing LLM activations, and visualizing attention scores during answer generation. We report the observations we made using these techniques."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özgün duygusal ifadeleri iletmek ve anlamak duygusal hesaplamanın en önemli yönlerinden biridir. Bale, sözsüz iletişimde bahsettiğimiz duygu dinamiklerini keşfetmek için uygun bir ortamdır. Bale sanatçıları koreografinin içsel hislerini seyirciye aktarmak için dans hareketlerini mükemmelleştirmeye çalışırlar. Bale sanatçılarının hareketleri, dans ettikleri müziğin duygusal özelliklerini benimser. Bu hareketler izleyicide performansın etkisini deneyimlemelerine yardımcı olan görsel bir uyarıya sebep olur. Bu tez, bale performansının hoşluk değerlerini sınıflandırmak için bale sanatçısının insan poz tahminlerini kullanan bir kavram kanıtı önermektedir. Önerdiğimiz yöntem, ifade nitelikli pozları ve ahenkli hareketleri müzikten ortaya çıkan hoşluk boyutuyla eşleştirmek için baledeki ses ve görüntü kiplerinin arasındaki eğilimi takip ediyor. Bu amaç ile, bale videolarından insan pozu tespit algoritmaları aracılığıyla çıkartılan 2 boyutlu insan pozu anahtar noktalarını ve derin poz yerleştirmelerini inceliyoruz. Yerleştirmeler daha sonra her videonun sesinden elde edilen düşük ve yüksek hoşluk seviyelerinin ikili sınıflandırmasını sağlamak için LSTM mimarisi ile oluşturulmuş hoşluk sınıflandırıcısına besleniyor. çalışmanın deneysel analizini gerçekleştirmek için çevrimiçi videolardan Ballet 116 veri kümesini oluşturduk. Deneysel çalışmalar, 2 boyutlu insan pozu anahtar noktalarının bale videolarındaki hoşluk tahmini için bir dayanak olarak görüldüğünü, en gelişkin insan pozu tahmin ağları yoluyla elde edilen uzay-zamansal derin poz yerleştirmelerinin kullanımının hoşluk değeri sınıflandırma doğruluğunu önemli ölçüde arttırdığını gösteriyor. Temel hoşluk sınıflandırma ağımızda öz-dikkat mekanizmasının uygulanmasıyla, her yerleştirme türü için doğruluk, karar değerlendirme grafiği, eğri altında kalan alan, ve F1 puanını arttıran bir güçlenme gerçekleşiyor.","One of the key aspects of affective computing is conveying and understanding authentic emotional expressions. To explore the dynamics of emotion in a non-verbal communication environment, ballet serves as an appropriate medium. Ballet performers strive to perfect their dance movements to transmit the choreography's internal sensations to the audience. The ballet performers' movements adopt the emotional characteristics of the music they are dancing to. These movements cause a visual stimulus in the spectator that helps them to experience the performance's affect. This thesis proposes a proof of concept to use human pose estimations of the ballet performer to classify valence values of the ballet performance. The proposed methodology follows the cross-modal bias of audio and vision in ballet to map the expressive poses and harmonious movements to the valence dimension of affection elicited from accompanying music. For this purpose, we investigate 2D human pose keypoints and deep pose embeddings extracted from ballet videos through human pose detection algorithms. The embeddings are then fed into a valence classifier of LSTM architecture to provide a binary classification of low and high valence levels obtained from each video's audio. To perform the experimental analysis of the work, the Ballet 116 dataset is introduced from in-the-wild online videos. Experimental studies show that while 2D human pose keypoints serve as a baseline for the valence estimation in ballet videos, spatio-temporal deep pose embeddings, extracted through a state-of-the-art human pose estimation network, significantly increase the valence classification accuracy. The LSTM valence classification network is further augmented by implementing a self-attention mechanism, increasing each embedding type's accuracy, AUC, and F1-score performances."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Allostery, the process by which binding at one site perturbs a distant site, is being rendered as a key focus in the field of drug development with its substantial impact on protein function. Allosteric drugs activate or inhibit proteins and offer advantages over non-allosteric drugs. However, the identification of allosteric sites is a challenging task due to unavailability of huge dataset, their distance, and lack of conservation across protein structures. A variety of computational techniques have been developed in the past to predict allosteric sites, such as Normal Mode Analysis (NMA), Molecular Dynamics (MD), and Machine Learning (ML), utilizing both static pocket characteristics and the dynamics of proteins; the performance of these methods needs further improvement. This research investigates the potential of incorporating Protein Language Models (pLMs) into ML and/or DL approaches to improve prediction of allosteric residues, based on the fact that the pLMs (e.g., ProtBERT from the family of ProtTrans pLMs: based on BERT architecture) effectively capture the spatial relationship among residues, eventually contributing to identification of allosteric sites/pockets. ProtBERT-BFD (ProtTrans) was fine-tuned on the Allosteric Dataset (ASD) of protein sequences, which predicts the allosteric residues with an F1 score of 61.54% on the test dataset. Several ML and DL approaches were utilized including XGBoost, SVM, AutoML, and GNNs. With the inclusion of fine-tuned pLM features, all of the aforementioned approaches improve the prediction performance of allosteric sites over previous studies by a considerable margin. XGBoost, being the highest performing model in this study, improves the results by combining the features extracted from finetuned ProtBERT with pocket features extracted by FPocket, resulting in an F1 score of 75.76% for allosteric pockets/sites. Case studies have been performed on proteins with known allosteric sites in addition to the case study to predict novel allosteric sites on new proteins.","Allosteri, proteinin bir bölgesindeki değişikliğin, mesela başka bir moleküle bağlanmanın, proteinin uzak bir bölgesini etkilediği süreç olarak tanımlanabilir. Allosteri protein fonksiyonu üzerindeki önemli etkisi sebebiyle ilaç geliştirme alanında önemli bir odak noktasıdır. Allosterik ilaçlar proteinleri aktive veya inhibe edebilir, allosterik olmayan ilaçlara göre avantajlar sunar. Bununla birlikte, allosterik bölgelerin tanımlanması zorlu bir iştir. Geçmişte allosterik bölgeleri tahmin etmek için Normal Mod Analizi (NMA), Moleküler Dinamik (MD) ve Makine Öğrenimi (MÖ) gibi hem statik cep özelliklerini hem de proteinlerin dinamiklerini kullanan çeşitli hesaplama teknikleri geliştirilmi olmakla birlikte bu yöntemlerin performansının daha da geliştirilmesi gerekmektedir. Bu araştırmada, pDM'lerin (örneğin, ProtTrans pDM ailesinden BERT mimarisine dayalı ProtBERT'in) allosterik kalıntıların tahminini iyilrştirmek için Protein Dil Modellerini (pDM'ler), MÖ ve/veya DÖ yaklaşımlarıyla birlikte kullanılma potansiyelini araştırılıyor. Tezde, amino asitler arasındaki mekansal ilişkiyi etkili bir şekilde öğrenerek, sonuçta allosterik alanların/ceplerin tanımlanmasını hedeflenmektedir. ProtBERT-BFD (ProtTrans), test veri kümesinde %61,54'lük bir F1 puanıyla allosterik kalıntıları tahmin eden protein dizilerinin Allosterik Veri Kümesine (AVK) göre ince ayar yapılmıştır. XGBoost, SVM, AutoML ve GNN'ler dahil olmak üzere çeşitli MÖ ve DÖ yaklaşımlarından yararlanılmıştır, İnce ayarlı pDM özelliklerinin dahil edilmesiyle, yukarıda belirtilen yaklaşımların tümü, allosterik bölgelerin tahmin performansını önceki çalışmalara göre önemli bir farkla artırdığı bulunmuştur. Bu çalışmada en yüksek performansa sahip model olan XGBoost, ince ayarlı ProtBERT'ten çıkarılan özellikleri FPocket tarafından çıkarılan cep özellikleriyle birleştirerek sonuçları iyileştiriyor ve allosterik cepler/bölgeler için %75,76'lık bir F1 puanı erişmektedir. Bilinen allosterik bölgelere sahip proteinler üzerinde örnek çalışmaların yanı sıra, farklı proteinler üzerindeki yeni allosterik bölgeleri tahmin etmek için de çalışmalar yapılmıştır."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Obje segmentasyon, tıbbi görüntü analizi ve görüntü/video düzenleme gibi çeşitli alanlardaki uygulamalarında yüksek doğruluk gerektirir; bu mevcut popüler tam otomatik obje segmentasyon modellerinin performansını da dikkate almamızı gerektirir. özellikle küçük ve karmaşık şekillere sahip nesneler için yeni modeller geliştirilmesi ya da mevcut modellerin iyileştirmesi gereksinimi son yıllarda hızla artmaktadır. Bu nedenle, bu alanlardaki uygulayıcılar zahmetli bir iş olan tamamen manuel etiketleme metoduna başvurmaktadır. Bu sorunların üstesinden gelmek amacıyla, daha kesin tahminler yapmak ve yüksek eğrilikli, karmaşık ve küçük ölçekli nesneler için daha kaliteli segmentasyon maskeleri oluşturmak amacıyla yeni bir yaklaşım öneriyoruz. İnsan destekli segmentasyon modelimiz HAISTA-NET, Strong Mask R-CNN ağını insan tarafından belirlenen kısmi sınırları dikkate alacak şekilde genişletir. Ayrıca İnsan Dikkat Haritaları (HAM) olarak adlandırdığımız elle çizilmiş kısmi nesne sınırları veri setini de sunmaktayız. PSOB (Kısmi çizim Nesne Sınırları) veri seti, bir nesnenin temel gerçek maskesinin sınırlarını birkaç pikselle temsil eden, elle çizilmiş kısmi vuruşları içerir. Kapsamlı değerlendirmeler sonucunda, PSOB veri seti ile eğittiğimiz HAISTA-NET'in, sırasıyla Mask R-CNN, Strong Mask R-CNN ve Mask2Former modellere kıyasla AP-Mask metriğinde +36,7, +29,6 ve +26,5 puanlık artış elde ederek bu gelişmiş yöntemlerden daha iyi performans gösterdiğini deneylerimizle sunuyoruz. Yaklaşımımızın, Tam Otomatik ve Etkileşimli Obje Segmentasyon mimarilerini birleştirerek gelecekteki insan destekli derin öğrenme modelleri için bir temel oluşturacağını umuyoruz. Kod ve PSOB veri seti herkese açık olarak paylaşılacaktır.","Instance segmentation is a fundamental computer vision task with a wide range of applications. Some instance segmentation tasks such as medical image analysis, and image/video editing require high levels of precision. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as ""human attention maps."" In addition, the Partial Sketch Object Boundaries (PSOB) dataset contains hand-drawn partial object boundaries which represent curvatures of an object's ground truth mask with several pixels. Through extensive evaluations, we show that HAISTA-NET outperforms state-of-the art methods such as Mask R-CNN, Strong Mask R-CNN, and Mask2Former, achieving respective increases of +36.7, +29.6, and +26.5 points in AP-Mask metrics for these three models. Our novel approach sets a baseline for future human-aided deep learning models by combining fully automated and interactive instance segmentation architectures."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Afet sonrası müdahalelerin etkin, adil ve hızlı olması mecburidir. Deprem gibi büyük çapta etkileri olan afetlerin ardından kaynakların sınırlı olması, bu kaynakların yönetimi noktasında afetle mücadeleye güçlük oluşturmaktadır. Bu tez çalışmasında, afetten etkilenen bir bölgeye müdahalede sınırlı olan bu kaynakların yönetimi için pekiştirmeli öğrenme tabanlı kaynak yönetimi yaklaşımımızı sunuyoruz. Pekiştirmeli öğrenme tabanlı yaklaşımımız şu şekildedir. İki boyutlu bir afet şiddet haritası bizim durum uzayımızı belirtmektedir.Bir kaynağın, destek biriminin, belirli bir noktaya atanması ise eylem uzayımızı ifade eder. Basit bir simulasyon yardımıyla sanallaştırdığımız afet senaryosunda destek birimlerinin sönümlendiği afet hasarları miktarı kaynak yönetiminin etkinliğini, kaynakların bölgeye yayılım miktarı kararların ne kadar adil olduğunu gösterir. Bu metrikleri kullanarak tüm atama sonrasında bir ödül hesaplaması yapıyoruz. Ayrıca, bu formulasyonda karşılaşı-lan problemleri azaltmak amaçlı her adımda ödüllendirme esaslı bir hesaplamamız daha mevcut. Çalışmamızda, iki farklı derin q-öğrenmesi tabanlı ajanlar eğittik. Biri sadece nihai durumda ödüllendirilirken, diğeri hem her adımda hem de nihai durumda ödüllendirildi. İki boyutlu olarak tasarladığımız dünya modellemesi durum ve eylem uzaylarının fazlaca geniş olmasını da beraberinde getirdi. Parametre sayılarını düşürmek ve model varsayımı eklemek için kullanacağımız modeli evrişimli sinir ağları üzerine kurguladık. Ayrıca, açgözlü yaklaşım ajanı geliştirip, her adımda ödüllendirme esaslı yaklaşımımıza bir taban oluşturduk. Modelleri değerlendirme süreçlerimiz iki farklı şekildedir. Birincisi, oyuncak haritalar diye isimlendirdiğimiz senaryolarımız üzerinde niteliksel davranış değerlendirme-leri yapıyoruz. İkinci olarak, kentsel senaryolarımızda ise niceliksel değerlendirmelerimizi yapıyoruz. Her iki değerlendirmede de kullandığımız senaryolar modellerin eğitilme aşamasında hiç karşılaşmadığı haritalardır. Nitelik değerlendirmelerimizde açgözlü yaklaşımın, beklendiği gibi, yayılımı dikkate almadığı, sadece afet şiddeti yoğun olan bölgelere kaynak ataması yaptığını gözlemledik. Nihai durumda ödüllendirdiğimiz ajanımızın ise çok yoğun olan bölgeleri gözden kaçırdığını, her adımda ve nihai durumda ödüllendirdiğimiz ajanımızın ise hem yayılımı hem de şiddetli noktaları dikkate aldığını ortaya koyduk. Niceliksel değerlendirmelerimizde de niteliksel değerlendirmelerimize paralel çıka-rımlar elde ettik. Açgözlü ajanımız yayılım hususunda başarısız olduğunu ve nihai durumda ödüllendirdiğimiz ajanımızın afet sönümlemelerinde geride kaldığını gözlemledik. Açgözlü ajanımız her lokasyonun değerini her adımda hesaplaması sebebiyle eğitilmiş modellerimize nazaran çok geride kaldığını, karar alım sürecinin çok uzun olduğunu belirledik. Genel olarak ifade etmek gerekirse, hem her adımda hem de nihai durumda ödüllendirdiğimiz ajanımız en iyi performansı gösterdi. Karar alımının hızlı olduğunu, atamalar sonrasında destek birimlerinin sönümlediği afet miktarının daha fazla olduğunu ve yayılımın daha geniş kapsama alanını etkilediğini niceliksel ve niteliksel olarak ortaya koyduk. Bu tez çalışmasında, ana amacımız pekiştirmeli öğrenmenin geniş çaplı kaynak yönetimi problemlerinde uygulanabilir olduğunu göstermektir. Öyle ki, çalışmamızda bazı basitleştirici varsayımlar yaptık. Örneğin, afetten farklı etkilenen farklı lokasyonlar için değişik tiplerde destek birimleri gerekiyor olsa da, çalışmamızda tek tip bir kaynak üzerine yoğunlaştık. Aksi durumda, her bir tip destek birimi için birden fazla yapay zeka ajanı eğitiyor olmamız gerekebilirdi. Ayrıca, kaynakların dağılım maliyetini de göz önüne almadık. Kaynak ataması sonrasında destek birimlerinin ilgili yerlere herhangi bir engel olmadan ulaşılabilir olduğu bir dünya tasarladık. Her ne kadar her adım ödüllendirme esaslı yaptığımız çalışmalarda ayrıca bir geliştirme yapma ihtiyacımız doğsa da, bu hali ile afet simulatörümüz ve nihai durum ödüllendirmelerimizi yeterli bir şekilde kapsayacak altyapı ihtiyacımızı karşıladı. Diğer bir varsayımımız ise, durağan bir afet doğası üzerinde çalışıyor olmamızdır. Çalışmamıza deprem odaklı başlamamız sebebiyle araştırma süreçlerimiz bu şekilde ilerledi. Büyük ölçekli yangınlar gibi dinamik felaketler de simülatöre dahil edilebilir ancak bu, felaketin nasıl geliştiğine ilişkin bilgilerin de yaklaşımımız ve altyapımıza dahil edilmesi için durum uzayında ek çalışma yapılmasını gerektirecektir. Bu tez çalışması, gelişigüzel karmaşık hedeflerin kullanılması ve ortam stokasti-sitesinin dahil edilmesi potansiyeline ek olarak, hedef yapıya ilişkin varsayımlar olmadan geniş durum ve eylem uzaylarıyla çalışabilen afet müdahalesine yönelik ilk kaynak tahsis yaklaşımını sunmaktadır. Çalışmalarımız, basitleştirici varsayımları ortadan kaldıracak daha karmaşık afet senaryoları ve hedef fonksiyonları ile bu alanda yapılacak çalışmaların önünü açacaktır.","Effective, fair and quick disaster response is imperative in the aftermath of disasters. Resource limitations, particularly after large-scale disasters like earthquakes, pose challenges in distributing material and human resources. In this thesis, we present a reinforcement learning (RL) based resource allocation approach for disaster response, where a finite amount of resources are dispatched to affected locations. Our RL formulation is as follows. A 2D map of continuous disaster severity constitutes our state space. Dispatching a single resource to a specific location constitutes the action space. We calculate rewards after allocating all the available resources by running a simple simulation to determine the amount of disaster relieved, reflecting effectiveness, and the spread of the resources across the map, reflecting fairness. We additionally define a per-step reward, based on the local disaster severity distribution, to alleviate issues with sparse rewards. We train two Deep Q-learning agents; one utilizing only terminal rewards and the other incorporating both rewards. Our 2D map formulation induces large state and action spaces. To reduce the number of learned parameters and to add inductive bias, we use convolutional neural networks to approximate the Q-values. We additionally devise a greedy algorithm incorporating per-step rewards as a baseline. Our evaluation encompasses qualitative behavior assessment on toy maps and quantitative performance assessment on urban maps, both on unseen maps and disaster distributions. Our qualitative assessment reveals that the greedy algorithm places resource units to high disaster severity locations but does not take spread into account as expected, the sparse-reward agent is prone to missing highly concentrated disaster regions, and the other RL agent spreads the units while catching the concentrated regions. Our quantitative assessment mirrors the qualitative ones; the greedy algorithm falls behind in resource spread and the sparse reward agent falls behind in the amount of disaster relieved. The greedy algorithm evaluates each location in each allocation step during testing/inference. This leads to two orders of magnitude slower allocation speed, which is related to quickness, compared to the trained agents. Overall, the RL agent trained with both rewards achieves the best performance in terms of allocation speed, disaster relieved and resource spread for novel disaster scenarios. In this thesis, our main aim is to show the feasibility of RL for large scale resource allocation. As such, we made some simplifying assumptions. We are assuming only one type of resource whereas different regions may require different types (e.g. excavators vs fire engines). This can be handled by training multiple RL agents for each resource type. We are also not taking the distribution cost of the resources into account and assume that the resources can get to where they want to without hindrance. Both of these can be readily incorporated into our disaster simulator and terminal rewards, while requiring additional work on the per-step rewards. Another assumption is about the static nature of the disaster as we started our work for earthquakes. Dynamic disasters such as large scale fires can be incorporated into the simulator stage as well but this would require additional work on the state space to incorporate information on how the disaster may evolve. This thesis presents the first resource allocation approach for disaster response that can work with large state and action spaces without assumptions on the objective structure, in addition to the potential of using arbitrarily complex objectives and incorporating environment stochasticity, to the best of our knowledge. Our work paves the way for further developments that can incorporate further developments such as more complicated disaster scenarios and objective functions to remove the simplifying assumptions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Pekiştirmeli Öğrenme (PÖ), herhangi bir uzman denetimine ihtiyaç duymadan otonom sürüş konusunda insan yeteneklerini aşma potansiyeline sahiptir. Buna rağmen, sensöre dayalı otonom sürüşteki en son teknolojiler büyük çoğunlukla taklit ile öğrenme yöntemine dayanmaktadır. Bunun sebebi PÖ algoritmalarının doğasında olan zorluklardır. Bununla birlikte, PÖ modelleri, ortama ilişkin gerçek referans değerlerden oluşan ayrıcalıklı durum temsilleri ile eğitildiğinde son derece başarılı politikalar keşfedebilmektedir. Bu çalışmada, ayrıcalıklı durum temsili kullanan ve sensöre dayalı PÖ modelleri arasındaki performans farklarının sebepleri araştırılmaktadır. Sensör verilerinden ayrıcalıklı durum temsillerini tahmin etmek için görüşe dayalı derin öğrenme modellerini kullanan çözümler önerilmektedir. Özellikle, istenen rota oluşturma ve trafik ışığı tahmini gibi PÖ modelinin başarısı için hayati önem taşıyan durum temsilleri belirlenmektedir ve mevcut bilgisayarlı görme yaklaşımlarıyla her biri için ayrıcalıklı bilgilerin kademeli olarak kaldırılmasına yönelik çözümler önerilmektedir. CARLA simülasyonu üzerinde yapılan değerlendirme sayesinde, otonom sürüş için PÖ'de durum temsilinin önemine ışık tutulmuştur ve gelecekteki araştırmalar için çözülmemiş zorlukların ana hatları çizilmiştir.","Reinforcement Learning (RL) has the potential to surpass human capabilities in self-driving without needing any expert supervision. Despite its promise, the state-of-the-art in sensorimotor self-driving is dominated by imitation learning methods due to the inherent challenges of RL algorithms. Nonetheless, RL agents are able to discover highly successful policies when provided with privileged ground truth representations of the environment. In this work, we investigate what separates privileged RL agents from sensorimotor agents for urban driving in order to bridge the gap between the two. We propose vision-based deep learning models to approximate the privileged representations from sensor data. In particular, we identify aspects of state representation that are crucial for the success of the RL agent such as desired route generation and traffic light prediction, and propose solutions to gradually remove privileged information for each with the existing computer vision approaches. Through rigorous evaluation on the CARLA simulation, we shed light on the significance of the state representation in RL for autonomous driving and outline unresolved challenges for future research."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son dönemlerde, kodlama verimliliği ve kalite iyileştirme açısından derin öğrenmenin video sıkıştırma ve iyileştirmede uygulanmasında önemli ilerleme kaydedilmiştir. Bu çalışmada, video sıkıştırma ve yeniden inşa etme için, göreceli kalite dikkate alınarak sıkıştırılmış video karelerinin kalitesini artırmak için Kalite Kapılı Evrişimsel Uzun-Kısa Süreli Bellek hücrelerini içeren öğrenilmiş bir işleme sonrası ağ önerilmektedir. Önerilen Kalite Kapılı Evrişimsel Uzun-Kısa Süreli Bellek hücre tabanlı son işleme ile ağın, komşu sıkıştırılmış kareler arasındaki kareler arası korelasyon ve kalite dalgalanmasından faydalanmakta ve bundan tam avantaj sağlanmaktadır. Yüksek kaliteli sıkıştırılmış çerçeveler, düşük kaliteli sıkıştırılmış çerçevelerden daha yararlı bilgiler sağladığından, önerilen ağ Kalite Kapılı Evrişimsel Uzun-Kısa Süreli Bellek hücrelerindeki unut ve giriş kapı ağırlıklarını ayarlayabilmektedir. Çerçeveler arasında göreceli kalite bilgisi kullanan önerilen ağın gelişimini göstermek için, girdi olarak ağa verilen video çerçeveleri, standart VVC (H.266) sıkıştırma algoritması kullanılarak farklı kalite değerleriyle hiyerarşik olarak sıkıştırılmaktadır. Önerilen ağda, Kalite Kapılı Evrişimsel Uzun-Kısa Süreli Bellek ağına verilen ağırlıklar, sıkıştırılmış video karelerinden kaliteyle ilgili özelliklerin çıkarılmasıyla belirlenmektedir. Ek olarak, kalite özelliği çıkarımı için bir referans çerçevesi bulunmadığından Transformer, göreceli sıralama ve öz-tutarlılık algoritmalarını kullanan referanssız bir görüntü kalitesi değerlendirme yöntemi önerilmektedir. Önerilen ağın kalite iyileştirme performansı, PSNR, MS-SSIM ve VMAF gibi, sıklıkla kullanılan metriklerle ölçülür.","Recently, significant progress has been shown in applying deep learning to video compression and enhancement in terms of coding efficiency and quality improvement. In this work, a learned post-processing network is proposed for video compression and restoration tasks, which contains Quality-Gated Convolutional Long Short-Term Memory (QG-ConvLSTM) cells to enhance the quality of the compressed video frames considering the relative quality. With the proposed QG-ConvLSTM cell-based post-processing, the network exploits and takes full advantage of the inter-frame correlation and quality fluctuation between neighboring compressed frames. Since high-quality compressed frames provide more helpful information than low-quality compressed frames, the network can adjust the input and forget gate weights in QG-ConvLSTM cells. To show the enhancement of the proposed network that uses relative quality information between frames, video frames given to the network as inputs are compressed hierarchically with different qualities using the standard VVC (H.266) codec. In the proposed network, the weights given to the QG-ConvLSTM network are determined by extracting quality-related features from compressed video frames. Additionally, there is no reference frame for the quality feature extraction, so a no-reference image quality assessment method via Transformers, relative ranking, and self-consistency is suggested. The quality enhancement performance of the proposed network is measured with frequently used metrics, namely PSNR, MS-SSIM, and VMAF."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnce taneli görsel anlamsal anlayış, otonom sürüş ve diğer birçok bilgisayarla görme görevi için gereklidir. Bu amaçla, segmentasyon görevlerinde, önerilen kıyaslamaların sayısının artmasının bir sonucu olarak hızlı gelişmelere tanık olunmaktadır. Bununla birlikte, mevcut segmentasyon kriterleri genellikle sabit bir anlamsal kategori kümesini varsaymaktadır. Sonuç olarak, segmentasyon yöntemlerinin geliştirilmesi bu varsayım etrafında yoğunlaşırken, gerçek hayat senaryolarında karşılaşılabilecek yeni veya dağıtım dışı (OoD) örneklerin ele alınmasına çok az önem verilmiştir. Arıza durumunda feci sonuçlardan kaçınmak için bir güvenlik uyarısının verilebilmesi amacıyla bilinmeyen nesnelerin tanımlanması çok önemli olduğundan, bu durum otonom bir araç için sorun teşkil etmektedir. Sonuç olarak OoD segmentasyon görevinin ayrı ayrı ele alınması, mevcut segmentasyon yöntemlerine uyum sağlayan ve ana segmentasyon görevlerinin performansını göz ardı eden yöntemlerin ortaya çıkmasına yol açmıştır. Ek olarak, bu yöntemler piksel başına sınıflandırma paradigmasını takip eden modellere dayanmalarından dolayı tahmin edilen anormallik haritalarında genel olarak düzgünlük ve nesnellik eksikliğinden de muzdariptir. Bu tezde, doğası gereği belirsizliği ifade etme yeteneğine sahip birleşik bir mimari olarak bilinmeyen bölümleme için bölge düzeyinde sınıflandırma modellerinin potansiyelini araştırıyoruz. Maske sınıflandırma modellerindeki nesne sorgularının, tüm sınıflandırıcılara göre tek bir sınıflandırıcı gibi davranma eğiliminde olduğunu gösterdik. Bu bulguya dayanarak, aykırı değer olma olayını bilinen tüm sınıflar tarafından reddedilmek olarak tanımlayarak hepsi Tarafından Reddedilen (RbA) adı verilen yeni bir aykırı değer puanlama fonksiyonu öneriyoruz. Ayrıca, kapalı küme performansına zarar vermeden sözde aykırı veriler kullanarak bilinmeyen bölümleme performansını artırmak için önerilen bu puanı optimize eden bir hedef de öneriyoruz. RbA, yüksek etki alanı değişimleri altında iyi performans gösterir ve bilinen sınıf belirsizliği nedeniyle sınırlar gibi belirsizlik kaynaklarını ayırma yeteneğine sahiptir. RbA'yı çeşitli bilinmeyen segmentasyon kriterlerine göre değerlendiriyoruz ve önceki piksel düzeyinde bilinmeyen segmentasyon yöntemleriyle karşılaştırıldığında önemli marjlarla en son teknolojiye sahip performansa ulaştığını gösteriyoruz. RbA'nın etkinliğini doğrulayan kapsamlı ablasyon deneylerini rapor ediyoruz.","Fine-grained visual semantic understanding is essential for autonomous driving and many other computer vision tasks. To this end, segmentation tasks have been witnessing rapid advancements as a result of the growing number of benchmarks proposed. However, existing segmentation benchmarks generally assume a fixed set of semantic categories. Consequently, the development of segmentation methods has been centered around this assumption, while little attention has been poured into handling novel or out-of-distribution (OoD) samples that can potentially be encountered in real-life scenarios. This poses an issue for an autonomous vehicle, as it is crucial to identify unknown objects so that a safety warning can be issued to avoid disastrous consequences in case of failure. As a result, the task of OoD segmentation has been addressed separately, leading to the emergence of methods that adapt to the existing segmentation methods and disregard the performance of the main segmentation tasks. Additionally, these methods also generally suffer from a lack of smoothness and objectness in their predicted anomaly maps due to the reliance on models that follow the per-pixel classification paradigm. In this thesis, we explore the potential of region-level classification models for unknown segmentation as a unified architecture with an inherent ability to express uncertainty. We show that the object queries in mask classification models tend to behave like one \vs all classifiers. Based on this finding, we propose a novel outlier scoring function called Rejected by All (RbA) by defining the event of being an outlier as being rejected by all known classes. We also propose an objective that optimizes this proposed score for boosting the unknown segmentation performance using pseudo-outlier data without hurting the closed-set performance. RbA performs well under high domain shifts and is capable of separating sources of uncertainty, such as at the boundaries, due to known class ambiguity. We evaluate RbA on several unknown segmentation benchmarks and show that it achieves state-of-the-art performance with significant margins compared to previous pixel-level unknown segmentation methods. We report extensive ablation experiments that validate the effectiveness of RbA."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Belirginlik tahmini, sahnelerdeki algısal olarak önemli bölgeleri vurgulamak için insanın görsel-işitsel dikkat mekanizmalarını modellemeyi amaçlamaktadır. Literatürde bu sorun ilk olarak sahne özelliklerine göre: statik (resimler için), dinamik (videolar için) ve görsel-işitsel belirginlik tahmini olarak üç dal altında ele alınmıştır. Son zamanlarda, sanal gerçeklik (VR) teknolojilerine artan ilginin sonucunda, tam görüş alanını yakalayan çok yönlü videolar (ODV'ler), bilgisayar görüsünde 360◦ belirginlik tahminine önem kazanmıştır. Bununla birlikte, insanların 360◦ sahnelerde nereye baktığını tahmin etmek, 360◦ sahnelerin temsili, küresel bozulma, yüksek çözünürlük ve sınırlı miktarda etiketli veri de dahil olmak üzere yeni zorluklar sunar. Bu tezde, SalViT360 adı verilen çok yönlü videolar için yeni bir görüntü dönüştürücü tabanlı belirginlik tahmin modeli önerilmişir. 360◦ video anlayışı için teğet görüntü temsilleri arasında küresel geometriye duyarlı uzay-zamansal bir öz-dikkat mekanizması sunulmuştur. Geri projeksiyon sonrasında tahminlerdeki bozulmaları azaltmak amacıyla projeksiyon tabanlı 360◦ yoğun tahmin modelleri için tutarlılık bazlı denetimsiz bir kayıp fonksiyonu sunulmuştur. Bu yaklaşım, bozulmamış 360◦ belirginlik tahmini için teğet görüntüleri kullanan ilk yaklaşımdır. Son olarak, birleşik bir 360◦ görsel-işitsel belirginlik tahmin modeli için bir boyutlu ve uzamsal ses modalitelerini dahil etmek üzere video belirginliği tahmin modeli SalViT360, görsel-işitsel adaptörlerle genişletilerek SalViT360-AV sunulmuştur. Dört 360◦ belirginlik veri seti üzerindeki deneysel sonuçlarımız, SalViT360 ve SalViT360-AV'nin en son teknolojiyle karşılaştırıldığında etkinliğini göstermektedir.","Saliency prediction aims to model human audio-visual attention mechanisms to highlight the perceptually important regions in the scenes. This problem was first addressed in the literature under three branches based on the scene characteristics: static (for images), dynamic (for videos), and audio-visual saliency prediction. Due to the growing interest in virtual reality (VR), omnidirectional videos (ODVs) that capture the full field-of-view have gained 360◦ saliency prediction importance in computer vision. However, predicting where humans look in 360◦ scenes presents novel challenges, including the representation of 360◦ scenes regarding spherical distortion, high resolution, and the limited amount of annotated data. This thesis proposes a novel vision-transformer-based saliency prediction model named SalViT360 for omnidirectional videos. We introduce a spherical geometry-aware spatio-temporal self-attention mechanism among tangent image representations for effective omnidirectional video understanding. We present a consistency-based unsupervised regularization term for projection-based 360◦ dense-prediction models to reduce artefacts in the predictions after inverse projection. Our approach is the first to employ tangent images for undistorted omnidirectional saliency prediction. Lastly, we propose SalViT360-AV by extending our video saliency prediction model with audio-visual adapters to incorporate mono and spatial audio modalities for a unified 360◦ audiovisual saliency prediction model. Our experimental results on four ODV saliency datasets demonstrate the effectiveness of SalViT360 and SalViT360-AV compared to the state-of-the-art."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu GPU sistemlerinin süper bilgisayar alanında daha yaygın hale gelmesiyle birlikte, bilimsel uygulamalar, artan performans için yüksek paralel hızlandırıcılardan faydalanmak amacıyla uyarlanmakta ve ölçeklendirilmektedir. Ancak, GPU programlamasının geleneksel modeli, bilimsel uygulamalardaki en büyük sorun ve dar-boğazlardan biri olan cihazlar arası iletişimi, çoklu GPU ortamlarında bırakılmak istenilen birçok şeyi geride bırakmaktadır - bu tür tek taraflı kontrol, sürekli olarak ana işlemci (CPU) ile cihazlar arasında senkronizasyon ve API çağrıları arasındaki gidip gelmenin neden olduğu aşırı gecikmelere yol açar ve cihaz sayısı arttıkça uygulama ölçeklenmesine zarar verir. Bu çalışma ilk olarak, çoklu GPU uygulamaları için, ilk başlatmanın ötesinde CPU'nun katılımını tamamen dışlayan, tamamen özerk CPU'suz yürütme modelini önermektedir. Bu amaçla Persistent kernel'ler, Thread Block özelleştirme, GPU-başlatılan iletişim ve senkronizasyon gibi çeşitli teknikleri sistematik olarak birleştirmekle ana işlemci kaynaklı gecikmeleri önemli ölçüde azaltıyor ve diger optimizasyonları etkinleştiriyoruz. Önerdiğimiz modeli, yaygın olarak kullanılan 2D/3D Jacobi Stencil iteratif çözücüde test ediyoruz ve 8 NVIDIA A100 GPU üzerinde CPU tarafından kontrol edilen temellere kıyasla 3D Stencil iletişim gecikmesini %58,8 oranında iyileştiriyoruz. Bu çalışmanın ikinci bölümü, Python'da performanslı CPU'suz kod yazmayı kolaylaştırmak için derleyici desteği ekler ve DaCe çerçevesini GPU-merkezli iletişim özellikleri ile genişletir. Otomatik olarak oluşturulan CPU'suz kodu, DaCe'deki mevcut dağıtık kodlarla karşılaştırır ve Stencil testlerinde %96'dan fazla performans iyileştirmesi görürüz.","As multi-GPU systems become more prolific in the field of supercomputing, scientific applications are adapted and scaled up to take advantage of the highly parallel accelerators for increased performance. However, the traditional model of GPU programming leaves much to be desired in multi-GPU settings, wherein communication among devices - one of the largest points of contention and bottlenecks in scientific applications - is controlled by the CPU. This kind of one-sided control leads to undue latencies incurred by the constant back-and-forth of synchronization and API calls between the host and devices, and harms application scaling as the number of GPUs grows. This work first proposes the fully autonomous CPU-Free execution model for multi-GPU applications that completely excludes the involvement of the CPU beyond the initial kernel launch. We systematically combine several techniques such as persistent kernels, thread block specialization, and GPU-initiated communication and synchronization to significantly reduce host-incurred latencies and facilitate further optimizations. We benchmark our proposed model on a broadly used iterative solver, 2D/3D Jacobi Stencil and improve 3D stencil communication latency by 58.8% compared to CPU-controlled baselines on 8 NVIDIA A100 GPUs. The second part of this work adds compiler support to easily write performant CPU Free code in high-level Python by extending the DaCe framework with GPU-centric communication intrinsics. We compare automatically generated CPU-Free code to existing distributed facilities in DaCe and observe over 96\% performance improvement in Stencil benchmarks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Durağan görüntülerden nesne-merkezli temsil öğrenme, derin öğrenme alanında umut vadeden bir yaklaşımdır. Ancak bu yaklaşımı hareketli görüntülere uyarlamak, video içeriğinin zamansal dinamiklerini yakalama gerekliliği nedeniyle bazı zorluklar içermektedir. Yakın zamanlı çalışmalar, sentetik video veri kümelerinde nesne keşfi konusunda bazı önemli ilerlemeler kaydetmiştir. Bununla birlikte, bu çalışmalarda videolardaki nesnelerin hareketleri ve zamanla ilgili ipuçları tam olarak dikkate alınmazlar. Bu tez çalışmasında, bu bilgilerin daha dikkatli kullanımıyla video görüntüleri üzerindeki nesne-merkezli temsil öğrenme başarımının arttırılması hedeflenmektedir. Bu amaca yönelik olarak, çoklu nesne keşfi ve takibi için bellek destekli slot dikkat modelini kullanan yeni bir gözetimsiz öğrenme yöntemi öneriyoruz. Yakla-şımımızın anahtarı, nesne slotlarına ek olarak öğrenme mimarisine entegre ettiğimiz ve video çerçevelerinden bilgi depolayan bellek slotlarıdır. Nesne slotları, geçmiş çerçevelerden bilgi almak için eş zamanlı olarak hem bellek slotlarına hem de o andaki görüntü girdisine dikkat ederler. Bellek slotlarının uzun video dizileri üzerinde eğitilmeleri gerekir. Ne var ki, bunun için en uygun öğrenme yapıları olan yinelemeli sinir ağları (RNN), patlayan ve/veya kaybolan gradyan problemine bağlı olarak, uzun süreli zamansal verileri öğrenmede çok etkili olamazlar. Bellek destekli modelimizi uzun video dizileri üzerinde RNN yapılarını kullanarak daha etkili bir şekilde eğitebilmek için, zamanda kısaltılmış geri yayılım tekniğini kullanıyoruz. Sentetik ancak gerçekçi görüntüler içeren video veri kümeleri üzerinde gerçekleştirdiğimiz deneyler umut verici sonuçlar üretmiştir; bu sonuçlar bellek slotlarının çoklu nesne takibi ve nesne bölütleme başarımını önemli ölçüde artırdığını göstermektedir. Tamamen gözetimsiz olarak çalışan öğrenme yöntemimiz, videolar üzerinde nesne-merkezli temsil öğrenme problemine katkı sağlamakta ve bu alanda yeni olanakların yolunu açmaktadır.","Learning object-centric representations from static images is a promising research direction in the field of deep learning. However, adapting this approach to videos poses certain challenges due to the necessity of capturing the temporal dynamics of video content. Recent works have made significant progress in object discovery within synthetic video datasets. Nevertheless, these works do not fully exploit the motion of objects in videos and temporal cues. In this thesis, we aim to enhance the performance of object-centric representation learning on video frames by using the temporal information more carefully. To achieve this goal, we propose a new unsupervised learning method that utilizes a memory-augmented slot attention model for multi-object discovery and tracking. The key component of our approach is the integration of memory slots, which store information from past video frames, alongside object slots into the learning architecture. Object slots simultaneously attend to both memory slots for information from past frames and the current image input. Training memory slots requires longer video sequences. However, the most suitable learning structures for this, recurrent neural networks (RNNs), are not very effective in learning long-term temporal data due to the issues of exploding and/or vanishing gradients. To train our memory-augmented model more effectively on long video sequences, we employ truncated back-propagation through time. Experiments conducted on synthetic yet realistic video datasets have yielded promising results, indicating that memory slots significantly improve multi-object tracking and object segmentation performance. Our fully unsupervised learning method contributes to the problem of object-centric representation learning in videos and opens up new possibilities in this field."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Federe Öğrenme (FL), dağıtık makine öğrenimi için önemli bir seçenek haline gelmiştir. Başlangıçta merkezi birleştirme üzerine odaklanan FL'deki son çalışmalar, yeni nesil iletişim ağlarında sunucusuz etkileşimin standartlaştırılmasını destekleyen merkezi olmayan yaklaşımlara vurgu yapmıştır. Ancak cihazların çeşitliliği, veri dağılımları ve iletişim ayarları, dinamik işletme koşulları tarafından karmaşık hale getirilmiş ve bu durum Merkeziyetsiz Federe Öğrenmenin (DFL) birden fazla zorlukla karşılaşmasına neden olmuştur. DFL konusunda, sınır sunucularını kullanmaktan tamamen cihazdan cihaza yaklaşımlara kadar farklı yöntemler bulunmaktadır. Merkezi olmayan durumda iletişim maliyeti ve öğrenme performansı genellikle bir arada değerlendirilir ve senaryolara dayalı belirli bir denge kurulur. Ancak, DFL yaklaşımlarını çok sayıda senaryo ve çalışma koşulu altında eşit durumda karşılaştırmak için literatürde eksiklik bulunmaktadır. Bu yöntemler ile karşılaştırılabilir analiz arasındaki boşluğu kapatmak amacıyla, Federe Öğrenme Algoritmaları Simülasyon (FLAGS) Platformunu tasarlayıp geliştirdik. Birçok DFL yönteminin özellikle tamamen merkeziyetsiz yaklaşımlar için cihaz erişilebilirliğindeki aşırı dalgalanmalar önemli bir zorluk olarak öne çıkmaktadır. Bu cihaz dalgalanması, zayıf öğrenme başarımına yol açar. Bu sorun kapsamında, çevre seçiminin, belleğin ve çoklu atlama bilgisi iletimin DFL başarımı üzerindeki etkilerini araştırdık. Gerçekçi ve yüksek derecede dalgalı cihaz katılım koşulları altında çalışabilen tamamen merkeziyetsiz bir FL yaklaşımı sunuyoruz. Bu tezin temel katkıları şu şekilde özetlenebilir: (i) birçok yöntemi karşılaştırmak için hafif bir FL simülasyon platformu geliştirilmesi, (ii) bu platformda bir dizi işletme koşulunda birçok FL yönteminin analizi ve karşılaştırılması, (iii) yoğun cihaz dalgalanması altında çeşitli düğüm seçimi stratejilerinin deneysel analizi ve (iv) bellek ve iletimli iletişimi kullanarak FL başarımını geliştirmek için gerçekçi işletme koşulları ve yoğun cihaz dalgalanması altında çalışabilen yeni bir algoritma geliştirilmesi. Federe Öğrenme, ağ kenarında geniş bir düğüm etkileşimi ve otonom işlemleri destekler. Bu çok yönlü heterojenliği kapsamak amacıyla FLAGS simülatörü, hafif bir FL uygulama ve test platformu olarak geliştirilmiştir. FLAGS platformu, geniş kapsamlı cihaz davranışları ve işbirliği mekanizmaları için olanak tanır ve böylelikle çeşitli FL algoritmalarının hızlı bir şekilde test edilmesine olanak verir. FLAGS özellikleri, mevcut ve yeni FL algoritmalarının geniş bir veri dağılımında test edilmesine olanak tanır, düğümleri birden çok sinir ağıyla ve homojen katılımdan yüksek derecede dalgalı katılıma kadar çeşitli cihaz katılım koşullarında simüle edebilir. Farklı ağ katmanları ve iletişim mekanizmaları, çeşitli FL algoritmalarının yukarıda bahsedilen faktörlerin birleşimlerini kullanarak yapılandırılmasına olanak tanır. Bu çok kapsamlı FL yaklaşımlarını bir araya getirmek ve temel FL algoritmalarının nesnel bir analizini sunmak amacıyla bir dizi kapsamlı analiz gerçekleştirilmiştir. Hiyerarşik FL (HFL), Merkeziyetsiz FL (DFL) ve Salgın FL (GFL) gibi üç temel FL algoritması ile başlayarak, bu çalışma tamamen merkezi olanlardan tamamen merkezi olmayanlara kadar uzanan altı algoritmanın analizi yapılmışır. Deneyler, özellikle asenkron birleştirme ve geciken düğümlerin varlığı gibi bir dizi çalışma koşulu altında tamamen merkeziyetsiz FL algoritmalarının karşılaştırılabilir doğruluk elde ettiğini göstermektedir. Ayrıca, DFL gürültülü ortamlarda da çalışabilir ve daha yüksek yerel güncelleme oranına sahip olabilir. Bununla birlikte, aşırı eğri veri dağılımlarının DFL üzerindeki etkisi, merkezi yaklaşımlara kıyasla daha olumsuzdur. Çapraz değerlendirmenin analizi, DFL başarımının düğüm katılımından oldukça etkilendiğini göstermektedir. Tezin bu bölümü, gerçekçi ve dalgalı cihaz davranışı altında DFL başarımını iyileştirmeye odaklanmaktadır. İletişim verimliliğini ve yakınsama hızını iyileştirmek için çeşitli düğüm seçimi mekanizmalarıyla deneyler yapılmıştır. Farklı düğüm seçim mekanizmaları ile deneyler gerçekleştirilerek, DFL için doğruluğu ve bunun tur başına değişimini kullanan zamanla değişen parametreli bir düğüm seçim yöntemi önerilmiştir. Bu ölçütler, seyrek ağlarda hem sert hem de stokastik/yumuşak seçim kullanılarak değerlendirilmiştir. Sonuçlar, düğüm seçimi ile ilişkilendirilen önyargının, eğitim ilerledikçe başarımı olumsuz etkilediğini ve aşırı sınırlı katılım koşulları altında rastgele bir seçimin tercih edilebilir olduğunu göstermektedir. Geciken düğümler ve katılmayan düğümlerin bulunduğu seyrek çizgeler üzerinde çalışan DFL başarımını iyileştirmek için mekanizmalar geliştirdik. Öncelikle, Bellek Destekli DFL (MA\_DFL) ve Artırılmış Çizge Destekli DFL (AG\_DFL) olmak üzere iki algoritma önerdik. Bu algoritmalar belleği ve seçici iletimi kullanarak DFL başarımını iyileştirmektedir. Her iki algoritma da dalgalı düğüm katılımı için temel DFL ve salgın etkileşimi yaklaşımlarından daha iyi başarım göstermektedir. Ardından, bu iki algoritmanın bir melezini önerdik: Bellek ve Artırılmış Çizge Destekli DFL (MAG\_DFL), yüksek derecede dalgalı cihazlar ve aşırı veri koşulları altındaki DFL başarımını iyileştirmek için belleği ve çizge artırımını kullanan bir algoritmadır. Bu tezde yürütülen araştırmalar, dalgalı koşullarda DFL işleyişine yönelik çok yönlü zorlukları değerlendirir ve başarımını iyileştirmek için mekanizmalar önerir. Çalışmamız, DFL'nin sınır ağı boyunca dağıtılan öğrenme işlemlerine yardımcı olma potansiyeline işaret etmektedir. Maliyetli yukarı yönlü iletişim veya sınırlı bağlantı durumlarında FL'yi iyileştirmel için kullanılabilir. Ancak, düğüm yoğunluğu DFL üzerinde önemli bir etkiye sahiptir ve seyrek ağlar, dalgalı cihaz davranışı ve homojen olmayan veri dağılımları, yakınsama hızını azaltma eğilimindedir. Geliştirilmiş komşuluk etkileşimi ve yerel bilginin akıllıca kullanımı, böyle olumsuz koşullar altında DFL başarımını artırma potansiyeline sahiptir. Bu tezde sunulan analizler, algoritmalar ve sonuçlar, DFL'nin gelecek nesil iletişim ağlarında daha fazla gelişmenin ve pratik uygulamanın yolunu açmaktadır.","Federated Learning (FL) has become a key choice for distributed machine learning. Initially focused on centralized aggregation, recent works in FL have emphasized greater decentralization supported by standardization of serverless interaction in the next-generation communication networks. However, the diversity of devices, data distributions, and communication settings, compounded by dynamic operating conditions, result in multiple challenges for Decentralized FL (DFL). There have been various approaches to DFL, from utilizing intermediate edge servers to fully device-to-device approaches. In decentralized settings, communication cost and learning performance are usually assessed together and certain trade-offs are made based on scenarios. However, there is a lack of existing work on comparing DFL approaches in an apples-to-apples manner in a multitude of scenarios and operating conditions. To bridge this gap between methods and their comparative analysis, we design and develop the Federated Learning Algorithms Simulation (FLAGS) Framework. One important challenge we noticed that most DFL methods struggle with is the extreme fluctuations in device availability, especially for purely decentralized approaches. This \textbf{device volatility} leads to poor learning performance. To address this issue, we investigate the effects of neighborhood selection, memory and multi-hop information passing on DFL performance. We introduce a fully decentralized FL approach that can operate under realistic and highly volatile device participation settings. The key contributions of this thesis are as follows: (i) development of a lightweight FL framework for benchmarking a large plethora of methods, (ii) analysis and comparison of multiple FL methods with this framework under multiple operating conditions, (iii) empirical analysis of various node selection strategies under heavy device volatility, and (iv) utilizing memory and relayed communication to enhance device-to-device FL by developing a novel algorithm that can operate under realistic operating conditions and heavy device volatility. Federated Learning supports a wide variety of node interactions and autonomous operations across the network edge. With the aim to encompass this multi-faceted heterogeneity, the FLAGS framework was proposed and developed as a lightweight FL implementation and testing platform. FLAGS framework allows for a wide range of device behaviors and cooperation mechanisms, enabling rapid testing of multiple FL algorithms. FLAGS's built-in features allow it to subject existing and novel FL algorithms to a wide range of data distributions, simulating the nodes with multiple neural networks as well as participation conditions ranging from homogeneous to highly volatile. Different network tiers and communication mechanisms enable various FL algorithms to be configured by employing various combinations of the aforementioned factors. In order to consolidate this very extensive FL landscape and offer an objective analysis of the major FL algorithms, comprehensive cross-evaluations for a wide range of operating conditions have also been conducted. Starting with the three foundational FL algorithms, including Hierarchical FL (HFL), Decentralized FL (DFL), and Gossip FL (GFL), this work evaluates six derived algorithms ranging from fully centralized to fully decentralized. The experiments indicate that fully decentralized FL algorithms achieve comparable accuracy under multiple operating conditions, including asynchronous aggregation and the presence of stragglers. Furthermore, DFL can also operate in noisy environments and with a comparably higher local update rate. However, the impact of extremely skewed data distributions on DFL is much more adverse than on centralized variants. The analysis of the cross-evaluation indicates that DFL performance is considerably impacted by node participation. This part of the thesis focuses on improving DFL performance under realistic and volatile device behavior. Node selection in various forms has been experimented with to improve both communication efficiency and convergence rate. We experimented with multiple node selection mechanisms and also proposed and evaluated a time-varying parameterized node selection method for DFL employing validation accuracy and its per-round change. The mentioned criteria are evaluated using both hard and stochastic/soft selection on sparse networks. The results indicate that the bias associated with node selection adversely impacts performance as training progresses, and a uniform random selection is preferable under extremely limited participation conditions. Continuing with volatile conditions, we investigate and propose mechanisms to improve DFL operating on sparse graphs in the presence of stragglers and non-participating nodes. We first propose two algorithms: Memory-Assisted DFL (MA_DFL) and Augmented-Graph Assisted DFL (AG_DFL). These algorithms employ memory and selective relaying to improve DFL performance. Both algorithms outperform the baseline DFL and gossip interaction for volatile node participation. Then, we propose a hybrid of these two algorithms, Memory and Augmented-Graph Assisted DFL (MAG_DFL), that employs memory and graph augmentation to improve the performance of DFL under highly volatile devices and extreme data conditions. The research conducted in this thesis evaluates the multi-faceted challenges to the DFL operation in volatile conditions and proposes mechanisms to improve its performance. Our work indicates that DFL holds the potential to assist learning operations distributed across the edge network. It may be used to augment the FL in the presence of costly upstream communication or limited connectivity. However, node density has a major impact on DFL, and sparse networks, along with volatile device behavior and non-IID distributions, tend to reduce its convergence rate. The enhanced neighborhood interaction and intelligent use of local information has the potential to improve DFL performance under such adverse conditions based on the presented results. The analysis, algorithms and results presented in this thesis pave the way for additional developments and more practical applications of DFL in the next-generation communication networks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, sosyal etkileşim ve doğallığı artırmada gülümsemeler, gülmeler ve baş sallamalar gibi sözel olmayan ipuçlarının oynadığı rollere özel bir odakla, insan-robot etkileşimi (İRE) alanında kapsamlı bir araştırma sunmaktadır. Araştırma, doğal ikili konuşmalardan el ile etiketlenmiş çok kipli verileri kullanarak Zaman Gecikmeli Sinir Ağları, Destek Vektör Makineleri, Uzun Kısa Vadeli Bellek ağları ve Dönüştürücü modelleri gibi ileri makine öğrenimi tekniklerini kullanmaktadır. Çalışma, gülme tespiti için yüz ifadesi takip bilgileri, baş hareketi ve ses özelliklerinin ortaya koyduğu faydayı incelemekte ve veri sınıfı dengesizliği sorununu etkili bir şekilde çözmek için torbalama gibi teknikleri entegre etmektedir. Ayrıca, İRE'de gülme algısı ve yanıtının etkileşim üzerindeki etkisini derinlemesine incelemekte, gülme-duyarlı ve gülme-duyarsız modlara sahip robotlarla yapılan deneysel çalışmalarda objektif ve subjektif ölçümlerle etkileşimleri değerlendirmektedir. Araştırma ayrıca, insan-insan konuşma verileri üzerinde eğitilmiş ve değerlendirilmiş baş sallama ve söz sırası alma olayları için bir görsel-işitsel tahminleme çerçevesi sunmaktadır. Karşılaştırmalı bir yaklaşım için Uzun Kısa Vadeli Bellek ağları baz model olarak kullanılmış ve esas önerilen yöntem olarak çapraz dikkat mekanizmalarına sahip Dönüştürücü modeller kullanılmıştır. Bu model, etkileşim sırasında bir arka-kanal sinyali olarak oluşturulabilecek aday gülümseme veya gülme olaylarının tahmin performansında önemli iyileştirmeler göstermektedir. Topluca, bu bulgular, diyalog yönetim sistemlerini anlamamızı önemli ölçüde ilerletmekte, ERİ'de sosyal etkileşimin mekaniği ve sözel olmayan ifadeler hakkında önemli içgörüler sunmakta ve gelecekteki araştırmalar için bir temel atmaktadır.","This thesis offers a comprehensive investigation into the realm of human-robot interaction (HRI), with a particular focus on the role of non-verbal cues such as smiles, laughs, and head nods in enhancing social engagement and naturalness. Utilizing meticulously annotated multi-modal data from naturalistic dyadic conversations, the research employs advanced machine learning techniques, including Time Delay Neural Networks (TDNNs), Support Vector Machines (SVMs), Long Short-Term Memory networks (LSTMs), and Transformer models. The work rigorously explores the utility of facial information, head movement, and audio features for the continuous detection of laughter, addressing the class imbalance problem through the effective incorporation of bagging techniques. It also delves into the impact of laughter perception and response on engagement in HRI, evaluated through objective and subjective measures in experimental setups featuring robots with laughter-responsive and non-responsive modes. Furthermore, the research presents an audio-visual prediction framework for head-nod and turn-taking events, trained and evaluated on human-human conversational datasets. A comparative approach is employed, using LSTMs as a baseline and Transformer models with cross attention mechanisms as the main proposed method, demonstrating significant improvements in the prediction performance of upcoming candidate smiles and laughs as backchannels. Collectively, these findings significantly advance our understanding of dialog management systems, offering crucial insights into the mechanics of social engagement in HRI and laying a robust foundation for future research."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Generative Adversarial Networks, özellikle StyleGAN ve farklı türkri, son derece gerçekçi görüntüler oluşturmada olağanüstü bir yetenek göstermiştir. Bununla birlikte, bu modellerin eğitimi, genellikle büyük veri kümeleri gerektirdiğinden, verilerin az olduğu alanlarda zor olmaya devam etmektedir. Bu tez çalışmasında, yalnızca az sayıda eğitim örneği kullanıldığında bile alan uyarlaması, referans kılavuzlu görüntü sentezi ve metin kılavuzlu görüntü manipülasyonu dahil olmak üzere çeşitli görevler için önceden eğitilmiş bir StyleGAN'ın yeteneklerini artıran çok yönlü bir çerçeve yöntemi sunuyoruz. Bunu, hiper ağlar kullanarak CLIP alanını StyleGAN üretecine entegre ederek başarmaktayiz. Bu hiper ağlar, önceden eğitilmiş StyleGAN'ın bir referans görüntü veya metinsel bir açıklama ile tanımlanan belirli alanlara etkili bir şekilde uygulanmasını sağlayan dinamik uyarlanabilirlik sağlar. Sentezlenmiş görüntüler ile hedef alan arasındaki uyumu daha da iyileştirmek için, yüksek kaliteli görüntülerin üretilmesini sağlayan CLIP kılavuzlu bir ayırıcı sunuyoruz. Özellikle, yaklaşımımız kayda değer bir esneklik ve ölçeklenebilirlik göstererek, metinsiz eğitim ve iki görüntü arasında kesintisiz stil aktarımıyla metin kılavuzluğunda görüntü manipülasyonuna olanak tanır. Kapsamlı niteliksel ve niceliksel deneyler yoluyla, performans açısından mevcut yöntemleri geride bırakarak yaklaşımımızın sağlamlığını ve etkililiğini ortaya koymekteyiz.","Generative Adversarial Networks, particularly StyleGAN and its variants, have shown exceptional capability in generating highly realistic images. However, training these models remains challenging in domains where data is scarce, as it typically requires large datasets. In this thesis work, we introduce a versatile framework that enhances the capabilities of a pre-trained StyleGAN for various tasks, including domain adaptation, reference-guided image synthesis, and text-guided image manipulation even when only a small number of training sample are available. We achieve this by integrating the CLIP space into the generator of StyleGAN using hypernetworks. These hypernetworks introduce dynamic adaptability, enabling the pre-trained StyleGAN to be effectively applied to specific domains described by either a reference image or a textual description. To further improve the alignment between the synthesized images and the target domain, we introduce a CLIP-guided discriminator, ensuring the generation of high-quality images. Notably, our approach shows remarkable flexibility and scalability, enabling text-guided image manipulation with text-free training and seamless style transfer between two images. Through extensive qualitative and quantitative experiments, we validate the robustness and effectiveness of our approach, surpassing existing methods in terms of performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Erişim, poz kontrolü ve ters kinematik, diğer görevlerin temelini oluşturan robotik manipülator görevleridir. Bu nedenle literatürde, bahsedilen görevler için çeşitli alanları kapsayan bir çok çalışma mevcuttur. Kontrol, planlama ve son zamanlardaki öğrenme metodları bu alanların arasındadır. Geleneksel kontrol algoritmaları tekillik ve eklem sınırları etrafında başarısız olma eğilimindedir ve kendi kendine çarpışma farkındalığını doğal olarak ele alamazlar. Planlama tabanlı yöntemler, reaktif davranışlar için yeterince hızlı değildir ve çalışmak için kontrolcüler dahil ek altyapı gerektirir. Bu sorunları ele almak için öğrenmeye dayalı yöntemler ortaya çıkmıştır. Ancak öğrenmeye dayalı yöntemlerin birçoğu serbest başlangıç ve hedef pozlarını ele almaz, kendi kendine çarpışma farkındalığını göz ardı eder, hedeflerinde oryantasyon bilgisi içermez, küçük çalışma alanlarında çalışır. Ek olarak bir çoğu sadece kaba başarı metrikleri ile değerlendirilmiştir. Bu tezde, erişim, poz kontrolü ve ters kinematik konularındaki boşlukları doldurmak için modelden-bağımsız Pekiştirmeli Öğrenme (PÖ) ve yalancı-ters kontrolü birleştiren, durum uzayı ve ödül fonksiyon tasarımını içeren yeni bir hibrit yaklaşım sunuyoruz. Yalancı-ters kontrol, istenilen görev uzayı (örneğin, uç işlevci pozu) hızları verildiğinde zaten eklem hızlarını hesaplar ve sadece robotun kinematik yapı- lanmasına ihtiyaç duyar. Yalancı-ters kontrol, genellikle eklem sınırlarından, tekilliklerden ve bireysel bağlantı elemanlarının birbirleriyle çarpışmadan uzak olduğu durumlarda güvenilirdir. Yaklaşımımızın ana fikri, PÖ'yü yukarıda bahsedilen durumların üstesinden gelmek için kullanmaktır. Bu amaçla, yeni bir durum uzayı ve ödül fonksiyonu tasarladık. Ödül fonksiyonumuz, pozisyon (erişim görevi) veya poz (ters kinematik ve poz kontrol görevleri) hatalarını, kendine çarpmaları azaltmayı ve hedefe yakın eklem hızlarını en aza indirmeyi amaçlamaktadır. Ek olarak, öğrenmeye yardımcı olması için bir müfredat öğrenme metodolojisi geliştirdik. Son olarak, görev performansını daha da artırmak için ""anahtarlama"" adını verdiğimiz basit bir değişiklik sunduk. Yaklaşımımızı, çeşitli durumlar ve görevler için benzetim ortamında dört robot ile değerlendirdik ve geleneksel ve öğrenmeye-dayalı yaklaşımlarla karşılaştırdık. Sonuçlarımız, yaklaşımımızın erişim görevlerinde ortalama hata, çeşitli eşiklerde başarı oranları ve son hız açısından temel yaklaşımları belirgin bir şekilde geride bıraktığını göstermektedir. Ek olarak, tüm senaryolarda kendi kendine çarpma sayısını azalttık. Oryantasyon eklendiğinde yaklaşımımız daha iyi sonuçlar elde etti, ancak yöntemlerin hiçbiri (özellikle öğrenmeye-dayalı temel aldığımız yöntemler) çok iyi performans göstermedi. Literatürdeki öğrenmeye-dayalı yöntemlerin neredeyse her zaman oryantasyonu göz ardı ettiğini görüyoruz. Bu nedenle, oryantasyon başarısızlığının nedenlerini ve potansiyel çözümleri kapsamlı bir şekilde tartışıyoruz.","Reaching, pose control, and inverse kinematics are fundamental robotic manipulator tasks that underpin other tasks and as such, there is a vast body of related literature from various fields. Control, planning, and more recently learning fields are among the main ones. Traditional control algorithms are prone to failure around singularities and joint limits and do not naturally handle self-collisions. Planning methods are not fast enough for reactive behaviours and require additional infrastructure, including controllers, to work. Learning-based methods have emerged to tackle these issues. However, most of them do not handle arbitrary initial and target poses, ignore self-collisions, do not include orientation information in their targets, work in small workspaces and evaluate themselves with coarse success metrics. In this thesis, we introduce a novel hybrid approach that combines Pseudo-inverse control (PinvC) and model-free reinforcement learning (RL), including state space and reward function design, to fill these gaps in the context of reaching, pose control and inverse kinematics. PinvC already calculates joint velocities given desired task-space (e.g. the end-effector pose) velocities and only requires the kinematic structure of the robot. PinvC is mostly reliable away from joint limits, singularities and when individual links are not prone to collisions. The main idea behind our approach is to use RL to handle these situations. Towards this end, we design a novel state space and reward functions. Our reward function aims to minimize position (reaching task) or pose (inverse kinematics and pose control tasks) errors, reduce self-collisions and reduce joint velocities near the target. Furthermore, we develop a curriculum learning methodology to aid learning. Lastly, we introduce a simple modification, which we call ""switching"" to further improve task performance. We evaluate our approach with four simulated robots for various problem settings and compare it against traditional and learning-based approaches. Our results show that our approach decidedly outperforms the baselines in terms of mean error, success rates at various thresholds and terminal speed for reaching tasks. In addition, we reduced the number of self-collisions across all the scenarios. Our approach achieved better results when orientation was included, but none of the methods performed very well, especially the learning baselines. We note that the learning-based methods in the literature almost always ignore orientation. As a result, we comprehensively discuss the reasons for orientation failure and potential remedies."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ağır nesnelerin insan-robot birlikte manipülasyonu sırasında görev performansını optimize etmek için derin pekiştirmeli öğrenme (DPL) modelinin öncüsü olarak bir yapay sinir ağı (YSA) modeli kullanan iki aşamalı bir makine öğrenimi yaklaşımı öneriyoruz. İlk aşamada, YSA modeli, insanın nesneyi hızlandırma veya yavaşlatma niyetini tahmin eder (büyük eylemsizliği nedeniyle nesnenin gerçek hızlanmasından veya yavaşlamasından önce gelir). Bu olasılıksal tahmin daha sonra robotun göreve katkısını değiştiren uyarlanabilir bir giriş kontrolörünün kazancını hesaplamak için kullanılır. İkinci aşamada, DPL modeli bu kazanca ince ayar yapar ve hareketteki sarsıntıyı ve insan tarafından harcanan fiziksel çabayı en aza indirerek görev performansını optimize eder. Fiziksel insan-robot etkileşimi görevi için bir DPL modelinin çevrimiçi eğitimi oldukça zaman alıcı olduğundan ve kontrolör kazancındaki ani değişiklikler nedeniyle insan için potansiyel olarak tehlikeli olabileceğinden, DPL modelinin simülasyonlar yoluyla çevrimdışı eğitimi için koşullu varyasyonel otomatik kodlayıcı ile veriye dayalı bir insan kuvveti modeli geliştirilmiştir. Bu amaçla, DPL modelini eğitmek ve doğrulamak için admitans kontrolörünün 3 farklı sabit kazancı (minimum, nominal ve maksimum) altında altı denekten deneysel veriler toplanmıştır. Tek başına YSA modeli ve önerilen iki aşamalı yaklaşım (YSA + DPL) tarafından üretilen uyarlanabilir admitans kazanç profilleri, birlikte manipülasyon simülasyonları aracılığıyla karşılaştırılmıştır. Sonuçlar, iki aşamalı yaklaşımla elde edilen kazanç profilinin, YSA modeli tarafından sağlanan başlangıç profiline kıyasla insan çabasında ve sarsıntıda bir azalmaya yol açtığını göstermektedir.","We propose a two-layer machine learning (ML) approach, which utilizes an artificial neural network (ANN) model as a precursor for a deep reinforcement learning (DRL) model, to optimize task performance during human-robot co-manipulation of heavy objects. In the first layer, the ANN model estimates the human intention to accelerate or decelerate the object (which precedes the actual acceleration or deceleration of the object due to its large inertia). This probabilistic estimation is then used to calculate the gain of an adaptive admittance controller, which alters the robot's contribution to the task. In the second layer, the DRL model fine-tunes this gain and optimizes the task performance by minimizing the jerk in movement and the physical effort made by human. Since online training of a DRL model for a physical human-robot interaction (pHRI) task is highly time-consuming and can potentially be dangerous for the human due to abrupt changes in controller gain, a data-driven human force model was developed by a conditional variational auto-encoder (C-VAE) for offline training of the DRL model via simulations. For this purpose, experimental data was collected from six subjects under 3 different fixed gains of the admittance controller (minimum, nominal, and maximum) to train and validate the DRL model. The adaptive admittance gain profiles generated by the ANN model alone and the proposed two-layer approach (ANN + DRL) were compared through co-manipulation simulations. The results show that the gain profile obtained by the two-layer approach leads to a decrease in human effort and jerk compared to the initial profile provided by the ANN model."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dinamik modellerin eğitimi, büyük ölçekli eğitimin hesaplama ve bellek gereksinimlerini azalttığı için DNN'lerde ilgi görüyor. Dinamik eğitim için öne çıkan yaklaşımlardan biri olan kademeli budama, eğitim sırasında bir modelin parametrelerini budar (veya seyreltir). Bununla birlikte, kademeli budamanın yan etkilerinden biri, seyrekleştirmenin hızlandırıcılar arasında dengesiz bir iş yükü getirmesi ve bunun da boru hattı paralellik verimliliğini etkilemesidir. Bu çalışma, budamanın olumsuz performans etkilerini gidermek için boru hattındaki yükleri dinamik olarak dengeleyen DynPipe'ı tanıtmaktadır. DynPipe, dinamik modellerde yük dengelemeye ek olarak, toplam yükü performansı düşürmeden daha az sayıda GPU'ya sığdırabilir. DynPipe, çok GPU'lu tek düğümlerde ve çok düğümlü sistemlerde çalışır. Deneysel sonuçlar, DynPipe'ın büyük dil modeli eğitiminde kullanılan son teknoloji çözümlere göre tek bir düğümde eğitimi %5,64'e ve çok düğümlü bir ortamda %8,43'e kadar hızlandırabildiğini göstermektedir. DynPipe aşağıdaki adreste mevcuttur: https://anonymous.4open.science/r/DynPipe-1EC5","Training of dynamic models is gaining traction in DNNs as it reduces computational and memory requirements of large-scale training. Gradual pruning, one of the prominent approaches for dynamic training, prunes (or sparsifies) the parameters of a model during training. However, one of the side effects of gradual pruning is that sparsification introduces an imbalanced workload across accelerators, which in turn affects the pipeline parallelism efficiency. This work introduces DynPipe which dynamically load balances the stages of the pipeline to offset the negative performance effects of pruning. On top of load balancing dynamic models, DynPipe can dynamically pack work into fewer GPUs, while sustaining performance. DynPipe works on single nodes with multi-GPUs and also on systems with multinodes. Experimental results show that DynPipe can speed up the training up to 5.64% in a single node, and 8.43% in a multi-node setting, over state-of-the art solutions used in training production large language models. DynPipe is available at https://anonymous.4open.science/r/DynPipe-1EC5"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Belirsizlik gelecek tahmininde çok kritik bir rol oynamaktadır. Gelecek belirsizdir ve bu birçok gelecek ihtimali olduğunu gösterir. Bir gelecek tahmini yöntemi güvenilir olabilmek için bütün ihtimalleri kapsamalıdır. Otonom sürüşte emniyet açısından kritik kararlar verebilmek için tüm modları kapsamak aşırı derecede önemlidir. Bilgisayarlı görü yöntemleri son yıllarda çok hızlı şekilde ilerse de gelecek tahmini hala zor olmaya devam etmektedir. Bu zorluklardan birkaç tanesi geleceğin belirsizliği, tüm sahnenin anlaşılmasının gerekliliği ve piksellerin gürültülü olmasıdır. Bu tezde, bu problemlere hareket bilgisini modelleyerek ve zamansal dinamikleri öğrenerek çözümler sunuyoruz. İlk olarak, sahnedeki gelecek bilgisinin açık bir şekilde işlenmesini sunarak video alanında gelecek tahmini yöntemi yapan bir yöntem sunuyoruz. Bu yöntem görünüşü ve hareketi ayrı ayrı modelleyerek sahnenin durağan ve hareketli kısımlarını ayırmaktadır. Bu sayede piksellerin gürültülü uzayından kaçarak hareket bilgisiyle tahmin yapmak mümkün olmuştur. Bu yöntemin kazançlarını detaylı bir şekilde göstermekteyiz. İkinci olarak, sahneyi 3 boyutlu yapı ve artakalan hareket olacak şekilde ikiye ayırmayı sunuyoruz. Bu ayrışım ile sahnedeki durağan ve hareketli kısımları ayrı ayrı modelleyebiliyoruz. Durağan kısımlar 3 boyutlu yapıyla ve hareketli kısımlar ise artakalan hareket bilgisiyle tahmin edilmektedir. Artakalan hareket için önce 3 boyutlu yapıdaki hareketi tahmin edip bu hareket üstüne koşullanarak tahmin yapılmaktadır. Bu ayrışımın ve koşullanmanın etkilerini detaylı bir şekilde göstermekteyiz. Son olarak, kompakt gösterimden etkilenerek, gelecek tahmini problemini kuşbakışı uzaya taşımayı sunuyoruz. Yeni bir formulasyon kullanarak gelecek tahmininden ziyade gelecek durum tahmini yapmanın problemi daha da kolaylaştıracağını sunmaktayız. Bu amaçla, gelecek bilgisinin kullanıldığı bir dağılım kullanarak daha dağınık ve kurallara uygun yörünge tahminleri yapılmaktadır. Yaptığımız detaylı deneyler ile bu yöntemin daha gerçekçi tahminler yaptığı gösterilmiştir.","Uncertainty plays a key role in future prediction. The future is uncertain. That means there might be many possible futures. A future prediction method should cover the whole possibilities to be robust. In autonomous driving, covering multiple modes in the prediction part is crucially important to make safety-critical decisions. Although computer vision systems have advanced tremendously in recent years, future prediction remains difficult today. Several examples are uncertainty of the future, the requirement of full scene understanding, and the noisy outputs space. In this thesis, we propose solutions to these challenges by modeling the motion explicitly in a stochastic way and learning the temporal dynamics in a latent space. Firstly, we propose to use the motion information in the scene in stochastic video prediction. By separately modeling the appearance of the scene and the motion in the scene, the static and the dynamic parts are partitioned. The dynamic part is predicted by the explicit motion. Since the motion is predicted, the noisy pixel space is not used. We demonstrate the benefits of using the motion information scene explicitly. Secondly, we propose to separate the scene into the 3D structure and the motion, which correspond to static and dynamic parts, in video prediction in driving scenarios. The static part is handled by the 3D structure and the motion of the ego-vehicle, whereas the dynamic part is handled by the remaining motion in the scene. The remaining motion is captured by explicitly conditioning the dynamic part on top of the static one. We demonstrate the improvements of the conditioning and structure-aware separation. Finally, motivated by the compact representation, we propose a method for stochastic future prediction in Bird's-Eye View representation. Using a new formulation, we approach the problem as a state prediction rather than a trajectory prediction. For that purpose, we choose to use more powerful label-aware latent variables to generate more diverse and admissible future trajectories. Extensive evaluations show that both the diversity and the accuracy of the future trajectories significantly improved, especially in challenging cases of spatially far regions and temporally long spans."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizimler, resimsel soyutlama ve iletişim için güçlü araçlardır. Dijital sanatlar, karikatürler ve çizgi romanlar dahil olmak üzere bu çeşitli çizim biçimlerini anlamak, bilgisayarla görme ve bilgisayar grafiği toplulukları için büyük bir ilgi sorunu olmuştur. Dijital ortamda çok sayıda çizim özellikle çizgi roman ve çizgi film şeklinde erişilebilir olsa da, bu örnekler geniş stilistik farklılıklar içermekte olduğundan, alana özel tanıma modellerinin eğitiminde pahalı etiketleme süreçleri gerekmektedir. Bu çalışmada, değiştirilmiş öğrenci ağı güncelleme dizaynına sahip bir öğretmen öğrenci ağına dayalı öz denetimli öğrenmenin yüz ve vücut dedektörleri oluşturmak için nasıl kullanılabileceğini gösterdim. Kurulumum, yalnızca az sayıda etiketlenmiş çizim resimleri seti kullanarak çok miktardaki etiketlenmemiş veriden faydalanmayı sağlıyor. Ayrıca, stil aktarma yöntemlerini öğrenme sürecine ekleyerek alan dışı doğal hayat resimlerinden faydalanmanın detektör performansını daha da arttırdığını kanıtladım. Kombine mimari, minimum etiketleme çabası kullanarak son teknoloji çizim detektörlerinden daha iyi ya da kıyaslanabilir performansa erişim sağlamakta. Bu detektör yapısından faydalanarak birtakım ekstra görevleri de tamamladım. İlk olarak, etiketlenmemiş veriler üzerinden geniş bir yüz çizim resim seti çıkarttım (~1.2 milyon örnek) ve yüz yaratmak için son teknoloji üretken çekişmeli ağ (GAN) modellerini, yeniden inşa etmek için ise son teknoloji bir GAN inversiyon modeli eğittim. Detektör destekli veriden yararlanıldığı takdirde, üretken modeller başarılı bir şekilde çeşitli stilistik özellikleri öğreniyor. İkinci olarak, var olan etiketlenmiş veriyi genişletmek için bir etiketleme aracı geliştirdim. Bu araç kullanıcılara panel, konuşma balonu, anlatım, yüz ve vücut sınırlayıcı kutularını etiketleme; yazıları yüz ve vücutlarla eşleştirme; yazıları transkripte dökme; resimdeki aynı karakterleri eşleştirme imkanı sunuyor.","Drawings are powerful means of pictorial abstraction and communication. Understanding diverse forms of drawings, including digital arts, cartoons, and comics, has been a major problem of interest for the computer vision and computer graphics communities. Although there are large amounts of digitized drawings from comic books and cartoons, they contain vast stylistic variations, which necessitate expensive manual labeling for training domain-specific recognizers. In this work, I show how self-supervised learning, based on a teacher-student network with a modified student network update design, can be used to build face and body detectors. My setup allows exploiting large amounts of unlabeled data from the target domain when labels are provided for only a small subset of it. I further demonstrate that style transfer can be incorporated into my learning pipeline to bootstrap detectors using a vast amount of out-of-domain labeled images from natural images (i.e., images from the real world). My combined architecture yields detectors with state-of-the-art (SOTA) and near-SOTA performance using minimal annotation effort. Through the utilization of this detector architecture, I accomplish a set of additional tasks. First, I extract a large set of facial drawing images (~1.2 million instances) from unlabeled data and train SOTA generative adversarial network (GAN) models to generate and a SOTA GAN inversion model to reconstruct faces. When the detector-aided data is leveraged, these generative models successfully learn diverse stylistic features. Secondly, I implement an annotation tool to enlarge the existing set of annotated data. This tool offers users to annotate bounding boxes of panels, speech bubbles, narrations, faces, and bodies; to associate text boxes with faces and bodies; to transcript the text; to match the same characters in the image."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan-aracı etkileşiminde kullanıcı ilgisinin nasıl canlı tutulabileceği, üzerinde hala çalışılmakta olan açık bir araştırma konusudur. Bu tezde, bu amaca yönelik olarak, konuşma güdümlü bir gülme arka kanalı üretme modeli önerilmektedir. Problemi, durumu vektörünün konuşma sinyaliyle, ödül işlevinin ise kullanıcı ilgisiyle temsil edildiği, böylelikle kullanıcı ilgisinin en büyüklendiği bir Markov karar süreci olarak formüle ediyoruz. Çevrimiçi eğitimin insan-aracı etkileşimi için sıklıkla uygulanamaz olması nedeniyle, insandan insana ikili etkileşime ilişkin video kayıtlarından oluşan mevcut veri kümelerini, arka kanal üretme görevine yönelik olarak bir aracıyı eğitmek için kullanıyoruz. Bu problemi, eğitim sırasındaki dağılım kayması sorununu azaltmak için Q değerlerinin fazla tahminini azaltan bir yöntem olan korunumlu Q-öğrenmeye (CQL) dayalı bir aktör-eleştirmen yöntemi kullanılarak ele alınmıştır. Önerilen CQL tabanlı yaklaşım, gülme üretme görevi için objektif yöntemlerle IEMOCAP veri seti üzerinde değerlendirilmiştir. Önceki politika dışı Q-öğrenme yaklaşımlarıyla karşılaştırıldığında, gülme üretim hızı açısından veri seti ile uyum iyileştirildi. Ek olarak, öğrenilen politikanın başarısını, beklenen kullanıcı ilgisini politika dışı politika değerlendirme teknikleriyle tahmin ederek değerlendirdik.","Sustaining engagement in human-agent interaction remains an open problem. The purpose of this thesis is to propose a model for maintaining engagement during human-agent interaction through speech-driven backchannel generation. The problem is modeled as a Markov decision process, with the speech signal representing the state and the reward of maximizing human engagement. Due to the fact that online training is frequently impracticable for human-agent interaction, existing datasets on human-to-human dyadic interaction are employed to train an agent for the backchannel generation task. The problem has been addressed using an actor-critic method based on conservative Q-learning (CQL), which reduces the distributional shift problem during training by suppressing Q-value overestimation. The suggested CQL-based approach is objectively evaluated for the laughter generating task on the IEMOCAP dataset. When compared to previous off-policy Q-learning approaches, compliance with the dataset is improved in terms of laugh production rate. Additionally, the learned policy's success is demonstrated by estimating expected engagement with off-policy policy evaluation techniques."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kesin olay örneklemesi, mevcut emtia işlemcilerde bulunan, donanımsal olayların örneklenmesinde ve bu olayların tetiklenmesine sebep olan komutların tanımlanmasını sağlayan bir profilleme özelliğidir. Bu özellik sayesinde, performans darboğazları düşük ek masraf ile saptanabilir ve bu darboğazların kaynak koddaki yerleri belirlenebilir. Çeşitli performans darboğazlarını belirlemek üzere birkaç profilleme aracı geliştirilmiştir. Ancak bu araçlardan hiçbiri, iş parçacıkları arasındaki veri hareketini belirleyemez, veya çok iş parçacıklı uygulamalarda veri yerelliğini ölçemez. Ek olarak, bu donanım özelliği birçok profilleme aracında kullanılmış olsa da, bu özelliğin doğruluğu ve masrafı analiz eden çok az çalışma vardır. Tüm bu çalışmalar sadece Intel mimarisine yöneliktir; ve bunlardan hiçbiri bu donanım özelliğinin hafıza masrafı, stabilite, ve fonksiyonellik taraflarını değerlendirmemiştir. Bu tezde, üç-yönlü büyük katkı öne sürülmüştür. İlk olarak, sırasıyla Intel ve AMD'nin kesin olay örneklemesi araçları olan PEBS ve IBS üzerinde derinlemesine nitel ve nicel analiz yapılmıştır. İkinci olarak, iş parçacıkları arasında haberleşmeyi saptayabilen ve bunları true-sharing ve false-sharing olarak sınıflandırıp haberleşme matrislerinde kaydedebilen bir profilleme aracı olan ComDetective öne sürülmüştür. Üçüncü olarak, çok iş parçacıklı uygulamalarda özel önbellek ve paylaşılan önbellek içerisinde veri yerelliğini ölçebilen bir profilleme aracı olan ReuseTracker öne sürülmüştür. ComDetective ve ReuseTracker, kesin olay örneklemesi kullanarak yüksek doğruluk oranı ve düşük ek masraf ile çok iş parçacıklı uygulamaları profilleyebilmektedir. Intel PEBS ve AMD IBS arasındaki kilit farkları analiz edebilmek adına, ilk olarak bir dizi dikkatle tasarlanmış microbenchmark geliştirilmiştir. Bu microbenchmarklar ile yapılan nicel analiz ve nitel çalışmalar sonucunda, Intel PEBS'in donanımsal olayları örneklem sayısı bakımından daha yüksek doğruluk ve stabilite ile örnekleyebildiği gözlemlenirken; AMD IBS'in ise bilgi bakımından daha kapsamlı örnekleme yaptığı görülmüştür. Ek olarak, PEBS ve IBS'in farklı komutlar üzerinde aynı donanım olayını örneklerken kötü yönde etkilendiği gözlemlenmiştir. Dahası, elde edilen deney sonuçlarımızın Intel ve AMD makinelerinde çalışabilecek tam teşekküllü bir profilleme aracı bağlamında ilişkilendirilmesi gösterilmiştir. Intel PEBS ve AMD IBS'in incelenmesinden sonra, iş parçacıkları arasındaki haberleşmeyi yüksek doğruluk, düşük ek masraf düşük çalışma zamanı ile saptayabilen bir profilleme aracı olan ComDetective öne sürülmüştür. ComDetective kesin olay örneklemesi ile hafıza erişimlerine örnekleyerek ve donanımsal debug yazmaçlarını kullanarak iş parçacıkları arasındaki haberleşmeleri saptayabilmektedir. Haberleşmeyi saptamaya ek olarak, ComDetective bu haberleşmeleri true-sharing ve false-sharing olarak sınıflandırabilmektedir. 18 farklı uygulamada 500K örnekleme aralığı ile çalıştırıldığında, ComDetective'nin sırasıyla zaman ve hafıza ek masrafları sadece 1.30times ve 1.27times olarak ölçülmüştür. ComDetective kullanarak, birkaç microbenchmark, PARSEC benchmark koleksiyonu ve bazı CORAL uygulamaları için haberleşme matrisleri oluşturulmuş, ve bu matrisler MPI karşıtları ile karşılaştırılmıştır. Bu sayede bazı uygulamalarda haberleşme darboğazları keşfedilmiş olup, düzeltilmeleriyle beraber 13%'e kadar hızlanma başarılmıştır. Ek olarak, bir veri yerelliği ölçütü olarak sıkça kullanılan yeniden-kullanım mesafesi'ni ölçebilen ReuseTracker öne sürülmüştür. Yeniden-kullanım mesafesi, herhangi bir hafıza adresine ard-arda yapılan iki erişim (kullanım ve yeniden-kullanım) arasında erişilen farklı adreslerin sayısıdır, ve dolayısıyla bir veri yerelliği ölçütüdür. ReuseTracker kesin olay örneklemesi ve de donanımsal debug yazmaçlarından faydalanarak yeniden-kullanım mesafesini ölçmektedir. Ek olarak, ReuseTracker önbellek-tutunum etkilerini göz önünde bulundurarak çok iş parçacıklı uygulamalarda, var olan diğer araçlara göre daha az ek masraf ile yeniden-kullanım mesafesini ölçebilmektedir. Öne sürülen bu araç, sadece 2.9x zaman ve 2.8x hafıza ek masrafına sebep olmaktadır. Kullanıcı tarafından belirlenebilen yeniden-kullanım mesafesine sebep olacak şekilde özel olarak yazılmış bir microbenchmark ile ölçlüdüğü üzere, ReuseTracker ortalama 92% doğruluk oranına sahiptir. Paylaşılmış önbelleklerde false-sharing olan mekansal yeniden-kullanım'ların saptanması, ve bazı uygulamaların komşu önbellek-satırı prefetch optimizasyonundan fayda sağlayabileceğine dair tahmin yapılması olarak iki farklı senaryoda ReuseTracker'nin, kod düzenlemesinde nasıl rehber alınabileceği gösterilmiştir. Bu tez içerisinde öne sürülen araçların ve analizlerin, donanım mimarlarının yeni kesin olay örnekleme özellikleri geliştirirken ve de performans mühendislerinin yazılım performansını ayarlarken faydalı olabileceği gibi; performans analiz ve donanım içerisindeki profilleme araçları alanında ileride olabilecek araştırmalar için yeni yollar açabileceği beklentimizdir.","Precise event sampling is a profiling feature in current commodity CPUs that allows sampling of hardware events and identifies the instructions that trigger the sampled events. It offers the ability to detect performance bottlenecks with low overhead as well as the locations of the bottlenecks in source code. There have been a number of profiling tools developed using this feature that detect various sources of performance bottlenecks. However, none of these tools detects inter-thread data movement nor measures data locality in multithreaded applications, which have become widely used due to the ubiquity of multicore architectures. Furthermore, though this hardware facility has been used in multiple profiling tools, there have been only few works that analyze it in terms of accuracy and overhead. All of these works target only the facility in Intel architecture, and none of these works evaluates other aspects of precise event sampling such as memory overhead, stability, and functionality of the facility. In this dissertation, we present threefold major contributions. First, we perform the most comprehensive and in-depth qualitative and quantitative analyses to date on PEBS and IBS, which are the precise event sampling facilities of two major vendors, Intel and AMD, respectively. Next, we show the potential for imaginative use of precise event sampling in developing low overhead yet accurate profiling tools for multicore and design two diagnostic tools with a particular focus on data movement as it constitutes the main source of inefficiencies. First of such tools is ComDetective that detects inter-thread communications, classifies them into true sharing or false sharing, and records them in the form of communication matrices. Second is ReuseTracker that measures data locality in private and shared caches of multithreaded applications. ComDetective and ReuseTracker leverage precise event sampling to profile multithreaded applications accurately and with low overheads compared to their state-of-the-art alternatives. To analyze key differences between Intel PEBS and AMD IBS, we firstly developed a series of carefully designed microbenchmarks. Through our qualitative analysis and quantitative study using the microbenchmarks, we found that Intel PEBS samples hardware events more accurately and with higher stability in terms of the number of samples that it captures, while AMD IBS records richer set of information at each sample. We also discovered that both PEBS and IBS are afflicted with bias when sampling the same event across multiple different instructions in a code. Moreover, we also show how our findings from the quantitative experiments using the microbenchmarks are relevant for a full-fledged profiling tool that runs on Intel and AMD machines. We develop ComDetective, a profiling tool that captures inter-thread communications accurately and with low runtime and memory overheads. ComDetective employs precise event sampling to sample memory accesses and utilizes hardware debug registers to detect inter-thread communications. In addition to detecting communications, ComDetective can also classify them into true or false sharing. Its time and memory overheads are only 1.30× and 1.27×, respectively, for the 18 applications studied under 500K sampling interval. Using ComDetective, we generate insightful communication matrices from several microbenchmarks, PARSEC benchmark suite, and some CORAL applications and compare the produced matrices against the matrices of their MPI counterparts. Using ComDetective, we identify communication bottlenecks in a few codes and achieve up to 13% speedup from code refactoring those codes. We also design ReuseTracker, which is a profiling technique that measures reuse distance - a widely used metric that measures data locality. Reuse distance is a measurement of data locality as it is the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location (use and reuse). ReuseTracker leverages precise event sampling to capture uses and debug registers to detect reuse in measuring reuse distance. ReuseTracker can measure reuse distance in multithreaded applications by also considering cache-coherence effects with much lower overheads than existing tools. It introduces only 2.9x time and 2.8x memory overheads. It achieves 92% accuracy when verified against a carefully crafted configurable microbenchmark that can generate user-specified reuse distance patterns. We demonstrate in two use cases how ReuseTracker can be used to guide code refactoring by detecting spatial reuses in shared caches that are also false sharing and how it can also be used to predict whether certain applications can benefit from adjacent cache line prefetch optimization. We expect that the analysis, algorithms, and the tools presented in this dissertation will benefit hardware architects in designing new precise event sampling features and performance engineers in performance tuning of their software while also paving the way for a new generation of low-overhead profiling tools. Moreover, the outcomes of the dissertation can be used by the end-users (e.g., data analysts, engineers, compiler developer) to identify the performance issues and increase the data locality aspects of their software."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Robotlar birçok farklı endüstride kullanılmalarının yanı sıra fabrikalardan çıkıp günlük yaşamlarımıza karşılama robotları, uzaktan bulunma robotları, oyuncaklar, otonom araçlar ve belki de en yaygın olarak elektrikli süpürgeler şeklinde girmektedir. Yakın zamanda mobil manipülatörler gibi daha yetenekli robotların, evlerimizde bize yardımcı olduğunu görebiliriz. Sabit ve kontrol altındaki endüstriyel ortamlarda robotları belirli görevler için uzmanlar aracılığı ile programlamak ve kontrol etmek, onları günlük ortamlarda kullanmaktan önemli ölçüde farklıdır. Bir robotu, herhangi bir uzman olmadan istenilen görevleri gerçekleştirecek şekilde programlayabilmek yakın gelecek için önem arz etmektedir. Gösterimlerdenden Öğrenme (GÖ) alanı, robotların insanlardan öğrenmesini sağ-lamayı amaçlar. Bu çerçevede, bir beceriyi analitik olarak türetip elle programlamak yerine robot, istenen beceriyi insan gösterimlerinden öğrenir. İnsan etkileşimi yönleri nedeniyle, GÖ'nün düşük miktarda veri ile başarılı olması gerekir. Deneme yanılma için kurulum bir yana, ödül fonksiyonu tasarlayacak bir uzman olmadığında, pekiştirmeli öğrenme yapmak gerçekçi değildir. Bu nedenle, sınırlı bir dizi gösterimden mümkün olduğunca fazla bilgi çıkarmak ve daha önce öğrenilen becerileri aktarım yoluyla kullanmak çekici bir seçenektir. Bu tezin GÖ çerçevesinde, önemli noktalardan hareket ve hedef/algısal beceri modelleri öğrenilir. Beceriyi yürütmek için eylem modelleri ve yürütmeyi izlemek için hedef modelleri kullanılır. Saklı Markov Modelleri (SMM'ler) ve türevleri, düşük miktarda önemli nokta gösteriminden eylem ve hedef modellerini öğrenmek için uygundur. Tezin ilk kısmı, tek bir kullanıcı için becerilerin aktarma yoluyla öğrenimini kolaylaştırmak için Durum Geçişi Aktarımı algoritmasını tanıtmaktadır. Bu algoritmanın, hedef modeller için sıfırdan öğrenmeye kıyasla daha başarılı olduğunu ancak eylem modeli performansını önemli ölçüde artırmadığını gösterilmiştir. Bununla birlikte, SMM'lerin, özellikle algısal durumlar için, aktarımlı öğrenim ve çoklu veri kaynakları (örneğin birden çok kullanıcı, aynı beceri için birden çok nesne, vb.) kullanımı konularında bazı sınırlamaları vardır. Bu sınırlamalar kısmen çok değişkenli Gauss emisyonlarının kullanılmasından ve doğru sayıda gizli durum seçmenin zorlu-ğundan kaynaklanmaktadır. Bu amaçla daha esnek hedef modelleri öğrenmek için, SMM'leri, normalleştiren akış modellerini, ve robotlara özel uyarlamaları birleştirererek Koşullu Akış Saklı Markov Modeli (C-FlowHMM) adı verilen bir üretici model tasarlanmıştır. Buradaki fikir, daha genel bir emisyon modelinin öğrenilebilmesi için Gauss emistonları yerine gizli durumlara göre koşullandırılmış tek bir normalleştiren akış modeli kullanmaktır. Gizli durumlar, tek bir model kullanıldığı için bilgi paylaşmış olurlar ki bu düşük veri rejimine uygundur. Ayrıca, sinir ağı modelleri, aktarımlı öğrenme için daha uygundur. İnsan gösterimlerinden C-FlowHMM öğrenmek için bir ""expectation-maximization (EM)"" tabanlı algoritma türetilmiştir. Yapılan hedef modeli öğrenme deneylerinde, C-FlowHMM'nin, SMM'lere kıyasla daha iyi yürütme izleme performansına yol açtığı gösterilmiştir. Ayrıca, veriler daha çeşitli olduğunda C-FlowHMM'lerin daha iyi aktarım öğrenme performansı olduğu gözlemlenmiştir. Son olarak, C-FlowHMM'nin geleneksel SMM'lere kıyasla gizli durumların sayısında-ki değişime daha dayanıklı olduğu bulunmuştur.","On top of being used in many different industries, robots are getting out of factories and into our everyday lives in the form of greeter robots, telepresence robots, toys, autonomous cars and perhaps most ubiquitously vacuum cleaners. Soon we may see more capable robots, such as mobile manipulators, helping us in our homes. Programming and controlling robots to achieve certain tasks in controlled industrial environments with field experts is significantly different than using them in everyday environments. Being able to program a robot to achieve desired tasks without the presence of an expert is of importance for the near future. Imitation learning or Learning from Demonstration (LfD) field aims to enable robots to learn from humans. In this framework, instead of analytically deriving and manually programming a skill, the robot learns the desired skill from human demonstrations. Due to the human-interaction aspects, LfD needs to contend with a low amount of demonstrations which leads to a low amount of data. Using reinforcement learning on top of demonstrations is not feasible when there is no one to engineer a reward function, let alone have the setup for trial and error. Thus, extracting as much information as possible from a limited set of demonstrations and utilizing previously learned skills via transfer is an attractive option. In the LfD framework of this thesis, action and goal/perceptual models of skills are learned from keyframes. Action models are used to execute the skill and goal models are used to monitor this execution. Hidden Markov Models (HMMs) and their derivatives are suitable to learn action and goal models from a low amount of keyframe demonstrations. The first part of the thesis introduces the State Traversal Transfer algorithm to facilitate transfer learning of skills for a single user. We show that this algorithm leads to successful transfer compared to learning from scratch for goal models but do not significantly increase action model performance. However, HMMs have some limitations with transfer learning and multiple sources of data (e.g. multiple users, multiple objects for the same skill, etc.), especially about dealing with perceptual states. These limitations partially stem from using multivariate Gaussian emissions and the difficulty of choosing the correct number of hidden states. Towards this end, a generative model called Conditional Flow Hidden Markov Model (C-FlowHMM) is designed by combining conventional HMMs, normalizing flows, and robotic specific adaptations to improve model flexibility in learning goal/perceptual models of skills. The idea is to use a single normalizing flow model, conditioned on hidden states, instead of Gaussians so that a more general emission model can be learned. By using a single model, states share information which is suitable in a low data regime. Furthermore, a neural network model is more amenable to transfer learning. We develop an expectation-maximization (EM) algorithm to train C-FlowHMMs from human demonstrations which lead to better execution monitoring performance compared to HMMs. We also show that C-FlowHMMs result in better transfer learning performance when data is more varied. Finally, we demonstrate that C-FlowHMM is more robust to change in the number of hidden states compared to conventional HMMs."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Olay bilgisi çıkarma sistemleri için metin verisi işaretlemesi yapmak hem zor, hem pahalı, hem de hata yapmaya oldukça açıktır. Bu tezde, yeni detaylı işaretleme yapmak yerine, çok daha kolay şekilde elde edilebilen daha az detaylı (döküman ve cümle etiketlemesi) veri kullanmanın fizibilitesini ölçüyoruz. Döküman ve cümle etiketlerini kullanmak için çok amaçlı modelimizi, ana işimiz olan kelime sınıflandırmasının yanında döküman ve cümle ikili sınıflandırması yan işleri ile eğitiyoruz. Bu amaçta, değişen veri rejimleri içeren birtakım deneyler icra ediyoruz. Deneylerin sonuçları bu eklenen daha az detaylı verinin daha iyi performans ve stabiliteye yol açtığını gösterirken, aynı zamanda orijinal veriye sadece içinde hiçbir şekilde olay bilgisi bulundurmayan negatif dökümanlar eklemenin göz ardı edilemeyecek katkısını da gözler önüne seriyor.","Annotating text data for event information extraction systems is hard, expensive, and error-prone. We investigate the feasibility of integrating coarse-grained data (document or sentence labels), which is far more feasible to obtain, instead of annotating more documents. We utilize a multi-task model with two auxiliary tasks, document and sentence binary classification, in addition to the main task of token classification. We perform a series of experiments with varying data regimes for the aforementioned integration. Results show that while introducing extra coarse-grained data offers greater improvement and robustness, a gain is still possible with only the addition of negative documents that have no information on any event."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İçinde bulunduğumuz doğrudan ve dijital pazarlama çağında, müşteri odaklı metrikler, özellikle müşteri yaşam boyu değeri (YBD), müşteri ilişkileri yönetimi ve genel olarak pazarlama stratejileri için en önemli iş metriklerinden biri haline gelmiştir. Bir müşterinin tüm ilişki süresi boyunca firmaya getireceği kâr veya gelirin bir ölçüsü olan müşteri YBD'si, her bir müşteri için büyük farklılık gösterir ve bu da firmanın tüm müşteri havuzu üzerindeki YBD tahminlerini oldukça zorlaştırır. Hızla genişleyen yapay öğrenme alanı, bu sorunu çözmek için çok uygun bir araçtır. Bu çalışmada, yerel bir hizmet pazarı olan Armut AŞ'nin dijital reklamlarının optimizasyonunda kullanılan müşteri YBD'sini tahmin edecek bir model geliştirdik. Model, doğrusal regresyon, karar ağacı, aşırı gradyan artırma (XGBoost) ve yapay sinir ağları (YSA'lar) dahil olmak üzere çeşitli yapay öğrenme algoritmalarıyla denemeler yapılarak ve veri kümesindeki kategorik özellikler için kodlama tekniklerinin bir kombinasyonu ile deneyler yapılarak geliştirilmiştir. Modeller, 5 katlı zaman serisi çapraz doğrulama kullanılarak değerlendirilmiş olup, 1,740 ila 1,776 arasında değişen normalleştirilmiş ortalama karekök hatası (NOKH) ile karşılaştırılabilir sonuçlar vermiştir. Bu modeller arasından kullanılmak üzere XGBoost seçilmiştir. Bu model, verileri iş modeline göre bölümlere ayırarak ve eğitim seti zaman penceresini 12 aya genişleterek daha da optimize edilmiştir. Bu optimizasyon sonucunda 1,725 NOKH skoruna ulaşılmıştır. Elde edilen model devreye alınmıştır ve şu anda Armut AŞ'de çalışmaktadır.","In this age of direct and digital marketing, customer-oriented metrics, especially customer lifetime value (LTV), have become one of the most important business metrics for customer relationship management (CRM) and marketing strategies in general. The customer LTV, a quantification of the profit or revenue that a customer will bring to the firm over their entire relationship period, differs wildly for different customers, which makes predictions over the entire pool of the firm's customers quite challenging. The rapidly expanding field of machine learning is uniquely suitable for solving this problem. In this study, we develop a model for predicting customer LTV for a local services marketplace, namely Armut AŞ. The predicted LTV is then used for the optimization of the company's digital ads. The model was developed through experimenting with several machine learning algorithms including linear regression, decision tree, extreme gradient boosting (XGBoost), and artificial neural networks (ANNs), as well as experimenting with a combination of encoding techniques for the categorical features in the data set. The models were evaluated using 5-fold time-series cross-validation and yielded comparable results with normalized root mean square error (NRMSE) ranging from 1.740 to 1.776. The chosen model was XGBoost, and it was further optimized by segmenting the data based on its business model as well as expanding the training set time window to 12 months yielding the final model with an NRMSE of 1.725. This model was deployed and is currently operating at Armut AŞ."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Küresel çevre sorunlarının telafi edilemezliğin eşiğine yaklaşıp yaklaşmadığı, canlı türlerinin ve insanlığın yaşayabilirliğini tehlikeye atıp atmadığı konusunda çözülmemiş bir tartışma vardır. Çevresel Kuznets Eğrisi (EKC) kavramı, çevresel sürdürülebilirliğin temel yakıtı olarak sürekli ekonomik büyümeyi desteklemek için yaygın olarak kabul görürken, karşıt kanıtlar da sanayileşme ve seri üretimi çevresel bozulmanın başlıca nedeni olarak belirlemektedir. Bugüne kadar, küresel politika yapımındaki kritik önemine rağmen, EKC'nin dinamiklerini ulusal ve küresel düzeyde ölçmek için çok az şey yapılmıştır. Burada, kişi başına yıllık CO2 emisyonları, kişi başına Gayri Safi Yurtiçi Hasıla (GSYH) ve yıl için mevcut küresel panel verilerinin üç boyutlu bir özellik uzayında bir ölçme metriği sunuyoruz. Veri noktaları ile kübik özellik uzayının köşeleri arasındaki Chebyshev mesafesini ulusal ve küresel EKC için bir metrik olarak öneriyoruz. Önerilen metodolojimize dayanarak, en büyük küresel kümülatif CO2 emisyonuna sahip otuz ülkeyi beş kategoride sınıflandırıyor ve her kategorideki EKC dinamiklerini daha ayrıntılı olarak açıklıyoruz. Sonuçlarımız Fransa, Almanya ve Birleşik Krallık'ın EKC hipotezi ile uyumlu olduğunu, ancak diğer ülkelerin EKC dışı senaryolarla daha tutarlı olduğunu göstermektedir. Ayrıca Polonya ve Meksika'nın ek CO2 emisyonları olmadan nasıl sürekli ekonomik büyüme kaydettiğini de gösteriyoruz. Son olarak, metodolojimize göre küresel düzeyde ülkelerin göreceli performansını tartışıyor ve gelecekteki araştırmalar için yönler öneriyoruz.","There is an unresolved imbroglio regarding whether the global environmental issues are approaching the verge of irrecoverability, endangering the viability of living species and humanity. While the concept of the Environmental Kuznets Curve (EKC) is widely accepted in support for perpetual economic growth as the essential fuel for environmental sustainability, the contrasting evidence also designates industrialization and mass production as the principal cause of environmental deterioration. To date, despite its crucial significance in global policymaking, little has been done to quantify the dynamics of the EKC on a national and global level. Here, we introduce a quantification metric in a three-dimensional feature space of the available global panel data for annual CO2 emissions per capita, Gross Domestic Product (GDP) per capita and year. We propose the Chebyshev distance between the data points and the vertices of the cubic feature space as a metric for the national and global EKC. Based on our proposed methodology, we classify thirty countries with the greatest global cumulative CO2 emissions in five categories and further explain the dynamics of EKC in each category. Our results demonstrate that France, Germany, and the United Kingdom are aligned with the EKC hypothesis, but other countries are more consistent with non-EKC scenarios. We also show how Poland and Mexico have gone through constant economic growth without additional CO2 emissions. Lastly, we discuss the relative performance of countries on a global level according to our methodology and propose directions for future research."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Videolarda obje tespiti, segmentasyonu ve takibi alanında mevcut yaklaşımlar ya tüm videoyu girdi olarak alıp çevrimdışı olarak işleyerek sonuçların kalitesine odaklanmakta; ya da kare kare işleyerek performastan feragat ederek hıza odaklanmaktadır. Bu çalışmada, çevrimdışı muadillerinin performansı ile yakın olan bir çevrimiçi yöntem öneriyoruz. Bu yöntemde özgün olarak nesneleri kodlayan ve onları zaman içinde ilişkilendiren, mesaj ileten bir grafik sinir ağı sunuyoruz. Ayrıca modelimizi, özellik piramidi ağındaki özellikleri artık bağlantılarla birleştirmek için yeni bir modül ile güçlendiriyoruz. Uçtan uca eğitilmiş modelimiz, çevrimiçi yöntemler dahilinde YouTube-VIS veri setinde muadil modeller arasında en iyi performansı elde etti. DAVIS üzerinde yapılan diğer deneyler, modelimizin video nesnesi bölümleme görevine genelleme kabiliyetini göstermektedir. Ayrıca otonom sürüş ayarı konusundaki çalışmalarımızı değerlendiriyor ve KITTI MOTS veri setinde karşılaştırılabilir sonuçlar gösteriyoruz.","In Video Instance Segmentation (VIS), current approaches either focus on the quality of the results, by taking the whole video as input and processing it offline; or on speed, by handling it frame by frame at the cost of competitive performance. In this work, we propose an online method that is on par with the performance of the offline counterparts. We introduce a message-passing graph neural network that encodes objects and relates them through time. We additionally propose a novel module to fuse features from the feature pyramid network with residual connections. Our model, trained end-to-end, achieves state-of-the-art performance on the YouTube-VIS dataset within the online methods. Further experiments on DAVIS demonstrate the generalization capability of our model to the video object segmentation task. We also evaluate our work on autonomous driving setting and show comparable results in KITTI MOTS dataset."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Konumsal yoğunluk sorguları, birçok coğrafi veri analizi işinde temel bir yapıtaşı olmakla birlikte, kalabalık alanların belirlenmesi, trafik yoğunluğunun tahmini, navigasyon, yolcu talep analizi gibi gerçek hayatta çok sayıda uygulama alanına sahiptir. Öte yandan, konumsal yoğunluk sorgularının kullanıcı verileri kullanılarak cevaplanması, kullanıcıların gerçek konumlarını güvenilir olmayan üçüncü kişilere (örneğin, servis sağlayıcı veya veri toplayıcı görevi gören bir sunucuya) ifşa ederek kullanıcı mahremiyetini ihlal edebilir. Bu tezde, konumsal yoğunluk sorgularının modern bir mahremiyet koruma standardı olan Lokal Diferansiyel Mahremiyet (LDP) korumalı cevaplanması için bir çözüm öneriyoruz. Çözümümüz dört ana adımdan oluşmaktadır: bölümleme, hassasiyet bulma, kullanıcı tarafı gürültülü cevap hesaplaması ve sunucu tarafı tahminleme. İlk adım için, öncelikle üç temel bölümleme stratejisi öneriyoruz: Tekil Bölümleme, Bütünsel Bölümleme ve Rastgele Bölümleme. Üç temel bölümleme stratejisi üzerinde yaptığımız nitel ve deneysel analizlere dayanarak, Gelişmiş Bölümleme adlı iyileştirilmiş bir strateji öneriyor ve geliştiriyoruz. İkinci adım için, merkezi DP literatüründen sorgu kümelerinin çizge-tabanlı modellemesi tekniğini kullanıyoruz. Gelişmiş Bölümleme yöntemi de çizge-tabanlı modelleme tekniğini kullanmakta ve bölümleme problemini sorgu kümesinin çizge modeli üzerinde düğüm renklendirme problemi olarak çözümleyerek geliştirmektedir. Üçüncü ve dördüncü adımlar için, çözümümüze iki popüler LDP protokolünü (GRR ve RAPPOR) uyarlamaya ek olarak, çözümümüze uygulanabilir hale gelmesi için Optimized Unary Encoding (OUE) protokolüne bir genişletme öneriyoruz. Genişletilmiş protokolümüzün ismi Optimized Bitvector Encoding (OBE)'dir. OBE sadece konumsal sorguların cevaplanması problemiyle sınırlı olmayıp, bitvektör kodlaması içeren herhangi bir LDP probleminde de kullanıma uygundur. OBE'nin kullanıcı tarafı gürültülü cevap hesaplamasının LDP'yi sağladığını ve sunucu tarafı tahminlemesinin tarafsız olduğunu matematiksel olarak kanıtlıyoruz. Farklı bölümleme stratejileri ve LDP protokollerinin kombinasyonları kullanılarak, konumsal sorguların LDP ile cevaplanması için 8 farklı yaklaşım elde edilmiştir; bu yaklaşımların hepsi dört-adımlı çözümümüzün adımlarında farklı seçimler yapılarak elde edilen örnekleri olarak incelenebilir. Elde edilen yaklaşımların kapsamlı deneysel değerlendirmesini, gerçek dünyadan alınmış 4 veri seti, değişen sayıda sorgu, değişen sayıda sorgu boyutu, değişen mahremiyet dereceleri ve birden fazla hata metriği kullanarak yapıyoruz. Sonuçlar, Gelişmiş Bölümleme stratejisi ve OBE protokolünün genelde en düşük hatayı verdiğini göstermekte, bu durum da önerdiğimiz yöntemlerin üstünlüğünü gözler önüne sermektedir.","Spatial density queries are fundamental in many geospatial data analysis tasks and have numerous applications in the real world, such as determining crowded areas, estimating traffic density, navigation, passenger demand analysis, and so forth. However, answering spatial density queries based on users' data may violate users' privacy by exposing their true locations to an untrusted third party (e.g., a server acting as the service provider or data collector). In this thesis, we propose a solution for answering spatial density queries while preserving Local Differential Privacy (LDP), a state-of-the-art privacy protection standard. Our solution consists of four main steps: partitioning, finding sensitivity, user-side noisy response computation, and server-side estimation. For the first step, we initially propose three basic partitioning strategies: Singleton Partitioning, Holistic Partitioning and Random Partitioning. Based on our qualitative and empirical analysis of the three basic strategies, we design and implement an improved strategy called Advanced Partitioning. For the second step, we adapt graph-based modeling of query sets from the centralized DP literature. Advanced Partitioning also leverages and extends this technique by formulating the partitioning problem as a vertex coloring problem on the graph representation of a query set. For the third and fourth steps, in addition to adapting two popular LDP protocols (GRR and RAPPOR) to our solution, we propose an extension for the Optimized Unary Encoding (OUE) protocol so that it can be employed in our solution. We call the extended protocol Optimized Bitvector Encoding (OBE). OBE is applicable to not only the problem of answering spatial density queries, but also in arbitrary LDP problems with bitvector encodings. We formally prove that the user-side perturbation step of our OBE protocol satisfies LDP and its server-side estimation step produces unbiased estimates. Combining the different partitioning strategies and LDP protocols, we obtain a total of 8 different approaches for answering spatial density queries under LDP, all of which can be parsed as instances of our four-step solution with different choices in the individual steps. We perform an extensive experimental evaluation of these approaches using 4 real-world datasets, varying number of queries, varying query sizes, varying degrees of privacy, and multiple error metrics. Results show that Advanced Partitioning and OBE protocol typically yield the lowest error, demonstrating the superiority of our proposed methods."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein - Protein Etkileşimleri (PPE), bir hücrenin büyümesi, bölünmesi veya bakımı gibi hayati süreçlere neden olmaktadır. Herhangi bir hücrenin kendisi ve diğerleri ile işleyişini anlamak için önemli bir konudur. Farklı yapıların özelliklerini araştırmak için en güvenilir teknikler deneysel metotlar olsa da, sayısal yöntemler ihmal edilebilir hata ile çok daha hızlıdır.Bununla birlikte, daha doğru sonuçların daha hızlı elde edilmesi için PPE tahminlerinin iyileştirilmesi gerekmektedir. Bu tezde, PPE yapılarının tahmin hızını artırmak için iki aşamalı bir karma fonksiyonu algoritması sunulmaktadır.Birinci bölümde, çok sayıda arayüz; bağ açıları, dihedral açılar ve Karbon Alfa (CA) atomları arasındaki mesafeler gibi farklı özelliklerine göre sınıflandırılmaktadır. Seçilmiş iki CA atomunun 4 $\AA$ ile 13 $\AA$ arasındaki mesafeye sahip ardışık olmayan her amino asit için, bu CA atomları ve onların komşu CA atomları ile gerekli açıları ve mesafeleri hesaplanır. Daha sonra, bir hash tablosu kullanarak algoritmanın ilk aşamasında benzer özelliklere sahip amino asit dörtlüleri özelliklerine ve arayüzlerine göre sınıflandırılır. İkinci aşamada, arayüzler için hesaplanan aynı özellikler kullanılarak belirli bir protein ile karşılaştırılır ve benzerliklerine göre puanlanır. Bu algoritma, Arayüz-Bazlı PPE tahminleme algoritmalarında seçilecek arayüzler arasından alakasız seçenekleri filtrelemek ve olası adayları sınırlandırmak için geliştirilmiştir. Protein yerleştirme algoritmalarında girdi olarak kullanılacak arayüzleri bir veri setinden seçmek için kullanılacak yapısal hizalama algoritmasının çalışma süresini veri setini filtreleyip küçülterek azaltmak için kullanışlıdır.","Protein - Protein Interactions (PPI) cause vital processes such as growing, division or maintenance of a cell. It is an important topic in order to understand the functioning of any cell with itself and others. Even though the most reliable techniques are experimental for investigating the properties of different structures, numerical methods are much faster with negligible error. Nevertheless, predictions of PPI needs improvements in order to derive more accurate results faster. In this thesis, we are implementing a two-step hashing algorithm in order to increase the speed of prediction of PPI structures. In the first part, a large number of interfaces are classified by their different properties such as bond angles, dihedral angles and distances between Carbon Alpha (CA) atoms. For each non-consecutive residue that has the distance of 4 $\AA$ to 13 $\AA$, between CA atoms,we are calculating necessary angles and distances with selected CA atoms and their consecutive neighbors. Then, we are classifying fragments of interfaces with similar properties in the first phase of the algorithm by using a hash table. In the second phase, we are comparing a given protein using the same properties that calculated for templates, and score them by their similarity. Proposed algorithm is developed for filtering the dissimilar interfaces and limiting the possible number of interfaces that can be used in Template-Based PPI prediction protocols. In addition, it is useful for reducing the computation time of any structural alignment algorithm to find input templates for a docking algorithm by returning a filtered subset from a given template dataset."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Morfolojik kuralların denetimsiz öğrenilmesi, doğal dil işleme (NLP) modellerinin beklenen yeteneklerinden biridir, çünkü insanlar bu kuralları ana dili edinimleri sırasında denetimsiz olarak öğrenmektedirler. Bu beklentiye dayanarak, Autoencoders (AE), Variational Autoencoders (VAE), Character-level Language Models (CharLM) ve Vector Quantized Variational Autoencoders (VQVAE) gibi denetimsiz birçok modelin morfolojik öğrenmesini değerlendirmek için kapsamlı bir deneysel kurulum sunuyoruz. Çalışmamızda morfolojik özelliklerin sondalanması, morfolojik segmentasyon ve morfolojik yeniden çekim deneylerine yer veriyoruz. Deneylerimizde, tüm modellerin, morfolojik bilgiyi kodladığının bir göstergesi olarak sondalama deneylerinde taban modellerden daha iyi performanslar gösterdiğini; morfolojik segmentasyon için VAE ve CharLM'lerin SOTA modelleriyle karşılaştırılabilir performanslara sahip olduğunu; birden fazla kod kitabına sahip VQVAE'nin, bir kelimenin kökünü ve son eklerini belirleme yeteneğine sahip olduğunu ve bu açıdan morjolojik çekimsel görevlerini gerçekleştirmek için iyi bir aday olduğunu gösteriyoruz.","Unsupervised learning of morphological rules is one of the expected abilities of natural language processing (NLP) models since children learn these rules during their native language acquisition without supervision. Based on this expectation, we present a comprehensive experimental setup for evaluating the morphological learning of several unsupervised models such as Autoencoders (AE), Variational Autoencoders (VAE), Character-level Language Models (CharLM) and Vector Quantized Variational Autoencoders (VQVAE) at the following tasks: probing for morphological features, morphological segmentation and morphological reinflection. In our study, we show that for probing, all models outperform baselines with an indication of encoding morphological knowledge; for morphological segmentation, VAE and CharLMs have comparable performances to unsupervised SOTA models; for morphological reinflection, VQVAE with multiple codebooks has the ability to identify the lemma and suffixes of a word and turns out to be a good candidate to perform inflectional tasks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Stil Temelli Çekişmeli Üretici Ağlar (StyleGAN), çözülmüş örtülü uzayları öğrenirken çok yüksek kaliteli görüntü sentezini mümkün kılar. Bu nedenle, örtülü uzay manipülasyonu ile anlamsal görüntü düzenlemeye odaklanan birçok yeni çalışma vardır. Özellikle öne çıkan bir alan, görselleri metinsel açıklamalara göre düzenlemektir. Mevcut çalışmalar, bu probleme ya çok verimli olmayan örnek düzeyinde örtülü vektör optimizasyonu gerçekleştirerek ya da önceden tanımlanmış metin istemlerini örtülü uzaydaki düzenleme yönergelerine eşleyerek yaklaşırlar. Buna karşılık, bu tez çalışmasında metinsel tanımlamalar tarafından yönlendirilen görüntü düzenlemeyi sağlayan iki yeni yaklaşım sunuyoruz. Yöntemlerimiz, bir artık örtülü vektörü ileribildirim ile tahmin eden bir metin koşullu kodlayıcı ağı ya da metin koşullu bağdaştırıcı ağı kullanır. Hem nicel hem de nitel sonuçlar, yöntemlerimizin manipülasyon isabeti açısından rakip yöntemlerden daha iyi performans gösterdiğini, yani sentezlenen görsellerin metinsel açıklamalarla ne kadar iyi uyuştuğunu ve son derece gerçekçi sonuçlar sağlarken orijinal görselin özelliklerini koruduğunu göstermektedir. Aynı zamanda, yöntemimizin insan yüzleri, kediler ve kuşlar dahil olmak üzere çeşitli görsel alanlara genellenebileceğini de gösteriyoruz.","Style-based Generative adversarial networks (StyleGAN) enable very high quality image synthesis while learning disentangled latent spaces. Hence, there is a lot of recent work focusing on semantic image editing by latent space manipulation. A particularly emerging field is editing images based on target textual descriptions. Existing approaches tackle this problem either by performing instance-level latent code optimization which is not very efficient or by mapping predefined text prompts to editing directions in the latent space. In contrast, in this thesis work, we present two novel approaches that enable image editing guided by textual descriptions. Our idea is to use either a text-conditioned encoder network or a text-conditioned adapter network that predicts a residual latent code in a feed forward manner. Both quantitative and qualitative results demonstrate that our methods outperform competing approaches in terms of manipulation accuracy, i.e., how well the synthesized images match the textual descriptions while ensuring highly realistic results and preserving features of the original image. We also demonstrate that our method can generalize to various domains including human faces, cats, and birds."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kendi kendini denetleyen monoküler derinlik tahmini, herhangi bir denetim olmaksızın tek bir görüntüden piksel başına derinliği tahmin etme görevidir. Tipik olarak, ardışık kareler arasındaki derinliği ve kamera pozunu tahmin etmek için iki ağ vardır ve bunlar daha sonra kendi kendini denetleme için bir görünümü diğerinden yeniden elde etmek için kullanılır. Bu yaklaşımla ilgili iki sorun vardır. İlk olarak, sahnenin statik olduğu ve tek hareketin kameradan kaynaklandığı varsayılır, bu varsayım statik sahne varsayımı olarak isimlendirilir, ancak bu, gerçek dünyadaki sürüş senaryolarında sıklıkla ihlal edilir. Bu varsayım nedeniyle, monoküler derinlik yöntemleri, sahnenin hareketli bölgelerinde doğru tahminler üretmekte zorlanır. Mevcut yöntemler ya hareketli bölgeleri yok sayar ya da hareketli bölgeleri belirlemek ve ayrı ayrı işlemek için ek bir örnek segmentasyonu girdisi gerektirir. Bu tezde, ilk olarak MonoDepthSeg'in derinliği tahmin etmesini ve bununla beraber dinamik nesnelerin hareketini modellemek için sahneyi hareketli bölgelere ayırmasını öneriyoruz. Statik sahne varsayımının ötesine geçmenin, özellikle hareketli bölgelerde derinlik tahmininin doğruluğunu geliştirdiğini gösteriyoruz. Kendi kendini denetleyen monoküler derinlik tahmin yöntemlerinin ikinci sorunu ise ölçek belirsizliğidir. Tahmini derinlik değerleri, genellikle sonuç elde etme esnasında asıl referans ölçeği değerine göre normalleştirme ile ele alınan bilinmeyen bir ölçektedir. Bu sorunu ele almak için geleneksel düzlem ve paralaks paradigmasını yeniden ele alıyoruz ve metrik ölçekte derinliği tahmin etmek için DepthP+P'yi öneriyoruz. Yöntemimiz, herhangi bir ek normalizasyon olmaksızın metrik ölçekte sonuçlar üretebilmektedir.","Self-supervised monocular depth estimation is the task of estimating per-pixel depth from a single image without any supervision. Typically, there are two networks to estimate depth and camera pose between consecutive frames, which are then used to reconstruct one view from another for self-supervision. There are two problems with this approach. Firstly, the scene is assumed to be static and the only motion is due to the camera, namely the static scene assumption, however, this is frequently violated in real-world driving scenarios. Due to this assumption, monocular depth methods struggle to produce accurate predictions in the moving regions of the scene. Current methods either ignore moving regions or require an additional instance segmentation input to identify and separately process moving regions. In this thesis, we first propose MonoDepthSeg to jointly estimate the depth and decompose the scene into moving regions to model the motion of dynamic objects. We show that going beyond the static scene assumption improves the accuracy of depth prediction, especially in moving regions. The second problem of self-supervised monocular depth estimation methods is the scale ambiguity. The estimated depth values are in an unknown scale which is typically handled with normalization with respect to the ground truth scale value during inference. We revisit the traditional paradigm of plane and parallax to address this issue and propose DepthP+P to estimate depth in metric scale. Our method shows promising results that are metric scale without any additional normalization."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Robot kolları yapılandırılmamış ortamlarda giderek daha fazla kullanılıyor. Bu nedenle, yüksek düzeyde yapılandırılmış, kontrollü ve kafesli iş hücrelerine yerleştirilen geleneksel fabrika robotlarına kıyasla görüntü sensörlerine daha fazla ihtiyaç duyuyorlar. çöp toplama, kutu taşıma ve yerleştirme, insan-robot ortak çalışma hücrelerinde montaj ve parça besleme, inceleme, kalite kontrol sistemleri görüntü sensörü kullanan uygulamalara örnek olarak gösterilebilir. Görüntü verileri esas olarak manipüle edilecek nesnelerin yerini belirlemek, ölçümler yapmak ve yakındakı insanları tespit etmek için gereklidir. Görüntü tabanlı robot sistemlerinin düzgün çalışabilmesi için, robot ve kamera arasında harici kalibrasyon işlemi yapılması gerekir. Bu, zaman alıcı ve sıkıcı aynı zamanda da pahalı olabilen bir prosedürdür. Hızlı, esnek ve hassas robot-kamera kalibrasyonu, yalnızca endüstriyel ortamlar için değil, aynı zamanda kamera ve/veya robotun konumunun sık sık değiştirilmesi gereken veya yanlışlıkla değiştirildiği akademik laboratuvar ortamları için de gereklidir. Robot-kamera harici kalibrasyonu, on yıllardır süre gelen ve bugün dahi üzerinde çalışılan bir problem. Geleneksel yöntemler, referans bir nesnenin kameraya göre afin dönüşümünü farklı robot pozisyonlarından hesaplayarak ve bu afin dönüşüm tahminlerini robotun yön ve konumuyla eşleştirerek çalışır. öğrenme tabanlı güncel yaklaşımların büyük bir kısmı simülasyon ortamında oluşturulan verileri kullanarak harici kalibrasyon tahminini gerçekleştiriyor. Bu tez çalışmasında; referans nesneye ihtiyaç duymayan, öğrenme tabanlı bir robot-3B kamera harici kalibrasyon sistemi sunuyoruz. Otomatik olarak oluşturulan gerçek dünya verilerilerini kullanarak, uç efektör (EE) segmentasyonu, EE yön tahmini ve kilit nokta tespiti için modeller eğitiyoruz. öğrenme modellerimiz MinkUNet ve PointNet++ mimarileri kullanılarak oluşturuldu. EE konum hesaplamasını, EE segmentasyon ve EE yön modellerinden elde edilen tahminlerini kullanarak yapıyoruz. Tespit edilen kilit noktaları referans kilit noktalarıyla eşleyerek, EE yön ve konum tahminini ikinci bir yöntemle de gerçekleştirmiş oluyoruz. Kalibrasyon kalitesini artırmak için yinelemeli yakın nokta (ICP) algoritmasından, birden fazla 3B fotoğraf verisinden ve aykırı değer analizinden faydalanıyoruz. Daha önce kullanılmamış test verileriyle yaptığımız ölçümlerde; 1 santimetreden daha az (0.74 cm) konum ve 0.05 radyandan daha az (1.69 derece) yön ortalama hata paylarıyla kalibrasyon işlemini gerçekleştirdik. Tek kareden yaptığımız EE yön ve konum hesaplamarında, 1.00 cm konum and 2.74 derece yön ortalama hata paylarına ulaştık. Ayrıca, bu tezde anlatılan tüm model ve algoritmaları entegre ederek; açık kaynaklı, kullanıcı dostu robot-3B kamera harici kalibrasyon yazılımını geliştirdik.","Robot arms are being used more and more in unstructured environments. As such, they are relying more on vision sensors compared to traditional factory robots which are placed in highly structured, controlled and caged work-cells. Some applications that rely on vision data include bin-picking, box picking and placing, assembly and part feeding in mixed human-robot work cells, inspection, quality control etc. Vision data is mainly required to localize the objects to be manipulated, perform measurements and detect near-by humans. Vision based robot systems require extrinsic calibration between the robot and camera in order to work properly. This is a time consuming and tedious procedure which can be expensive as well. Fast, flexible and precise robot-camera calibration is essential for not only industrial environments but also academic lab environments where the location of the camera and/or robot needs to be frequently or is accidentally changed. Extrinsic calibration between a robot arm and camera is a decades old challenge still prevalent to this day. Traditional techniques work by estimating pose of the camera relative to a fiducial marker from multiple points and matching these estimations with the robot's pose. Recent learning based approaches predict extrinsic calibration from images relying heavily on simulation data. In this thesis, we present a learning based markerless extrinsic calibration system that uses a depth camera. We learn models for end-effector (EE) segmentation, single-frame rotation prediction and keypoint detection, from automatically generated real-world data. Our models are based on MinkUNet and PointNet++ architectures. We use a transformation trick to get EE pose estimates from rotation predictions and a matching algorithm to get EE pose estimates from keypoint predictions. We further utilize the iterative closest point (ICP) algorithm, multiple-frames and outlier detection to increase calibration robustness. Our results on the test set with previously unseen camera locations give sub-centimeter (0.74 cm) and less than 0.05 radians (1.69 degrees) average calibration errors and 1.00 cm and 2.74 degrees average pose estimation errors. In addition, we released an open source easy to use tool for robot users to handle robot-camera calibration with a few mouse clicks by seamlessly integrating all the models and algorithms discussed in this thesis."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Blokzincir teknolojilerinin hayatımıza girmesi ve gelişmesi para gönderimini gönderici ile gönderen arasındaki mesafeden bağımsız olarak maliyetini düşürerek kolaylaştırmıştır. Dahası, blokzincir teknolojisi sadece günümüz ekonomik modelini etkilememiştir, aynı zamanda daha önce güvenilir bir otorite (devlet veya bilinen bir firma gibi) olmadan imkansız olduğu düşünülen birçok yeni uygulamaya da ilham kaynağı olmuştur. Fakat yeni bir teknoloji olan blokzincir; madencilik, hesap işlemleri ve uygulamaları açısından güvenliğini ve teşvik uyumluluğunu (yani katılımcıların beklenen protokol adımlarını takip etmesinin motive edilmesi) garanti edecek kadar araştırılmamıştır. Kriptografi ve oyun teorisi bize bu mevzuda yardımcı olabilecek ispat yöntemleri olan geleneksel iki araçtır. Bu tezde güvenli blokzincir uygulamaları için oyun teorisi ve kriptografik güvenlik modellerini kullanmaktayız. Birinci olarak, e-bağış adını verdiğimiz merkezi olmayan ve adil çevrimiçi bağışlar için ispatlanabilir güvenli bir blokzincir uygulamasını geliştirmekteyiz. İkinci olarak, pratik ve çok yönlü özellik tabanlı bir dijital imza protokolü önermekteyiz. Üçüncü olarak, çalışma-ispatı blokzincirine Eyal ve Sirer'in (CACM '18) bencil madencilik saldırısına karşı teşvik uyumlu bir savunma çözümü sunarak blokzincirin güvenliğini analiz etmekte ve geliştirmekteyiz. Dördüncü olarak, bu ve benzeri saldırıların ve karşı savunmalarının simüle edilebileceği ağ tabanlı bir simülasyon aracını da beraberinde tasarlamaktayız. Beşinci olarak, eşik kriptografisini ve oyun teorisinin devredilebilir yarar kavramını başarıyla birleştiren bir sistem sunmaktayız. Bu sistem sayesinde koalisyon boyutu bir eşikle sınırlandırılabildiğinde çok oyunculu mekanizmaların daha iyi oyun teorik analizi mümkün olmuştur. Altıncı olarak, bu sistemi teşvik edilmiş başkasına hesaplatma problemine çözümümüzde uygulamakta, teşvik uyumluluğunu göstermekteyiz.","With the introduction and advent of blockchain technologies, money transfers have become easier with lower costs, no matter how far the recipient is from the sender. Moreover, the blockchain technology has not only affected the contemporary economical model, but also inspired many novel applications that have been thought impossible without the help of a trusted third party (such as governments or well-recognized companies). However, being a newly developed technology, blockchain still lacks enough research to ensure its security (in terms of mining, transactions, or its applications) and incentive compatibility (i.e., the participants are motivated for following the expected protocol steps). Cryptography and game theory are two well-known tools with traditional proving methods to help us for this task. In this thesis, we utilize game theory and cryptography based security models for building secure applications on blockchain. First, we develop a provably secure blockchain application that we name as e-donation, for secure, fair and decentralized online donations. Second, we develop a practical attribute based digital signature scheme. Third, we analyze and improve the security of blockchain by providing an incentive compatible defense solution against the selfish mining attack of Eyal and Sirer (CACM '18) on proof-of-work blockchain. Forth, we develop a network-based simulation tool for both attacks and defenses. Fifth, we propose a framework for better game-theoretical analysis of multi-player mechanisms when the sizes of the coalitions can be bounded by a threshold, via successfully combining threshold security ideas of cryptography and transferable utility of game theory. Sixth, we apply our threshold coalition notions to show the incentive compatibility of our solution to the outsourced incentivized computation problem in blockchain with multiple outsourced parties."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, davranış analitiği ve nedensel çıkarımda büyük veri ve makine öğrenimi yöntemlerinin kullanımına odaklanmaktadır. Tezin temel motivasyonu, geleneksel ekonometrik yöntemlerle çalışan araştırmacıların büyük veri ve nedensel Makine Öğrenmesi yöntemlerinden nasıl yararlanabileceğini göstermektir. Eğer bir konuda kapsamlı bir literatür yoksa, doğru regresyon spesifikasyonunu bulmak, özellikle yüksek boyutlu veri seti ile çalışırken zorlu bir iştir. Bu çalışmada nedensel Makine Öğrenimi tekniklerini açıklanabilir Yapay Zeka yöntemleriyle birleştirdim ve heterojen tretman etkilerinin doğru regresyon spesifikasyonu oluşturularak nasıl ölçüleceğine dair kılavuzlar (yani, bir regresyonda hangi ana değişkenler ve etkileşim değişkenleri kullanılacağı, hangi kontrol değişkenleri modele dahil edileceği) oluşturdum. Bu yönergeleri ampirik olarak test etmek için, futbolda maç içi geri bildirimler, maça özgü koşullar, takım özellikleri ve en önemlisi yönetici özellikleri hakkında ayrıntılı değişkenler içeren büyük bir veri seti oluşturdum. Ortaya koyduğum ampirik kanıtlar, futbol yöneticilerinin risk alma davranışlarının maç sırasında alınan geri bildirimlerden ve maç öncesinde gözlemlenen bilgilerden ne zaman ve nasıl etkilendiğini göstererek spor analitiği literatürüne katkıda bulunmaktadır. Ayrıca, bu tez, bilinen heterojen tretman etkileri ile üretilen sentetik verileri kullanarak iyi bilinen iki nedensel Makine Öğrenimi tekniğinin (Son zamanlarda popüler olan ve ortalama tretman etkilerini bulmaya odaklanan FLAME ve doğrudan heterojen tretman etkilerini bulmaya çalışan Nedensel Orman) performanslarını değerlendirerek nedensel Makine Öğrenimi literatürüne katkıda bulunmaktadır. Spor analitiğine ek olarak, eğitim verileriyle de çalıştım ve bilişsel olmayan bir beceri olan azmin öğrenciler için akademik başarıyı nasıl öngördüğünü gösterdim. Davranışsal bir azim ölçüsü oluşturmak için dijital bir öğrenme platformundan benzersiz bir veri kümesi kullandım ve davranışsal olarak ölçülen azmin, araştırmacılar tarafından geleneksel anketlerle ölçülen azim ölçüsüne kıyasla öğrenci performansının daha iyi bir prediktörü olduğunu gösterdim. Ayrıca, büyük verinin gücü sayesinde, makine öğrenimi algoritmalarının, herhangi bir yapısal model veya regresyon spesifikasyonu oluşturmadan bile akademik dayanıklılığı tahmin etmede iyi performans gösterdiğini buldum. Spor ve eğitimdeki vakalar üzerinde çalışarak elde ettiğim ampirik bulguların, nedensel çıkarım yapmak için geleneksel ve teoriye dayalı modellerle çalışan araştırmacıların Makine Öğrenimi ve büyük veriden sağlayabileceği faydaları açıkca ortaya koyduğuna inanıyorum.","This thesis focuses on the use of big data and machine learning methods in behavioral analytics and causal inference. The main motivation of the thesis is to illustrate how the researchers working with traditional econometric methods can benefit from big data and causal ML methods. In the absence of well-established literature, finding the right regression specification is a challenging task, especially when working with high dimensional data set. In this study, I have combined causal ML techniques with explainable AI methods and provided guidelines on how to measure heterogeneous treatment effects with the right regression specification (i.e. which main effects and interactions to be used, what control variables to be included). To empirically test these guidelines, I have curated a large data set in football including detailed variables about interim feedback, match-specific conditions, team features, and most importantly manager characteristics. Empirical evidence contributes to the sports analytics literature suggesting when and how risk-taking behavior of football managers pays off in light of interim and ex-ante information revealed to the manager (i.e. the decision maker). Moreover, this thesis contributes to the causal ML literature by evaluating the performances of two well-known causal ML techniques (a recently popular matching algorithm focusing on finding average treatment effects (FLAME) and Causal Forest that directly aims to estimate heterogeneous treatment effects) are evaluated by using synthetic data generated with known heterogeneous treatment effects. In addition to sports analytics, I have also worked with education data and demonstrated how grit, a non-cognitive skill, predicts academic achievement for students. I used a unique dataset from a digital learning platform to construct a behavioral measure of grit and showed that behavioral grit is a better predictor of student performance compared to survey grit that has been traditionally used by the researchers. I have also found that machine learning algorithms perform well in predicting academic resilience even without constructing any structural model or regression specification, thanks to the power of big data. I believe that my findings from cases in sports and education put forward the benefits of using Machine Learning and big data for researchers working with traditional and theory-based models for causal inference."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Pekiştirmeli öğrenme (PÖ), robotlara beceriler kazandırmak için gelecek vaadeden bir yaklaşımdır. Bununla birlikte, PÖ kullanarak iyi sonuçlar elde etmek için birçok deneme yapmak gerekir. Gösterimlerden Öğrenme (GÖ) destekli PÖ, insan gösterimlerinden başlangıç becerisi öğrenerek bu sorunun etkisini azaltır. Ne yazık ki, GÖ destekli PÖ hala daha robotların beceri öğrenmesi için fazla denebilecek miktarda deneme gerektirir. Bu çalışmada, bu deneme sayısını daha da azaltmak için bir yaklaşım geliştirdik. Başlıca katkılarımız, (1) keşfe odaklanan bir algoritma ve (2) Yol İntegralleriyle Politika İyileştirme yöntemini geliştiren kapalı kutu politika bulma yöntemi olan Bayessel Optimize Edilmiş Politika Öğrenimi methodur (BO-PI$^{2}$). GÖ yapımız, becerinin hareketini (robot kol pozu) ve hedefini (nesneye özgü algısal veriler) birlikte modellemek için anahtar nokta gösterimlerini ve bunlardan öğrenilen bir Dinamik Bayessel Ağ'dan (DBA) yararlanır. Bu ağın hareket bölümü robot hareketini modellemek için, hedef bölümü ise hareketin başarısını izlemek ve Kısmen Gözlenebilir Markov Ödül Modeli oluşturarak ödül sinyali oluşturmaya yarar. BO-PI$^{2}$, öğrenilen ödül sinyali sayesinde DBA'nın hareket bölümünü geliştirir. BO-PI$^{2}$'nin yeniliği, keşif stratejisinden gelir. Hareket ve hedef arasındaki bağlantı deneme miktarını azaltmak için yeteneğin odaklanılacak kısmını seçmek için kullanılır. Bu yaklaşım, bir anlamda kredi atama sorununu çözmeye benzer. Odaklanacak kısım seçildikten sonra, BO-PI$^{2}$, PÖ yaklaşımlarının tipik olarak kullandığı denemeleri üretmek için hareket modelini kullanır. BO-PI$^{2}$, bu işlem sırasında yürütülen hareketler ile gelişen kümülatif ödülleri öğrenmek için Gaussal Süreç modeli (GS) kullanır. BO-PI$^{2}$ bir sonraki hareket noktalarını Üst Güven Sınırı (ÜSB) algoritmasını kullanarak seçer. Bu algoritma adayların tahmin edilen ödül ve belirsizliğine dayanır. Bu, çoğu PÖ yaklaşımında kullanılan rastgele hareket yaratmaktan farklıdır. BO-PI$^{2}$ ayrıca yeteneği öğrendiğinde erken durmak için hedef modeline dayanan bir sonlandırma kriterinden faydalanır. Bu çalışmada BO-PI$^{2}$'yi 3 beceri için uzman ve uzman olmayanlar alınmış anahtar nokta gösterimleriyle test ettik. Uzmanlar tarafından verilmiş gösterimlerden öğrenilen hareket modelleri başarılı oldukları için, elle yapılan müdahalelerle başarısız hale getirdik. Uzman olmayan kişilerden alınmış gösterimlerde ise başlangıçta başarısız olan haraket modellerini seçtik. Yaklaşımımızı şu zamana kadar ki en başarılı sonuçları elde etmiş PI$^{2}$-ES-Cov algoritmasına karşı üç metrikde karşılaştırdık, bunlar: (1) beceri başarı oranı, (2) toplam birikmiş ödül ve (3) deneme sayısı. Hem uzman, hem de uzman olmayan durumlarda, yaklaşımımız ortalama olarak her üç metrikte de PI$^{2}$-ES-Cov dan daha iyi performans gösterdi. Sonuçlarımız, anahtar nokta gösterimlerinin bütün gezingeden ziyade başarısız olan kısımlara odaklanmamıza izin verdiğini ve bunun ödül tahminine dayalı keşif stratejileriyle birleştirildiğinde, PÖ performansını iyileştirdiğini ve robot kollarını gerçek hayatta kullanılacak yeteneklerle donatmak için deneme sayısını azaltmaya faydalı olduğunu gösteriyor.","Reinforcement learning (RL) is a promising approach to endow robots with skills. However, RL requires many trials to get satisfactory results. Learning from Demonstration (LfD) seeded RL alleviates this problem by learning an initial skill from human demonstrations. Nevertheless, this approach still requires robots to perform a non-trivial amount of trials. In this thesis, we develop an approach to further reduce these for manipulation skills with perceptual goals. Our main contributions are (1) an algorithm to focus the exploration by using the learned relationship between action and perception and a (2) Black-Box RL Policy Search (PS) method that improves upon the popular Policy Improvement with Path Integral (PI²) algorithm, called the Bayesian Optimized PI² (BO-PI²), that uses reward predictive UCB-type exploration. Our underlying LfD framework utilizes a Dynamic Bayesian Network (DBN) learned from keyframe demonstrations to jointly model the action (end-effector pose) and the goal (object-specific perceptual data) of the skill. The action part is used to generate robot trajectories, and the goal part is used to monitor the success of trajectory executions and to create a Partially Observable Markov Reward Model in order to learn rewards. BO-PI$^{2}$ is used to improve the action part of the DBN with trial-and-error using the learned returns. The novelty of BO-PI$^{2}$ comes from its exploration strategy. The coupling between the action and the goal is used to pick the part of the model to focus on to reduce the effort, in a sense to solve the credit attribution problem. After picking the part to focus on, BO-PI$^{2}$ samples trajectories from the action model to get rollouts, which is typical of PS approaches. In addition, BO-PI$^{2}$ uses a Gaussian Process (GP) to learn local returns from these rollouts, which is improved with each executed trajectory. The next samples are selected by utilizing an Upper Confidence Bound (UCB) approach, using the predicted return and uncertainty of the possible candidate points. This is in contrast to random sampling, used in most PS approaches. BO-PI$^{2}$ also utilizes a skill success based termination criteria, using the goal model to monitor success autonomously. We evaluate BO-PI$^{2}$ with expert and non-expert keyframe demonstrations for three skills. In the expert case, the models are perturbed so that the initial skill execution starts from a failure condition. In the non-expert case, we pick skill models that fail to begin with. We test our approach against the current state-of-the-art PI$^{2}$-ES-Cov algorithm using three metrics: (1) skill success rate, (2) total accumulated reward, and (3) number of trials. In both the expert case and the non-expert case, on average, our approach performed better than the baseline on all three metrics. Our results show that utilization of keyframes allows us to focus on failed sub-goals rather than the entire trajectory, and combined with reward predictive exploration strategies, are beneficial to improve RL performance and reduce the number of trials to endow robot arms with real-life manipulation skills."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kendi kendini denetleyen öğrenme, genellikle zaman, çaba ve maliyet açısından pahalı olan veri etiketleme gerçekleştirmeden, büyük miktarda veriden etkili temsilleri öğrenmek için bir çözüm sağlar. Genel olarak kendi kendini denetleyen öğrenme yaklaşımıyla ilgili temel sorun çökmedir, yani aynı girdiden üretilen farklı temsilleri eşleştirirken tüm girdiler için aynı temsilleri elde etmektir. Bu tezde, aynı girdinin farklı versiyonlarının gizli temsilleri arasındaki bilgi maksimizasyonunun doğal olarak çöküşü önlediğini ve farklı alt görevlerde rekabetçi ampirik sonuçlar elde ettiğini savunuyoruz. Bu amaçla, gizli temsil argümanları arasındaki korelasyon derecesini yansıtan, ikinci dereceden istatistik tabanlı karşılıklı bilgi ölçüsünü maksimize etmeye dayalı CorInfoMax adında yeni bir kendi kendini denetleyen öğrenme yöntemi öneriyoruz. Aynı girdinin alternatif gizli temsilleri arasında bu bağıntılı bilgi ölçüsünü en üst düzeye çıkarmak iki temel amaca hizmet eder: (1) dejenere olmayan kovaryanslara sahip özellik vektörleri üreterek çökme problemini önler; (2) alternatif temsiller arasındaki doğrusal bağımlılığı artırarak, birbirleri ile alakalı olmasını sağlar. Önerilen bilgi maksimizasyonu hedefi, özellik kovaryans matrisinin log-determinantı tarafından düzenlenen Öklid mesafesine dayalı bir amaç fonksiyonuna basitleştirilmiştir. Öznitelik alanı bozulmasına karşı doğal bir engel görevi gören düzenlileştirme terimi nedeniyle CorInfoMax, temsillerin tüm özellik alanı boyunca yayılmasını zorlayarak boyutsal çöküşü de önler. Ampirik deneyler, CorInfoMax'in farklı görevlerde ve veri kümelerinde en gelişmiş kendi kendini denetleyen öğrenme metotlarına göre daha iyi veya rekabetçi performans sonuçları elde ettiğini göstermektedir.","Self-supervised learning provides a solution to learn effective representations from large amounts of data without performing data labeling, which is often expensive in terms of time, effort, and cost.The main problem with the self-supervised learning approach, in general, is collapse, i.e., obtaining identical representations for all inputs while matching different representations generated from the same input. In this thesis, we argue that information maximization among latent representations of different versions of the same input naturally prevents collapse. To this end, we propose a novel self-supervised learning method, CorInfoMax, based on maximizing the second-order statistics-based measure of mutual information that reflects the degree of correlation between the latent representation arguments. Maximizing this correlative information measure between alternative latent representations of the same input serves two main purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it increases the linear dependence between alternative representations, ensuring that they are related to each other. The proposed information maximization objective is simplified to an objective function based on Euclidean distance regularized by the log-determinant of the feature covariance matrix. Due to the regularization term acting as a natural barrier against feature space degeneracy, CorInfoMax also prevents dimensional collapse by enforcing representations to span across the entire feature space. Empirical experiments show that CorInfoMax achieves better or competitive performance results over state-of-the-art self-supervised learning methods across different tasks and datasets."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Modern yapay sinir ağlarının insan seviyesine yakın bir görü kabiliyeti kazanması için karmaşık ve çoklu nesne içeren sahnelerden nesne-odaklı temsiller öğrenebilmeleri kritik bir öneme sahip. Fakat, günümüz yapay sinir ağları nesne-odaklı temsiller öğrenebilmek için önceden sabitlenmiş nesne sayısına göre çıkarımlar yapmaya çalışmakta veya yüksek çözünürlüklü görseller üzerinde bu görevi yerine getirememektedirler. İlgili çoğu yöntemin aksine, bu araştırma ile Sorgu Kümeleyici Görsel Dikkat (SKGD) modelini denetimsiz sahne bölütlemesi problemini çözmek ve bu esnada nesne-odaklı temsiller öğrenebilmek üzere sunuyoruz. SKGD yapısı itibari ile bahis edilen görevleri başarıyla tamamlayabilecek etkili ve verimli bir yapı taşıdır. SKGD denetimsiz sahne parçalandırması problemini böl ve yönet stratejisi kapsamında cevaplamaya çalışan bir kümeleyici dikkat modülüdür. Kendisinden birkaç adet ardarda sıralandığında ve hiyerarşik bir mimari inşa edildiğinde, SKGD hem yüksek çözünürlüklü resimlerde denetimsiz sahne parçalandırması yapabilir hem de daha önceden sabitlenmiş bir nesne sayısına bağımlı kalmadan ihtiyaç anında temsil kapasitesini düzenleyebilir. SKGD bu özelliklerini Sorgu Kümeleyici İşlem (SKİ) adını verdiğimiz özgün, türevlenebilir ve parametre içermeyen bir kümeleme süreci ile kazanır. Bu araştırma ile sadece SKGD katmanlarından oluşturulan bir kodlayıcının literatürdeki diğer modellere kıyasla nasıl rekabetçi bir performansa sahip olduğunu ve diğer avantajlı özelliklerini sahnelemiş olacağız.","Extracting object-centric representations from a complex multi-object scene is indeed a crucial milestone for modern neural network architectures to achieve near human level cognition capabilities. Nevertheless, most of the contemporary neural networks that address object-centric representation learning problem require apriori initialization of a fixed set of object describing vectors or cannot manage to handle images of higher resolution. Contrary to long-standing paradigms in the literature, this work proposes Query Breaking Visual Attention (QBVA) module, an efficient and effective building block that introduces a divide and conquer strategy to object-centric representation learning while solving the unsupervised scene segmentation task. QBVA is essentially a stand-alone attention based clustering module that is capable of extracting object-centric representations from a multi-object scene when cascaded into a hierarchical network architecture. QBVA leverages a novel, fully differentiable and non-parametric clustering scheme named Query-Breaking Clustering (QBC) which eliminates the need for initializing a fixed set of clusters and holds the promise to provide dynamic representation for a variable number of objects. We demonstrate that QBVA-Net is indeed a competitive approach to address object-centric representation learning paradigm and prove to be advantageous compared to the state-of-the-art in the sense that it can provide better segmentation performance at the end of the encoder network and theoretically scale up to images of higher resolution."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan konuşması sadece dilsel içerik ve konuşmacı kimliği değil aynı zamanda duygusal içerik de taşır. Konuşmanın duygusal rengini değiştirme yeteneği, akıllı diyalog sistemleri için duygusal konuşma üretme ve insanlara duygusal ifade yeteneklerinde rehberlik etme gibi çeşitli görevleri mümkün kılma potansiyeline sahiptir. Duygusal ses dönüşümü gerçekleştirilirken, hem konuşma kalitesine (örneğin doğallık, anlaşılabilirlik) hem de üretilen konuşmanın algılanan duygusuna özel olarak odaklanılması gerekir. Bu çalışmada, tek bir eğitimli model ile konuşmanın duygusunu çoklu duygu kategorisine dönüştürebilen bir yöntem öneriyoruz. DTW algoritması ve yardımcı konuşmacı sınıflandırıcısı ile geliştirilmiş StarGAN tabanlı modelimiz, verilen bir konuşma sinyalinin duygusunu kızgın, mutlu ve üzgün olmak üzere 3 duygu sınıfına dönüştürebilir. Modelimizi oluştururken, kayıp fonksiyonlarını özgünlük, dilsel içerik, konuşmacı kimliği ve duygusal ifade gibi konuşmanın farklı niteliklerini hedefleyecek şekilde belirliyoruz. Modelimizin performansını, dönüştürülen konuşmanın hem ses kalitesi hem de duygusal içeriği için nesnel ve öznel değerlendirme kriterleri aracılığıyla değerlendiriyoruz. Sonuçlar, yöntemimizin hem konuşma kalitesi hem de duygusal ifade açısından son teknoloji yöntem ile avantajlı kalacak şekilde kıyaslanabilir olduğunu göstermektedir.","Human speech carries not only linguistic content and speaker identity but also emotional content. The ability to alter emotional colouring of speech has the potential to enable a variety of tasks such as producing affective speech for intelligent dialogue systems and guiding people in their emotional expression abilities. While performing emotional voice conversion, special focus needs to be given to both the quality of speech (e.g., naturalness, intelligibility) and the perceived emotion of the generated speech. In this study, we propose a method for converting the emotion of speech across multiple emotion categories with a single trained model. Our StarGAN-based model, enhanced by the DTW algorithm and auxiliary speaker classifier, can change the emotion of a given speech signal into 3 emotion classes: angry, happy and sad. When building our model, we determine the loss functions targeting distinct attributes of speech including the authenticity, linguistic information, speaker identity, and emotional expression. We evaluate the performance of our model through objective and subjective evaluation criteria for both audio quality and emotional content of the converted speech. The results show that our method compares favourably with the state-of-the-art method in terms of both speech quality and emotional articulateness."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sirkadiyen ritim, organizmalarda ~24 saatlik fizyolojik ve davranışsal süreçleri düzenleyen içsel bir süreçtir. Memelilerde sirkadiyen ritim, çekirdek saat proteinleri arasındaki etkileşimin bir sonucu olarak transkripsiyon ve translasyonel geri besleme döngüsü (TTFL) mekanizması tarafından üretilir. TTFL'de, CLOCK ve BMAL1 birbirleriyle etkileşime girer ve Period (Per) ve Cryptochrome (Cry) dahil olmak üzere saat kontrollü genlerin transkripsiyonunu başlatmak için promotör bölgesindeki E-box dizilerini bağlamak için bir heterodimer oluşturur. Zaman içinde, CRY'ler ve PER'ler sitozolde birikir ve daha sonra Casein kinase Iε ile çekirdeğe girer ve BMAL1: CLOCK temelli transkripsiyonu baskılar. Sirkadiyen ritmi kontrol eden başka yardımcı TTFL'ler de vardır. Genetik ve epidemiyolojik çalışmalar, sirkadiyen ritmini bozan faktörlerin obezite, diyabet, kardiyovasküler hastalıklar, yaşlanma, kanser, duygudurum ve uyku bozuklukları gibi çeşitli hastalıklara doğrudan neden olabileceğini düşündürmektedir. Çekirdek saat genleri için birkaç tek nükleotid polimorfizmi (SNP) tanımlanmış ve farklı hastalık türleri ile ilişkili olduğu gösterilmiştir. Bununla birlikte, bu SNP'lerin hastalığın farklı tipine nasıl neden olduğu tam olarak tanımlanmamıştır. Bu yaklaşımlardaki zorluklardan biri, genom çapında ilişki dizileri (GWAS) çalışmalarının fonksiyonel tahminde sınırlamalara sahip olmasıdır. Bu sorunu çözmek için, belirli missense mutasyonlarının CLOCK proteininin ve dolayısı ile sirkadiyen ritim üzerindeki etkisini göstermek adına, 1000 Genom Projesinde yer alan CLOCK SNP'lerini fonksiyonel olarak tanımlamak ve karakterize etmek için in vitro tekniklerini izleyen in-siliko çalışmaları kullandım. Bu tür sistematik yaklaşımlar, patolojik etkileri olan SNP'leri keşfetmemize ve bu SNP'lerin proteinlerin işlevini nasıl etkilediğini anlamamıza olanak sağlayacaktır.Bu tezde, Ensembl veri tabanından tanımlanan nadir CLOCK missense varyasyonlarının (p.Phe104Cys, p.Leu118Arg, p. Asp119Val, p.Gly120Val ve p.Phe121Cys) fonksiyonel açıdan karakterizasyonlarını gerçekleştirdim. Başlangıçta bu varyasyonları hesaplama araçlarını kullanarak analiz ettim. Sonuçlar, varyantların CLOCK'un işlevsel olarak önemli bölgesinde yer aldığını ortaya koydu. In vitro deneysel yaklaşımı kullandım ve p.Leu118Arg, p.Asp119Val ve p.Phe121Cys CLOCK'un işlem etkinliğini azalttığını, p.Gly120Val CLOCK'un BMAL1 ile birlikte işlemi artırdığını gösterdim. Ancak, p.Phe104Cys CLOCK, karşılaştırılabilir transkripsiyonel aktiviteye sebep olmadı. CLOCK SNP'leri ve BMAL1 arasındaki afinite etkisini göstermek için, aralarında ortak immünopresipitasyon deneyi yaptım. Sonuçlar şunu gösterdi ki p.Leu118Arg, s. Asp119Val, p.Gly120Val CLOCK'lar BMAL1'e afiniteyi azaltmıştı ve ilginç bir şekilde p.Phe121Cys CLOCK, BMAL1'e afiniteyi artırmıştı. Daha geniş bilgi elde etmek için, CRY1'in p.Leu118Arg ve p.Phe121Cys CLOCK'larda baskılayıcı aktiviteyi azalttığını da gösterdim. Bu arada, MD simülasyonlarının tahmini bağlama enerjisi analizi biyokimyasal sonuçları destekledi ve bu tür SNP'lerin etkilerinin mekanizmasını incelemek için amino asid başına bağlanma enerjisi analizi kullanıldı. Toplu olarak, CLOCK'taki tek nükleotid değişikliklerinin bile CLOCK işlevlerini doğrudan etkilediğini keşfettim. Bu nedenle, CLOCK SNP'lerinin etkilerinin aydınlatılması, CLOCK'un sirkadiyen saat mekanizmasındaki yapı-işlevi için değerli bilgiler sağlayacak, daha ileri çalışmalar da saat bozulması ile ilgili hastalıklar yeni tedavi stratejilerinin geliştirilmesine de yardımcı olacaktır.","Circadian rhythm is an internal process regulating ~24-h physiological and behavioral processes in organisms. In mammals, circadian rhythm is generated by transcription and translational feedback loop (TTFL) mechanism as a result of the interaction between core clock proteins. In TTFL, CLOCK and BMAL1 interact with each other and form a heterodimer to bind E-box sequences within the promoter region to initiate the transcription of the clock-controlled genes, including Period (Per) and Cryptochrome (Cry). Within the time, CRYs and PERs accumulate in the cytosol and then translocate into the nucleus with Casein Kinase Iε and repress BMAL1: CLOCK driven transcription. There are other auxiliary TTFLs exist that control circadian rhythm. Genetics and epidemiolocal studies suggest factors that disturb circadian rhythm result in susceptibility or may directly cause several diseases such as obesity, diabetes, cardiovascular diseases, aging, cancer, mood, and sleep disorders. Several single nucleotide polymorphisms (SNPs) for core clock genes have been identified and shown to be associated with different types of diseases. However, whether these SNPs contribute to the different type of the disease are ill-defined. One of the challenges in these approaches is that genome-wide association sequences (GWAS) studies have limitations in functional prediction. To address that, I developed using in vitro studies following the in-silico techniques to identify and characterize functional CLOCK SNPs from 1000 Genomes Ensemble to show the effect of a particular missense mutation on CLOCK protein on function. Such systematic approaches would allow us to discover SNPs with pathological effects and understand how these SNPs affect proteins' function. In this thesis, I performed a functional characterization of rare CLOCK missense variations (p.Phe104Cys, p.Leu118Arg, p.Asp119Val, p.Gly120Val, and p.Phe121Cys) identified from the Ensembl database. I initially analyzed these variations using computational tools. Results revealed that variants are located on the functionally important region of CLOCK. I used in vitro experimental approach and showed p.Leu118Arg, p.Asp119Val, and p.Phe121Cys CLOCK had reduced transactivation activity while p.Gly120Val CLOCK had increased transactivation along with BMAL1. However, p.Phe104Cys CLOCK had not been comparable transcriptional activity. To attrubitue these functional difference on the affinity between CLOCK SNPs and BMAL1, I performed co-immunoprecipitation between them Results indicated that p.Leu118Arg, p. Asp119Val, p.Gly120Val CLOCKs had reduced affinity to BMAL1and interestingly p.Phe121Cys CLOCK had increased the affinity to BMAL1. To gain more insight I further showed that the CRY1 had reduced repressor activity on p.Leu118Arg and p.Phe121Cys CLOCKs. Meanwhile, the estimation binding energy analysis of MD simulations supported the biochemical results, and binding energy analysis per residues was used to examine the mechanism of such SNPs effects. Collectively, I discovered that even single nucleotide changes in CLOCK directly affect the CLOCK functions. Hence, illumination of the effects of CLOCK SNPs would also help develop novel treatment strategies for diseases related to clock disruption for further studies and provide valuable information for the structure-function of CLOCK in circadian clock mechanism."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kavisli bir yüzey üzerine belirli bir açıyla manuel olarak delik açma işlemi, matkap hizalanmasında karşılaşılacak olası problemler ve görevin doğasından kaynaklanan kararsızlık sebebiyle oldukça zordur. Aynı zamanda bu tür bir manuel görev, kullanıcının yaralanmasına veya çabuk yorulmasına sebep olabilir. Ayrıca, gerçek bir üretim ortamında bu görevin tamamen otomatik olarak tasarlanması da mümkün olmayabilir. Çünkü, üretim hattına gelen parçalar farklı kompleks şekillere sahip olabilir ve dolayısıyla seçili noktaları otomatik olarak delmek için yörünge planlaması yapmak kolay olmayabilir. Bu çalışmada, 6 serbestlik derecesine sahip olan uyarlamalı bir admitans kontrolcüsü tasarlanmıştır. Ucuna matkap monte edilmiş KUKA LBR iiwa 7 işbirlikçi robot, tasarlanan kontrolcü aracılığıyla operatöre konforlu bir şekilde ve tek elle kavisli bir yüzey üzerinde delikler açabilmesini sağlamaktadır. Operatöre delik açma sırasında, robot tarafından dokunsal (haptik) yönlendirme ve bir artırılmış gerçeklik arayüzü vasıtasıyla görsel yönlendirme sağlanmaktadır. Admitans kontrolcüsünün sönümleme parametresinin gerçek zamanlı adaptasyonu, robotun çalışma uzayındaki çevre ile etkileşim olmaksızın gerçekleşen hareketi sırasında şeffaflık (robotun insana olan direncinin düşük olması) sağlarken delme işlemi sırasında kararlılık sağlamaktadır. Operatör, matkabı delme noktasına yeteri kadar yaklaştırdıktan ve istenilen delme açısına kabaca hizaladıktan sonra ilk olarak haptik yönlendirme modülü, hizalamayı hassas şekilde ayarlamakta ve operatörün hareketi sadece delme ekseni boyunca olacak şekilde kısıtlamaktadır. Ardından operatör, minimal bir eforla matkabı delinecek parçaya doğru iterek delme işlemini tamamlamaktadır. Haptik yönlendirme modülünün potansiyel yararlarını nicel olarak araştırmak için bir deney (Deney 1) ve önerilen fiziksel insan-robot etkileşim sisteminin gerçek bir üretim ortamındaki pratik değerini katılımcıların kişisel görüşüne dayanarak araştıran başka bir deney olmak üzere (Deney 2) iki farklı deney tasarlanmıştır. Bu alanda tecrübesi olmayan 3 katılımcı ile yapılan birinci deneyin sonucunda (Deney 1), haptik yönlendirme modülünün görev tamamlama süresini %26 oranında iyileştirdiği, insanın uyguladığı eforu %16 ve kas aktivasyon seviyelerini %27 oranında azalttığı bulunmuştur. 3 deneyimli işçi ile yapılan ikinci deney (Deney 2), delme işlemi sırasında önerilen sistemin kullanımının kolay, güvenli ve yararlı olduğunu göstermiştir.","Drilling a hole on a curved surface with a desired angle is prone to failure when done manually, due to the difficulties in drill alignment and also inherent instabilities of the task, potentially causing injury and fatigue to the workers. On the other hand, it can be impractical to fully automate such a task in real manufacturing environments because the parts arriving at an assembly line can have various complex shapes where drill point locations are not easily accessible making automated path planning difficult. In this work, an adaptive admittance controller with 6 degrees of freedom is developed and deployed on a KUKA LBR iiwa 7 cobot such that the operator is able to manipulate a drill mounted on the robot with one hand comfortably and open holes on a curved surface with haptic guidance of the cobot and visual guidance provided through an AR interface. Real-time adaptation of the admittance damping provides more transparency when driving the robot in free space while ensuring stability during drilling. After the user brings the drill sufficiently close to the drill target and roughly aligns to the desired drilling angle, the haptic guidance module fine tunes the alignment first and then constraints the user movement to the drilling axis only, after which the operator simply pushes the drill into the workpiece with minimal effort. Two sets of experiments were conducted to investigate the potential benefits of the haptic guidance module quantitatively (Experiment I) and also the practical value of the proposed pHRI system for real manufacturing settings based on the subjective opinion of the participants (Experiment II). The results of Experiment I conducted with 3 naïve participants, show that haptic guidance improves task completion time by 26% while decreasing human effort by 16% and muscle activation levels by 27% compared to no haptic guidance condition. The results of Experiment II, conducted with 3 experienced industrial workers, show that the proposed system is perceived to be easy to use, safe, and helpful in carrying out the drilling task."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek Matris-Vektör Çarpımı (SpMV), endüstriyel ve bilimsel uygulamalarda sıklıkla kullanılan bellek-bağımlı kilit çekirdeklerden biridir. Veri hareketini iyileştirip daha yüksek işlem gücünden yararlanmak üzere karışık-hassasiyetin SpMV'de kullanılması için birtakım çalışmalar yapılmıştır. Geçmiş çalışmaların çoğunluğu, bir iteratif çözücü (ör. CG, GMRES) gibi içerisinde düşük hassasiyet kullanılabilen, daha büyük bir uygulama boyunca farklı hassasiyetlerde SpMV kullanmaya odaklanmıştır. Daha yakın zamanda ise, sadece bir SpMV içerisinde karışık-hassasiyet kullanmanın yolları değerlendirilmiştir. Örneğin, matris içerisindeki her bir sıfır-olmayan değer için hassasiyet seçilip, girdi bu hassasiyetlere göre birden fazla matrise ayrılabilir; veya, bir blok-diagonal matrisin her bloğu için hassasiyete karar verilebilir. Bu çalışmada, bahsedildiği gibi daha ince-taneli karışık-hassasiyet yaklaşımı ile ilgileniyoruz. Bu minvalde, iplik gruplarının satır-sıkıştırılmış seyrek matrisleri satır bazında işlediği GPU'lardaki paralellik taneliğinden yola çıkarak, var olan bir eleman bazında hassasiyet karar verme yaklaşımı, satır bazında karar verebilmek üzere genişletiyoruz. Satır permutasyonları kullanarak karışık-hassasiyetli CSR depolama metodları öneriyor, ve bu metodların var olan metoda göre daha üstün yük-dengeleme özelliğini açıklıyoruz. Dahası, tek ve çift hassasiyetli matrislerin halihazırda depolandığı bir çoklu-hassasiyet senaryosunu göz önünde bulundurarak, önerdiğimiz karışık-hassasiyet SpMV yaklaşımımızı bu senaryoda da kullanılmak üzere genişletiyoruz. Metodlarımızı değerlendirmek için, bir çoklu-hassasiyet Jacobi yöntemi ve bir de Kardiyak modelleme uygulaması olmak üzere iki uygulamada kullanıyoruz. Ek olarak, karışık-hassasiyet yöntemimizi ELLPACK-R formatıyla kullanılmak üzere genişletiyoruz. Önerilen SPMV metodumuzun etkililiğini, NVIDIA V100 GPU kullanarak SuiteSparse Matrix Collection içerisindeki gerçel-değerli büyük seyrek matrislerden oluşan kapsamlı bir veri seti üzerinden gösteriyoruz.","Sparse Matrix-Vector Multiplication (SpMV) is one of the key memory-bound kernels commonly used in industrial and scientific applications. To improve its data movement and benefit from higher compute rates, there are several efforts to utilize mixed precision for SpMV. Most of the prior-art focus on performing the SpMV in different precisions throughout the entire application, such as an iterative solver (e.g., CG, GMRES) where certain steps can be done with lower precisions. More recently, methods of using mixed-precision within a single SpMV has been of consideration. For instance, one can decide precision for each non-zero value in the matrix, and then split the input into multiple matrices with their respective precisions; or, decide the precision for each block in a given block-diagonal matrix format. In this work, we are interested in this more fine-grained approach of mixedprecision SpMV. To this extent, we extend an existing entry-wise precision based approach by deciding precision for each row, motivated by the granularity of parallelism on a GPU where groups of threads process rows in row-compressed sparse matrices. We propose mixed-precision CSR storage methods with row permutations and describe its greater load-balance compared to the existing method. We also consider a multi-precision case where single and double-precision copies of the matrix are stored priorly, and further extend our mixed-precision SpMV approach to comply with it. To evaluate our methods, we apply them in two real-life applications: a multiprecision Jacobi method and a multi-precision Cardiac modeling application. We further extend our mixed-precision methodology to be used with the ELLPACK-R format. We demonstrate the effectiveness of the proposed SpMV methods on an extensive dataset of real-valued large sparse matrices from the SuiteSparse Matrix Collection using an NVIDIA V100 GPU."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ağız kanseri, her yıl dünya çapında yaklaşık 330.000 ölüme yol açan önemli bir sağlık sorunudur. Erken teşhis ve tedavi, çoğu durumda %75-90'a ulaşan hayatta kalma oranları ile ağız kanserinden ölüm ve morbiditeyi azaltmada en etkili önlem olmaya devam etmektedir. Yıllar içinde, kansere dönüşme olasılığı yüksek olan öncül lezyonların tespitini iyileştirmek için tarama programları uygulamaya konmuş, ancak bu programların gerçek dünyada uygulanmasında zorluklarla karşılaşılmıştır. Bunun başlıca sebebi olarak, tarama programlarının dayandığı birinci basamak pratisyenlerinin sunumda önemli değişkenlik gösteren bu lezyonları tanımak için gereken yeterli eğitimi almamış olması gösterilmiştir. Tarama ve klinik karar vermeye yardımcı olabilecek görme temelli yardımcı teknolojilerin geliştirilmesi, ağız kanserinin erken teşhisini iyileştirmek için çok önemlidir. Bu çalışmada, çeşitli bilgisayarlı görme tekniklerinin ağız kanseri alanındaki potansiyel uygulamalarını fotoğrafik görüntüler kapsamında araştırıyor ve ağız kanseri taraması için otomatik bir sistemin olasılıklarını inceliyoruz. Derin öğrenmedeki ilerlemelerden yararlanarak, birinci aşamada bir tespit ağı ile oral lezyonları tespit etmek ve ikinci aşamada da tespit edilen bölgeyi bir sınıflandırıcı ağ ile iyi huylu, prekanseröz veya kanser olarak sınıflandırmak için iki aşamalı bir çerçeve öneriyoruz. Ağız kanseri için kamuya açık veri setinin bulunmaması nedeniyle çalışma için özel bir veri seti geliştirilmiştir. İlk sonuçlarımız, oral lezyonların gerçek zamanlı olarak otomatik olarak algılanması ve sınıflandırılması için derin öğrenmeye dayalı yaklaşımların uygulanabilirliğini göstermektedir. Önerilen modelin, tarama süreçlerini destekleyebilen ve ağız kanserinin erken teşhisini iyileştirebilen, düşük maliyetli ve invaziv olmayan bir araç olarak büyük bir potansiyel sunacağını öngörüyoruz.","Oral cancer is a major health concern in many countries leading to approximately 330,000 deaths every year worldwide. Early detection and treatment remain to be the most effective measures in reducing mortality and morbidity from oral cancer with survival rates achieving 75-90% in most cases. Over the years, screening programmes have been put in place to improve detection of precursor lesions that have high probability of progressing into cancer. The implementation of these programs, however, has proved to be challenging in a real-world setting as they rely on primary care practitioners who are often not adequately trained to recognize these lesions that exhibit substantial variability in presentation. The development of vision-based adjunctive technologies that can assist screening and clinical decision making is crucial for improving early detection of oral cancer. In this study, we explore the potential applications of various computer vision techniques to the oral cancer domain in the scope of photographic images and investigate the prospects of an automated system for oral cancer screening. Exploiting the advancements in deep learning, we propose a two-stage framework to detect oral lesions with a detection network in the first stage and classify the detected region as benign, precancer, or cancer with a classifier network in the second stage. A custom dataset was developed for the study due to the lack of publicly available dataset for oral cancer. Our preliminary results demonstrate the feasibility of deep learning-based approaches for automated detection and classification of oral lesions in real-time. We envisage that the proposed model offers great potential as a low-cost and non-invasive tool that can support screening processes and improve early detection of oral cancer."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dokunmatik ekranlar, cep telefonları, ATM'ler, tabletler, otomatlar ve araç navigasyon sistemleri gibi günlük hayatımızın her alanına entegre edilmiştir. Bu ekranlar, dokunsal geri bildirim içermeyen veya sınırlı dokunsal geri bildirim ile dokunmatik etkileşimler için sezgisel bir arayüz sağlar. Sonuç olarak, göz olmadan etkileşim neredeyse imkansızdır ve kullanıcıların bir görevi gerçekleştirmek için görsel ve sesli geri bildirime güvenmeleri gerekir, bu da kullanıcıların performansını ve deneyimini azaltır. Bununla birlikte, yüzey haptik teknolojilerindeki son gelişmelerle, artık dokunmatik ekranlarda karmaşık dokunsal efektler oluşturmak ve daha sofistike dokunsal geri bildirim görüntüleyerek kullanıcı etkileşimlerini geliştirmek mümkün hale geliyor. Bu tezin genel amacı, yeni yüzey haptikleri teknolojileri kullanılarak dokunmatik ekranlarda gerçekçi dokunsal düğmelerin ve şekillerin nasıl oluşturulacağını anlamaktır. Bu bağlamda, tezin ilk kısmı, ekrana bağlı piezo aktüatörleri kullanarak fiziksel düğmelerin hissini simüle eden bir dokunmatik ekran üzerinde vibrotaktil geri bildirim oluşturmaya odaklanmaktadır. Bu amaçla, önce üç farklı fiziksel düğmeyle etkileşime giren on iki katılımcının kuvvet, ivme ve voltaj verilerini kaydettik ve analiz ettik: mandal, geçiş ve basma düğmeleri. Daha sonra, kaydedilen verilere dayalı olarak her düğme için düğmeye özel bir titreşimli uyaran üretildi. Sonuçlarımız, katılımcıların üç dijital düğmeyi %83'lük bir başarı oranıyla fiziksel benzerleriyle eşleştirebildiklerini gösterdi. Ek olarak, katılımcılar bu çalışmada incelenen tüm fiziksel ve dijital düğmeler için yedi sıfat çifti kullanarak öznel duygularının derecesini derecelendirdiler. Sonuçlarımız, katılımcıların üç dijital düğmeden ikisini fiziksel karşılıklarına benzer şekilde derecelendirdiği en az üç sıfat çifti olduğunu gösterdi. İkinci bölümde, dokunmatik ekranda elektrovibrasyon kullanarak, üç farklı dokunsal göreselleştirme yöntemi ile (iç, dış, kenar) oluşturulan ve prototip ve prototip olmayan yönlerde görüntülenen beş dokunsal şeklin (yani üçgen, kare, beşgen, altıgen ve sekizgen) tanıma oranı ve süresi araştırıldı. Sonuçlar, haptik olarak aktif alan (elektrotitreşimin açık olduğu alan) daha büyük olduğunda şekillerin doğru tanıma oranının daha yüksek olduğunu gösterdi. Bununla birlikte, kenar sayısı arttıkça, tanıma süresi arttı ve tanıma oranı önemli ölçüde düştü, örenğin prototip olmayan sekizgen için %20 şans oranından biraz daha yüksek bir değere ulaştı. Ayrıca, iç görselleştirme koşulu için tanıma süresi, kenar ve dış görselleştirme koşullarına kıyasla önemli ölçüde daha kısaydı ve kenar görselleştirme koşulu, en uzun tanıma süresine yol açtı. Dokunsal keşif stratejileri analizlerimiz, katılımcıların önce görüntülenen şekillerin kaba özelliklerini çıkarmak için global taramayı kullandıklarını ve daha sonra daha ince ayrıntıları belirlemek için yerel tarama uyguladıklarını, ancak prototip olmayan şekiller durumunda nihai onay için başka bir global taramaya ihtiyaç duyduklarını ortaya koydu. Keşif için tek bir parmak kullanıldığında, elektrovibrasyon altında şekillerin kenarlarını takip etmenin ve beşten fazla kenarı olan şekilleri tanımanın oldukça zor olduğunu gözlemledik. Bu çalışmaların sonuçları, dokunmatik ekranlarda dijital düğmelerin ve şekillerin gerçekçi bir şekilde oluşturulması için daha zengin dokunsal uyaranların gerekli olduğunu göstermektedir. Örneğin, düğmelerin oluşturulmasında kinestetik geribildirimin olmaması ve şekillerin oluşturulmasında dokunsal geribildirimin sadece teğetsel yönde olması, çalışmamızdaki katılımcıların algısını olumsuz etkilemiştir. Bu bulgular, dokunmatik ekranlardaki grafik öğelerle verimli ve etkili dokunsal etkileşim için teknikler geliştirmede dokunsal arayüz tasarımcılarına rehberlik edebilir.","Touchscreens are integrated in every aspect of our daily life as mobile phones, ATMs, tablets, vending machines, and car navigation systems. These screens provide an intuitive interface for touch interactions with no or limited haptic feedback. As a result, eye-free interaction is nearly impossible and users have to rely on visual and audio feedback to perform a task, which reduces the users' performance and experience. However, with the recent advances in surface haptics technologies, it is now becoming possible to generate complex tactile effects on touchscreens and enhance the user interactions by displaying more sophisticated haptic feedback. The overall goal of this thesis is to understand how to render realistic tactile buttons and shapes on touchscreens using novel surface haptics technologies. In this regard, the first part of the thesis focuses on creating vibrotactile feedback on a touchscreen that simulates the feeling of physical buttons using piezo actuators attached to the screen. For that purpose, we first recorded and analyzed the force, acceleration, and voltage data from twelve participants interacting with three different physical buttons: latch, toggle, and push buttons. Then, a button-specific vibrotactile stimulus was generated for each button based on the recorded data. Our results showed that participants were able to match the three digital buttons with their physical counterparts with a success rate of 83%. In addition, participants rated the degree of their subjective feelings using seven adjective pairs for all the physical and digital buttons investigated in this study. Our results showed that there exist at least three adjective pairs for which participants have rated two out of three digital buttons similar to their physical counterparts. In the second part, we investigated the recognition rate and time of five tactile shapes (i.e., triangle, square, pentagon, hexagon, and octagon) rendered by electrovibration on a touchscreen using three different methods and displayed in prototypical orientations and non-prototypical orientations (i.e., 15 degrees CW and CCW to the prototypical orientation). The results showed that the correct recognition rate of the shapes was higher when the haptically active area (area where electrovibration was on) was larger. However, as the number of edges increased, the recognition time increased and the recognition rate dropped significantly, arriving to a value slightly higher than the chance rate of 20% for non-prototypical octagon. Moreover, the recognition time for inside rendering condition was significantly shorter as compared to edge and outside rendering conditions and edge rendering condition led to the longest recognition time. Our analyses of exploration strategies revealed that participants first used global scanning to extract the coarse features of the displayed shapes, and then they applied local scanning to identify finer details, but needed another global scan for final confirmation in the case of non-prototypical shapes. We also observed that it was highly difficult to follow the edges of shapes and recognize shapes with more than five edges under electrovibration when a single finger was used for exploration. The results of these studies show that richer haptic stimuli is necessary for realistic rendering of digital buttons and shapes on touchscreens. For example, lack of kinesthetic feedback in rendering buttons and displaying haptic feedback in tangential direction only in rendering shapes adversely affected the perception of the participants in our study. These results are considered as a starting point for the development of tactile stimuli, haptically improved user interfaces, and touchscreen applications. These findings can also provide some guidance to haptic interface designers in developing techniques for efficient and effective interaction with graphical elements on touchscreens."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Google'ın kuantum bilgisayarların klasik bilgisayarlara göre üstünlüğü iddiası, kuantum hesaplama tarihinde büyük bir kilometre taşıdır. İddialara rağmen, kuantum bilgisayarların pratik uygulanabilirliği, düşük kuantum bit sayısı ve yüksek gürültü oranları nedeniyle tartışmalı olmaya devam etmekte. Kuantum hesaplamanın alternatif modeli, yalnızca belirli bir formatta bir optimizasyon problemini çözebilen kuantum tavlamadır. Kuantum tavlama, bu tür cihazların ölçeğinin binlerce kübite kadar çıkması nedeniyle aktif olarak araştırılmaktadır. Bu nispeten yüksek kübit sayısı, kuantum tavlayıcıların daha büyük boyutlardaki sorunları çözmesini sağlar, bu nedenle onları gerçek hayat senaryolarında kullanılabilir hale getirir. Çözülmüş optimizasyon probleminin spesifik formatı, aynı zamanda ikinci dereceden kısıtlanmamış ikili optimizasyon (QUBO) olarak da temsil edilebilen Ising formülasyonu olarak adlandırılır. QUBO, karşılık gelen önyargı ağırlıklarına ve kübitler arasındaki kuadratik ağırlıklara sahip bir kübit kümesinden oluşur. Bu tez, iki farklı kombinatoryal optimizasyon probleminin QUBO formülasyo- nunda ağırlık optimizasyonu için iki şema sunmaktadır. Her iki şema, QUBO'ları çözen bir tavlama cihazı ile arayüzlenen QUBO ağırlıklarını yeniden tanımlayan klasik bir bilgisayarı içerir. İlk kombinatoryal problem, önyargıların görevlerin hesap- lama maliyetlerini temsil ettiği ve ikinci dereceden terimlerin görevler arasındaki iletişimi modellediği görev atama problemidir. İkinci problem, önyargıların kuantum kapılarının aslına uygunluğunu temsil ettiği, ikinci dereceden terimlerin kübit hareketi modellediği devre haritalama problemidir. Ağırlık optimizasyon algoritması (WOA) olarak adlandırılan ilk yaklaşım, kuantum kapılarını fiziksel kübit topolojisine eşlemenin uygunluğundan sorumlu kübit önyargıları ile kübit hareketinden sorumlu ikinci dereceden terimler arasında istenen bir oranı arar. Oranın istenebilirliği, hem kübit hareketinden hem de haritalamadan kaynaklanan toplam aslına uygunluk ile tanımlanır. WOA'nın kuantum devre haritalaması için kuantum tavlama iş akışına eklenmesi, tüm sorunlu örneklerin %72,9'unda kübit hareketinin azalmasına neden oldu. Ayrıca, eşlenen devrenin toplam doğruluğunu IBM Vigo cihazında %39 ve IBM QX2'de %107 oranında artırmaya izin verdi. Deneyler, D-Wave kuantum tavlama yazılım yığınından tabu arama QUBO çözücüsünde gerçekleştirilmiştir. Karınca kolonisi ağırlık optimize edicisinin sonuçları, bir kuantum tavlama cihazının bulunmaması nedeniyle sınırlıdır.","Google's claim of quantum supremacy is a big milestone in the history of quantum computing. Despite the claim, the practical applicability of quantum computers remains questionable due to the low number of quantum bits and high noise rates. An alternative model of quantum computing is quantum annealing, which is capable of solving only an optimization problem in a specific format. Quantum annealing is being actively researched due to the fact that the scale of such devices has increased up to thousands of qubits. This relatively high number of qubits enables quantum annealers to solve problems of larger sizes, hence makes them usable in real-life scenarios. The specific format of the solved optimization problem is called an Ising formulation, which can also be represented as quadratic unconstrained binary optimization (QUBO). The QUBO consists of a set of qubits with corresponding bias weights and the quadratic weights between the qubits. This thesis presents two schemes for weight optimization in the QUBO formulation of two different combinatorial optimization problems. Both schemes involve a classical computer redefining the QUBO weights that is interfaced with an annealing device, which solves the QUBOs. The first combinatorial problem is the task assignment problem, in which the biases represent the computational costs of tasks and quadratic terms model communication between tasks. The second problem is the circuit mapping problem, where the biases represent the fidelity of quantum gates, while the quadratic terms model qubit movement. The first approach named weight optimization algorithm (WOA) searches for a desirable ratio between the qubit biases responsible for fidelity of mapping quantum gates to physical qubit topology and the quadratic terms responsible for qubit movement. The desirability of the ratio is defined by the total fidelity resulting from both qubit movement and mapping. The second presented model uses ant colony optimization (ACO) to update the quadratic terms of the QUBO that solves the task assignment problem. However, this model can be generalized to any combinatorial optimization problem solvable by the ACO. Efficient updates of the weights based on the answers from previous QUBOs are expected to guide the reformulated QUBOs towards the optimum of the objective function. At the same time, this algorithm would allow utilizing the stochasticity of quantum annealers for better exploration of the solution space and their speed for faster generation of candidate solutions. The introduction of the WOA into the quantum annealing workflow for quantum circuit mapping resulted in reduced qubit movement in 72.9% of all problem samples. Moreover, it allowed to increase the total fidelity of the mapped circuit by 39% on the IBM Vigo device and 107% on IBM QX2. The experiments have been performed on the tabu search QUBO solver from the D-Wave quantum annealing software stack. The results for the ant colony weight optimizer are limited due to the unavailability of a quantum annealing device."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Robotiğin giderek daha geniş bir görev ve ortam yelpazesinde uygulanmasına yönelik artan talep göz önüne alındığında, robot programlamanın aksine robot öğrenmesi gün geçtikçe daha önemli hale gelmektedir. Ancak robotlarla veri toplamak pahalı ve zaman alıcı bir iştir, bu nedenle öğrenme yöntemleri sınırlı miktarda veri ile çalışmak durumundadır. Yüksek boyutlu algısal verilerle çalışırken, düşük boyutlu, kullanışlı temsiller öğrenmek, düşük veri ortamının çıkardığı zorluklarla başa çıkmada önemli bir husustur. Bu tezde, kendi kendini denetleyen bir yapıda az sayıda insan beceri gösteriminden algısal temsiller öğrenen bir sinir ağı mimarisi geliştiriyoruz. Geliştirilen modeller, verilerin sıralılığını ve problemin düşük veri yapısını dikkate almaktadır. Öğrenilen temsiller, gösterilen becerilerin algısal hedef modellerini öğren-mek için kullanılır. Bu modeller, öğrenilen beceri yürütmelerini izleyebilir (başarılı/ başarısız şeklinde değerlendirebilir) ve ödül mühendisliği olmadan ödül sinyali oluş-turmak için pekiştirmeli öğrenmede kullanılabilir. Nesne tabanlı manipülasyon becerileri üzerine yaptığımız simüle edilmiş ve gerçek robot değerlendirmeleri, öğrenilen temsillerin, genel boyutluluk azaltma yöntemlerine kıyasla izleme performansı ve pekiştirmeli öğrenme performansı açısından daha iyi hedef modelleri ile sonuçlan-dığını göstermektedir. Bu çalışmada ek olarak, gösterimlerden öğrenme bağlamında aktarımlı öğrenme yaklaşımları tanıtıyoruz ve aynı beceri için farklı nesneler arasında, belirli koşullar altında aynı nesne için farklı beceriler arasında ve ayrıca farklı görsel girdi uzayları arasında pozitif aktarım gözlemlendiğini gösteriyoruz. Modüler sinirsel mimarimizin mümkün kıldığı bilgi aktarımı teknikleri, geleceğe yönelik öğrenme için mevcut verilerden yararlanarak veri gereksinimini daha da azaltmamızı sağlamaktadır.","Given the growing demand for the application of robotics in an increasingly wide range of tasks and environments, robot learning as opposed to programming is getting more relevant by the day, since programming robots is tedious and usually only feasible in controlled domains. However, gathering data with robots is an expensive and time-consuming task, so the learning methods must cope with the limited amount of data available. When working with high dimensional perceptual data, learning low-dimensional, useful representations is a key aspect in dealing with the low-data setting. In this thesis, we develop a neural network architecture to learn perceptual representations from few human skill demonstrations in a self-supervised manner. The developed models take the sequentiality of the data and the low-data nature of the problem into account. These representations are used to learn perceptual goal models of the demonstrated skills. These models can monitor the learned skill executions and be used in reinforcement learning to generate reward signals, without explicit reward engineering. Our simulated and real robot evaluations with object manipulation skills show that the learned representations result in better goal models in terms of monitoring and reinforcement learning performance compared to generic dimensionality reduction methods. We further introduce transfer learning approaches in the context of learning from demonstration and show positive transfer between different objects for the same skill, between the same object for different skills under certain conditions, and also between different perceptual domains. Transferring knowledge as enabled by our modular neural architecture allows us to leverage existing data for continual learning into the future. Overall, we show that our proposed self-supervised representation learning architecture has the potential to improve learning from demonstration approaches with a perceptual component."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Duyguların uyarılma-valans düzleminde temsil edildiği, müzikal ses verisinde duygu tanıma için sürekli uyarılma saptaması problemini ele alıyoruz. Çalışmamızda, öznitelik olarak mel-spectrogram katsayılarını kullanan iki yinelemeli yapay sinir ağının füzyonu olan yeni bir yöntem sunuyoruz: Öznitelik çıkarıcı olarak kullanılan, frekans ekseninde işleyen bir çift yönlü GRU ve ardından zaman ekseninde işleyen, modelin gerçek zamanda gerçeklenebilir olabilmesi için tek yönlü olarak seçilen i-kinci bir GRU. Bu yöntem MediaEval2015- Emotion in Music veri kümesi üzerinde değerlendirilmekte ve literatürde bulunan gerçek zamanda gerçeklenebilir modellerin raporladığı sonuçlardan daha iyi olan 0.215 RMSE değerine ulaşmaktadır.","We address the problem of continuous arousal detection for emotion recognition in musical audio pieces where emotions are represented in the two-dimensional arousal-valence space. We propose a novel method which is a combination of two recurrent neural networks using mel-spectrogram features: A bidirectional GRU network along the frequency dimension as a feature extractor, stacked with a GRU network along the temporal dimension, which is unidirectional for real-time adaptability. The method is evaluated on the MediaEval2015- Emotion in Music Dataset, achieving an RMSE of 0.215 which is better than the results reported by real-time adaptable state-of-the-art models."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"GPU iletişimi, çoklu GPU hızlandırmalı uygulamaların performansında ve ölçeklene-bilirliğinde kritik bir rol oynar. Giderek artan iletişim yöntemleri ve türleriyle, programcının bir uygulamada gerçekleşen iletişim miktarını ve türünü bilmesi genellikle zordur. MPI için dağıtık sistemlerdeki iletişimi ve paylaşılan bellek sistemlerinde çok iş parçacıklı uygulamaları algılayan önceki çalışmalar olsa da, bildiğimiz kadarıyla, bu çalışmaların hiçbiri düğüm içi GPU iletişimini tanımlamamaktadır. Bu çalışmada, bir düğümdeki tüm GPU-GPU ve CPU-GPU çiftleri arasındaki iletişim türlerini tanımlayan ve sınıflandıran bir araç olan ComScribe'u sunuyoruz. Aracımız, açık iletişim ilkelleri, Birleşik Bellek işlemleri ve Sıfır Kopyalı Bellek aktarımlarından kaynaklanan düğüm içi noktadan noktaya iletişimi yakalamak için NVIDIA'nın profil oluşturucusu nvprof üzerine inşa edilmiştir. Bir düğümde toplu GPU-GPU iletişimini izlemek için ComScribe, NCCL'nin toplu ilkelerini çalışma zamanında GPU'lar arasındaki veri aktarımlarını kaydeder. Veri hareketini hem aktarılan bayt sayısı hem de aktarım sayısı için bir iletişim matrisi olarak görselleştirir. Aracımızı 16 GPU'da doğrulamak için, NVIDIA, Comm|Scope ve MGBench benchmark paketlerinden 13 mikro ve 3 makro karşılaştırmalı iletişim modelleri oluşturduk. Aracın yeteneklerini gerçek hayattaki uygulamalarda göstermek için, üç derin sinir ağı modelinin içgörülü iletişim matrislerini oluşturduk. Sonuç olarak, ComScribe programcıya iletişim kuran GPU gruplarını, iletişim hacmini ve kullanılan ilkel türlerini belirlemede rehberlik eder. Bu, performans darboğazlarını ve daha da önemlisi bir uygulamadaki iletişim hatalarını tespit etmek için yollar sunar.","GPU communication plays a critical role in performance and scalability of multi-GPU accelerated applications. With the ever increasing methods and types of communication, it is often hard for the programmer to know the exact amount and type of communication taking place in an application. Though there are prior works that detect communication in distributed systems for MPI and multi-threaded applications on shared memory systems, to our knowledge, none of these works identify intra-node GPU communication. In this work we present ComScribe, a tool that identifi es and categorizes types of communication among all GPU-GPU and CPU-GPU pairs in a node. Our tool is built on top of NVIDIA's pro lfier nvprof for capturing intra-node point-to-point communication resulting from explicit communication primitives, Uni ed Memory operations, and Zero-copy Memory transfers. For monitoring collective GPU-GPU communication in a node, ComScribe intercepts NCCL's collective primitives at runtime and records data transfers among GPUs. It visualizes data movement as a communication matrices for both number of bytes transferred and the number of transfers. To validate our tool on 16 GPUs, we present communication patterns of 13 micro and 3 macro-benchmarks from NVIDIA, CommjScope, and MGBench benchmark suites. To demonstrate tool's capabilities in real-life applications, we also present insightful communication matrices of three deep neural network models. All in all, ComScribe can guide the programmer in identifying groups of communicating GPUs, the volume of communication, and types of primitives used. This o ers avenues to detect performance bottlenecks and more importantly communication bugs in an application."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüz ifadelerini düzenleme yeteneği, bilgisayar ara yüzlerinde çok çeşitli uygulamalara sahiptir. İdeal yüz ifadesi düzenleme algoritmasının iki önemli kriteri karşılaması gerekir. İlk olarak, bireysel yüz eylemlerinin kesin ve hedefli düzenlenmesine izin vermelidir. İkincisi, yapaylık olmadan yüksek kaliteli çıktılar üretmelidir. Biz yüz ifadesi düzenlemesi için, yüzlerin anlamsal manipülasyonu alanında yaygın olarak kullanılan StyleGAN'a dayalı bir çözüm oluşturuyoruz. Bunu yaparken, StyleGAN'da çeşitli anlamsal niteliklerin nasıl kodlandığına dair anlayışımıza katkıda bulunuyoruz. Özellikle, gizli uzayda düzenleme yapmak için naif bir stratejinin belirli eylem birimleri arasında, birbirlerinden bağımsız olsalar bile, istenmeyen bağlantılarla sonuçlandığını gösteriyoruz. Örneğin, kaş indirme ve dudak gerdirme farklı eylem birimleri olmasına rağmen, eğitim verilerinde ilişkili görünüyorlar. Bu nedenle, StyleGAN onları çözmekte zorluk çekiyor. Her bir eylem birimi için ayrılmış etki bölgelerini hesaplayarak bu tür eylem birimlerinin bağımsız düzenlenmesine izin veriyoruz ve düzenlemeyi bu bölgelerle sınırlandırıyoruz. Toplam 43 denekle gerçekleştirilen algı deneyleri ile yerel surat ifadesi düzenleme yöntemimizin etkinliğini doğruluyoruz. Sonuçlar, yöntemimizin yerel düzenleme üzerinde daha yüksek kontrol sağladığını ve son teknoloji yöntemlerle karşılaştırıldığında üstün ve resmin aslına uygun görüntüler ürettiğini gösteriyor.","The ability to edit facial expressions has a wide range of applications in computer graphics. The ideal facial expression editing algorithm needs to satisfy two important criteria. First, it should allow precise and targeted editing of individual facial actions. Second, it should generate high fidelity outputs without artifacts. We build a solution based on StyleGAN, which has been used extensively for semantic manipulation of faces. As we do so, we add to our understanding of how various semantic attributes are encoded in StyleGAN. In particular, we show that a naive strategy to perform editing in the latent space results in undesired coupling between certain action units, even if they are conceptually distinct. For example, although brow lowerer and lip tightener are distinct action units, they appear correlated in the training data. Hence, StyleGAN has difficulty in disentangling them. We allow disentangled editing of such action units by computing detached regions of influence for each action unit and restrict editing to these regions. We validate the effectiveness of our local editing method through perception experiments conducted with a total of 43 subjects. The results show that our method provides higher control over local editing and produces images with superior fidelity compared to the state-of-the-art methods."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İstenmeyen rafineri olayları daha düşük kaliteli ürüne, süreç kesintilerine veya ekipman arızalarına yol açar. Operatörler, rafineri süreçlerini izler ve gerektiğinde bu tür durumları önlemek veya etkilerini azaltmak için müdahale ederler. Rafineri süreçleri için gerekli donanımlar büyük ve karmaşıktır. Genellikle dikkat edilmesi gereken birçok sensör vardır ve bu da izleme işini zorlaştırır. Bu çalışmada, izleme yükünü azaltmayı amaçlayan operatör karar destek sistemleri geliştirmek için veriye dayalı bir metodoloji önermekteyiz. Karar desteği, belirli bir zaman çerçevesi içinde istenmeyen bir olayın meydana geleceğini operatöre bildirmek için yumuşak alarmlar verir. Olay tanımı sürece özel (ör. 15 dakika içinde 5 dereceden fazla sıcaklık artışı) veya genel (ör. 15 dakika içinde anormal sensör ölçümü) olabilir. Buradaki fikir, geçmiş verilerden eğitilmiş yapay öğrenme modelleri kullanarak bu olayları tahmin etmektir. Belirli bir ekipman birimi için sensör verileri, çok boyutlu bir zaman serisi olarak ele alınır. Olaylar, ya zaman serisi regresyonu ve tahminlere olay mantığının uygulanması ya da belirli bir zaman diliminde bir olayın olup olmayacağının sınıflandırılması yoluyla tahmin edilir. Bu olaylar genellikle nadirdir ve bu da onları tahmin etmeyi zorlaştırır. Bu, tüm olayları yakalamak (doğru tahmin edilen olayların toplam olay sayısına oranı) ile sadece doğru olayları yakalamak (doğru tahmin edilen olayların bütün gerçekleşen olaylara oranı) arasında, yani hatırlama/hassasiyet ve kesinlik arasında, ödünleşmeye neden olur. Karar destek sistemi tüm olayları yakalar, ancak daha fazlasını yanlış tahmin ederse, operatörün güveni azalacaktır. Bu nedenle, makul hatırlama değerlerini tutturarak kesinlik değerlerini arttırmaya odaklanıyoruz. Bu yüzden, sonuçlar F 0.1 puanını kullanarak ve ikincil olarak en az 0,5 kesinlik ve 0,1 hatırlama elde edilmesine bakılarak değerlendirilmektedir. Önerilen yaklaşım, gerçek bir rafinerinin dört yatağında birden fazla yapay öğrenme modeli ile değerlendirilmiştir. İlk değerlendirme olay tanımlarını doğrudan uygulayarak tahminlemek üzerinedir. Sonuçlar, her yatak için yüksek F 0.1 puanları elde eden en az bir yöntem bulunduğunu ve böylece karar destek sistemi oluşturmanın uygulanabilir olduğunu göstermektedir. Ancak yapay öğrenme modelleri arasında belirgin bir kazanan yoktur ve farklı yataklar için farklı performanslar elde edilmektedir. Ayrı ayrı sensörler yerine yataklar için olayları tahmin etmek önemli bir kazanım sağlamamıştır. Ayrıca, operatörlerin daha fazla esnekliğe sahip olması adına hatırlama ve kesinlik ödünleşimini ayarlamak için uyarlanabilir bir eşikleme yöntemi önerilmektedir. Bu, tahmin hatalarının güncel standart sapmasına dayanmaktadır. Değerlendirmeler, ödünleşimin mümkün olduğunu ve hatırlama değerlerini azaltsa da F 0.1 performansını önemli ölçüde artırmak için kullanılabileceğini göstermektedir. Son değerlendirme için, doğrudan sıcaklık tahmini yerine sıcaklık farkı tahmini denenmiştir. Yöntemlerin alt-kümesine uygulanan bu fikir, performans arttırmıştır ve daha üzerine gidilebilir olduğunu göstermiştir.","Refinery upsets lead to lower quality product, process shutdowns or equipment failure. Human operators monitor refinery processes and intervene if needed to prevent such upsets or reduce their effects. The equipment required for these processes are large and complex. There are usually many sensors to pay attention to, making the monitoring job difficult. In this work, we propose a data-driven methodology to develop operator decision aids that aim to decrease the monitoring burden. The decision aid raises soft-alarms to notify the operator that an undesirable event will happen within a certain time-frame. The event definition can be process specific (e.g. more than 5 degrees temperature rise within 15 minutes) or generic (e.g. the sensor measurement will be anomalous within 15 minutes). The idea is to predict these events using machine-learning models trained from historical data. The sensory information for a given equipment unit is treated as a multi-dimensional time-series. The events are predicted either by time-series regression and applying the event logic on the forecasts or by classification of whether there will be an event or not in a given time-frame. These events are usually rare which makes them hard to predict. This results in the trade-off between capturing all the events (ratio of correctly predicted events over total number of events) versus capturing the true events (correctly predicted events to all the positive predictions) or the trade-off be-tween recall/sensitivity and precision. If the decision aid captures all the events but mispredicts many more, the trust of the operator will be eroded. Thus, we concentrate on getting good event prediction precision values while still keeping reasonable recalls. We evaluate our approach with multiple machine learning models on the four beds of a real refinery hydrocracker unit. Our first evaluation is applying the event definitions directly. Our results show that there is at least one method that can achieve around 0.5 or higher F0.1 for each bed, implying that building a decision aid is viable. However, there isn't a clear winner among the machine-learning methods and performance changes between different beds. Predicting events for beds, instead of sensors, did not yield significant gains. Tuning the precision-recall trade-off to the end of improving the decision aid performance, we suggest an adaptive thresholding method using the running standard deviation of the prediction errors. The evaluations show that the adaptive approach can increase the performance significantly. For the last evaluation, we directly predict the change of temperature values within the target time horizon in our regression methodology. We show that there are performance gains for a subset of our data utilizing this perspective of time series prediction."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein arayüzlerinin yapısal mimarisini anlamak, proteinlerin nasıl etkileşime girdiği ve fonksiyonlarını nasıl yerine getirdiğini anlamak için oldukça önemlidir. Protein-protein arayüzleri, ilaç keşfi ve yeniden konumlandırma çalışmaları için önemli hedefler olabilir; bu ancak mevcut yapısal verilerin anlamlandırılabilmesiyle mümkündür. Deneysel tekniklerdeki gelişmeler, daha büyük ve daha karmaşık protein yapılarının tespit edilmesini mümkün kılmaktadır. Bu araçlar daha erişilebilir hale geldikçe, PVB'de saklanan protein yapılarının sayısı da hızla artmaktadır. Protein komplekslerinin boyutundaki ve verilerin sayısındaki bu artış, protein etkileşimlerini anlamak için paha biçilmez bir araçtır; ancak bu veri bir takım hesaplamalı zorlukları beraberinde getirmektedir. Bu çalışma, öncelikle protein-protein arayüzlerini belirlemeye ve PVB'de bulunan yapı boyutlarının büyümesinden kaynaklanan ve verinin protein arayüzleri açısından analizinde ortaya çıkan bazı zorlukların çözümü için öneriler sunmaktadır. Bu çalışmada, PVB'de bulunan 169,681 protein yapısını değerlendirdik ve 499,169 protein-protein ara yüzü elde ettik. Benzersiz bir protein arayüzleri seti, tüm arayüzlerin yapısal olarak karşılaştırılmasıyla oluşturulabilir. Artan arayüz yapısı, tüm arayüz yapılarını makul bir zaman çerçevesinde karşılaştırmayı neredeyse imkansız kılmaktadır. Bu nedenle, karşılaştırma sayısını yapısal bilgileri kaybetmeden değerlendirilebilir bir aralığa getirebilmek için hesaplamalı çözümlere ihtiyaç vardır. Bu çalışmanın ikinci bölümü, protein sekans ve yapı bilgileri kullanılarak oldukça benzer proteinleri filtrelemek için kullanılabilecek bir yöntem sunmaktadır. Bu süreç sonucunda sıra ve yapı bakımından benzersiz 331.231 arayüz belirledik. Bu filtreleme sonucunda gereken karşılaştırma sayısı, herhangi bir yapısal bilgi kaybetmeden, 2,3 kat azalmıştır. Bu tekniklerin uygulanması, büyük veri kümeleri için protein arayüz karşılaştırması üzerine gelecekte yapılması planlanan çalışmaları mümkün kılabilir.","Understanding the structural architecture of protein interfaces is one of the key challenges in explaining how proteins interact and function. Protein-protein interfaces can be important targets for drug discovery and repurposing studies; this is only possible if the structural data available can be utilized in a meaningful way. Advancements in experimental techniques make it possible for scientists to determine larger and more complex protein structures. As these tools are becoming more accessible, the number of protein structures deposited in PDB is also increasing rapidly. This increase in the size of the protein complexes and the size of the data is an invaluable tool for understanding protein interactions. Still, it does not come without its computational challenges. This study first focuses on identifying protein-protein interfaces and overcoming some of the difficulties rooting from growing structure sizes in PDB. We evaluated 169,681 available protein structures in PDB and extracted 499,169 protein-protein interfaces. A unique set of protein interfaces can only be created by structural comparison of interfaces. The increasing number of interface structures makes it almost impossible to compare all interface structures in a reasonable time frame. Therefore, computational solutions are needed to reduce the comparison number within a manageable range without losing structural information. The second part of this study explains an approach for filtering highly similar proteins using sequential and structural information. As a result of this process, we identified 331,231 interfaces, unique in terms of sequence and structure. The reduced number of interfaces results in almost a 2.3-fold reduction in the number of comparisons needed, without losing any structural information. Implementing these techniques can make future studies on protein interface comparison possible for large data sets."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Matrisleri çarpanlarına ayırma yöntemleri sinyal işleme ve makine öğrenmesi uygulamalarında yaygın olarak kullanılmaktadır. Bu yöntemler, bu alanlardaki problemlerde kullanılan çok sayıda algoritmanın temelini oluşturur. Biz de temel problemlerden biri olan girdi verisinin içine gömülü bilgiyi keşfetmeye çalışıyoruz. Bu problem için genel bir çözüm stratejisi olarak, girdi verisi iki faktör/matrisin çarpımı olarak modellenmiştir. Bu, sinyal işleme literatüründe Kör Kaynak Ayrımı (Kör Veri Ayrıştırma) olarak bilinir. Bu tezde, yeni bir veri ayrıştırma yaklaşımı olarak Polytopic Matrix Factorization'ı (PMF) tanıtıyoruz. Girdi verilerini, bir politoptan gelen bazı gizli vektörlerin bilinmeyen doğrusal dönüşümleri olarak modelliyoruz. Politop seçimi, gizli vektörlerin varsayılan yapısını ve bunların ilişkilerini belirliyor. İlk olarak, gizli vektörlere ilişkin olarak PMF'nin tanımlanabilirlik kriterini ve tanımlanabilirlik koşullarını sunuyoruz. Tanımlanabilirlik için, politopa sığabilecek maksimum hacimli elipsoidin, özel bir sıkışma kısıtlaması ile gizli vektörlerin dışbükey kabuğunda bulunmasını gerektiren yeterli bir koşul tanıtıyoruz. Uygulamalarda yaygın olarak kullanılan niteliklere karşılık gelen özel politop durumlarının PMF tanımlanabilirlik sonuçlarını sunuyoruz. Ardından, PMF'in uygulanabileceği politopların karakterizasyonunu ile PMF'nin genelleştirilmiş bir versiyonunu sunuyoruz ve bu politopları belirlemek için bir karar algoritması kullanmayı öneriyoruz. PMF'i, PMF'e uygun politoplar üzerinde özel bir doğrusal işlem sınıfını düşünerek genişletiyoruz. PMF'i Bounded Matrix Factorization olarak daha da genişletiyoruz ve politoplara atıfta bulunarak bazı sınırlı kümelerin tanımlanabilirlik sonuçlarını sağlıyoruz. Ayrıca bir PMF algoritması ve PMF'i veri ayrıştırma aracı olarak kullanan ilginç örnekler sunuyoruz. PMF, gizli vektörleri karakterize etmede sonsuz sayıda politop ve sınırlı seti seçenek olarak kullanmamızı sağlıyor. Bu nedenle, alt vektör düzeyinde negatif olmama, seyreklik ve benzeri nitelikler gibi gizli vektörler için farklı varsayılan yapılar ve ilişkiler tanımlamak mümkündür. Özetle, gizli vektörlerin yapısal varsayımları açısından yüksek derecede esneklik sağlayan yeni bir veri ayrıştırma aracı sunuyoruz ve bu esnekliği PMF'e uygun farklı setler ve bu setlere karşılık gelen nitelikler ile anlatan örnekler sunuyoruz.","Matrix factorization methods are widely used in signal processing and machine learning applications. These methods lay the foundation of a large number of algorithms utilized in problems of those areas. As one of the main problems, we try to discover the information hidden inside input data. As a general solution strategy for this problem, input data is modeled as a product of two factors/matrices. This is known as Blind Source Separation (Blind Data Decomposition) in the signal processing literature. In this thesis, we introduce Polytopic Matrix Factorization (PMF) as a novel data decomposition approach. We model input data as unknown linear transformations of some latent vectors drawn from a polytope. The choice of polytope determines the presumed structure of latent vectors and their relationships. We first propose the identifiability criterion and identifiability conditions of PMF regarding the latent vectors. We introduce a sufficient condition for identifiability, which requires that the maximum volume inscribed ellipsoid of the polytope is contained in the convex hull of the latent vectors with a particular tightness constraint. We propose PMF identifiability results of special polytope cases corresponding to widely utilized feature attributes in applications. Then, a generalized version of PMF is presented with the characterization of eligible polytope choices and we propose to use a decision algorithm to determine these eligible polytopes. The proposed PMF tool is extended by considering a special class of linear mappings on eligible polytopes. We further extend PMF as Bounded Matrix Factorization and provide identifiability results of some bounded sets referring to polytopes. We furthermore present a PMF algorithm and interesting examples that utilize PMF as a data decomposition tool. PMF enables us to use infinitely many polytope choices and bounded sets in characterizing latent vectors. Therefore, it is possible to define different presumed structures and relations for latent vectors such as nonnegativity, sparsity and such attributes in subvector level. In brief, we present a novel data decomposition tool that provides a high degree of flexibility in terms of presumed structures of latent vectors and offer examples illustrating this flexibility with different eligible sets and corresponding feature attributes."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri bilimi metodolojisi ve özellikle makine öğrenmesi teknikleri günümüzde birçok farklı alanda yaygın olarak kullanılmaktadır. Özel sektörde de kullanımı oldukça avantajlı olan makine öğrenmesini uygulama konusunda aynı eğilim bulunmaktadır. Birden fazla girdi parametresinin olduğu bilinen işletmedeki kilit değişkenlerin tahmini önemlidir ve bu, firmanın kar düzeyini etkileyebilmektedir. Bu araştırma, belirli bir veri çerçevesi ve algoritmik bir yaklaşım gerektiren iş dünyasındaki çeşitli sorunları; bu kapsamda müşteri kaybı tahmini, konut fiyat tahmini ve duygu analizini araştırmayı amaçlamaktadır. Uygulanan algoritmalar her bir vaka için karşılaştırılmaktadır. Ele alınan vakalar, makine öğrenmesi modellerinin sınıflandırma, regresyon ve metin analizi türlerinin birer uygulamasını içermekte ve bunların endüstrideki çeşitliliğiyle birlikte faydalı uygulamalarını da göstermektedir.","Data science methodology and, particularly, machine learning techniques, are being widely used in many different fields today. There is the same trend in the private sector, where using machine learning is highly advantageous. The prediction of key variables in business given multiple input parameters is important and may affect the firm's profitability. This research aims to investigate several problems in business, being churn estimation, housing price prediction and sentiment analysis, which require a certain data framework and an algorithmic approach. The applied algorithms are compared for each case. The regarded cases involve an application of classification, regression, and text analysis types of machine learning models, demonstrating their diversity and useful application in the industry."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bizi çevreleyen dünyayı algılayabilen ve insan dilini kullanarak bizimle etkileşim kurabilen makineler tasarlamak, yapay zekanın köklü hedeflerinden biridir. Dilbilimsel anlamları hesaplamalı olarak modellemek için önemli ilerleme kaydedilmiş olsa da, dilsel ve algısal işlemenin çok modlu görevlerde en iyi nasıl birleştirileceği önemli bir problemdir. Bu tez, dilin biliş, görsel algı ve görev yürütmedeki rolünün farklı yönlerini göz önünde bulunduran, bilişsel esinli birkaç yapay sinir ağı mimarisini sunmaktadır. Önerilen modeller, bilişsel bilim çalışmalarından esinlenilmiş ve görü-dil problemlerindeki ortak örüntülere dayanan tasarım kararlarını içermektedir. İlk olarak, özgün kanal tabanlı bir ilgi mekanizmasına sahip bir kodlayıcı-çözücü mimarisi ve bu mimarinin gezinim komutlarını anlama problemine uygulanışı sunulmuştur. Bu mimarinin algı işleyen birimi, dil önceliklerini kullanarak uzamsal ilişkileri kaybetmeden ortamdaki nesne ve özelliklere odaklanacak şekilde tasarlanmıştır. Ayrıca, modelin algı üzerinde uzamsal olarak muhakeme gerçekleştirmesine olanak sağlayan gelişmiş bir dünya temsili geliştirilmiştir. Daha sonraki bölümde, ilk kez gerçek bir robotik sistemde Sinir Modül Ağları yaklaşımının nasıl kullanılabileceği gösterilmiştir. Büyük boyutta gerçek dünya verisini toplamak maliyetli olduğundan dolayı, sınırlı veri problemine takılmamak için sistem modülleri simüle edilmiş bir veri üzerinde öğrenirken dünya temsilini ayrı olarak öğrenmektedir. Fakat bu durum, kullanıcının dünya modeli ile robotun dünya modeli arasında uyumsuzluklara sebep olmaktadır. Bu problemin üstesinden gelmek için, kullanıcının gördüğü dünya modeli ile robotun algıladığı dünya modelini eşleştirebilmek için, komutlardaki örtük bilgileri kullanarak algı temsilini güncelleyen bir Bayesçi öğrenme yaklaşımı sunulmuştur. Her iki bölümde de, yüksek seviyeli algı temsilleri üzerinde çalışan ve dilin yüksek seviyeli görsel işleme üzerindeki etkisini kullanan sistemler gösterilmiştir. Buna ek olarak, son bölümde, dilin düşük seviyeli görsel işleme üzerindeki etkisi irdelenmiştir. Bu amaçla, hem yüksek seviyeli hem de düşük seviyeli görsel işleme dalları olan bir mimari temel alınmıştır. Bu mimarinin bir veya her iki dalının dil filtreleri ile koşullandırılmış versiyonları atıf ifadelerinden görüntü bölütleme problemine uygulanmıştır. Deneyler sonucunda hem düşük seviyeli hem de yüksek seviyeli görsel işleme süreçlerini dil ile modüle etmenin, dil temellendirme performansını önemli ölçüde geliştirdiği gösterilmiştir.","Designing machines that can perceive the surrounding world and interacting with us using human language is one of the long-standing goals of artificial intelligence. Although tremendous progress has been made to model the linguistic meanings computationally, how to best integrate linguistic and perceptual processing in multi-modal tasks is a significant open problem. This thesis explores several cognitively-inspired neural architectures that consider the different aspects of the language's role in cognition, visual perception, and task execution. Proposed models incorporate design choices motivated by cognitive science studies and are based on the common patterns in vision-language tasks. We begin by presenting an encoder-decoder network with a novel channel-based perceptual attention mechanism and its application to the navigational instruction following task. The perceptual processing component of this architecture is designed to focus on individual objects and properties within the environment using the language priors while preserving the spatial relations. To benefit from the designed component, we also propose an improved agent-centric world representation to allow the model to reason over the perception spatially. Next, we explore the usage of the Neural Module Networks approach in a real robotic system for the first time. Since collecting large-scale real world data is a labor-intensive and expensive work, the system learns the language grounding on simulated data and the perceptual representation separately to overcome the scarce data problem. However, because of the separate learning processes, inconsistencies arise between the user's and robot's world models. To overcome this, we propose a Bayesian learning approach that uses the implicit information in the instruction to update the perceptual belief to align what the user sees and what the robot perceives. In both parts, we demonstrate systems that use the high-level effect of language on visual processing, which operates on high-level representations. In addition to this, in the last part, we investigate the effect of language on low-level visual processing. To this end, we condition one or both low-level and high-level visual processing branches of a backbone architecture on language using language filters and apply these models to the image segmentation from referring expression task. Experiments show that modulating both low-level and high-level visual processing with language significantly improves the language grounding performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sosyal robotlar, yapay zeka destekli becerileriyle, insan-bilgisayar etkileşim (İBE) sistemlerinde yaygın olarak kullanılmaya başlamıştır. İlgi seviyesi ve duygu durumunun modellenmesi sosyal İBE için önemli problemlerdir. Bu tezde bu önemli problemlerin üç yönünü araştırıyoruz, bunlar ilgi seviyesi ve duygu durumunun İBE çerçevesinde takibi ve daha doğal İBE uygulamaları için duygusal konuşan kafa sentezidir. İlgi seviyesinin takibi ortak bir çalışma olan çok kipli insan robot etkileşiminde ilgi seviyesi veri setinin (eHRI veriseti) oluşturulması çerçevesinde çalışılmıştır. İkinci olarak gerçek zamanlı İBE ortamında duygu durumunun takibi incelenmiştir. Bir CNN-GRU mimarisi ses sinyalinden duygu durum özelliklerini tahmin etmek için oluşturulmuştur. Önerilen duygu durumu tahmini yapan model RECOLA ve CreativeIT veri setleriyle eğitilmiş, IEMOCAP veri setiyle test edilmiş- tir. Son olarak konuşma ve yüz noktalarının konuşan kafa sentezindeki katkıları incelenmiştir. Yüz noktaları tabanlı modellerin ses tabanlı modellere göre daha iyi performans gösterdiği gözlemlenmiştir. Bununla birlikte sesin yüz noktaları yanında ek bir girdi olarak kullanılması performansı daha da arttırmaktadır.","Social robots are becoming widely used in human-computer interaction (HCI) systems with their artificial intelligence powered skills. Modeling and monitoring of engagement and affective state in the course of social HCI set important problems. In this thesis, we investigate three aspects of these important problems that are monitoring engagement and affective state in HCI settings, and affective talking head generation for more natural HCI applications. Monitoring of engagement has been studied as a part of the joint work on constructing multimodal database of engagement in human-robot interactions (eHRI Database). The second aspect is affective state monitoring in real-time HCI settings. We construct a CNN-GRU architecture to estimate affective state attributes from the speech signal. The proposed affective state estimation model has been trained with RECOLA and CreativeIT databases and tested on the IEMOCAP database. Lastly, we investigate contributions of speech and facial landmarks in talking head generation. We find that landmark based models have better performance compared to speech based models. However, speech can be used as an additional input along with landmarks to improve the performance further."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Biyomoleküllerin analizinde standart bir araç olan moleküler dinamik simülasyonları, ayrıntılı ve doğru karakterizasyonlar sağlamakla birlikte bu simülasyonların hesaplama maliyeti yüksektir. İlgilenilen sistemleri anlamayı sağlamak için çeşitli daha verimli hesaplama yöntemleri geliştirilmiştir. Bu tez, proteinlerin pertürbasyon-tepki ve gürültü dinamiklerini analiz etmek için elektrik devresi analizinde kullanılan araçları adapte ederek ve yeniden kullanarak ve makine öğrenmesinde kullanılan boyut azaltma teknikleri aracılığı ile yüksek boyuttaki simülasyon verisinin altında yatan esas nitelikleri çıkartmaya imkan sağlayarak bu alandaki çalışmalara katkı sağlamaktadır. Proteinlerin ligandlarla etkileşimleri, sadece statik, zamanla değişmeyen işlemlerin aksine dinamik özellikleri ile belirlenir. Elektronik devre tasarımında yaygın olarak kullanılan bir frekans analiz tekniğinden esinlenerek, küçük ölçekli fonksiyonel protein hareketlerinin yanı sıra proteinlerin ligandlarla doğrudan etkileşimlerinin analizinde kullanılabilecek ProteinAC (PAC) adını verdiğimiz yeni bir frekans etki alanı hesaplama tekniği öneriyoruz. Bu teknik daha önce önerilen statik pertürbasyon-tepki yöntemlerinin pertürbasyon frekansının kilit rol oynadığı genelleştirilmesi olarak kabul edilebilir. Pertürbasyon frekansının protein dinamiklerinde önemli bir faktör olabileceğini gösteriyoruz. Ayrıca, tepki davranışını karakterize etmek için frekansa bağlı birkaç yeni metrik sunuyoruz. Alosteri-ligand bağlanması ile uzak fonksiyonel bölgelerin aktivitesinde değişiklikler gözlemlendiği fenomen-kavramsal olarak bir girişteki (bağlanma bölgesi) sinyalin (ligand) yayılarak çıkışa (uzak etkilenen bölge) ulaştığı ağlı bir iletişim ortamında noktadan noktaya telekomünikasyon olarak kavramsallaştırılabilir. Sinyalin uzak bölgelere kadar güvenilir bir şekilde iletimi, proteini etkileyen tüm bozan etkenlere (gürültü) rağmen gerçekleşir. Bu bakış açısına dayanarak, proteinin bir bölgesinde meydana gelen bağlanma bölgesine ligand uyarımına bağlı yer değiştirmeleri ve gürültüden kaynaklanan dalgalanmaları frekans etki alanında inceleyen hesaplamalı bir çerçeve öneriyoruz. Ligand varlığındaki yer değiştirmeleri ve yokluğundaki dalgalanmaları karakterize ediyoruz. İlk durumda, ligandın etkisi harici bir dinamik salınım kuvveti uyarımı olarak modellenirken, ikincisinde tek dalgalanma kaynağı, çevre ortamı ile etkileşimlerden kaynaklanan ve dahili protein ağı dinamikleri tarafından şekillendirilen gürültüdür. Uyarım frekansını sadece liganddan kaynaklanan yer değiştirmelerin sadece gürültü nedeniyle oluşana oranı olarak tanımladığımız Sinyal-Gürültü oranına (SNR) dayalı bir analizde anahtar bir faktör olarak tanıtıyoruz. Daha sonra yeni allosterik bölgeleri keşfetmek için bir yol sağlayacak SNR tabanlı karakterizasyonu genişleten bir bilgi teorik (iletişim) kanal kapasite analizi kullanıyoruz. Moleküler simülasyonlardan elde edilen muazzam miktarda veriden içgörü elde etmek, karşılık gelen düşük boyutlu serbest enerji manzaralarında, altta yatan sistemin temel özelliklerini koruyan az sayıda kolektif değişkenin tanımlanmasını gerektirir. Veriye dayalı teknikler, ilgili itici kuvvetlere dair kapsamlı sezgiye ihtiyaç duymadan bu manzarayı oluşturmak için sistematik bir yol sağlar. Özellikle, otokodlayıcılar, doğal olarak bir bilgi darboğazını ve dolayısıyla temel özelliklerin düşük boyutlu olarak gömülmesini zorladıkları için boyutsallığın azaltılmasında kullanılan güçlü araçlardır. Varyasyonel oto-kodlayıcılar, öncül olarak tek modlu bir Gauss varsayarak gömülmenin sürekliliğini sağlarken, bu, tipik olarak anlamlı kolektif değişkenlerin tanımlanmasından kaynaklanan çok havzalı serbest enerji manzaraları ile çelişmektedir. Bu çalışmada, gömülme içinde metastabil durumların ayrılmasını teşvik eden bir Gauss karışımı varyasyonel oto-kodlayıcısı (GMVAE) kullanarak bu fiziksel sezgiyi öncüle dahil ediyoruz. GMVAE, tek bir birleşik çerçeve içinde boyutsallık azaltma ve kümeleme işlemlerini gerçekleştirir ve verileri sınıflandırmak için gereken Gauss dağılımı sayısı bakımından girdi verilerinin doğal boyutsallığını belirleme yeteneğine sahiptir. Ortaya çıkan gösterimler ayrıca, boyut indirgemesinin statik denge özelliklerinden dinamiklere aktarılabilirliğini vurgulayarak Markov durum modellerinin oluşturulması için temsiller sağlar.","Molecular Dynamics simulations, the standard tool for analyzing biomolecules, provide detailed and accurate characterizations but at the expense of tremendous computational cost. A variety of more efficient computational methods have been developed in order to enable the understanding of practical systems of interest. This thesis contributes to this body of work by adapting and repurposing tools from electrical circuit analysis for analyzing the perturbation-response and noise dynamics of proteins, and by applying dimensionality reduction techniques from machine learning for identifying and extracting the essential features of biomolecules from large amounts of simulation data. The interactions of proteins with ligands are determined by their dynamic characteristics as opposed to only static, time-invariant processes. Inspired by a frequency domain analysis technique from electronic circuit design, we propose a novel computational technique that can be used to analyze small scale functional protein motions as well as interactions with ligands directly in the frequency domain. It can be considered as a generalization of previously proposed static perturbation-response methods, where the frequency of the perturbation becomes the key. We show that the frequency of the perturbation may be an important factor in protein dynamics. Furthermore, we introduce several novel frequency dependent metrics in order to characterize response behavior. Allostery-a phenomenon in which the binding of a ligand induces alterations in the activity of remote functional sites-can be conceptually viewed as point-to-point telecommunication in a networked communication medium, where a signal (ligand) arriving at the input (binding site) propagates through the network (interconnected and interacting atoms) to reach the output (remote functional site). The reliable transmission of the signal to distal points occurs despite all the disturbances (noise) affecting the protein. Based on this point of view, we propose a computational frequency-domain framework to characterize the displacements and the fluctuations in a region within the protein, originating from the ligand excitation at the binding site and noise, respectively. We characterize the displacements in the presence of the ligand, and the fluctuations in its absence. In the former case, the effect of the ligand is modeled as an external dynamic oscillatory force excitation, whereas in the latter, the sole source of fluctuations is the noise arising from the interactions with the surrounding medium that is further shaped by the internal protein network dynamics. We introduce the excitation frequency as a key factor in a Signal-to-Noise ratio (SNR) based analysis, where SNR is defined as the ratio of the displacements stemming from only the ligand to the fluctuations due to noise alone. We then employ an information-theoretic (communication) channel capacity analysis that extends the SNR based characterization by providing a route for discovering new allosteric regions. Extracting insight from the enormous quantity of data generated from molecular simulations requires the identification of a small number of collective variables whose corresponding low-dimensional free-energy landscape retains the essential features of the underlying system. Data-driven techniques provide a systematic route to constructing this landscape, without the need for extensive a priori intuition into the relevant driving forces. In particular, autoencoders are powerful tools for dimensionality reduction, as they naturally force an information bottleneck and, thereby, a low-dimensional embedding of the essential features. While variational autoencoders ensure continuity of the embedding by assuming a unimodal Gaussian prior, this is at odds with the multi-basin free-energy landscapes that typically arise from the identification of meaningful collective variables. In this work, we incorporate this physical intuition into the prior by employing a Gaussian mixture variational autoencoder (GMVAE), which encourages the separation of metastable states within the embedding. The GMVAE performs dimensionality reduction and clustering within a single unified framework, and is capable of identifying the inherent dimensionality of the input data, in terms of the number of Gaussians required to categorize the data. The resulting embeddings also provide representations for constructing Markov state models, highlighting the transferability of the dimensionality reduction from static equilibrium properties to dynamics."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"2 boyutlu imajlardan ve çizimlerden 3 boyutlu modeller üretmek, bilgisayar grafikleri alanında genişçe çalışılan önemli bir problemdir. Biz bu tezde, tek bir çizilmiş çubuk şeklinden 3 boyutlu bir insan modeli üreten ilk metotu tanımlıyoruz. Mevcut olan insan modelleme tekniklerine karşın bizim metotumuz, ne bir istatiksel vücut şekli modeline ne de bir giydirilmiş 3 boyutlu karakter modeline ihtiyaç duymaktadır. Biz bu çalışmada basit bir 2 boyutlu çubuk şekil çiziminden, bu çizimle uyumlu olan bir 3 boyutlu insan modeline dönüşüm kabiliyetine sahip yeni bir sistem geliştirmek için varyasyonel otokodlayıcılardan faydalanırız. Bizim makine öğrenmesi modelimiz, girdi çizim ile çıktı 3 boyutlu model arasındaki eşleştirmeyi öğrenmektedir. Bunun yanında modelimiz, bu modeller etrafında kurulan gömme uzayı da öğrenmektedir. Biz sistemimizin 3 boyutlu modeller üretmenin haricinde, bu gömme uzaydaki enterpolasyon ve ekstrapolasyonlar ile birlikte 3 boyutlu animasyonlar da üretebildiğini göstermekteyiz. 3 boyutlu insan modellerine ek olarak, sistemimizin genelleme yeteneğini göstermek için 3 boyutlu at modelleri de üretiriz. Kapsamlı deneyler, modelimizin mantıksal 3 boyutlu modeller ve animasyonlar ürettiğini göstermektedir.","Generating 3D models from 2D images or sketches is a widely studied important problem in computer graphics. We describe the first method to generate a 3D human model from a single sketched stick figure. In contrast to the existing human modeling techniques, our method requires neither a statistical body shape model nor a rigged 3D character model. We exploit Variational Autoencoders to develop a novel framework capable of transitioning from a simple 2D stick figure sketch, to a corresponding 3D human model. Our network learns the mapping between the input sketch and the output 3D model. Furthermore, our model learns the embedding space around these models. We demonstrate that our network can generate not only 3D models, but also 3D animations through interpolation and extrapolation in the learned embedding space. In addition to 3D human models, we produce 3D horse models in order to show the generalization ability of our framework. Extensive experiments show that our model learns to generate reasonable 3D models and animations."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kontrakt briç, veya sadece briç, her biri iki oyuncudan oluşan iki ortaklığının birbiriyle rekabet ettiği kusurlu bilgili bir kart oyunudur. Oyun iki aşamadan oluşur: deklarasyon ve kart oyunu. Bilgisayar oyuncuları yirmi yıl önce oyun aşamasında insan seviyesinde performanslara ulaşmış olsalar da, deklarasyon aşaması hala zorlayıcı bir problem. Bu aşamada oyuncular, bir açık arttırmaya katılırken yalnızca kendi kartlarının bilgisine sahipler. Burada iyi performans göstermek, oyuncuların aksiyonları ile nasıl iletişim kuracaklarını belirleyip ortak bir kontrakta karar verebilmeyi gerektirir. Bu iletişim tekliflerin sürekli artma zorunluluğu ile limitli olup, rakip ortaklık da kendi teklifleri ile iletişimin arasına karışabilmektedirler. Bu tezde, minimum öznitelik mühendisliğine sahip birkaç yeni mimari ile deney yapıyoruz ve bunları uzman düzeyindeki insan oyunlarından oluşan bir veri kümesi ile gözetimli eğitim kullanarak değerlendiriyoruz. Bundan sonra, elde edilen modeli gerçek oyun oynanışları ile iyileştirmek için farklı derin pekiştirmeli öğrenme biçimlerini inceliyoruz. Son olarak, rakipli briç oyuncuları için rekabet etmek için ayrı bir oyuncu gerektirmeyen bir değerlendirme ölçütü öneriyoruz.","The game of contract bridge, or just bridge, is a four-player imperfect information card game where two partnerships of two players compete against each other. It has two main phases: bidding and play. While the computer players have approached human-level performance two decades ago in the playing phase, bidding is still a very challenging problem. This makes bridge one of the last popular games where computers still lag behind the expert human-level performance. During bidding, players only know their own cards while participating in a public auction. Performing well in this phase requires the players to figure out how to communicate with their partners using the limited vocabulary of bids to decide on a joint contract. This communication is restricted by the strict ordering of legal bids and can be negatively interfered by bids made by the opponent partnership. In this thesis, we experiment with several novel architectures with minimal feature engineering and evaluate them by using supervised training over a data set of expert-level human games. After that, we further study different forms of deep reinforcement learning to refine the resulting model by simulated gameplay. Lastly, we propose an oracle evaluation metric that can measure the quality of any bidding sequence with respect to the game-theoretical optimum."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir robotun uygun şekilde yardım edebilmesi için, fiziksel işbirliği içeren görevlerde insan niyetinin kavranabilmesi önemlidir. Bu çalışmada, ortaklaşa yürütülen çoğu görevin sıralı alt görevlerden oluştuğunu ve insanın niyetinin yürütülecek olan bu alt görevlere denk geldiğini öne sürüyoruz. Bu alt görevler farklı gereksinimlere sahip olabilirler, dolayısıyla tek bir denetleyici, etkileşimi düzenlemek için en iyi seçenek olmayabilir. Bu alt görevler gerçek zamanlı olarak tanınabildiği zaman, uygun denetleyici veya denetleyici parametreleri seçilebilecektir. Biz, bu çalışmada, alt görevleri tanıma fikrini öneriyor, problemimizi bir zaman serisi sınıflandırması olarak formüle ediyor ve bunu başarmak için derin öğrenme yaklaşımını kullanıyoruz. Bu çalışma kapsamında üç farklı deney gerçekleştirdik. Birinci deneyde (n = 10 katılımcı), alt görevleri tanıma yaklaşımımızın uygulanabilir olduğunu ve böyle bir yaklaşımla elde edilen tahmin oranımızın %90'ın üzerinde olduğunu gördük. İkinci deney (n = 5 katılımcı) kapsamında yaklaşımımızı çok zorlu koşullarda test ettik ve yaklaşımımızın farklı kullanıcılara, insan kolu konfigürasyonlarına, denetleyici parametrelerine ve çevre katılığına karşı gürbüz olduğunu gösterdik. Üçüncü deneyde (n = 18 katılımcı) ise yaklaşımımızı gerçek zamanlı olarak test ettik. Sonuçlar, önerdiğimiz yaklaşım kullanıldığında görev performansının iyileştirilebile-ceğine dair deneysel bulgular sunmaktadır.","Understanding the human's intention in a physical collaborative task is important for a robot to provide the right type of assistance. We claim that most tasks are made of sequential subtasks and define the human intention as the current desired subtask to be executed. These subtasks have different requirements and a single controller will not be the best way to regulate the interaction. If these subtasks can be recognized in real time, an appropriate controller or controller parameters can be selected. In this work, we propose the idea of subtask recognition, formulate it as a time series classification problem, and develop a deep learning approach to accomplish it. We perform three experiments. The first one (n=10 subjects) verifies the viability of our subtask recognition approach, achieving above 90% test accuracies. The second one (n=5) tests our approach in multiple challenging conditions and shows that it is robust to different human operators, human arm configurations, controller parameters, and environment stiffnesses. The third experiment (n=18) tests our approach in real time. The results provide empirical evidence that the task performance is improved using the proposed approach."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yaptıǧım çalışmada, verilen cümlelerin dil bilgisi analizini, cümledeki her bir kelimenin yapısını ve cümle içindeki görevini tespit edebilecek bir sistem geliştirdim. Baǧlılık ayrıştırma adı verilen bu iş için geliştirilen sistem, temelde çizge tabanlı algoritmaları ve derin öǧrenme tekniklerini kullanmaktadır. Geçiş ve çizge tabanlı yöntemler ile yapılan baǧlılık ayrıştırma işleminde, kelime vektörlerine, kelime baǧlamlarını eklemenin etkisini gözlemledim. Ayrıca, SMeta adını verdiǧim çözücü yapısını, çizge tabanlı baǧlılık ayrıştırma yöntemleri ile test ettim. Dil bilgisi analizi için oluşturulan çizge tabanlı model, kodlayıcı ve çözücü olmak üzere iki ana bileşenden oluşmaktadır. Kodlayıcı, metinde yer alan cümleleri modelin işleyebileceǧi şekilde temsil edecek vektörleri oluşturur. çözücü kısım ise, her bir cümleyi temsil eden vektörleri kullanarak, cümlenin yapı ve dil bilgisi analizini gerçekleştirir ve bunun sonucu olarak cümle içerisinde yer alan kelimeler arasındaki baǧlantıları temsil eden baǧlılık çizgesini oluşturur. Koç üniversitesi takımı olarak oluşturduǧumuz model ile 2018 Evrensel Baǧlılık Analizi yarışmasında yer aldık ve modelimizi 41 farklı dilde oluşturulmuş 61 veri seti üzerinde çalıştırma imkanı yakaladık. Modelin oluşturulma aşamasında derin öǧrenme tekniklerinden ve doǧal dil işleme tekniklerinden, özellikle çizge tabanlı baǧlılık ayrıştırma yönteminden yararlandık.","I demonstrate the effect of contextual embeddings on transition and graph-based parsing methods and test our contribution, the structured meta biaffine decoder, using various graph-based parsing algorithms. As Koç University Graph-Based parsing team, we implemented a graph-based dependency parsing model in order to perform syntactic and semantic analysis of given sentences. Our neural graph-based parser consists of two main parts, which are encoder and decoder. The encoder forms continuous feature vectors from provided sentences for the neural graph-based parser to process the texts properly, whereas the decoder produces the parse tree from the output of the neural parser, by first producing a graph representation of the output. We participated in CoNLL 2018 Shared Task with the parsing model we created, and had the opportunity to run our model on 61 different data sets formed with texts in 41 different languages. We took advantage of natural language processing and deep learning techniques, including graph-based dependency parsing algorithms."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin Sinir Ağlarını (DNN'ler) ölçeklemek, birçok gerçek dünya uygulamasında doğruluklarını artırmak için çok önemli olmuştur. Bu nedenle, son teknoloji ürünü DNN'lerin çoğu derinleşiyor ve / veya genişliyor; daha büyük parametre setlerine ve / veya daha fazla katmana sahip oluyorlar. Ancak, bu modellerin eğitimi büyük miktarda bellek gerektirdiğinden bunun bir bedeli vardır. Parametreleri ve ara sonuçları depolamak için gereken bellek, genellikle eğitmekte kullanılan işleme öğeleri, yani Grafik İşleme Birimleri (GPU'lar) tarafından sunulan belleği aşmaktadır. Bu, bellek gerektiren bu modellerin, kullanılan işlem öğelerinin getirdiği kapasite kısıtlamaları altında verimli bir şekilde eğitilmesini sağlamak için algoritmalar ve teknikler aramayı gerektirir. Bu tez, veri akışı grafikleri olarak temsil edilen derin öğrenme modelleri için otomatik, genel ve müdahaleci olmayan bir bölümleme stratejisi olan ParDNN'ni önermektedir. Veri akışı grafiği gösterimine, evrenselliği nedeniyle ve TensorFlow, PyTorch ve MXNet gibi en popüler genel amaçlı derin öğrenme çerçeveleri tarafından benimsenmesinden dolayı bölümleme tekniğimizi veri akış grafiklerine dayandırdır. Stratejimizdeki temel algoritma, DNN'nin temelindeki veri akışı grafiğini bir dizi işleme öğesi arasında böler, böylece bellek kısıtlamaları karşılanır ve eğitim süresi en aza indirilir. Önerilen algoritma, bölümlenecek modelin derin öğrenme yönlerinden tamamen bağımsızdır, bu nedenle ortaya çıkan her tür modelle kullanılabilir. Dağıtılmış eğitim çerçevelerinin çoğunun aksine, ParDNN tamamen otomatiktir, kullanıcının model yapısı hakkında bilgi sahibi olmasını gerektirmez. Dahası, ne modelde ne de operasyon çekirdeklerinin sistem düzeyinde uygulanmasında herhangi bir değişiklik gerektirmez. Popüler bir DNN'nin veri akışı grafiği, birkaç bin ila yüz binlerce işlem içerir ve bu sayı hızlı bir artışa tabidir. Sonuç olarak, grafik işlemlerinin bir dizi işleme elemanı üzerinde verimli bir şekilde yerleştirilmesi, çok iyi kalitede bir bölümleme elde etmek ve düşük ek yüke sahip olmak arasında doğru dengeyi sağlayan bir algoritma gerektirir. Bu tezde önerilen strateji bu akılda tutularak tasarlanmıştır. Grafik bölümleme düşük ek yük algoritmalarından ve yüksek kaliteli statik programlama tekniklerinden kavramları içerir. Bu tezde, stratejimizin tasarımını ve deneysel sonuçlarını ortaya koyuyoruz. ParDNN kullanarak, derin öğrenmenin başlıca uygulama alanlarını temsil eden bir dizi model denedik. Deneylerimizde Tensorflow derin öğrenme çerçevesini kullanıyoruz. Deneylerimiz, ParDNN tarafından elde edilen bölümlemenin, birden çok cihaz üzerinde yüz binlerce işlem ve milyarlarca parametreye sahip DNN'lerin verimli eğitimine izin verdiğini göstermiştir. ParDNN son teknoloji alternatiflere göre daha iyi performans gösterir veya niteliksel bir avantaj sağlar. Dahası, ParDNN 'nun ihmal edilebilir bir ek yükü vardır, çalışma süresi haftalarca sürebilen eğitime kıyasla saniyeler ile birkaç dakika arasında değişir.","Scaling Deep Neural Networks (DNNs) has been crucial for enhancing their accuracy in many real-world applications. Hence, many state-of-the-art DNNs are becoming deeper and/or wider; having larger parameter sets and/or more layers. However, this comes at a cost, since training these models requires huge amounts of memory. The memory required to store their parameters and intermediate results exceeds what is offered by the processing elements usually used in training, i.e. Graphics Processing Units (GPUs). This necessitates seeking algorithms and techniques to enable efficient training of these memory-demanding models under the capacity constraints imposed by the underlying processing elements. This thesis proposes 'Parallelize DNN' (ParDNN), an automatic, generic, and non-intrusive partitioning strategy for deep learning models that are represented as dataflow graphs. Our partitioning relies on the dataflow graph representation of the DNN because this representation has been adopted by most of the popular general-purpose deep learning frameworks. The core algorithm in our strategy partitions DNN's underlying dataflow graph among a set of processing elements so that their memory constraints are met and the training time is minimized. The proposed algorithm is completely independent of the deep learning aspects of the model to be partitioned, so it can be used with any type of emerging models. Unlike most of the distributed training frameworks, ParDNN is completely automatic and it requires no knowledge about the model structure. Moreover, it requires no modification neither on the model nor at the implementation of its operation kernels. The dataflow graph of a widely-used DNN contains few thousands up to hundreds of thousands of operations, and this number is subject to rapid increase. As a result, finding an efficient placement of the graph operations on a set of processing elements requires an algorithm that strikes the right balance between obtaining a good quality partitioning and having low overhead. In this thesis, we explain the shortcomings of the existing heuristics to attain this balance, and how our strategy is designed to overcome these shortcomings. Our heuristics incorporate concepts from graph partitioning low-overhead algorithms and high-quality static scheduling techniques. In this thesis, we demonstrate the design choices and experimental results of our strategy. Using ParDNN, we experimented with a set of models representing the major application domains of deep learning. We used TensorFlow deep learning framework in our demonstration. Our experiments have shown that the partitioning obtained by ParDNN permitted efficient training of DNNs having hundreds of thousands of operations and billions of parameters on multiple GPU devices. In these experiments, ParDNN either outperforms or provides a qualitative advantage over state-of-the-art alternatives. Moreover, ParDNN has a negligible overhead, its running time ranges between seconds to few minutes compared to the training that may last for weeks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde OTT ve IPTV servisleri de dahil olmak üzere Internet üzerinden canlı video hizmetleri giderek daha popüler hale gelmektedir. Bu hizmetler, hiper metin aktarım protokolü (HTTP) uyarlamalı akış (HAS) kullanan kullanıcı-sunucu modeline dayanmaktadır. Bir sunucunun kullanabileceği eşzamanlı akış sayısında maksimum bir kısıt olduğundan, isteğe bağlı ölçeklenebilirlik sağlayan popüler çözümler arasında içerik dağıtım ağları (CDN) ve eşler arası (P2P) akış bulunmaktadır. Bugün OTT video hizmetleri, içeriği dağıtılmış uç noktalarda önbelleğe almak için bulut CDN çözümlerini kullanmaktadır. Bulut CDN'leri yalnızca kaynak sunucudaki yükü azaltmakla kalmayıp, aynı zamanda düşük gecikmeyle içerik dağıtımını hızlandırmaktadır. Ancak, CDN altyapısını barındırmak veya kiralamak pahalı olabilir. P2P ortamının ve veri kanallarının kullanımını kolaylaştıran WebRTC protokolündeki son gelişmeler, P2P video akışına olan ilginin artmasını sağlamıştır. Fakat, Internet üzerinden P2P video gönderiminin kontrolsüz doğası kritik bir problem olarak kalmaktadır. Bir yandan da, iki büyük endüstri trendi gözlemlenmektedir: i) bulut hizmeti sağlayıcıları düşük gecikme süresi ve daha yüksek bant genişliği erişimi için ağ kenarlarına doğru ilerlemektedir (sis bilişim olarak da bilinir), ii) ağ hizmeti sağlayıcıları (NSP) kenar erişim ağlarını yönetmek için kapalı ve özel donanım tabanlı erişim teknolojilerini kenar bulutlarda çalışan ayrıştırılmış ve sanallaştırılmış yazılım ile değiştirmektedir. Mevcut CDN ve P2P video çözümlerinin eksikliklerini ve SDN tabanlı bulut ve uçtan erişimli ağ teknolojilerinin potansiyelini kabul ederek, ağ servis sağlayıcıları tarafından işletilen kenar erişim veri merkezlerinde barındırılan hibrit bir P2P destekli CDN mimarisi öneriyoruz. Önerilen mimaride, NSP, her bir kenar ağ alanını yönetmek için SDN özellikli bir bulut platformu kullanmaktadır. Önerilen hibrit servis mimarisinin önemli bir özelliği, video servisi anahtar performans göstergelerini (KPI) optimize etmek için hem kullanıcılar tarafından CDN erişiminin hem de kullanıcılar arasında P2P video akışının, her bir SDN tabanlı uç ağ etki alanındaki ağ servis sağlayıcısı tarafından kontrol edilmesidir. Bu kontrol edilebilir P2P destekli kenar yönetimli video servisi, CDN sunucularındaki yükü azaltırken, akış başına deneyim kalitesi (QoE) dalgalanmalarının ve ayrılmış bir ağ dilimi üzerindeki birden fazla heterojen video çözünürlüğü kullanıcısı arasındaki adaletsizliğin üstesinden gelir. Bu hizmetin diğer avantajları şunlardır: i) daha iyi video kalitesi, düşük gecikme ve kullanıcılar için tamponlama olmaması; ii) çapraz ISS trafiğini en aza indirmesi; iii) P2P hizmetlerinin izinsiz kullanılmasından kaçınması. Deneysel sonuçlar, SDN destekli kenar ağ platformlarında çalıştırılan önerilen kontrol edilebilir P2P destekli CDN servisinin, KPI'mıza göre tek başına son teknoloji CDN veya P2P hizmetlerinden daha iyi performans gösterdiğini göstermektedir. Son olarak, SDN, WebRTC ve kenar bilişimini birleştiren P2P destekli servisleri kullanarak tüm bu sorunları gideren bir çözüm literatürde bulunmamaktadır.","Live video services over the Internet, including over-the-top (OTT) and IPTV services, are becoming ever more popular. These services are based on the client-server model using hyper-text transmission protocol (HTTP) adaptive streaming (HAS). Since there's a maximum number of concurrent streams that a server can handle, popular solutions to provide demand-scalability include content-distribution networks (CDN) and peer-to-peer (P2P) streaming. Today OTT video services employ cloud CDN solutions to cache content in distributed edge points of presence. Cloud CDNs not only ease the load on the origin server, but also accelerate content delivery with lower latency. However, hosting or renting CDN infrastructure can be costly. Recent developments in the WebRTC protocol that offers easy to use P2P media and data channels have led to renewed interest in P2P video streaming. However, the uncontrolled nature of P2P video delivery over the Internet remains as a critical problem. In the meantime, we observe two major industry trends: i) cloud service providers are moving towards network edges for lower latency and higher bandwidth access (also known as fog computing), ii) network service providers (NSP) are replacing closed and proprietary hardware-based access technologies with disaggregated and virtualized software running on edge clouds to manage their edge access networks. Recognizing the shortcomings of current CDN and P2P video solutions and the potential of SDN-based unified cloud and edge-access network technologies, we propose a hybrid P2P-assisted CDN architecture hosted at edge-access datacenters operated by network service providers. In the proposed architecture, the NSP employs an SDN-enabled cloud platform to manage each edge network domain. An important feature of the proposed hybrid service architecture is that both the CDN access by clients and P2P video streaming between clients are controlled by the network service provider within each SDN-based edge network domain to optimize video service key performance indicators (KPI). This controllable P2P-assisted edge-managed video service reduces the load on CDN servers while overcoming quality of experience (QoE) fluctuations per flow and unfairness between multiple heterogeneous video-resolution clients over a reserved network slice. Other advantages of this service include: i) better video quality, lower delay and no buffering for clients; ii) minimization of cross-ISP traffic; iii) avoiding illegal, unauthorized usage of P2P services. Experimental results show that the proposed managed P2P-assisted CDN service deployed at SDN-enabled network edge platforms performs better than the state of the art CDN or P2P services alone according to our KPI. Finally, there are no solutions to the best of our knowledge that address all of these problems in the literature using P2P-assisted services combining SDN, WebRTC and edge computing."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sınır bilişim, bulut bilişimin yüksek gecikmeli iletişim probleminin önüne geçebilmek için hesaplamayı veriye ve verileri kullanıcıya yakınlaştırır. Yerel veri depolama alan- larında depolama; otonom sürüş, makine görüşü, akıllı şebeke ve sanal gerçeklik gibi alanlarda gecikmeye duyarlı uygulamalar sağlayan yüksek hızlarla veri erişimine izin verir. Ancak, dağıtılmış hizmetler çoğunlukla bulut mimarisi için tasarlanmıştır. Ayrıca, etkin bir sınır özellikli depolama sistemi oluşturmak, kenarın dağıtılmış ve heterojen yapısı ve sınırlı kaynakları nedeniyle zorlayıcıdır. Bu tezde, ağ kenarı için tasarlanmış merkezi olmayan bir depolama sistemi olan EdgeKV'yi öneriyoruz. EdgeKV herhangi bir merkezi yönetim gerektirmez, ayrıca lokal altyapı kaynakları ve kullanıcı sayısı ile ölçeklenebilir. Buna ek olarak, EdgeKV, güçlü tutarlılık garantileri sağlarken veri çoğaltmayı kullanarak arızalara karşı hızlı ve güvenilir depolama sunar. EdgeKV, konum içermeyen ve arayüz tabanlı tasarımı sayesinde, heterojen kenar düğümleri sistemi ve değişen kaynaklar ile ölçeklenebilir özelliktedir. Hataya dayanıklı depolama sağlayan RSM (Çoğaltılmış Durum Makineleri) ile adil yük dengelemesi yapan DHT (Dağıtılmış Komut Çizelgesini) yapısını birleştirerek farklı kullanım durumları için yapılandırılabilen coğrafi dağıtılmış bir sistem elde edilmesini sağlar. Gecikmeye duyarlı uygulamaların tipik olarak iki veri türü vardır: yalnızca birbirine yakın az sayıda kullanıcıyla ilgili olan ve sıkı gecikme gereksinimleri gerektiren yerel veriler, sistemdeki tüm kullanıcılara açık olması gereken ve gecik- melere daha yüksek tolerans gösterebilen küresel veriler. EdgeKV gecikme gereksin- imlerindeki bu değişimi göz önünde bulundurur ve yerel sınır gruplarında depolanan yerel veriler için düşük gecikmeli erişim sağlar, ayrıca farklı konumlardaki sınır gru- plarına adil bir şekilde dağıtılan küresel veriler için de sistem genelinde kabul edilebilir bir başarımla erişim sağlamaya çalışır. Tez kapsamında, Golang ile EdgeKV modüllerinin sistem prototipi geliştirilmiş ve Grid'5000 dağıtık ortamında gerçekleştirilen deneylerle kapsamlı EdgeKV başarım analizi ve bulut depolama ile karşılaştırması yapılmıştır. Sistem performansını gerçekçi iş yükleri altında analiz etmek için YCSB (Yahoo! Cloud Serving Benchmark) kul- lanılmıştır. Analiz sonuçları EdgeKV'nin, aynı kurulum koşullarında hem yerel hem de küresel veri erişimiyle yapılan bulut depolama ayarından, ortalama yanıt süresinde %26, verimlilikte ise %19 daha iyi başarım göstermektedir. Ayrıca, EdgeKV'nin başarımdan ödün vermeden kullanıcı sayısıyla ölçeklenebileceğini göstermektedir. Son olarak, merkezi bir bulut yerine, EdgeKV ile yerel kaynakların kullanılması duru- munda enerji verimliliğini iyileştirmesini tartışıyoruz.","Edge computing moves the computation closer to the data and the data closer to the user to overcome the high latency communication of cloud computing. Storage at the edge allows data access with high speeds that enable latency-sensitive appli- cations in areas such as autonomous driving, machine vision, smart grid, and virtual reality. However, several distributed services are typically designed for the cloud. In addition, building an efficient edge-enabled storage system is challenging because of the distributed and heterogeneous nature of the edge and its limited resources. In this thesis, we propose EdgeKV, a decentralized storage system designed for the network edge. EdgeKV does not require any central management and can scale with the edge infrastructure resources and the number of users. Besides, EdgeKV offers fast and reliable storage against failures, utilizing data replication while provid- ing strong consistency guarantees. With a location-transparent and interface-based design, EdgeKV can scale with a heterogeneous system of edge nodes and varying resource capabilities. Combining Replicated State Machines (RSM) that provide consistent fault-tolerant storage with a Distributed Hash Table (DHT) that allows fair load balancing results in a geo-distributed system that can be easily configured to serve different use cases. Latency-sensitive applications typically have two types of data: local data that is relevant only to a small number of users close to each other and requires strict latency requirements, and global data that should be avail- able to all users in the system and can tolerate higher latencies. EdgeKV considers this variation in latency requirements and provides low-latency access to local data, stored in local edge groups, and availability throughout the system to global data, fairly distributed over edge groups in different locations, with acceptable performance. We implement a prototype of the EdgeKV modules in Golang and evaluate it in both the edge and cloud settings with an emulated setup on the Grid'5000 testbed. We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to analyze the system's performance under realistic workloads. Our evaluation results show that EdgeKV outperforms the cloud storage setting with both local and global data access with an average write response time and throughput improvements of 26% and 19% respec- tively under the same settings. Our evaluations also show that EdgeKV can scale with the number of clients, without sacrificing performance. Finally, we discuss the energy efficiency improvement when utilizing edge resources with EdgeKV instead of a centralized cloud."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Biyolojik süreçler, protein-protein arayüzleri aracılığıyla meydana gelen protein etkileşimlerine bağlıdır. Protein-protein arayüzlerinin kolay ve güvenilir bir şekilde belirlenebilmesi, protein bağlanma mekanizmalarını anlamak ve yeni etkileşimleri tahmin etmek için çok önemlidir. Bir arayüz çıkarabilmek için, protein-protein kompleks yapısına ihtiyaç vardır. Tek bir protein kompleksinin yapısını belirlemek bile deneysel olarak uzun bir zaman ve büyük bir çaba gerektirdiğinden; hesaplamalı yöntemlerin binlerce olası protein kompleksi yapısını tahmin edebileceğini görmek umut vericidir. Fakat, hesaplamalı yöntemlerde karşımıza çıkan en büyük zorluk, yöntem tarafından önerilen binlerce protein kompleksi arasından biyolojik olanlarını ayırt edebilmektir. Günümüze kadar, biyolojik protein modellerini belirlemek için, dekoyları puanlayan ve sıralayan birçok yöntem geliştirilmiştir. Bu tezde, DeepInterface adı verilen üç boyutlu evrişimli sinir ağı tabanlı dekoy puanlama ve sıralama yaklaşımını geliştirdik. En iyi performans gösteren DeepInterface modelini bulmak için birden çok evrişimli sinir ağı mimarisini karşılaştırdık ve hiperparametre alanlarını taradık. Sonuç olarak, VGG16'nın mimarisine benzeyen yeni bir model ortaya çıkarttık. Protein Veri Bankasında (PDB) depolanan komplekslerden pozitif veri kümelerini ve PPI4DOCK ve DOCKGROUND veritabanlarında depolanan yanlış dekoylardan negatif veri kümelerini oluşturduk. Önerdiğimiz modelin, PDB'de depolananlara benzer pozitif protein arayüzlerini, yaklaşık %81 doğrulukla, CAPRI kriterlerine göre yanlış negatif protein arayüzlerinden ayırt edebildiğini gösterdik. Modellerimizi ZDOCK benchmark 4.0 üzerinden IRAD ve DOVE gibi diğer dekoyların puanlama / sıralama araçlarıyla da karşılaştırdık ve modellerimizin bu araçlarla yarışabilir nitelikte olduğunu gösterdik. Kullandığımız bu yöntem, protein-protein etkileşimlerini tahmin eden programların hesaplama maliyetini azaltmak için kullanılabilir.","Biological processes depend on protein-protein interactions that occur through protein-protein interfaces. Identification of protein-protein interface regions is crucial to understand mechanisms of protein binding and predict new interactions. Therefore, it is critical to be able to determine protein-protein interfaces easily and reliably. To extract a protein interface, protein-protein complex structure is needed. Since even determining a single protein complex structure experimentally takes time and effort; it is promising to see that computational docking tools can provide thousands of possible complex structures, called decoys, in a short time. A challenge that comes with computational methods is to be able to discriminate near-native decoys among thousands proposed. Multiple methods have been developed for scoring and ranking the decoys to identify the biologically relevant ones that can be used in further biological research. In this thesis, we improved a three-dimensional convolutional neural network-based decoys` scoring and ranking approach called DeepInterface. We compared multiple convolutional neural network architectures and searched the hyperparameters` space to find the best performing DeepInterface model that resulted in having the VGG16 resembling architecture. We built positive datasets from the complexes stored in the Protein Data Bank, and negative datasets from the incorrect decoys stored in the PPI4DOCK and DOCKGROUND docking databases. We showed that the model we suggested can discriminate positive interfaces, similar to ones stored in the PDB, from the incorrect ones, according to the CAPRI criteria, with approximately 81% accuracy. We also compared our models with other decoys` scoring/ranking tools including IRAD and DOVE using ZDOCK docking benchmark 4.0 decoys and showed that our models are competitive. This method could be used to reduce the computational cost of the protein-protein interactions` predictions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapısal protein-protein etkileşimi ağlarındaki yaygın uygulama, her protein için sadece bir spesifik konformasyonu araştırmaktır. Ancak, bu yöntem farklı protein etkileşimlerine, fonksiyonlarına ve aşağı akım sinyallerine yol açabilecek yapısal değişiklikleri ihmal ettiği için kapsamlı bir temsil değildir. Bu tezde, proteinlerin alternatif konformasyonlarını inceleyen yapısal protein etkileşim ağları için yeni bir gösterim önerilmiştir. Bu gösterim, proteinlerin tüm kullanılabilir yapılarını protein veri bankasından elde etmek için bir yöntem kullanır ve bunları sekanslarına ve yapısal benzerliklerine göre kümeler. Daha sonra, her proteinin alternatif konformasyonlarını araştırır. Meme kanseri, akciğer ve beyin metastazı alt ağları oluşturularak ve proteinlerin alternatif konformasyonları ile donatılarak büyük ölçekli bir çalışma yapıldı. Protein-protein etkileşim ağı analizleri, her alt ağda önemli roller oynayan yeni genler ve hücresel yolaklar gösterdi. Proteinlerin alternatif konformasyonlarını inceleyerek kenetlenme sonuçları kapsamı %54'ten %76'ya yükseldi. Ek olarak, konformasyonel değişikliklerin spesifik etkileşimler üzerindeki etkileri gösterildi. KPNB1'in yapısal değişiklikleri, onu açık ve kapalı konformasyonlarda sırasıyla SNAI1 ve SNUPN'ye bağlanmaya yönlendirir. CXCL12'nin alternatif konformasyonlarının araştırılması, bağlanma yüzeyi üzerindeki bir nokta mutasyonunun karmaşık bir yapı olarak CXCL12 homodimerizasyonunu inhibe edebileceğini ve işlevlerini değiştirebileceğini göstermektedir. PRISM, alt ağlarda protein yerleştirme amacıyla kullanıldı. PRISM'in temel kısıtlamaları uzun çalışma süresi ve sınırlı arayüz veritabanıdır. Bu sınırlamalar esas olarak çok zaman alan protein yapısı karşılaştırma işleminden kaynaklanmaktadır. Bu nedenle, bu süreci hızlandırmak için, protein yüzeyi ve arayüz yapılarının 1D tanımlayıcılarını oluşturmak için yeni bir yöntem önerilmektedir. Bu yöntem, protein tanımlayıcı vektörleri oluşturmak için fazlalık içermeyen küçük protein parçalarının bir kütüphanesini ve Geometrik karma tekniğini kullanır. Sonuçlara dayanarak, bu tanımlayıcı vektörler kullanılarak binlerce arayüz karşılaştırması saniyeler içinde yapılabilir. Çok benzer olmayan arayüzlerin %70-%80'i hızla filtrelenebilir ve diğer aday benzer arayüzler bir hizalama yöntemi ile karşılaştırılabilir.","The common practice in structural protein-protein interaction (PPI) networks is to investigate just one specific conformation for each protein. Yet it is not a comprehensive representation as it neglects the conformational changes of proteins which may lead to different protein interactions, functions, and downstream signaling. In this dissertation, a new representation is proposed for structural PPI networks which inspects the alternative conformations of proteins. This representation uses a method to get all available structures of proteins from protein data bank (PDB) and clusters them based on their sequence and structural similarities. Then, it investigates the alternative conformations of each protein. A large-scale study is done by creating breast cancer lung and brain metastasis sub-networks and equipping them with alternative conformations of the proteins. PPI network analyses showed novel genes and cellular pathways which play important roles in each sub-network. By examining alternative conformations of proteins, the docking results coverage increased from 54% to 76%. In addition, the effects of conformational changes on specific interactions are shown. The conformational changes of KPNB1 directs it to bind to SNAI1 and SNUPN in the open and close conformations, respectively. Exploring the alternative conformations of CXCL12 shows that a point mutation on the binding surface can inhibit CXCL12 homodimerization, as a complex structure, and alter its functions. PRISM is used for protein docking purposes in the sub-networks. The primary limitations of PRISM are its long running time and restricted interface database. These limitations derive mainly from the all-to-all protein surface and interface structure comparison process which is very time-consuming. So, to speed up this process, a new filtering method is proposed which creates 1D descriptors of protein structures to rapidly filter the dissimilar interface. This method uses a library of non-redundant small protein fragments and Geometric hashing technique to create the protein descriptor vectors. Based on the results, thousands of interface comparisons could be done in seconds using these descriptor vectors. 70%-80% of grossly dissimilar interfaces could be filtered rapidly and the remaining candidate similar interfaces would be compared by an alignment method."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sınır bilişim, verilerin kaynağına daha yakın bir şekilde işlenmesini sağladığı için birçok sektörde veri işleme sürecini değiştirmektedir. Farklı endüstriler için çeşitli uç hesaplama mimarileri mevcut olsa da, bildiğimiz kadarıyla, petrol rafinerileri ve özellikle alarm yönetim sistemleri için uç hesaplamanın kapsamlı ve pratik kullanımı mevcut değildir. Alarm yönetim sistemi, çeşitli işlemleri güvenli bir şekilde yürütmek için her endüstrinin ayrılmaz bir parçasıdır. Etkisiz bir alarm sistemi, operatörün iş yükünü artırabilir ve bu da felaket olaylarına yol açabilir. Alarm seli, anlık alarmlar ve rahatsız edici alarmlar gibi faktörler operatörün üretkenliğini engeller ve dolayısıyla iş yükünü artırır. Bu tez çalışmasında, bir petrol rafinerisinin mevcut altyapısını değiştirmeden, EdgeAlarm adlı uç hesaplama modeline dayanan verimli bir gerçek zamanlı alarm yönetim sistemi öneriyoruz. Önerilen EdgeAlarm sistemi üç modülden oluşur: Depolama Azaltma (SR) modülü, Operatör İş Yükü Azaltma (OWR) modülü ve Yapay Zeka asistanı (AI) modülü. SR modülü, uçtaki anlık alarmları filtrelemekten sorumludur ve yalnızca yararlı alarmları bulut birimine (Historian sunucusu) gönderir. OWR modülünde, operatör iş yükünü azaltmak için gerçek ve rahatsız edici alarmları sınıflandıran iki yeni teknik öneriyoruz. AI modulü, operatöre gerçek zamanlı olarak yardımcı olmak için gelecekteki alarmları tahmin etmek için bir Tekrarlayan Sinir Ağından (RNN) oluşur. Önerilen EdgeAlarm sistemini Tüpraş petrol rafinerisi tesisi alarm yönetim sisteminin 13 aylık alarm veri setini kullanarak değerlendirdik. Bulgularımız, SR modülünün Historian sunucusunun depolama kullanımında yaklaşık %10 oranında azalma sağladığını göstermektedir. OWR modülü, doğru ve rahatsız edici alarmları bularak operatörün iş yükünü yaklaşık %50 oranında azaltmaktadır. AI modülü ise, operatörün yaklaşan alarmların çoğunu RNN yardımıyla uçta gerçek zamanlı olarak alarm kaynağı başına %55 doğrulukla tahmin ederek herhangi bir anormal durumdan önceden kaçınmasına veya buna hazırlanmasına yardımcı olmaktadır.","Edge computing is changing the course of data processing in many industries as it enables the processing of data closer to its origin. Although various edge computing architectures exist for different industries, to the best of our knowledge, there is no extensive and practical utilization of edge computing for oil refineries and specifically for alarm management systems. Alarm management system is an integral part of every industry to carry out various operations safely. An ineffective alarm system can increase the operator workload, which can lead to catastrophic events. The factors such as alarm flooding, momentary alarms, and nuisance alarms impede the operator productivity and hence increase his/her workload. In this thesis, we propose an efficient real-time alarm management system based on the edge computing paradigm, namely EdgeAlarm, without altering the existing infrastructure of an oil refinery. The proposed EdgeAlarm system consists of three modules: Storage Reduction (SR) module, Operator Workload Reduction (OWR) module, and an Artificial Intelligence assistant (AI) module. The SR module is responsible for filtering the momentary alarms at the edge and only sends useful alarms to the cloud (Historian server). In the OWR module, we propose two novel techniques to reduce the operator workload by classifying the true and nuisance alarms. The AI module consists of a Recurrent Neural Network (RNN) to predict the future alarms to assist the operator in real-time. We evaluated the proposed EdgeAlarm system on the 13 months alarm dataset of an alarm management system of T¨upra¸s oil refinery plant. Our findings indicate that the SR module achieves a reduction in the storage utilization of the Historian server by approximately 10%. The OWR module reduces the operator workload approximately by 50% by finding the true and nuisance alarms. Finally, the AI module assists the operator to avoid or prepare for any abnormal situation in advance by predicting the majority of the upcoming alarms with the help of RNN with an accuracy of 55% per alarm source at the edge in real-time and some alarm sources are predicted with more than 90% accuracy."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Fosil enerjinin t¨ukenmesi ve artan ¸cevre sorunları ile enerji piyasaları, r¨uzgar ve g¨une¸s gibi yenilenebilir enerji kaynaklarına y¨onelmi¸stir. Burada ama¸c, ¨uretim ve talep arasındaki bo¸slu˘gu kapatmaktır. Bu kaynaklar k¨u¸c¨uk ¨ol¸cekte toplanabildi˘ginden, artan sayıda ¨uretici artık enerjiyi satabilir hale gelmektedir. Bu, t¨uketicilerin iki rol ¨ustlenmesini te¸svik ederek (hem ¨uretip hem de fazla enerji satabilen) g¨orevde¸s (P2P) enerji ticareti ve payla¸sımı kavramını ortaya ¸cıkarmı¸stır. G¨un¨um¨uzde akıllı ¸sehir teknolojilerinin geli¸smesiyle birlikte enerji ticareti ve payla¸sımı kavramları hayal olmaktan ¸cıkmaktadır. Bununla birlikte, bu enerji kaynaklarının heterojenli˘gi (enerji ¨uretimi ve ba˘gımlılıktaki farklılık) ve d¨u¸s¨uk derecede g¨uvenilirlik nedeniyle, bu enerji kaynaklarını verimli bir ¸sekilde y¨onetme g¨orevi zorla¸smaktadır. Geli¸stirilen enerji ticaret sistemleri ve y¨onetim ara¸cları ¸co˘gunlukla merkezi bulut sistemlerini kullanır ve y¨uksek gecikme, d¨u¸s¨uk ¨ol¸ceklenebilirlik ve g¨uvenlik sorunları gibi do˘gal sorunlarla kar¸sı kar¸sıya kalırlar. Dolayısıyla, geli¸sen teknolojiyi kullanarak t¨uketici verilerini koruyabilen, de˘gi¸siklikleri denetim amacıyla g¨unl¨u˘ge kaydedebilen ve sistemin ba¸sarımını iyile¸stiren g¨u¸cl¨u bir araca ihtiya¸c vardır. Bu ba˘glamda da˘gıtık bir defter olarak blokzincir teknolojisi, merkezi sistemlere alternatif olarak ortaya ¸cıkmı¸stır. Bu tezde, son teknoloji P2P enerji ticareti ve payla¸sım ¸c¨oz¨umleri, avantaj, dezavantaj ve a¸cık problemleri ara¸stırılarak Altyapı Tabanlı Enerji Ticareti, B¨uy¨uk Ol¸cekli ¨ Enerji Depolamasına Dayalı Ticaret ve Tasarsız P2P Enerji Ticareti grupları ¨onerilmi¸stir. ˙Incelenen a¸cık konular g¨oz ¨on¨unde bulundurularak, iki ana mod¨ulden olu¸san, blokzincir destekli P2P enerji ticareti ve t¨uketici y¨onetimi ¸cer¸cevesi SynergyGrids geli¸stirilmi¸stir. Sistemin ilk mod¨ul¨u olarak, P2P enerji ticareti ba˘glamında prosumer gruplama mekanizmasının ¨ol¸ceklenebilirli˘gini ve ademi merkeziyetini geli¸stirmek i¸cin SynergyChain adlı da˘gıtık bir gruplama mekanizması ¨onerilmi¸stir. Akıllı s¨ozle¸smeler, i¸slem bilgilerini depolamak ve m¨u¸steri gruplarının olu¸sturulması i¸cin kullanılmı¸stır. SynergyChain , genel sistem ba¸sarımını ve karlılı˘gını arttırmak i¸cin uyarlanabilir bir gruplama tekni˘gi olu¸sturmak i¸cin bir peki¸stirmeli ¨o˘grenme mod¨ul¨un¨u i¸cermektedir. Onerilen ¨ SynergyChain modeli, kullanılabilir enerjinin arama alanını n'den (¨uretici sayısı) k'ye (olu¸sturulan grup sayısı) d¨u¸s¨urd¨u˘g¨u i¸cin her enerji talebi i¸cin zaman karma¸sıklı˘gını iyile¸stirmektedir. ˙Ikinci mod¨ul olarak, enerji ticaretine ve payla¸sımına izin vermek i¸cin mikro ¸sebekelerden ¨uretim yapanları y¨oneten FederatedGrids adlı karma bir sistem ¨onerilmektedir. Bu sistem, t¨uketicilerin yerel kaynaklardan enerji satın almasına ve ana ¸sebekelere olan ba˘gımlılı˘gı azaltmasına olanak tanır. FederatedGrids t¨uketicilerin enerji talebini tahmin etmek i¸cin kullanılan da˘gıtık blokzincirtabanlı birle¸sik bir ¨o˘grenme modeli ¨onermektedir. Onerilen ¨ SynergyGrids sistem mod¨ulleri Python ve Solidity kullanılarak geli¸stirilmi¸s ve Ethereum test a˘gları kullanılarak analiz edilmi¸stir. Saatlik Enerji T¨uketimi veri setini kullanan kapsamlı deneyler ger¸cekle¸stirilmi¸stir. SynergyChain , merkezi sisteme kıyasla sistemin ba¸sarımı ve ¨ol¸ceklenebilirli˘ginde %39,7 ve t¨uketicilerin karlılı˘gında % 18,3 iyile¸sme g¨ostermektedir. Talep y¨uk¨un¨u tahmin etmek i¸cin FederatedGrids kapsamında ¨onerilen blokzincir-tabanlı birle¸sik ¨o˘grenme modeli, ortalama %97,8 iyile¸stirilmi¸s a˘g y¨uk¨u ile kar¸sıla¸stırılabilir do˘gruluk sa˘glamakatadır. Literat¨urdeki di˘ger modellerle kar¸sıla¸stırıldı˘gında t¨uketiciler i¸cin enerji maliyetinde %17,8 ve ana ¸sebekelere g¨ore y¨ukte %76,4 oranında azalma oldu˘gunu g¨ostermektedir.","With depletion in fossil energy and increasing environmental problems, energy markets have shifted towards renewable energy resources, such as wind and solar. The ultimate aim is to bridge the gap between generation and demand. As these sources can be harvested on a small scale, increasing number of producers are becoming capable of selling surplus energy. This fostered the concept of peer-to- peer (P2P) energy trading and sharing, by encouraging consumers to become prosumers (i.e. capable of both producing and selling surplus energy). Nowadays, the concept of energy trading and sharing is no longer a dream due to the emergence of the smart city concept. However, due to the heterogeneity of these energy resources (i.e. difference in energy generation and dependency) and a low degree of reliability, the task of efficiently managing these energy resources becomes difficult. Various energy trading systems and management tools have been developed, however, they mostly use centralized cloud systems and face inherent problems of high latency, low scalability, and security problems. Hence, a strong tool that can protect the prosumer data using emerging technology, log the changes for audit purposes, and eventually improve the performance of the system is needed. In this context, blockchain technology as a distributed ledger has emerged as an alternative to centralized systems. In this thesis, state-of-the-art P2P energy trading and sharing solutions are investigated, highlighting their benefits, drawbacks and open issues by classifying them into sub-categories of Infrastructure-based Energy Trading, Large Scale Energy Storage Based Trading, and Ad hoc P2P Energy Trading. Considering the investigated open issues, a blockchain assisted P2P energy trading and prosumer management framework, namely SynergyGrids, that consists of two main modules is developed. As the first module of the framework, a decentralized grouping mechanism, named SynergyChain, is proposed for improving scalability and decentralization of the prosumer grouping mechanism in the context of P2P energy trading. Smart contracts are used for storing transaction information and for the creation of the prosumer groups. SynergyChain integrates a reinforcement learning module to create a self-adaptive grouping technique for improving the overall system performance and profitability. The proposed SynergyChain model improves the time complexity for each energy request as it reduces the search space of available energy from n (number of prosumers) to k (number of groups formed). As the second module, a hybrid trading system, named FederatedGrids, that manages both the prosumers from local microgrids and across the microgrids to allow energy trading and sharing is proposed. This allows consumers to buy energy from local resources and reduce reliance on utility grids. FederatedGrids proposes a decentralized blockchain-based federated learning model that is used to predict the energy demand of the consumers. The proposed SynergyGrids modules are developed using Python and Solidity and have been analyzed using Ethereum test nets. The comprehensive experimental analysis using the Hourly Energy Consumption data-set has been conducted. SynergyChain shows a 39.7% improvement in the performance and scalability of the system as compared to the centralized system, and 18.3% improvement in the profitability of the prosumers. The blockchain-based federated learning to predict the demand load proposed in FederatedGrids shows comparable accuracy with an average 97.8% improved network load. The energy trading and sharing results show a 17.8% decrease in energy cost for consumers and a 76.4% decrease in load over utility grids when compared to other models from the literature."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"This dissertation aims to present a multitouch interaction surface to enable a more natural human-computer interaction. Within the last decade, touch screens largely replaced indirect input devices, and became widely integrated in our daily tasks. As user interactions moved from mechanical controls to touch controls, the performance and accuracy of users dropped due to the lack of tactile feedback. In mechanical controls, users continuously receive tactile information according to the stage of their interaction. Therefore, we suggest that adding haptic modality suitable for the interaction to current touch systems could potentially improve task performance. Touch screens currently only use visual modality to provide information to users. In this case, we suggest that haptic systems could be used to provide additional information to users, and help to relieve visual information overload. We explored how we can augment current touchscreen based systems using haptic modality without compromising direct manipulation. For this purpose, we designed and manufactured HapTable -a large multimodal interactive tabletop. HapTable allows users to interact with digital content through natural touch gestures, and to receive visual and haptic feedback according to their interaction. HapTable first recognizes the gesture performed by the user. To provide suitable haptic feedback, it combines two different haptic technologies -piezoelectric actuator-based vibration in normal direction and electrostatic-based frictional vibration in tangential direction. We explored the first actuation technology to provide additional information on touch surfaces, such as a directional wind flow. Our findings indicate that users successfully recognized the direction of flow between two fingers, as well as under their palm. The latter actuation technology is used to provide tactility while turning a visual knob. Although the quantitative metrics did not show improvement in task performance with haptic feedback, the qualitative metrics showed that users strongly preferred knobs with haptic feedback over the purely visual knob.","Bu tez, insan-bilgisayar etkileşiminin daha doğal hâle getirebilecek geniş ve çok dokunuşlu bir etkileşim yüzeyi sunmayı hedeflemektedir. Son on yıl içinde dokunmatik arayüzler, klavye ve fare gibi dolaylı etkileşim araçlarının yerini almaya başlamış ve kişilerin günlük etkileşimlerinde yaygınca kullandığı bir teknoloji olmuştur. Fakat, fiziksel etkileşim araçlarından uzaklaşarak sadece dokunmatik araçlarla gerçekleştirdiğimiz insan-bilgisayar etkileşimleri, bu araçlarda dokunsal geri bildirim olmaması sebebiyle kişilerin görevi tamamlama başarılarının ciddi derecede düşmesine sebep olmuştur. Kişiler, fiziksel etkileşim araçlarını kullanırken, yaptıkları el hareketlerine ve etkileşim aracının o anki duruma göre sürekli olarak dokunsal geri bildirim alırlar ve gelecek hareketlerini bu bildireme göre ayarlarlar. Bu nedenle biz, dokunmatik ekranlara eklenilebilecek dokunsal geri bildirimlerle kişilerin görevi tamamlama başarılarının artabileceğini düşünüyoruz. Bunun yanı sıra şu anda, dokunmatik ekranlar sadece görsel kanaldan bilgi verebilmektedir. Bu sebeple eklenecek dokunsal sistem, kullanıcıya ek bilgi sunabilmek ve böylece görsel bilgi yüklenmesini hafifletmek için kullanılabilir. Bu tezde, dokunsal yöntemler ile günümüzdeki dokunmatik ekranları nasıl geliştirebileceğimizi araştırdık. Bu sebeple, HapTable adını verdiğimiz geniş ve birden fazla duyuya hitap eden dokunmatik bir masa geliştirdik. HapTable, kullanıcıların doğal el hareketlerini kullanarak sanal içerik ile etkileşim haline girmesini sağlamakta, ve bu etkileşim sırasında uygun görsel ve dokunsal geri bildirimleri kullanıcıya sunmaktadır. HapTable, piezo elektrik motorlar ve elektrostatik-bazlı titreşim yöntemlerini birleştirerek hem normal yönde, hem de teğetsel yönde dokunsal geribildirim sağlamaktadır. Bu tez, ilk yöntemi kullanarak yüzeyde, rüzgârın yönü gibi ek bilgi sunmayı araştırmıştır. Araştırmalarımızın sonucunda, kişilerin hem iki parmak arasında, hem de ellerinin altında bir akıntının yönünü başarıyla tanıdığını bulduk. İkinci dokunsal teknolojiyi ise görsel bir düğmeye dokunsal geribildirim eklemek için kullandık. Nicel sonuçlarda düğmenin çevrilmesindeki görev başarısı artmasa da, nitel sonuçlarda kullanıcıların, dokunsal bildirimi olan düğmeleri, dokunsal bildirimi olmayana göre daha fazla tercih ettiklerini gözlemledik."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son zamanlarda, uçtan uca modeller, doğal dil çıkarımı (NLI) veri kümelerinde insan seviyesine yakın bir performans sergilemiştir. Bununla birlikte, eğitim veri setlerindeki meyiller nedeniyle sığ sezgisellikler öğrenme eğiliminde oldukları için dağıtım dışı değerlendirme setlerinde düşük genelleme gösteriyorlar. Bir araya getirilebilirlik veya basit sezgiselliklere karşı dayanıklılığı ölçen tanı kümelerinde performans önemli ölçüde düşmektedir. Bu soruna yönelik mevcut çözümlerde, eğitim veri kümesini, değerlendirilen çekişmeli kategorilerden örneklerle genişleterek veri kümesi genişletmesi kullanılmaktadır. Fakat, bu yaklaşımın sadece sınırlı bir dizi çekişmeli sınıf için geçerli olmasının yanı sıra, en kötü ihtimalde genişletme setinde yer almayan diğer çekişmeli örnekler üzerindeki performansı zedelemesi gibi dezavantajları bulunmaktadır. Bunun yerine önerdiğimiz çözüm, açık olarak anlamsallığın ortak öğrenimi ile cümle anlayışını (dolayısıyla dağıtım dışı genellemeyi) geliştirmektir. Bu tezde, İngilizce anlamsal rol etiketleme (SRL) ve NLI ile ortaklaşa eğitilen BERT tabanlı bir modelin, genelleme performansını ölçen dış değerlendirme setlerinde önemli ölçüde daha yüksek performans elde ettiğini gösteriyoruz.","Recently, end-to-end models have achieved near-human performance on natural language inference (NLI) datasets. However, they show low generalization on out-ofdistribution evaluation sets since they tend to learn shallow heuristics due to the biases in the training datasets. The performance decreases dramatically on diagnostic sets measuring compositionality or robustness against simple heuristics. Existing solutions for this problem employ dataset augmentation by extending the training dataset with examples from the evaluated adversarial categories. However, that approach has the drawbacks of being applicable to only a limited set of adversaries and at worst hurting the model performance on other adversaries not included in the augmentation set. Instead, our proposed solution is to improve sentence understanding (hence out-of-distribution generalization) with joint learning of explicit semantics. In this thesis, we show that a BERT based model trained jointly on English semantic role labeling (SRL) and NLI achieves signiﬁcantly higher performance on external evaluation sets measuring generalization performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kör denkleştirmede dışbükey bir maliyet fonksiyonu olarak l∞ normun gücü, çıktının l∞ -normunu minimize etmenin, kombine kanal-denkleştirici dürtü yanıtı icin seyrek bir çözüm seçmesinden kaynaklanır. Şıkıştırılmış Öğrenme Uyarlamalı Den- kleştirme yaklaşımı, l∞ normu ve kare maliyet fonksiyonlarını kullanarak, sıkıştırılmış algılama yaklaşımını ISI kanaldenkleştirme problemine bağlar. [Yilmaz and Erdogan, 2017], Masif MIMO sistemleri icin, öğrenme sembollerinin log2(K) civarında sıkıştırılabileceğini kanıtlar, ki burada K kullanıcı sayısıdır. Bu tezde, Sıkıştırılmış Öğrenme Tabanli Masif MIMO için hızlı ve düşük karmaşıklıklı çözümler incelenmiştir. Bu amacla, l∞ -normun proksimal operatörünün dogrudan türetilmesi sunuldu ve kırpma özelligi gösterildi. Daha sonra, l∞ -norm ile duzenlenmiş kare maliyet fonksiyonlu optimizasyon problemleri için proksimal algoritmalar araştırılır. Spesifik olarak, Sıkıştırılmış Öğrenme Tabanli Masif MIMO Gürültüsüz Optimizasyon Problemi icin ADMM yontemi sunulur. Ayrica, Sıkıştırılmış Ögrenme Tabanli Masif MIMO Gürültülü Ayar icin hızlandırılmış bir projeksiyon tabanlı algoritma onerilmistir. Ileriki araştrmalar için, K normun proksimal operatörü turetilmiştir. Ayrica, K-norm ile l∞ -norm ve l1 -normun proksimal operatörleri arasındakı ilişki analiz edilir.","The power of l∞-norm as a convex cost function in blind equalization arises from the property that minimizing l∞ -norm of the output selects the sparse solution for the combined channel-equalizer impulse response. Compressed Training Adaptive Equalization approach proposed in [Yilmaz and Erdogan, 2016] combines the power of l∞ -norm with the supervised channel equalization technique in order to reduce the required training length. [Yilmaz and Erdogan, 2019] proves that for Massive MIMO systems, the training symbols can be compressed about log2(K), where K is the number of users. This thesis investigates fast and low complexity solutions for Compressed Training Based Massive MIMO. For the purpose, the direct derivation of the proximal operator for l∞-norm is presented firstly and its clipping property is shown. The proximal methods are investigated for the solutions of l∞-norm regularized least-square problems. Specifically, Alternating Direction Method of Multipliers is proposed for the solution of Compressed Training Based Massive MIMO Noiseless Setting. Besides, an accelerated alternating projection based algorithm is proposed for solution of the Noisy Setting of Compressed Training Based Massive MIMO. Moreover, for further applications, the proximal operator of the K-norm is derived and its relation with l∞-norm and l1-norm is analyzed."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Görevdeş (P2P) bulut depolama sistemleri, kullanıcıların birbirlerinin verilerini depolamaktan sorumlu oldukları, merkezi olmayan veya birleşik yönetim alanı gerektirmeyen dağıtık sistemlerdir. Bu tez çalışmasında, P2P bulut depolama alanı Servis Olarak Platform (PaaS) işlevselliği sağlayan SEALA adında bir dağıtık yazılım mimarisi önerilmektedir. SEALA tarafından sunulan bileşenler, P2P bulut depolama sisteminin ölçeklenebilirlik, kullanılabilirlik, güvenlik, tepki süresi, hata toleransı ve enerji verimliliği açısından başarımını artırmayı amaçlamaktadır. Tezin ilk bölümünde, ölçeklenebilir, düşük gecikmeli, kullanılabilir ve hataya dayanıklı Skip Graph tabanlı P2P ağ bağlantısı sağlamayı amaçlayan SEALA yapısal ilkeleri önerilmektedir. Ayrıca, SEALA kimlik doğrulama mekanizması olarak ölçeklendirilebilir ve verimli DHT tabanlı bir blok zinciri önerilmektedir. Tezin ikinci bölümünde, önerilen SEALA yapısal ilkelerini taban alan çeşitli replikasyon protokolleri önerilmekte ve geliştirilmektedir. SEALA replikasyon yöntemleri, yer farkındalığı, kullanılabilirlik bilinci ve kopyaların kullanım bilincini sağlamayı amaçlamaktadır. Her bir SEALA modülü, mevcut son teknoloji merkezi çözümlerden daha iyi başarım gösteren yeni bir merkezi olmayan çözüm sunmaktadır. Bu tez çalışmasının diğer bir katkısı olarak, modelleme ve başarım analizi için çevrimdışı, ölçeklenebilir ve kullanıma açık bir Skip Graph simülatörü geliştirilmiştir. SEALA modülleri ve mevcut Skip Graph tabanlı P2P ağlarda merkezi olmayan çözümlerin modellemesi ve benzetimleri gerçekleştirilmiş, karşılaştırmalı ve kapsamlı başarım analizleri yapılmıştır. Mevcut en iyi çözümlerle karşılaştırıldığında, SEALA dinamik sistem özelliklerinde ağ bağlantılarını, erişilebilir replika sayısını ve replikaların mevcut bant genişliğini artıran bileşenler sunmaktadır. Ayrıca, SEALA ağdaki sorguların ortalama tepki süresini en aza indirirken veri toplama işleminin enerji verimliliğini korumaktadır. DHT tabanlı blokzincir modülüyle asimptotik iyileştirme sağlamakta ve bu modülü güvenlik ve doğrulanmış veri toplama için kullanmaktadır. SEALA bileşenlerinin P2P bulut depolama sistemlerindeki uygulamalarına ek olarak, diğer dağıtık sistem modellerindeki kullanım durumları için örnek yapılandırmaları önerilmektedir. Benzer şekilde, SEALA bileşenleri, Skip Graph dışındaki DHT-tabanlı servislerde mevcut çözümlerin alternatifleri olarak kullanılabilir. Ayrıca, ölçeklendirilebilir DHT tabanlı blokzinciri önerimiz, kripto para birimleri ve akıllı sözleşmeler gibi sistemlere uygulanabilir özelliktedir.","Peer-to-peer (P2P) cloud storage systems are decentralized systems with no centralized or federated administrative domain, where users are in charge of storing each others' data in exchange for having their data stored on some other peers of the system. In this thesis, a middleware architecture, namely SEALA, which provides P2P cloud storage Platform-as-a-Service (PaaS) functionality is proposed. The components offered by SEALA aim at improving the performance of the P2P cloud storage systems with respect to scalability, availability, security, response time, fault tolerance, and energy efficiency. In the first part of the thesis, we propose the structural primitives of SEALA, which aim at providing a scalable, low-latency, highly-available, and fault-tolerant Skip Graph-based P2P overlay. We also propose a scalable and efficient Distributed Hash Table (DHT)-based blockchain as the authentication mechanism of SEALA. In the second part of the thesis, we develop various replication functionalities based on the proposed structural primitives. The replication services of SEALA aim at providing locality-awareness, availability-awareness, and utility-awareness of replicas. Each module of SEALA presents a novel fully decentralized solution that outperforms the existing state-of-the-art decentralized solutions. For modeling and performance analysis, we develop an offline, scalable, and publicly available Skip Graph simulator as an independent contribution of this thesis. We implement and simulate our proposed modules as well as the related works, and compare their performances with the associated modules of SEALA. Compared to the best existing solutions, SEALA maximizes the overlay connectivity, the number of available replicas, and the available bandwidth of the replicas under churn. SEALA also minimizes the average response time of the queries while preserving the energy efficiency of the aggregation operation. SEALA provides asymptotic improvement with its DHT-based blockchain module, and employs it for security and authenticated aggregation. In addition to its applications on P2P cloud storage systems, we present configurations of the SEALA's components for the potential use cases in clusters and grids. Likewise, the individual components of SEALA are capable of being used as alternatives of the associated existing solutions in DHT-based services other than Skip Graphs. Finally, our scalable DHT-based blockchain proposal can be applied in other settings including the cryptocurrencies and smart contracts."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Şu anda, çok partili WebRTC video konferansı, en iyi erişim gösteren İnternet üzerinden, örgü (mesh) ve seçici yönlendirme birimi (SFU) mimarilerinde yapılandırılır. Ölçeklenebilir video kodlaması (SVC), heterojen ağ bağlantıları ve farklı erişim cihazlarına sahip eşler için verimli hizmet sağlar. Her bir eşin diğer eşlere ayrı bir video akışı gönderdiği örgü mimarisinde veri gönderme bant genişliği kapasitesi yeterli değildir. Ayrıca SVC olmayan tek bir kodlayıcı kullanılması sebebiyle veri indirme hızı en düşük olan eş diğer eşlerin hizmet kalitesini kısıtlayıcı bir etki yaratır. SVC kullanılan SFU mimarisinde her eş tek bir video akışını merkezi bir SFU sunucusuna gönderir fakat SFU sunucuları yüksek bant genişliği olan ağ bağlantısına ihtiyaç duyar ve yapısından ötürü tek arıza noktası haline gelir. Ayrıca SFU mimarisi örgü mimarisine kıyasla hem İnternet altyapı kaynaklarını daha fazla kullanır hem de uçtan uca gecikme miktarı daha fazladır. Bunlara bağlı olarak SVC kullanılan bir tane en iyi erişimi gösteren İnternet ile örgü video akış mimarisi ve üç tane de İnternet hizmet sağlayıcısı ile yönetilen çok partili WebRTC mimarisi önerdik: i) örgü, ii) dağıtılmış uç (edge) SFU ve iii) yazılım tabanlı ağ destekli çoklu internet protokolüne yayın (IP multicast). Önerilen en iyi erişim gösteren internet tabanlı örgü mimarisi düşük indirme hızı ile genel hizmet kalitesini kısıtlayan eş sorununu örgü mimarisine hareket uyumlu SVC'yi gömerek çözmektedir. Ancak en iyi erişim gösteren İnternet tabanlı görüntülü toplantı hizmetletleri değişen ağ koşullarına çok hızlı uyabilen bir yapıyı gerektirmektedir. İnternet hizmet sağlayıcısı tarafından yönetilen örgü mimamirisi önerilen en iyi erişim göstern İnternet tabanlı ve halihazırdaki örgü mimarisine kıyasla daha iyi hizmet kalitesi sağlamaktadır, lakin hâlâ daha yüksek kapasiteli veri gönderme bant genişliği gerektirmektedir. Dağıtılmış uç SFU mimarisi tek bir görüntülü konuşma oturumunda birden fazla SFU sunucusu kullanarak yeni bir SFU veri akışı tipi sunmaktadır. İnternet hizmet sağlayıcısı ile yönetilen dağıtılmış uç SFU mimarisi toplam ağ altyapı kaynak kullanımını ve uçtan uca hizmet gecikmesini halihazırda kullanılan SFU uygulamasına kıyasla önemli miktarda düşürmektedir. Buna rağmen bu mimarinin gerçeklenmesi uç ağlarda dağıtılmış hizmet tanzim edilmesine dayanmaktadır. Yazılım tabanlı ağ destekli çoklu internet protokolüne yayın mimarisi tamamen yeni bir çok partili WebRTC yayın akışı mimarisidir. Bu mimari kısıtlı veri gönderme bant genişliği, dağıtılmış hizmet tanzim edilmesi ve tek arıza noktası sorunlarını giderirken daha az İnternet altyapı kaynaklarının kullanılmasını sağlar ve olabilecek en iyi uçtan uca hizmet gecikme kabiliyeti gösterir.","At present, multi-party WebRTC videoconferencing can be configured in mesh or selective forwarding unit (SFU) architectures over the best-effort Internet. Scalable video coding (SVC) enables efficient service to peers with heterogenous network connections and terminals. In the mesh architecture each peer sends a separate video stream to each other peer where peer upload bandwidth capacity is not enough as the number of peers increase, and the peer with least download bandwidth capacity becomes limiting factor for the rest of the peers because of single non-SVC encoder. SVC enabled SFU architecture, where each peer sends a single upstream video to a central SFU, requires high capacity servers with high bandwidth network connection and the SFU becomes a single point of failure. Also, SFU architecture has more network resource consumption and end-to-end delay performance comapring to mesh architecture. To these effects, we propose one SVC enabled best-effort streaming architecture for mesh connection and three network service provider (NSP) managed SVC enabled multi-party WebRTC architectures with i) mesh, ii) distributed edge-SFU and iii) software-defined network (SDN) assisted IP multicast. Proposed best-effort mesh connection technique solves limiting overall service quality effect of least receiving peer by embedding motion-adaptive SVC. Yet, best-effort videoconferencing services require very quick responsiveness for variant network conditions. Proposed NSP-managed architectures solves this basic problem by bandwidth allocation for WebRTC services. NSP-Managed mesh architecture has the best service quality comparing to proposed best-effort and default WebRTC mesh implementations, however, it still suffers from required high upload bandwidth capacity of the peers. Distributed edge-SFU architecture introduces a novel SFU streaming type by using multiple SFU servers in a single videoconferencing session. NSP-managed distributed edge-SFU architecture reduces total network resource consumption and end-to-end service delay considerably comparing to default SFU implementation. Nonetheless, this architecture is dependent on distributed service deployment over edge networks. SDN-assisted IP-multicast architecture is a completely new streaming type for multi-party WebRTC services. This architecture overcomes limited uplink capacity, service deployment, and single point of failure drawbacks while having better network resource consumption and the best possible end-to-end service delay performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uzamsal artırılmış gerçeklik, sanal nesneleri veya herhangi bir bilgiyi projektörler yardımıyla gerçek dünyaya adapte etme tekniğidir. Gerçek dünyadaki bir değişim eş zamanlı olarak sanal nesnelerin veya bilginin de değişmesine neden olur. Bu teknik çoğunlukla eğlence amaçlı kullanılsa da, eğitim için de büyük bir potansiyele sahiptir. Bilişsel teorilere göre, soyut veya mekansal kavramlar bedensel eylemlerle daha iyi anlaşılabilir. Bu kavramların erken yaşta öğrenilmesi çocukların ileri donemdeki akademik başarıları için bir hayli önemlidir. Bu noktada, uzamsal artırılmış gerçeklik, dokunulabilir etkileşime yatkın doğası nedeniyle, bu kavramları öğrenmeye katkı sa\u{g}la-yabilir. Biz de, bu motivasyonla, bu tür oyunların geliştirilmesi için, ekipman seçimi, projektör ile kameranın eşleştirilmesi, arka plan modellemesi, dokunulabilir objelerin algılanması ve tanımlanması, ve el ve parmak ucu algılamadan oluşan 5 adımlık bir teknik kılavuz oluşturduk. Bu kılavuzu yardımıyla, projektör ve derinlik kamerası kullanarak, okul öncesi eğitimi için bir matamatik ve uzamsal öğrenme oyunu geliştirdik. Ayrıca, 3-5 yaş arası 14 okul öncesi öğrencisinin katılımıyla bir kullanıcı çalışması gerçekleştirdik. Bu çalışmalarda çocukların oyunlar sırasında güçlü bağlanma ve uzun süreli dikkat sergilediği gözlemlendi. Sonuç olarak, eğitim alanındaki öğretmenler ve akademisyenler geliştirdiğimiz platformun gelecek vaadettiğini ve ilerideki araştırmalarda ve ticari ürün olarak incelenmesini önerdiler.","Spatial augmented reality (SAR) is a technique that makes use of projectors to combine virtual and real worlds by displaying computer-generated objects or information onto the physical environment where alterations of the real objects also affect the virtual ones simultaneously. Although this technique is mostly used for entertainment, it has great potential for education as well. According to cognitive theories, abstract or spatial concepts can be better understood through bodily actions, which is crucial for the later academic achievements of a preschooler. Therefore, SAR can have significant help to learn such concepts due to its embodied nature involving tangible interactions. With this motivation, we developed a technical guideline for developing SAR games. The technical guideline involves the following 5 steps: choosing the equipment for a SAR setup, calibration and mapping between the projector and the camera, background modeling, tangible detection and identification, hand and fingertip detection. We demonstrated games for preschool math education and preschool spatial learning using mobile projector and depth camera-based hardware setups. User studies were performed with 14 children with 3-to-5 years old and the team observed very strong engagement and prolonged attention during the activities. In conclusion, the platform technology we developed was found very promising by educators and scholars in this field and will be exploited further both as a research tool and as a commercial product."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İfade ediciliğinden ötürü doğal dil, insanlar arasında açık ve örtülü duygusal durumun aktarımı için çok önemlidir. Konuşmacıların duygusal durumuna ve konuşmanın içeriğine bağlı olarak, aynı sorunun (örneğin: Nasılsınız?), farklı duygu içerikli cevapları olabilir. Duygunun cevap oluşumunda yapılandırıcı bir özellik olarak kullanıldığı bir takım çalışmalar mevcut olsa da, diyalog sistemlerinin çoğu, duygusal durumdan bağımsız bir şekilde verilen bir soruya aynı cevabı üretmektedir. Bu tezde, duyguyu diyalog oluşturmada entegre eden bir yaklaşım olan, AffectON-u, tanıtıyoruz. Hedeflenen bir duyguda dil üretmek için yaklaşımımız, olasılıksal bir dil modelini, bir duygu uzayını ve kelime yerleştirmelerini kullanmaktadır. AffectON'un kullandığı olasılıksal dağılım herhangi bir olasılıksal dil modelinden kaynaklanabileceğinden (örneğin, diziden diziye modeller, sinir dili modelleri, n-gramlar) yaklaşımımız dil modeli agnostiğidir. Bu nedenle, hem duygusal diyalog hem de duygusal dil üretimi için uygundur. Bu çalışmada, duygusal diyalog oluşturma ile deneyler yapıp, öznel ve nesnel değerlendirmeler gerçekleştirdik. Değerlendirmenin öznel kısmında, derecelendirme yapılabilmesi için bir kullanıcı arayüzü tasarlayıp, bu tür arayüzlerin tasarımı için öneriler sunduk. Hem öznel hem de nesnel değerlendirme sonuçları, yaklaşımımızın oluşturulan dili hedeflenen duyguya doğru çekmede başarılı olduğunu ve sözdizimsel tutarlılık konusunda çok az fedakarlıkta bulunduğunu göstermektedir.","Due to its expressivity, natural language is paramount for explicit and implicit affective state communication among humans. The same linguistic inquiry (e.g. How are you?) might induce responses with different affects depending on the affective state of the conversational partner(s) and the context of the conversation. Yet, despite few studies which perceive affect as constitutive aspect of response generation, most of the existing dialog systems produce identical responses to a given inquiry, irrespective of affective information. In this thesis, we introduce AffectON, an approach for integrating affect into dialog generation. For generating language in a targeted affect, our approach leverages a probabilistic language model, an affective space and word embeddings. AffectON is language model agnostic since the probabilistic distribution can originate from any probabilistic language model (i.e. sequence-to-sequence models, neural language models, n-grams). Hence, it can be employed for both affective dialog and affective language generation. We experiment with affective dialog generation and conduct subjective and objective evaluations. For the subjective part of the evaluation, we design a designated user interface for rating and provide recommendations for the design of such interfaces. The results, both subjective and objective show that our approach is successful in pulling the generated language toward the targeted affect, with little sacrifice in syntactic coherence."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin öğrenme, bilgisayar görüşünden doğal dil işlemeye kadar pek çok alanda bilgi çıkartmak ve keşfetmek için öne çıkan bir araç haline gelmiştir. GPU ve FPGA gibi modern hızlandırıcıların artan ulaşılabilirliği sayesinde derin öğrenme araştırmacıları daha güçlü ve daha karmaşık sinir ağı mimarileri tasarlama fırsatı bulmuştur. Ancak eğitim süreleri ve hafıza kısıtlamaları halen derin öğrenme araştırma alanında darboğaz olmaktadır. Sistem kaynaklarını kullanmak, eğitim verimliliğini iyileştirmek açısından gereklidir.Konvolüsyon ve matris çarpımı gibi anahtar sayısal işlemleri bütün işlemciler (ör: CPU çekirdekleri ve GPU) arasında planlamak bu hususta çok önemli bir hale gelmiştir. Bu kısıtlamaları hedef alan mevcut çözümler şeffaf değildir yani kullanıcının manüel parçalamasını gerektirerek derin öğrenme çerçevelerinin eğitim performansını kısıtlamaktadır. Bu tez bu problemi iki farklı açıdan ele almaktadır: çekirdek iyileştirmesi ve model parallelliği. İlk önce sinir ağı eğitiminde sıkça kullanılan çok boyutlu indirgeme işlemi analiz edilip iyileştirilmiştir. İndirgeme işleminin matematiksel özellikleri birden fazla boyutun aynı anda indirgenip gereksiz çekirdek ateşlemelerinin yok edilmesi için kullanılmıştır. Geliştirdiğimiz çekirdek yazılım performansını, iyileştirme yapılmış Knet derin öğrenme çerçevesi ile karşılaştırdığımızda, 56 kata kadar hızlanma ve NVIDIA K20m GPU makinesinin teorik bant genişliğinin 75%'ini elde etmekteyiz. Bunlara ek olarak, yazılımcının programlama eforunu azaltmak ve kaynak kullanımını arttırmak için bir aygıt atama algoritması geliştirilmiştir. Algoritma, ağ ve katmanlarındaki işlemsel bağlılıkları ve hafıza aktarımlarını değerlendirerek, bir maliyet modeli oluşturmaktadır. Daha sonra değiştirilmiş çerçeve çalışma zamanı bu maliyet modeli ve bazı buluşsal koşulları kullanarak işlemleri aygıtlara yerleştirmektedir. Ağın farklı kısımlarını birden çok aygıta yerleştirme işleminin bütün karmaşıklığı çerçevemiz tarafından kontrol edilmektedir. Önerilen çalışma zamanı, bilinen bir derin öğrenme çerçevesi olan TensorFlow'un üstüne geliştirilmiştir ve sonuçları, popüler birer çizge parçalama ve yük paylaştırma kütüphaneleri olan, METIS ve Zoltan sonuçları ile karşılaştırılmıştır. Yapılan deneylerde, performans ölçümleri farklı ve modern derin sinir ağlarının kullanılması ile gerçekleştirilmiştir","Deep learning has become a prominent tool for extracting and exploring information in a wide selection of areas ranging from computer vision to natural language processing. With the increasing availability of modern hardware accelerators like GPUs and FPGAs, deep learning researchers had the opportunity to design more powerful and complex neural network architectures. Training time and memory restrictions, however, still act as a bottleneck in deep learning research. Utilizing system resources is essential in order to maximize training efficiency. Scheduling key computational operations such as convolution and mat-mul efficiently across all processing units (e.g. CPU cores and/or GPUs) in the system while minimizing the time spent on data communication is crucial. Existing solutions addressing these restrictions are not transparent and require manual partitioning of the model limiting the training performance of the frameworks. This thesis, attacks this problem from two angles: kernel optimization and model parallelism. First the multidimensional reduction operation which is a frequently used operation in neural network training is analyzed and optimized. The mathemetical properties of the reduction operation are used to concurrently reduce many of its dimensions and eliminate unnecessary kernel launches. We compare our kernel's performance with the optimized Knet deep learning framework and achieve up to 56x speed-up and 75% of the theoretical device bandwidth of an NVIDIA K20m GPU. In addition, a device placement algorithm based on depth-first search is proposed to reduce the programming effort for the researcher and improve resource utilization. The partitioning algorithm creates a cost-model for the target neural network based on the computational dependencies and memory transfers between the operations involved in the network and its and layers. Then the altered framework runtime uses this cost model and heuristics to place the operations on devices. All the complexity of assigning different parts of the network into multiple devices is handled by our run-time, with high resource utilization. The proposed runtime is implemented on top of popular deep learning framework Tensorflow for evaluation and compared with the popular graph partitioning and load balancing tools METIS and Zoltan. Different state-of-the-art neural network architectures and training datasets are used in the experiments to measure the performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar teknolojilerinin olanaklarından faydalanırken, aynı anda bedensel hareket ve etkinlikler bakımından zengin tecrübeler yaşamayı nasıl başarabiliriz? Bu soru, artırılmış gerçeklik ve hareket algılama gibi teknolojiler mümkün olduğundan beri, insan-bilgisayar etkileşimi ve etkileşim tasarımı araştırmacılarını meşgul etmektedir. Fakat, kaydedilen teorik ve teknolojik gelişmelere rağmen, beden ve fiziksellik odaklı bilgisayar arayüzleri araştırma projeleri ve niş uygulamalar ile sınırlı kalmıştır. Bu tez, bu soruna odaklanarak, bilgisayar arayüzlerinde fiziksel zenginliği hedef alan beş araştırma projesini sunar. Bu projeler sırasında edinilen tecrübeler, ilgili literatürde mevcut tartışmalar ile harmanlanarak, beden ve fiziksellik odaklı insan-bilgisayar etkileşimi doğrultusundaki teknolojik ve teorik çalışmaları ilgilendiren yorumlar sunulur. Bu beş projenin konuları kısaca şöyledir: (1) canlı elektronik müzikte izleyici tecrübesi üzerine deneysel bir çalışma; (2) hareket algılama ile tasarım için bir eğitim programı; (3) teknik ve profesyonel bir kullanım vakası için artırılmış gerçeklik uygulamaları; (4) gelişmekte olan teknolojiler ile geleneksel zanaat kesişiminde tasarım olanakları; ve (5) interaktif otonom hava araçları üzerine literatür taramasına dayalı tasarım yönergeleri. Tezin literatüre temel katkısı, bu çalışmalar sonucunda ortaya çıkan ve tasarım çalışmaları için girdi olabilecek kavramlardır.","How might we enjoy the benefits afforded by computers, and experience an abundance of physical experiences while doing so? This question has intrigued human-computer interaction (HCI) and interaction design (IxD) scholars since the inception of technologies like augmented reality and gesture sensing. Yet, despite abundant discourse and a stream of technologies to match, the vision of physical diversity in HCI is still not realized beyond research and niche applications. To address this issue, this dissertation presents a critique on and implications for the trajectories of technologies and discourse aimed at enriching the physicality of HCI, based on reflections on a compilation of five research publications. These publications report on: (1) an empirical study on the spectator experience in live electronic music; (2) an introductory educational program pertaining to user interface design with emerging technologies; (3) an augmented reality application for professional use; (4) a philosophical and conceptual exposition on design with emerging technologies and traditional craft; and (5) a literature review and design directions for interactive autonomous drones. A vocabulary of five notions distilled from the literature on embodied HCI, substantiated through reflections on the five projects, emerges as a principal contribution with the potential to inform future designs."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde, TOP500 listesindeki süper bilgisayarların % 25'inden fazlası, büyük ölçüde paralel ve güç açısından verimli mimarileri nedeniyle GPU'ları kullanmaktadır. Ancak, büyük ölçekli bir sistemde GPU'ların verimli bir şekilde programlanması, yalnızca hesaplamalı bilim dalındaki insanlar için değil, aynı zamanda programlama uzmanları için de zorlu bir iştir çünkü bu işte GPU'ya özel kod üretilmeli, farklı bellekler yönetilmeli ve iletişim ele alınmalıdır. Bu gereksinimler üretkenliği azaltırken, uygulamanın taşınabilirliğini de sınırlar. Kod yönergeleriyle taşınabilirlik ve üretkenlik sunan OpenACC gibi direktif tabanlı programlama modelleri bulunmaktadır. Ancak bu modeller performansı iyileştirme amaçlı ayarlamalar gerektirir ve GPU'lu kümelenmiş sistemlerin programlanmasını desteklemezler. Kümelenmiş sistemlerde iletişimin verimli bir şekilde ele alınması gerekir. İletişimi ele alan ve GPU programlamayı kolaylaştırmak için iskelet tabanlı soyutlamalar sunan çalışmalar bulunmaktadır, ancak bu soyutlamalar programlamada yeterli esnekliği sağlayamazlar. Üstelik, literatürdeki çalışmaların çoğu, GPU'ları ana işlemciye bağlayan ara bağlantının bant genişliği kısıtını dikkate almamaktadır. Bazı görev tabanlı yaklaşımlar, bu kısıta çözüm olması amacıyla hesaplama ile örtüşen veri aktarımlarını kullanmaktadır fakat bu yaklaşımlarda programlayıcı GPU'ya özgü kodun yazılmasından ve görev zamanlamasından sorumludur. Programlama çabasını kolaylaştırmak, taşınabilirliği artırmak ve transferleri optimize etmek amacıyla GPU'lu kümelenmiş sistemlerde yürütülecek yapısal çözüm alanı problemleri için bloklamaya dayalı programlama modeli sunuyoruz. Model, üstü kapalı bir şekilde, TiDA ile veriyi ayrıştırır ve CUDA ile farklı bellekleri yönetir. CPU ve GPU'lar için tek tip bir arayüzün arkasında OpenACC'nin yönergelerinden yararlanarak otomatik bir şekilde GPU'ya has kod üretir. Ek olarak, veri aktarımlarını ve iletişimi yönetir ve bunları, CUDA akışları ve bloklamayan MPI rutinlerini kullanarak hesaplamalarla örtüştürür. Programlama modelinin etkinliğini bir ısı simülasyonu ve gerçek hayatta kullanılan bir kardiyak modellemesi üzerinde gösterdik. Sonuçlar, programlama modelinin iletişimi başarıyla örtüştürdüğünü ve iyi hızlanma sağladığını göstermektedir.","Currently, more than 25\% of supercomputers in TOP500 list employ GPUs due to their massively parallel and power-efficient architectures. However, programming GPUs efficiently in a large-scale system is a demanding task not only for computational scientists but also for programming experts because it requires generating GPU-specific code, managing distinct address spaces and handling communication. While these requirements reduce productivity, they also limit the portability of the application. There are pragma-based programming models such as OpenACC offering portability and productivity with code annotations. However, they require performance tuning and lack support for programming GPU clusters. Handling communication efficiently is essential in the cluster environment. There are related works such as Cluster-SkePU handling communication and offering skeleton-based abstractions to ease GPU programming, but such abstractions arguably limit programming flexibility. Moreover, most of the works in the literature do not consider the bandwidth bottleneck of the interconnect that links GPUs to hosts. Some task-based approaches provide overlapping data transfers with computation as a solution to the interconnect bandwidth limit, but they hold programmer responsible for GPU-specific code and task scheduling. To ease the programming effort, increase the portability and optimize communication, we propose a tiling-based programming model for structured grid problems running on a GPU cluster. The model implicitly applies data decomposition with TiDA and manages distinct address spaces with CUDA. It automatically generates GPU-specific code itself by leveraging OpenACC annotations behind a uniform interface for CPUs and GPUs. Furthermore, it handles data transfers and communication and overlaps them with computation by exploiting CUDA streams and non-blocking MPI routines. We demonstrate the effectiveness of the programming model on a heat simulation and a real-life cardiac modeling. The results show that it successfully overlaps communication and achieves good speedup."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Web üzerinde çoklu ortam içeriğinin artışına paralel olarak, video aramaları, yazı tabanlı arama yöntemleri yerine, video içeriğine gore düzenleme olanağı sağlayan içerik tabanlı arama yöntemleri kullanılarak gerçekleştirilmeye başlanmıştır. Bu eğilim, büyük video kümeleri üzerinde etkili ve verimli arama gerçekleştirebilecek video arama sistemleri üzerinde bir araştırma sürecinin başlangıcı olmustur. Birçok video arama sistemi, sadece el yordamıyla oluşturulan özniteliklere ve etiketlemelere bağlı olarak arama gerçekleştirmektedir. Video gibi devimsel içerikleri birbirinden ayıran en önemli özellik olan nesnelerin hareket bilgisi görmezden gelinmektedir. Hareket, çizim ve konuşmanın eş zamanlı olarak kullanılmasıyla belirtilebilecek bir bilgidir. Konuşma, içeriğin, olayların ve nesnelerin birbirleriyle olan ilişkilerinin kolaylıkla belirtilebilmesine olanak tanırken, çizim uzamsal ifade kabiliyeti sunmaktadir. Fakat, söz konusu etkileşim yapısına sahip bir video arama sistemi bulunmadığından, bu kiplerin video aramalarında nasıl kullanılabileceğine dair bir bilgi eksikliği mevcuttur. Bu çalışmada, kullanıcıların çizim ve konuşma tabanlı video arama görevlerine aktif katılımlarını sağlayacak bir Oz Büyücüsü yönergesi ve bazi araçlar geliştirilmiştir. Söz konusu araçların ve arama yönergesinin birbirleriyle olan uyumu, bir kullanım alanı üzerinde (futbol maçlarının aranması) değerlendirilmiştir. Ardından, toplanan kullanıcı etkileşim verileri kullanılarak, eş zamanlı olarak verilen çizim ve konuşma girdilerinden kullanıcının bahsetmiş olduğu hareket olaylarının sıralamasının elde edilebildiği bir makine öğrenmesi modeli geliştirilmiştir. Bu modelin performans sonuçları, video arama yönergesinin ve araçların farklı türlerde videoların aranmasında çoklu etkileşim mekanizmalarının irdelenmesi icin uygun olduğunu göstermektedir. Bunun yanında, oluşturulmuş çok kipli yorumlayıcı, çoklu ortamlar icin hazırlanmış ölçeklenebilir ve hızlı bir veritabanı sistemi ile birleştirilmiş ve bir video arama sistemi meydana getirilmiştir. Söz konusu video arama sistemi, kullanıcı değerlendirme çalışmaları ile değerlendirilmiştir. Çalışmalardan elde edilen sonuçlar, oluşturulan çok kipli yorumlama mekanizmasının ve veritabanı sisteminin büyük video kümeleri üzerinde hareket tabanlı video araması için iyi bir ikili olduğunu göstermektedir.","With the increasing amount of multimedia content available on the web, the focus on video retrieval engines has been shifting from text-based systems to content-based methods that allow indexing and retrieval based on video contents. This trend has sparked a quest for efficient and effective video retrieval systems on large video collections. Most video retrieval systems rely only on hand-crafted features and manual annotations. Motion of the individual objects, the most decisive information conveyed in videos, is usually overlooked in video retrieval. From a user interaction perspective, motion can be given as a query using speech and sketch simultaneously. Speech allows easy specification of content, events and relationships, while sketching brings in spatial expressiveness. Unfortunately, we have insufficient knowledge of how sketching and speech can be used for video retrieval, because there are no existing retrieval systems that support such interaction. In this paper, we describe a Wizard-of-Oz protocol and a set of tools that we have developed to engage users in a sketch- and speech- based video retrieval task. We report how the protocol and the tools fit together to establish an ecologically valid testbed using retrieval of soccer videos as a use case scenario. Using the data collected in the studies, we developed a model capable of interpreting simultaneous speech and sketching to infer the sequence of motions described by a user. The performance results of the model suggest that the protocol and the tools together have the potential to serve as effective means for studying a wide range of multi-modal use cases. Moreover, a video retrieval system was built by integrating the multimodal interpretation model to a database back-end designed for big multimedia collections. The retrieval system was assessed through user evaluation studies. The evaluation results demonstrate that the given query interpretation mechanism and the database system make a good couple for motion-based video retrieval on big video collections."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Facebook ve Twitter gibi Çevrimiçi Sosyal Ağların (ÇSA'lar) hizmet sağlayıcıları, kullanıcılara depolama ve hesaplama kaynakları sunar ve kişisel bilgilerini paylaşmalarını ve arkadaşlıklar kurmalarını sağlar. Merkezi bir ÇSA'da kullanıcı verilerinin toplanması güvenlik riskleriyle birlikte gelir. Bu sebeple federe sunucu mimarisine sahip dağıtık ÇSA'lar önerilmiştir. Merkezi bir sunucu yerine farklı sağlayıcılar tarafından çalıştırılan birden çok sunucu kullanılması önerilmiştir. Sunucuların hiçbiri tam olarak güvenilir değildir. Bu tezde, federe sunucu mimarisini ve sunucular arasındaki güven dağılımını kullanarak ve merkezi yapıda aksi takdirde imkansız olacak verimli ve gizliliği koruyan bir ÇSA hizmetleri çerçevesi öneriyoruz. Çerçevenin ilk modülü Privado, ÇSA sunucuları tarafından, reklam verenlerin hedef müşterilerini ÇSA kullanıcılarının şifreli profillerini kullanarak bulabilecekleri, gizliliği koruyan bir grup tabanlı reklam yöntemidir. Kişiselleştirilmiş sistemlerde mümkün olmayan kullanıcı gizliliğini korumak için grup temelli reklam kavramını öneriyoruz. Tek bir sunucuyu, tüm reklam verenleri ve kullanıcıların büyük bir bölümünü kontrol eden aktif bir saldırgana karşı kullanıcı gizliliğini resmi olarak tanımlıyor ve kanıtlıyoruz. Tasarımımız aynı zamanda reklam şeffaflığını da sağlar; Hedef müşterileri belirleme prosedürü, kullanıcıları ve reklam verenleri dahil etmeden, sadece ÇSA sunucuları tarafından çalıştırılmaktadır. Çerçevenin ikinci modülünde, Anonyma, ÇSA'larda davetiye ile grup katılımını ele alıyoruz. Anonyma ile kullanıcılar sistem yöneticisine, kim tarafından davet edildiklerini açıklamadan belirli sayıda davet aldıklarını kanıtlayabilmektedir. Kötü niyetli bir saldırgana karşı davetiyenin anonimliğini ve davetiyelerin orijinalliğini resmi olarak tanımlıyor ve kanıtlıyoruz. Ek olarak tasarımımız performans olarak en iyi rakiplerinden daha iyi sonuçlar göstermektedir. Bir kullanıcı sisteme katıldıktan sonra, yöneticinin anında ve mevcut üyeleri yeniden anahtarlamadan, yeni gelen kişinin davetci olarak hareket edebilmesi için gerekli bilgileri vermesi anlamında verimli bir şekilde ölçeklenebilir. Ayrıca, bir sosyal ağın üyeleri tarafından verilen davetiyelerin başka bir sosyal ağa kayıt için kullanılabileceği davetiye tabanlı bir sistem olan Anonymax çözümümüzü de tasarladık. Çerçevenin üçüncü modülü Integrita, ele geçirilmiş sunucuların varlığında görünüm tutarlılığını koruyan ortak bir veri paylaşım platformudur (Facebook grupları gibi). Diğer bir deyişle, paylaşılan verinin (örneğin, grup sayfasının gönderileri) farklı kullanıcılara (örneğin, grup üyeleri) farklı gösterilemeyeceği garanti edilir. Mevcut çözümlerden farklı olarak, Integrita, yinelemeli depolama veya kullanıcıların bant dışı iletişimini gerektirmeden tutarsızlığın tespitini sağlar. Her kullanıcı, gerçekleştirdiği işlemle ilgili görünüm tutarlılığını doğrulamak için tek başına (diğer kullanıcıların varlığına güvenmeden) işlem yapabilir. Bu durum için q-saptanabilir tutarlılık adı verilen yeni bir görünüm tutarlılığı tanımı ve çözümü ortaya koyuyoruz. Tezin her modülü için önerilen yaklaşımlarımızın çalışma süresi ve performansı kapsamlı simülasyonlar ve bunlara göre elde edilen sonuçlar üzerinden incelenmiştir. Privado, istenen reklamcılık doğruluğunu ve kullanıcı gizlilik seviyesini ayarlamak için parametrelere sahip bir reklam sistemini sunucu tarafında minimum iletişim karmaşıklığı ile reklam şeffaflığını ve kullanıcı gizliliğini aynı anda sağlayarak teknolojik üstünlük sunar. Anonyma, davetiye anonimliğini ve orijinalliğini kanıtlanabilir bir biçimde sağlarken yalnızca gerekli davetiye sayısına bağlı bir hesaplama karmaşıklığı gerektirir (önceki sistemlerde ise bu durum sistemdeki toplam kullanıcı sayısı ile doğru orantılıydı). Integrita veri paylaşım platformu, merkezileştirilmiş ve dağıtılmış emsallerine kıyasla görünüm tutarlılığını geliştirir ve ÇSA sunucularının depolama yükünü frac {1}{N} katsayısı ile hafifletir (N sunucu sayısıdır) ve sunucu başına depolama ve sunucular arası iletişim konularında bütün rakipleri arasında en düşük yükü gerektirir.","Online Social Network (OSN) providers like Facebook and Twitter supply storage and computational resources for the users and enable them to share personal information and establish friendships. The collection of users data at a central OSN provider causes security issues due to which the distributed OSNs with federated server architecture is proposed. The central provider is replaced by multiple servers (running by distinct providers) which run OSN services collaboratively while no server is fully trusted. We utilize the federated server architecture and the trust distribution among the servers to propose a framework of efficient and privacy-preserving services for OSNs that would be otherwise impossible in the centralized versions. The first module of the framework, Privado, is a privacy-preserving group-based advertising method through which OSN servers can find the advertiser's target customers using users' encrypted profiles. We propose the group-based advertising notion to protect user privacy, which is not possible in the personalized variant. We formally define and prove user privacy against an active malicious adversary controlling all but one server, all the advertisers, and a large fraction of the users. Privado also achieves advertising transparency; the procedure of identifying target customers is operated solely by the servers without getting users and advertisers involved. The second module of the framework, Anonyma, copes with the inference attack in the invitation-only nature of OSNs' group formation. Anonyma is an invitation-based system where users prove to the administrator that they are invited by a certain number of group members without revealing their inviters. We formally define and prove the inviter anonymity and unforgeability of invitations against a malicious adversary. Also, Anonyma outperforms the state-of-the-art concerning the computational overhead. Besides, Anonyma is efficiently scalable in the sense that the administrator can issue credentials to the newcomers, enabling them to act as an inviter, without re-keying the existing members. We also design Anonymax, an anonymous cross-network invitation-based system where the invitations issued by the members of one social network can be used for the registration to another social network. The third module of the framework, Integrita, is a collaborative data-sharing platform (e.g., Facebook groups) that preserves view consistency against corrupted servers i.e., servers cannot show divergence view of the shared data (e.g., posts of the group page) to the users (e.g., group members) without being detected. Unlike the state-of-the-art, Integrita enables detection of inconsistency neither by using storage inefficient data replication solution nor by requiring users to exchange their views out of the band. Every user, without relying on the presence of other users, can verify any server-side equivocation regarding her performed operation. We introduce and achieve a new level of view consistency called q-detectable consistency in which any inconsistency between users view cannot remain undetected for more than q posts. For each module of this thesis, the running time and performance of our proposed approaches are examined through extensive simulations, and the results provided accordingly. Privado enables an advertising system with tweak-able parameters to adjust the advertising accuracy w.r.t. user privacy. Privado also excels the state-of-the-art by efficiently and simultaneously achieving advertising transparency and user privacy with the minimum communication complexity at the server-side. Anonyma supports provable invitation unforgeability and inviter anonymity by a computation complexity upper-bounded only by the number of required inviters while the computation complexity of the prior studies grows linearly with the system size. The data-sharing platform of Integrita advances the centralized and distributed counterparts by improving the view-consistency and storage overhead (by the factor of 1/N where N is the number of the servers), respectively. Nevertheless, concerning per server storage overhead and cross-server communication, Integrita's overhead is the minimum among all its counterparts."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, c ̧ok az go ̈sterimden o ̈du ̈l o ̈gˇrenip, bu o ̈du ̈lleri politika aramasında, be- ceriyi geli ̧stirmek i ̧cin kullanan bir robotik beceri o ̈gˇrenme sistemi tanıtılmaktadır. Go ̈sterimlerden c ̧ıkartılan hareket ve algı verisi, beceriyi ger ̧cekle ̧stirmek i ̧cin kul- lanılan polic ̧e ve hareketi go ̈zlemek ic ̧in kullanilan hedef modeli parametrelerinin o ̈ˇgrenimi i ̧cin kullanılmı ̧stır. Beceriler, Dinamik Hareket Pirimitivleri (DHP) ile parametrize edilirken, Saklı Markov Modeli (SMM) hedef modeli olarak kullanılmı ̧stır. O ̈duller ise SMM'in yapısı ve go ̈zlem kabiliyerleri vasıtası ile ogrenilmistir. Bir son- raki adım olarak SMM, sonlu ufka sahip Markov O ̈du ̈l Su ̈reci'ne (MO ̈S) ̧cevrilmi ̧stir. Monte Carlo yo ̈ntemi ile SMM'in saklı durumlarının ̈odul deˇgerleri hesaplanmı ̧stır ve bu deˇgerler SMM'i Kısmen Go ̈zlenebilir MO ̈S'e (KGMO ̈S) ̧cevirmek i ̧cin kul- lanılmı ̧stır. KGMO ̈S sayesinde robot, polic ̧e aramasında kullanılmak u ̈zere do ̈nu ̈tler elde etmi ̧stir. O ̈du ̈l o ̈ˇgreniminin yanı sıra, bu tezde, kara kutu en iyilemesi tabanlı bir poli ̧ce araması metoduna adaptif ke ̧sif stratejisi uygulanmı ̧stır. Ortaya ̧cıkan poli ̧ce araması yapısının performansı, robotik simu ̈lasyon ortamında be ̧s farkli poli ̧ce araması metodu kullanılarak iki tane beceri ile o ̈l ̧cu ̈lmu ̈ ̧stu ̈r. Bu beceriler robotun bir kutu a ̧ctıgˇı A ̧cma ve bir kutu kapadıgˇı Kapama becerileridir. Sonu ̧clar g ̈ostermektedir ki, KGMO ̈S'den ̧cıkartılan o ̈du ̈ller, hedef modelinden elde edilen aralıklı go ̈zlem sinyal- lerine go ̈re daha iyi performans sergilemektedir. Buna ek olarak, bu tezde tanıtılan poli ̧ce araması metodu digˇer poli ̧ce araması metodlarına go ̈re daha hızlı ve daha az varyans ile yakınsamaktadır. Son olarak, simu ̈lasyonda g ̈ozlemlenen sonu ̧clar, gerek robotta u ̈ ̧c tane beceri ile test edilmi ̧stir. Bu beceriler Ac ̧ma, Kapama ve robotun tahta bir ̧cekmeceyi c ̧ekerek a ̧ctıgˇı C ̧ekme becerisidir. Robot, bu ̈tu ̈n becerileri tam ba ̧sarısızlıktan tamamen ba ̧sarılı olacak ̧sekilde o ̈gˇrenmi ̧stir.","In this thesis, a novel skill learning framework that learns rewards from very few demonstrations and uses them in a policy search setting to improve the skill is intro- duced. The action and perceptual data that are extracted from the demonstrations are used to learn a parameterized policy to execute the skill and a goal model to monitor the executions respectively. The skills are parameterized with Dynamical Movement Primitives (DMP) and a Hidden Markov Model (HMM) is used as the goal model. The rewards are learned from the HMM structure and its monitoring capability. The HMM is then converted to a finite horizon Markov Reward Process (MRP). A Monte Carlo approach is used to calculate the values corresponding to each hidden state of the HMM. Then, the HMM and the extracted values are merged into a Partially Observable MRP (POMRP). POMRP enabled us to obtain execution returns that are then used in policy search to improve the policy. In addition to reward learning, an adaptive exploration strategy is introduced to a black box opti- mization based PS method. The resulting framework is evaluated with five different policy search methods in a robotic simulation environment for two skills: Open in which robot learns to open a box and Close in which robot learns to close a box. The results show that the returns extracted from the POMRP lead to better performance compared to sparse monitoring signals, and the introduced policy search approach converges faster with higher success rates and lower variance than the rest. Finally, the efficacy of the framework is validated in a real robot setting with the introduced policy search method for three skills: Open, Close and Draw in which robot learns to pull to open a wooden drawer. We show that, in the real robot, the three skills are improved to complete success from complete failure."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yakın gelecekte ev, hastane, fabrika gibi çeşitli ortamlarda, insanların ve robotların birlikte fiziksel işbirliğine dayalı ortaklaşa işler yapması beklenmektedir. İnsanlar, üstün zihinsel yeteneklere sahip iken, robotlar dayanıklılık, konumlamadaki yüksek doğruluk ve tekrarlanabilirlik gibi konulardaki avantajlarıyla ön plana çıkarlar. Fiziksel insan-robot işbirliği sayesinde insanların ve robotların yetenekleri bir araya getiri\- lerek karmaşık görevlerin yürütülmesi sırasında hız, esneklik ve ergonomi açılarından iyileşmeler sağlanabilir. Fiziksel insan-robot işbirliği, birbirleriyle çelişen yapılarda olan yüksek şeffaflık ve yüksek güvenlik hedeflerinin aynı anda gerçekleşmesini gerektirmektedir ve bu durum araştırmacılar için zor bir denetim problemidir. Mükemmel şeffaflık pratikte erişilemez olduğu için bu hedefler arasında en iyi ödünleşime izin veren denetleyiciler tercih edilmektedir. Bu tezin ilk kısmında, fiziksel insan-robot etkileşimi için yeni bir denetleyici mimarisi olarak kesir dereceli admitans denetleyicileri öneriyoruz. Önerilen denetleyiciyi içeren sistemin kararlılık ve şeffaflık çözümlemeleri, insan da döngü içine dahil edilerek yapılmıştır. Kesir dereceli denetleyicinin değişkenlerini tam sayılı denetleyici değişkenlerine eşleştirmek için empedans eşleme yöntemi önerilmiş ve bu sayede sistemlerin gürbüz kararlılığı analitik olarak karşılaştırılmıştır. Ayrıca, etkileşim başarımı, insan deneklerin katıldığı doğrusal olan ve olmayan çevrelerle temas içeren iki farklı deneysel çalışma ile incelenmiştir. Sonuçlar, fiziksel etkileşimde kesir dereceli admitans denetleyicileri kullanıldığında tam sayı dereceli olanların kullanımına göre daha fazla gürbüzlük ve şeffaflığa erişilebileceğini göstermiş, denetleyicideki kesir dereceli terimin çevre ile temas içeren etkileşim görevlerinde insanın harcadığı eforu azalttığınına dair kanıtlar sunulmuştur. Tezin ikinci kısmında, belirli bir etkileşim denetleyici yapısı için, kapalı çevrim fiziksel insan-robot etkileşim sisteminin şeffaflığını ve gürbüz kararlılığını birlikte en iyileyecek çok kriterli bir optimizasyon çerçevesi önerilmiştir. Önerdiğimiz Pareto optimizasyon çerçevesi, şeffaflık ve gürbüz kararlılık arasındaki ödünleşimi kapsamlıca inceleyip, tasarımcının ödünleşim hakkında yeterli bilgiye ulaştıktan sonra bilgiye dayalı karar vermesini sağlamaktadır. Bu yapı, gürbüz kararlılık ve şeffaflık için tanımlanmış performans ölçütlerinin (amaç fonksiyonlarınının) ayrıklaştırılmış denetleyici değişken uzayında hesaplanmasını içermektedir. Çok kriterli optimizasyon kullanılarak elde edilen Pareto cephesi eğrisi, şeffaflık ve gürbüz kararlılık arasındaki ödünleşimi tarif etmektedir. Bu ödünleşim çalışılarak, ulaşılabilecek en yüksek şeffaflık ve gürbüz kararlılığı sağlayacak denetleyici değişken kümesi bilinçli şekilde seçilebilir. Önerilen tasarım yönteminin pratik kullanımını göstermek için tam sayılı ve kesir dereceli admitans denetleyiciler örnek olarak çalışılmış, bu iki denetleyici mimarisinde yapılan tasarımlar teorik ve deneysel olarak karşılaştırılmıştır. Deneysel kıyaslamalar, esnekliği doğrusal olmayan bir çevre ile temas içeren fiziksel insan-robot ortaklaşa görevi sırasında yapılmıştır. Sonuçlar, önerilen tasarım çerçevesinin geçerliliğini doğrulamaktadır ve ayrıca, iki denetleyici sistemin gürbüz kararlılığı aynı olacak şekilde tasarlandığında, kesir dereceli admitans denetleyicinin tam sayılı olana göre daha fazla şeffaflık sağlayabileceğini ortaya koymaktadır.","In the near future, humans and robots are expected to perform collaborative tasks involving physical interaction in various environments, such as homes, hospitals, and factories. Robots are good at precision, strength, and repetition, while humans are better at adaptation and cognitive tasks. Physical human-robot interaction (pHRI) takes advantage of both robots and humans to improve speed, flexibility, and ergonomics of complex tasks. pHRI requires design of controllers to achieve safe and transparent operations which is challenging mainly due to the contradicting nature of these objectives. Knowing that attaining perfect transparency is practically unachievable, controllers allowing better compromise between these objectives are desirable. In the first part of this dissertation, we propose a new controller, fractional order admittance controller, for pHRI. The stability and transparency analyses of the new control system are performed computationally with human-in-the-loop. Impedance matching is proposed to map fractional order control parameters to integer order ones to enable comparisons, and the stability robustness of the system is studied analytically. Furthermore, the interaction performance is investigated experimentally through two human subject studies involving continuous contact with linear and nonlinear viscoelastic environments. The results indicate that the fractional order admittance controller can be more robust and transparent than the integer order admittance controller and the use of fractional order term can reduce the human effort during tasks involving contact interactions with environment. In the second part of this dissertation, we propose a multi-criteria optimization framework for interaction controller design, which jointly optimizes the stability robustness and transparency of a closed-loop pHRI system for a given interaction controller structure. In particular, we propose a Pareto optimization framework that allows the designer to make informed decisions by studying the tradeoff between stability robustness and transparency thoroughly. The proposed framework involves a search over the discretized controller parameter space to compute the performance metrics. Using multi-criteria optimization, Pareto front curve depicting the tradeoff between stability robustness and transparency is obtained. Studying this tradeoff, an informed decision can be made to select the set of controller parameters that yield maximum attainable transparency and stability robustness. To demonstrate the practical use of the proposed design approach, integer and fractional order admittance controllers are studied as a case study and compared both analytically and experimentally. Experimental comparisons are performed for a pHRI task that involves contact interactions with an environment displaying nonlinear stiffness. The results validate the proposed design framework, and show that the achievable transparency under fractional order admittance controller is higher than that of integer order one, when both controllers are designed to ensure the same level of stability robustness."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım Tanımlı Ağ (SDN) modeli, denetim ve yönlendirme düzlemlerininin ayrılması özelliğiyle programlanabilir ağ elemanları, ağ yönetiminde esneklik ve verimlilik avantajlarına sahiptir. Enerji maliyetleri, ağlardaki genel maliyetlere büyük oranda katkıda bulunurken, enerji verimliliği modern ağ mekanizmaları için önemli bir tasarım gereksinimi haline gelmiştir. Bununla birlikte, enerji verimli çözümler tasarlarken enerji verimliliği ve ağ performansı arasındaki dengenin dikkate alınması önemlidir. Bu tez çalışmasında, enerji verimliliği için trafik farkında, makine öğrenmeye dayalı ve son sistem farkında SDN denetleyici modülleri önerilmektedir. Kontrol ünitesinin ilk modülü olarak, bağlantı kullanım aralıklarına göre enerji verimliliğini ölçen SDN Enerji Tasarrufu Oranı (RESDN) adlı yeni bir enerji verimliliği ölçütü önerilmektedir. Ağın RESDN değerini artırma amacıyla tamsayılı programlama formülasyonu ve yöntemi sunulmaktadır. RESDN yaklaşımı, bağlantıların harcadıkları enerji miktarı açısından karlı bir şekilde nasıl kullanıldığını ölçmesi yönüyle özgündür. Kontrol ünitesinin ikinci modülü olarak, enerjiyi verimli kullanan yönlendirme için yeni bir hibrit makine öğrenme tabanlı HyMER sistemi önerilmektedir. HyMER denetimli ve pekiştirici öğrenme modellerinin avantajlarını birleştirmektedir. Denetimli öğrenme bileşeni, özellik çıkarma, eğitim ve testlerden oluşur. Pekiştirici öğrenme bileşeni mevcut verilerden veya sıfırdan ağ ortamıyla yinelemeli olarak etkileşerek öğrenir. HyMER yaklaşımı, yazılım tanımlı ağlarda enerji verimliliği ve ağ performansı için bir karma makine öğrenme çözümü özelliğiyle yenilikçidir. Kontrol ünitesinin son sistem farkında modülünün bir parçası olarak, sunucuların ve ağ bileşenlerinin enerji verimliliği birlikte ele alınmaktadır. Fiziksel sunucuların, içerdikleri sanal makinelere göre enerjinin ne kadar verimli olduğunu ölçen, Fiziksel Makinelerin Enerji Tasarrufu Oranı (RESPM) adlı bir fiziksel sunucu fayda tabanlı ölçüt önerilmektedir. Deneyler, gerçek topolojiler ve trafik izleri kullanılarak POX kontrol ünitesi ve Mininet ağ emülatöründe gerçekleştirilmiştir. Çeşitli başarım ölçütleri ve farklı SDN özellikli anahtar türleri dikkate alınarak elde edilen sonuçlar RESDN değerini en üst düzeye çıkarmanın kabul edilebilir ağ başarımını sağlarken enerji verimliliğini artırdığını göstermektedir. En son teknolojiye dayalı fayda tabanlı sezgisel araştırmalara kıyasla RESDN yöntemi, enerji tasarrufu için %30'a varan oranlarda başarım, anahtarlama gücü başına 14,7 watt, %38 bağlantı tasarrufu, ortalama yol uzunluğunda 2 atlama azalması ve trafik orantılılığında %5'lik iyileştirme sağlamıştır. HyMER denetimli öğrenme bileşeni, %65 oranında özellik boyutu azalması ve parametre tahmininde %70'den fazla doğruluk sağlamaktadır. Arıtma sezgisel algoritması, 25X hızlanma ile tahminin doğruluğunu %100'e yükseltmiştir. Sonuçlar, HyMER yönteminin anahtar başına güç tasarrufu, bağlantı tasarrufu ve ortalama yol uzunluğunda azalma sağladığını göstermektedir.","Software Defined Networking (SDN) paradigm has the benefits of programmable network elements by separating the control and the forwarding planes, efficiency through optimized routing and flexibility in network management. As the energy costs contribute largely to the overall costs in networks, energy efficiency has become a significant design requirement for modern networking mechanisms. However, designing energy efficient solutions is non-trivial since they need to tackle the trade-off between energy efficiency and network performance. In this thesis, traffic aware, machine learning based, and end system aware SDN controller modules for energy efficiency are proposed. As the first module of the controller, we propose a novel energy efficiency metric named Ratio for Energy Saving in SDN (RESDN) that quantifies energy efficiency based on link utility intervals. We provide integer programming formulation and method for maximizing the RESDN of the network. To the best of our knowledge, RESDN approach is unique as it measures how links are profitably utilized in terms of the amount of energy they consume with respect to their utility. As the second module of the controller, we propose HyMER: a novel hybrid machine learning based framework for traffic aware energy efficient routing in SDN. The framework combines the advantages of supervised and reinforcement learning models. The supervised learning component consists of feature extraction, training, and testing. The reinforcement learning component learns from existing data or from scratch by iteratively interacting with the network environment. HyMER approach is the first that proposes a hybrid machine learning solution considering both energy efficiency and network performance in SDN. As part of the end system aware module of the controller, we jointly address the energy efficiency of servers and network components. We propose a physical server utility-based metric named Ratio for Energy Saving of Physical Machines (RESPM) which measures how energy efficient the physical servers with respect to virtual machines residing in them. Experiments are conducted on POX controller and Mininet network emulator using real topologies and traffic traces. Considering various metrics of interest and different types of SDN enabled switches, the results show that maximizing the RESDN value improves energy efficiency while maintaining acceptable network performance. In comparison to state-of-the-art utility-based heuristics, RESDN method performs up to 30% better ratio for energy saving, 14.7 watts per switch power saving, 38% link saving, 2 hops decrease in average path length and 5% improved traffic proportionality. The supervised component of HyMER achieves more than 65% feature size reduction and 70% accuracy in parameter prediction. The refine heuristics algorithm increases the accuracy of the prediction to 100% with 25X speedup as compared to the brute force method. Results show that HyMER achieves improvements in per switch power saving, link saving, and decrease in average path length."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mikroplar konak hücrelerde sayısız işlevi kontrol eder. Mikroplar, konakçı hücredeki sinyal mekanizmalarını değiştirebilir ve protein-protein ara yüzlerini taklit edip, konakçı proteinlerle etkileşime girerek bağışıklık sisteminin denetlenmesini modüle edebilir. Mikropların sağlığa ve hastalığa olan katkısına ışık tutmak için, mikrobiyal proteinlerin konak hücrelerdeki sinyal mekanizmalarını nasıl değiştirdiğini ve bu durumun hangi konak proteinlerini içerdiğini anlamak çok önemlidir. Mevcut konak-mikrop proteinlerinin etkileşimi hakkındaki veriler henüz çok yetersizdir ve bu etkileşimlerin tanımlanmasında deneysel yöntemlerin kullanımı oldukça zordur. Konak-mikrop etkileşimlerinin tahmini için kullanılan mevcut yöntemlerin çoğu, global hizalama veya yapısal benzerliğe dayanmaktadır. Öte yandan, bu yöntemlerin araştırmacılar tarafından kullanımını sınırlayan sebeplerden biri de bu amaç için kullanılan yalnızca bir web sunucusu olmasıdır. Her iki sorunu da çözmek için, kullanıcı dostu bir web sunucusu olan Host-Microbe Interaction PREDictor (HMI-PRED)'i geliştirdik. Bu web sunucusu konakçı (insan) ve herhangi bir mikrobiyal tür (bakteri, virüs, mantar ve protozoa) arasındaki protein-protein etkileşimlerinin şablon bazlı yapı tahmini için kullanılabilmektedir. HMI-PRED'in çalışma prensibi, mikrobiyal proteinlerin konakçı proteinlerin bağlanma yüzeylerini taklit etmesine dayanmaktadır. HMI-PRED sunucusu, tahminler için temel olarak kullanılan şablon ara yüz seti kümelenerek optimize edildi. HMI-PRED, ilgilenilen bir mikrobiyal proteinin 3B yapısı göz önüne alındığında, potansiyel konak-mikrop etkileşiminin detaylı 3B yapısal modellerini, mikrop proteini tarafından bozulabilen konak endojen ve eksojen protein-protein etkileşimlerinin listesini, mikrop hedefli konakçı proteinlerin fonksiyonel açıklamasını ve bu proteinlerin ekspresyonunun gerçekleştiği bütün dokuların listesini vermektedir. Buna ek olarak, sunucu, vurgulanmış etkileşim kalıntılarını, öngörülen yapıların 3B görselleştirmelerini ayrıca orijinal şablon ara yüzü ile üst üste gösterilen tahmini yapının şeklini sunmaktadır. Sunucu ayrıca, kullanıcıların mikrobiyal proteinlerin homoloji modellerini de girdi olarak kullanılmasına olanak tanır. Tahmini sonuçlar, kullanıcıların gelecek erişimleri için bir arşivde depolanır ve bu sayede kullanıcılar bu arşivdeki sonuçları inceleyip arayabilirler. HMI-PRED, https://interactome.ku.edu.tr/hmi adresinde halka açıktır. Ayrıca, öngörülen protein-protein ara yüzlerinin biyolojik geçerliliğini tanımlamak için derin öğrenme modeli kullanarak eğitilmiş bir sıralama yöntemi de tarafımızdan sunulmaktadır.","Microbes, commensals and pathogens, control numerous functions in host cells. They can alter host signaling and modulate immune surveillance by interacting with host proteins through mimicking the host protein-protein interfaces. To shed light on the contribution of microbes to health and disease, it is vital to discern how microbial proteins rewire host signaling and through which host proteins. Current host-microbe interaction data is a long way from complete, and experimental methods for large-scale identification of HMIs is challenging. Most of the currently available methods for HMI prediction are based on global sequences or structural similarity. On the other hand, there is only one available webserver for these methods, which limits the usage of these tools by the researches and scientists. To address both issues, we developed Host-Microbe Interaction PREDictor (HMI-PRED), a user-friendly webserver for template-based structural prediction of protein-protein interactions (PPIs) between host (i.e., human) and any microbial species, including bacteria, viruses, fungi, and protozoa. HMI-PRED relies on ""interface mimicry"" through which the microbial proteins hijack host binding surfaces. HMI-PRED server was optimized by clustering the template interface set, which is used as bases for predictions. Given the 3D structure of a microbial protein of interest, HMI-PRED will return detailed 3D structural models of potential host-microbe interaction (HMI) complexes, the list of host endogenous and exogenous PPIs that can be disrupted by the microbe protein, and the functional annotation and tissue expression of the microbe-targeted host proteins. Also, the server offers 3D visualizations of the predicted structures with highlighted contact residues, as well as visualization of the predicted structure superimposed on the original template interface. The server also allows users to upload homology models of microbial proteins. The prediction results are stored in a repository for the community access. Users can examine and search the accumulated results. HMI-PRED is available for public at https://interactome.ku.edu.tr/hmi. We also introduce a ranking method for the predicted interactions using a deep learning model based on 3D structures which is trained to identify valid protein-protein interfaces."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Stereoskopik ekranlar sabit bir odak düzlemine sahiptir ve uyum-yakınsama çatışması (UYÇ) olarak da bilinen gözün odak ve hareketlerinden kaynaklanan uyumsuzluktan dolayı görsel rahatsızlıklardan mustariptir. UYÇ olgusu, geleneksel yöntemlerle hazırlanmış kafaya-takılabilen ekranlarda (KTE) kaçınılmazdır. Bu tezde; gerçek zamanlı bakış izleyicisi ile donatılmış ve yeni bir deneyim sunan bi-oküler (iki göz için ortak optik elemanlar), iki odak-düzlemli bir artırılmış gerçeklik (AG) ekranı sunulmaktadır. İki ayrı sıvı kristal ekran (SKE) Fresnel lens ile farklı mesafelere yerleştirilerek sanal görüntülerin kullanıcıya göre 25 santimetre ve 50 santimetrede oluşması sağlanmıştır. İki SKE de iki göz ile tamamen görülebilmektedir. Sistem iki derinlik ile sınırlandırılmış olmasına ve bu durum sanal sahnede bir süreksizliğe yol açmasına rağmen, bahsi geçen derinliklerde doğru odak kanıtlarını ve doğal bulanıklık etkilerini sunmaktadır. Bu durum, farklı derinliklerde gösterilen sanal bilgilerin aynı görsel eksende olsalar bile kullanıcı tarafından ayırt edilebilmesini sağlamaktadır. SKEler sadece bir bilgisayar ile sürülebilmektedir ve sanal sahnedeki objeler derinliklerine göre bu ekranlar üzerine dağıtılmaktadır. Ayrıca, bir yol ve sürüş simülasyonu da uygun bir kullanım alanı olarak sunulmaktadır. Oyleki; büyük bir monitör, arka plan sahnesini yaratmak için kullanılmış ve SKElerdeki içerikler bu arka plan uzerine düşürülmüştür. Görüş alanı (GA), 60 x 36 derecedir ve kullanıcı pozisyonunun değişimine rağmen sanal sahnenin görüntülenebilmesini sağlayan alan 100 milimetreden fazladır. Böylece, iki göz için konforlu bir sistem elde edilmiştir. Sistem, iki gözü de görüntüleyebilen tek bir kamera ve görüntü işleme teknikleri ile uygulanmış bir bakış izleyicisi içerir. Farklı kullanıcılar ile test edilmiş bu bakış izleyicisi, göz bebeği merkezleri arasındaki mesafeyi hesaplayarak kullanıcının baktığı derinlik düzlemini herhangi bir kızıl-ötesi aydınlatmaya ihtiyaç duymadan hesaplayabilmektedir. Oluşturulan içerik, iki farklı derinlik düzlemine ve arka plana eş zamanlı olarak dağıtılmaktadır. Böylece, kullanıcı gözlerini kullanarak farklı derinlik düzlemlerindeki içerik ile etkileşime konforlu bir şekilde girebilmektedir. Sunulan sistem prototipi, geniş görüş alanı ve çoklu odak düzlemi gerektiren uygulamalarda veya bir AG araştırma aracı olarak kullanılabilir.","Stereoscopic displays have a fixed focus plane and they suffer from visual discomfort due to the mismatch between the focus and vergence of the eyes, known as the vergence-accommodation conflict (VAC). VAC is unavoidable in conventional stereoscopic head-mounted displays (HMDs). In this thesis research, I proposed a biocular (i.e, common optics for two eyes), two focal-plane based augmented reality (AR) system with real-time gaze tracker, which provides a novel interactive experience. Two separate liquid crystal displays (LCDs) are placed at slightly different distances to a Fresnel relay lens such that virtual images of LCDs appear at 25 cm and 50 cm to the user. Both LCDs are totally viewed by both eyes. While the system is limited to two depths and discontinuity occurs in the virtual scene, it provides correct focus cues and natural blur effect at the corresponding depths. This allows the user to distinguish virtual information through the accommodative response of the eye, even when the virtual objects overlap and partially occlude in the axial direction. Displays are driven by a single computer and the objects in the virtual scene are distributed over the LCDs according to their depths. Furthermore, a road scene simulation is realized as a convenient use-case of the proposed display so that a large monitor is used to create a background scene and the rendered content in the LCDs is augmented into the background. Field-of-view (FOV) is 60 x 36 degrees and the eye-box is larger than 100 mm, which is comfortable enough for two-eye viewing. The system includes a pupil and gaze tracker, which is implemented with a single camera and using computer vision algorithms. The gaze tracker, which is experimented on different users, is able to select the correct depth plane based on the shift in the interpupillary distance when the convergence angle of the user's eyes changes. The rendered content can be distributed to both depth planes and the background scene simultaneously. Thus, the user can select and interact with the content at the correct depth in a natural and comfortable way. The prototype system can be used in tasks that demand wide FOV and multiple focal planes and as an AR and vision research tool."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birçok biyolojik işlemin temelini protein-protein etkileşimleri (PPE) oluşturur. PPElerin biyolojik aktivitelerin yerine getirilmesindeki önemi yeni PPElerin tanımlanmasını önemli kılar. Hataya karşı dayanıklı ve güvenilir bilimsel araçlar yeni etkileşimlerin bulunmasını kolaylaştırmak için kullanılabilir. Bilgisayar üzerinden yeni etkileşimlerin bulunmasında en büyük zorluklardan biri arayüzlerden oluşan bir veri kümesinde verilen protein yapısına en uygun arayüzü bulmaktır. Bu tezde, bir veri kümesindeki arayüzleri verilen proteine olan yapısal benzerliklerine göre sıralayan QuickRet algoritmasını sunuyorum. Algoritma arayüzlerden çıkartılan yapısal karakteristik özellikleri verilen proteinden çıkartılan yapısal karakteristik özelliklerle karşılaştırır. QuickRet'i yapısal özelliklerine göre kümelenmiş arayüzlerden oluşan veritabani PIFACE'si ve örnek arayüzleri kullanarak PPE tahmini yapan algoritma PRISM'in tahminlerini kullanarak test ettim. Sonuçlar QuickRet'in verilen proteinler için yapısal olarak benzer arayüzler bulmakta başarılı olduğunu gösteriyor. PIFACE veri kümesinde, QuickRet verilen 166 kümesi üyesinden 56'si için küme temsilcilerini 1. sırada veriyor. PRISM tahminleri içinse, verilen 80 proteinden 56'si için PRISM tarafından tahmin edilen arayüz yapısını %50'lik dilimin içerisinde sıralıyor. QuickRet'e ek olarak, arayüz bölgesini tahmin eden bir derin öğrenme modeli sunuyorum. Model, 3 boyutlu konvolüsyon ağı, verilen yapıyı analiz eder ve verilen yapının arayüz olma olasılığını hesaplar. Model PIFACE, PPI4DOCK, DOCKGROUND'un içinde bulunduğu veri kümelerindeki arayüz yapılarını %80 doğruluk ile tahmin etti.","Protein-protein interactions (PPIs) form the basis of many biological processes in living organisms. The significance of PPIs in mediating biological activity necessitates the identification of novel interactions. Template based structural alignment is one of the computational approaches to predict protein-protein interactions using known protein interfaces. One challenge in template-based prediction is the computational cost due to the one-to-all comparison of the query protein against a database of all known interfaces. In this thesis, two different approaches have been developed a) QuickRet, a hashing based algorithm, b) and a deep learning based algorithm. QuickRet, a fast screening algorithm, ranks interfaces due to their structural similarity to a query protein. It extracts features (angles and distances derived from four atoms) from structures of interfaces and compares them with the features extracted from the query protein. QuickRet is tested with the PIFACE database, a clustered protein-protein interface database, and predictions made by the template interface based PPI prediction algorithm, PRISM. The results indicate that QuickRet is successful in filtering structurally dissimilar interfaces for a given protein. With at least 80% match, 99% (320/43500 interface structures remained) of the database is eliminated and the average RMSD value of the remaining structures is 2.4 Å. With at least 90% match, 99.9% (50/43500 structures remained) of the database is eliminated and the average RMSD value drops to 2.28 Å. In addition, a deep learning based method which predicts, for a given protein complex, if the interface between the proteins of the complex is a true interface or not (based on known interfaces in Protein Data Bank). The model, a 3-dimensional convolutional model, analyzes the given structure and outputs the probability of the given structure being an interface. The accuracy of the model for several interface data sets, including PIFACE, PPI4DOCK, DOCKGROUND is approximately 80%. Both algorithms can be used to reduce the computational cost of template-based PPI predictions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Elle çizilmiş nesneler genellikle çok sayıda anlamlı alt kısımdan oluşur. Örneğin bir çöp adam; bir kafa, bir gövde ve bacak ile kol çiftlerinden oluşur. Elle çizilmiş bir sembolü bu alt parçalara bölme işlemine sembol segmentasyonu denir. Diğer yandan, alt parçaları çizerek bir anlamlı sembole dönüştürme sürecine sembol üretimi denir. Bu iki prosedür birbiriyle ilişkili olmasına rağmen, önceki tüm çabalar, ya segmentasyon ya da üretim problemi üzerine odaklanmıştır. Bu tezde, StrokeRNN adlı Değişimsel Oto Kodlayıcı (DOK) mimarisine dayanan çizim üretme modelini öneriyoruz ve bu modeli, çizimdeki anlamlı alt parçaları tanımaya yönelik kullanışlı hale getiriyoruz. Mevcut generatif sistemler elle çizilmiş tam objeleri modellerken, StrokeRNN ise onların bileşen çizgilerini modeller. Bu nedenle, önceki modellerden farklı olarak StrokeRNN, tek bir sınıf ile eğitildiği takdirde bile, birçok değişik kategoriden çizimler üretebilir. Çizimleri alt parçalara bölmek için halihazır tekniklerde kullanılan piksel resim formatı yerine daha az hafızaya ihtiyaç duyan vektör formatını kullanıyoruz. Segmentasyon modelimiz, StrokeRNN tarafından oluşturulan kodlamaya dayanarak çizgileri sınıflandıran basit ama güçlü bir yapay sinir ağıdır. Mevcut veri kümesiyle yapılan deneylerde, bizim segmentasyon modelimizin var olan metodolojiye göre daha iyi bir başarı sergiliyor. Üstelik, alt parçalarını yeni etiketlediğimiz çizim veri setimizde yapılan geniş çaplı değerlendirmeler, yapay sinir ağımızın en iyi referans modele kıyasla daha iyi bir performans sergilediğini gösteriyor. Etikelenmiş veri kümemizi kamuya açıyoruz.","Hand-drawn objects usually consist of multiple semantically meaningful parts. For example, a stick figure consists of a head, a torso, and pairs of legs and arms. The process of breaking a hand-drawn symbol into those subparts is called symbol segmentation. On the other hand, the process of drawing subparts and unifying them into a single entity is called symbol generation. Despite the fact that these two procedures are interrelated, all previous endeavors focused on a single task, either segmentation or generation. In this thesis, we propose a StrokeRNN that is a generative model based on a Variational Auto-Encoder (VAE) architecture and we extend it to recognize semantically meaningful components. While existing generative systems model complete hand-drawn objects, the StrokeRNN models their constituent strokes. Hence, in contrast to prior frameworks, the StrokeRNN is capable of drawing multiple symbol categories even when trained on a single class. To segment drawings, we adopt a memory efficient vector representation of symbols instead of the raster image format used by existing techniques. Our segmentation model is simple yet powerful neural network, it classifies stroke-level components based on the encoding generated by the StrokeRNN. Experiments show that our segmentation accuracies surpass exist- ing methodologies on the available state of the art dataset. Furthermore, extensive evaluations on our newly annotated dataset demonstrate that our neural network obtains significantly better scores as compared to the best baseline model. We release our dataset to the community."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hastalıklar genellikle çok sayıda gen ve bu genlerin ürünü olan proteinlerin diğer hücresel bileşenlerle işbirliğiyle oluşturduğu karmaşık etkileşim mekanizmalarında meydana gelen bozulmalar sonucu gelişir. Protein – protein etkileşimlerini hem etkileşim ağı seviyesinde hem de mutasyon bilgisi ile birlikte moleküler seviyede incelemek ve yorumlamak, farklı kaynaklardan beslenen kapsamlı bir araştırma süreci gerektirir. Bu tezde, geniş ölçekli protein-protein etkileşim ağını, üç boyutlu protein yapısı bilgilerini ve etkileşim ara yüzlerinde görülen mutasyon bilgisini entegre ederek, araştırmacılara farklı fenotiplerin moleküler mekanizmalarını keşfetme ve diğer fenotiplerle karşılaştırmalarında yardımcı olacak web tabanlı bir araç olan Gene2Phen'i geliştirdik. Gene2Phen, protein-protein etkileşimlerini yapılarına ve mutasyon verilerine bağlı olarak inceleyebilmek için geliştirilmiş olup fenotipe özgü alt ağların oluşturulması, görselleştirilmesi ve karşılaştırılması için otomatikleştirilmiş bir iletişim hattı işlevi görür. Gene2Phen web aracı, bir fenotipe özgü tohum genlerine dayalı olarak insan protein-protein etkileşim ağını önceliklendirir. Kullanıcılar, önceliklendirilmiş etkileşim ağından fenotipe özgü bir alt ağ oluşturabilirler. Fenotipe özgü alt ağlar görselleştirilebilir ve interaktif olarak karşılaştırılabilir. Bu interaktif ağ gösteriminde her protein, genom ek açıklamaları ve topolojik özellikleri ile birlikte gösterilir. Gene2Phen'i eşsiz kılan bir özelliği, protein-protein etkileşimlerinin üç boyutlu yapısal modellerini ve öngörülen protein-protein ara yüzlerini görüntüleme yeteneğidir. Kullanıcılar, tahmin edilen protein - protein ara yüzleri üzerine eşlenmiş olan mutasyonların listesini görebilirler. Bu özellik kullanıcıların protein - protein ara yüzlerini değiştiren mutasyonları ve bu mutasyonların fenotipe özgü alt ağlardaki yerlerini öğrenebilmelerini sağlar. Gene2Phen, protein-protein ağlarının, protein yapısının ve hastalıkla ilgili mutasyonların büyük ölçekte entegrasyonunu otomatikleştirerek yalnızca verimliliği ve etkililiği artırmakla kalmayıp yeni çözümler ve araştırmalar için manivela gücü sağlayabilir.","Diseases are commonly the result of dysregulated complex interactions involving large sets of genes and proteins as products of these genes, and their cooperation with other cellular components. Interpreting protein-protein interactions at both network and molecular interaction levels with mutation knowledge requires a comprehensive research process that is fed from different sources. In this thesis, we developed a web-based tool, Gene2Phen, by integrating large-scale protein-protein interaction network, 3D protein structure information and interface mutation knowledge to aid researchers in exploring and comparing the molecular mechanism of different phenotypes. Gene2Phen works as an automatized pipeline tool to build, visualize and compare phenotype specific subnetworks, to examine protein- protein interactions associated with their structure and mutation data. Gene2Phen web tool prioritizes the human protein-protein network based on seed genes specific to a phenotype. From the prioritized-PPI network, users can generate a phenotype specific subnetwork. The phenotype-specific subnetworks can be visualized and compared interactively. Genome annotations and topological properties of each protein are shown in this interactive network representation. A unique feature of Gene2Phen is its ability to display 3D structural models of protein-protein interactions and their predicted protein-protein interfaces. Users can see the list of mutations which are mapped on predicted protein-protein interfaces. This allows users to study mutations altering protein-protein interfaces and their role in the phenotype-specific subnetworks. Gene2Phen, by automating the integration of protein-protein networks, protein structure, and disease - related mutations at large scale, will not only boost the productivity and efficiency, but it may be the leveraging step to the novel solutions/studies."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uyarlanabilir Izgara Geliştirimi (Adaptive Mesh Refinement - AMR) belirli uygulamalar için hesaplama ve bellek maliyetini düşüren, kısmi ayrışık denklemlerin çözümü için geliştirilen bir yaklaşımdır. AMR, hesaplama alanını, ardışık olarak rafine edilmiş ızgaraların bir hiyerarşisi olarak temsil eder ve hiyerarşide hem iç hem de dış iletişimi ele alır. Basit bir AMR algoritması bir zaman adımı içerisinde bile, iletişim yönünden zaten pahalı olan pek çok eşzamanlama noktasını içerir. Bu problem, iletişim maliyetini daha da artıran milyar derecede paralleliğe sahip exascale süper bilgisayarlarda daha da belirgin hale geliyor. Asenkron yürütme aracılığıyla haberleşmeyi hesaplama ile örtüştürmek, iletişim maliyetini azaltmak için yaygın olarak kullanılan bir tekniktir. Senkronizasyondan kaçınmak için AMR algoritmalarını yeniden tasarlamak ve bir AMR uygulamasında iletişimin ve hesaplamanın örtüşmesini sağlamak için uygulamayı yeniden yapılandırmak fazlasıyla karmaşıktır. Bunlar, geniş kod büyüklüğünden ve karmaşık kontrol yapılarından dolayı yazılım geliştirmeyi ve yazılım bakım maliyetlerini daha da artırır. AMR uygulamalarında haberleşmenin etkilerini gizlemek için kesintisiz (nonintrusive) asenkron bir yaklaşım sunmaktayız. Yaklaşımımız, AMR uygulamaları ile ilgili alan bilgisini kullanarak veri bağımlılıklarını otomatik olarak algılar ve makul miktarda kod değişikliği ile asenkronizasyonu sağlar. Bu yaklaşımı kullanarak, faz asenkron bir AMR algoritması önermekte ve onu yaygın kullanılan AMReX adında AMR çerçevesine dahil etmekteyiz. Tüm haberleşme bitene kadar hesaplamanın geciktirildiği senkron algoritmaların aksine, faz asenkron algoritmasında, bağımlı haberleşme tamamlandığında AMR seviyesindeki bir alt ızgarada hesaplama yapılır. Bununla birlikte bir AMR seviyesindeki tüm alt ızgaraların hesaplanması bir sonraki AMR seviyesindeki hesaplama başlamadan önce tamamlanır. Faz asenkron AMR algoritmamız, hem senkron bir yaklaşımın verimliliğine sahiptir hem de tam asenkron uygulama performansını elde etmeyi amaçlamaktadır. AMR uygulamaları için önerilen faz asenkron yürütmeye olanak sağlayan yürütme sistemi gerçekleştirdik. Yürütme sistemi hiyerarşideki hem iç hem de dış haberleşmenin hesaplama ile örtüşmesine olanak vermektedir. Yürütme sistemi hem iç hem de dış haberleşmeyi gerçekleştiren haberleşme denetçilerini (communication handlers) içermektedir. Uygulamalar haberleşme verisini yürütme sisteminin haberleşme denetçisine devretmektedir. Bir ızgara için haberleşme bağımlılıkları tamamlanır tamamlanmaz, yürütme sistemi o ızgaranın hesaplanmasını planlamaktadır. Büyük köklü kodların senkronize olmayan yürütmeden senkronize yürütmeye geçişini kolaylaştıran, faz asenkron yürütme sistemi için uygulama programı arabirimini (UPA) dikkatli bir şekilde tasarladık. Yeni asenkron AMReX çerçevemizi tanıtmak için gerçek-dünya kodu olan, astrofizik akışları için çok bileşenli sıkıştırılabilir hidrodinamik denklemleri çözen CASTRO kodunu bir vaka çalışması olarak ele aldık. CASTRO'nun dönüşüm stratejisini tartıştık ve yeni UPA'mızı kullanmak için gereken programlama çabasını ve performansı değerlendirdik. Yaklaşımımız, uygulama programcısının verimliliğini ciddi biçimde etkilemeden okunurluğu ve uzun vadeli sürdürülebilirliği korurken, performans gerekliliklerini de yerine getirmektedir. Süper bilgisayar mimarileri heterojenliğe doğru yönelmektedir. Şuanda 500 süper bilgisayardan 100 tanesi hızlandırıcı kullanmaktadır ve bu sayı ilk 10 içerisindeki beş makineyi de kapsamaktadır. Bu nedenle heterojen mimariler için destekten yoksun olan programlama modellerinin bu makinelere uyarlanma ihtimali düşüktür. Heterojen mimariler üzerinde yürütmeyi desteklemek için yürütme sistemimizi genişlettik. Programlama modelimizin ve yürütme sistemimizin, AMR uygulamaları heterojen mimariler üzerine taşındığında ortaya çıkan güçlüklerle nasıl başa çıktığını göstermekteyiz. Yürütme sistemi, verimliliği korurken tüm hesaplama kaynaklarının etkin kullanımı için eşzamanlı olarak hem CPU'larda hem de GPU'larda hesaplama yapar. Performans çalışmalarında, küçük bir ısı adveksiyon uygulamasını ve geniş bir gerçek dünya üretim kodu olan CASTRO'yu kullanmaktayız. Intel Haswell ve Intel Xeon Phi (Knights Landing- KNL) kullanan, Hazel Hen ve Cori süper bilgisayarlarındaki performansı gösterdik. Makul bir programlama çalışmasıyla, Intel Haswell üzerinde 49K çekirdek ve KNL üzerinde 278,528 çekirdek kullanarak yüzde 50 daha iyi performans almayı başardık. Ayrıca yürütme sisteminde heterojen mimari desteğin performans analizini SummitDev'de ısı tutma kodunu kullanarak yaptık. SummitDev, IBM Power8 CPU'lar ve NVIDIA Tesla P100 GPU'lar ile donatılmış heterojen bir mimariye sahiptir.","Adaptive Mesh Refinement (AMR) is an approach to solving partial differential equations that reduces the computational and memory cost for certain applications. AMR represents the computational domain as a hierarchy of successively refined grids, introducing both intra- and inter-level communications in the hierarchy. A straight-forward AMR algorithm typically exhibits many synchronization points even during a single time step, where costly communication often degrades the performance. This problem will be even more pronounced on exascale supercomputers containing billion way parallelism, which will raise the communication cost further. One common technique to reduce the communication overhead is to overlap communication with computation through asynchronous execution. Re-designing AMR algorithms to avoid synchronization and manually restructuring an AMR application to realize communication overlap is extremely complicated and increases software development and maintenance costs due to the large code size and complex control structures. We present a nonintrusive asynchronous approach to hide the effects of communication in an AMR application. Specifically, our approach reasons about data dependencies automatically using domain knowledge about AMR applications, allowing asynchrony to be discovered with only a modest amount of code modification. Using this approach, we proposed a phase asynchronous AMR algorithm and incorporated it in a widely used AMR framework, AMReX. Unlike synchronous algorithm where computation is delayed until all communication is finished, in phase asynchronous algorithm, a subgrid at an AMR level is scheduled for computation as soon as its dependent communication is completed. However, computation of all subgrids at an AMR level are completed before starting computation on a next AMR level. Our phase asynchronous AMR algorithm aims to achieve the performance of a fully asynchronous execution while retaining the productivity of a synchronous approach. We implemented a runtime system to facilitate the proposed phase asynchronous execution for AMR application. The runtime enables overlapping of both intra and inter-level communication with the computation within a level. Runtimes include communication handlers that perform both intra and inter-node communication. Applications hand over the communication data to the runtime's communication handler. As soon as communication dependencies for a subgrid are completed, the runtime schedules that subgrid for computation. We carefully designed an API for phase asynchronous execution model to facilitate the transition of large legacy codes from synchronous to asynchronous execution. As a case study, we pick the CASTRO code, which solves multicomponent compressible hydrodynamic equations for astrophysical flows, as a real-world code to demonstrate our new asynchronous AMReX framework. We discuss the transformation strategy of CASTRO and evaluate the performance and programming effort required to use our new API. Our approach achieves the performance requirements while retaining readability and long-term maintainability without severely affecting the productivity of the application programmer. Supercomputer architectures are trending towards heterogeneity. At the moment, 110 out of top 500 supercomputers uses accelerators this number includes five machines from the top 10. Thus programming models without support for heterogeneous architectures are not likely to be adopted on these machines. We extend our runtime to support execution on heterogeneous architectures. We present how our programming model and runtime system deal with the challenges raised when porting AMR application on heterogeneous architectures. The runtime schedules computation on both CPUs and GPUs concurrently to effectively use of all computational resources while maintaining the productivity. For performance studies, we use a small heat advection application and a large real-world production code, CASTRO. We demonstrate performance on Hazel Hen and Cori supercomputers; on the Intel Haswell cores and on the Intel Xeon Phi (Knights Landing- KNL). With reasonable programming effort we were able to achieve up to 50% performance improvements using 49K cores on Intel Haswell and 278,528 cores on KNL. We also carried out performance analysis of the heterogeneous architecture support in the runtime using the heat advection code on SummitDev. SummitDev is a heterogeneous architecture machine equipped with IBM Power8 CPUs and NVIDIA Tesla P100 GPUs."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mevcut ¸cok ¸cekirdekli sistemler ¸cok sayıda i¸slemci ¸cekirde˘gine sahiptir ve yakın zamanda kullanılmaya ba¸slanacak olan exascale sistemlerde ¸cekirdek sayısının daha da artması beklenmektedir. Paralel i¸sleri i¸slemcilere yerle¸stirmek ¸cip i¸cindeki islemciler arasındaki ileti¸sim miktarını azalttı˘gından ¸cok ¸cekirdekli makinelerde performansı arttırıcı bir etkendir. Ayrıca bu y¨ontem i¸sletim sisteminin i¸s par¸cacıklarının g¨o¸c etmesini ¨onleyerek veri yerelli˘ginin iyile¸stirilmesi de sa˘glamaktadır. Ancak, farklı uygulamalar ve makineler arasında en iyi performansı sa˘glayacak tek bir paralel i¸sleri i¸slemcilere yerle¸stirme algoritması bulunmamakta ¸c¨unk¨u her makinenin farklı bir topolojisi bulunmakta ve her uygulama farklı bir ileti¸sim ¨or¨unt¨us¨u sergilemektedir. Bir uygulama veya bir makine i¸cin en uygun algoritmayı belirlemek, fazladan programlama eforu gerektirmektedir. Yazılımcının bu y¨uk¨un¨u ortadan kaldırmak i¸cin, uygulama ¨or¨unt¨us¨une ve makine topolojisine ¨ozel e¸sleme algoritması ¨uretecek otomatik bir i¸s atama aracının gereklili˘gini savunmaktayız. Bu ama¸c do˘grultusunda bu tezde yazılımcıya i¸s par¸cacıklarını donanıma atamasında yardımcı olacak i¸s par¸cacı˘gı atama k¨ut¨uphanesini, BindMe aracını, sunuyoruz. BindMe, etkili bir i¸s yerle¸stirme ilkesini form¨ulize etmek i¸cin uygulamadaki ileti¸sim ¨or¨unt¨ulerini kullanır ve en yeni e¸sleme algoritmalarını b¨unyesinde bulundurmaktadır. Buna ilave olarak, paralel i¸s par¸cacıklarının ortak ¨onceliklerini de˘gerlendirerek e¸sleme dizisi ¨ureten ileti¸sim bilin¸cine dayalı bir e¸sleme algoritması olan ChoiceMap'i sunuyoruz. ChoiceMap, ¸cekirdekler arası ileti¸sim hacmini azaltarak dengeli bir e¸sleme ger¸cekle¸stirmektedir. Algoritma hem da˘gıtık, hem payla¸sımlı sistemlerde kullanılır. BindMe, b¨unyesinde ChoiceMap algoritmasını da e¸sleme se¸ceneklerinden biri olarak bulundurmaktadır. BindMe aracını, NAS paralel benchmark kapsamındaki bir ¸cok uygulamayla de˘gerlendirdik. Bulgularımız g¨osteriyor ki, uygulama ¨or¨unt¨ulerine en iyi uyacak e¸sleme ilkesini se¸cmek, uygulamanın verimini arttırmakta ve tek bir ¸ce¸sit e¸sleme algoritması farklı uygulamalarda en iyi performansı sa˘glayamamaktadır.","Current multicore machines have a large number of cores and the number of cores is expected to increase in upcoming exascale multicore machines. Binding parallel tasks to cores according to a placement policy is one of the key aspects to achieve good performance in multicore machines because it can reduce on-chip communication among parallel threads. Binding also prevents operating system from migrating threads, which improves data locality. However, there is no single mapping policy that works best among all different kinds of applications and machines because each machine has a different topology and each application exhibits different communication pattern. Determining the best policy for a given application and for a given machine requires extra programming effort. To relieve the programmer from that burden, we argue the need for an automated task binding tool that generates mapping policy specific to the machine topology and application behaviour. We present BindMe, a thread binding library, that assists programmer to bind threads to underlying hardware. BindMe incorporates state-of-the- art mapping algorithms which use communication pattern in an application to formulate an efficient task placement policy. We also introduce ChoiceMap, a communication aware mapping algorithm that generates a mapping sequence by respecting mutual priorities of parallel tasks. ChoiceMap performs a fair mapping by reducing communication volume among cores. The algorithm can be used both in shared memory and distributed memory systems. ChoiceMap is incorporated in BindMe and can be used as one of the mapping options. We have tested BindMe with various applications from NAS parallel benchmark. Our results show that choosing a mapping policy that best suits the application behavior can increase its performance and no single policy gives the best performance across different applications."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde bir dil modelinden tu ̈retilen bir kelimenin sol ve sag ̆ bag ̆lamını temsil eden vekto ̈leri tanıtıyorum. Bu vekto ̈rlerin faydasını c ̧ok katmanlı algılayıcı (MLP) bag ̆lılık ayrıs ̧tırıcı model ile test ettim. 2017 Evrensel Bag ̆lılık Analizi yarıs ̧masında 33 katılımcı arasından 7. olan bu model, sag ̆/sol bag ̆lam vekto ̈rlerinin bag ̆lılık ayrıs ̧tırmasında kul- lanıldıg ̆ındaayrıs ̧tırmanındog ̆rulug ̆unuo ̈nemlio ̈lc ̧u ̈dearttırdıg ̆ınıgo ̈steriyor. Butezde daha detaylı anlatacag ̆m c ̧ok katmanlı ayrıs ̧tırıcı model bir takım el-yapımı o ̈zelliklerden de faylandıg ̆ında en iyi performansı vermektedir. Derin o ̈g ̆renmedeki son gelis ̧melerden fay- dalanarak el-yapımı o ̈zellik belirlemek yerine bunu otomatikles ̧tiren bir model gelis ̧tirdim. Bu model yine bu tezde tanıttıg ̆ım bic ̧imsel o ̈zellik vekto ̈rleri ile birles ̧tig ̆inde c ̧ok katmanlı ayrıs ̧tırıcı modelden daha yu ̈ksek bas ̧arım elde etti. Bu model ile 2018 Evrensel Bag ̆lılık Analizi yarıs ̧masına katıldım. Modelim 30 farklı sistem ic ̧erisinden 16. oldu.","I introduce word and context embeddings derived from a language model representing left/right context of a word instance and demonstrate that context embeddings significantly improve the accuracy of transition based parser. Our multi-layer perceptron (MLP) parser making use of these embeddings was ranked 7th out of 33 participants (ranked 1st among transition based parsers) in CoNLL 2017 UD Shared Task. However MLP parser relies on additional hand-crafted features which are used to summarize sequential information. I ex- ploit recurrent neural networks to remove these features by implementing tree-stack LSTM, and develop new set of continuous embeddings called morphological feature embeddings. According to official comparison results in CoNLL 2018 UD Shared Task, our tree-stack LSTM outperforms MLP in transition based dependency parsing."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Anahtar-değer veritabanlarından diğer bulut servislerine kadar olan bir çok dağıtık sistem uygulaması hata dayanıklılığı ve güvenilirlik özelliklerini sağlamak zorundadır. Hızlı kurtarma ve kesintisiz geçiş gibi özellikleri sayesinde, birincil-yedek replikasyon protokolü dağıtık veritabanları, web servisleri ve nesnelerin interneti gibi farklı alanlarda sıklıkla kullanılmaktadır. Bu tez kapsamında, birincil-yedek replikasyon protokolünün yüksek iletişim maliyeti göz önünde bulundurularak, etkinliği ve performansı arttırmak amaçlı denetim noktası alma yöntemine odaklanmaktayız. Bunu takiben, Facebook tarafından açık-kaynak olarak geliştirilmekte olan RocksDB anahtardeğer veritabanınını kullanarak özgün bir sistem önermekteyiz. Geliştirilen bu sistemi PlanetLab test ağında coğrafi olarak replike ederek, periyodik olmayan, periyodik, artımlı ve sıkıştırılmış denetim noktası alma gibi farklı yöntemleri test etmekteyiz. Farklı test senaryolarını, bu iş için sıklıkla tercih edilen YCSB test aracını kullanarak gerçekçi iş yükleri dahilinde değerlendirmekteyiz. Bekletme süresi, denetim noktası alma süresi, denetim noktası boyutu, sıkıştırma oranı ve sistem başarımı gibi farklı ölçüm noktalarını ve gerçekçi iş yüklerini kullanarak yapılan çalışmalarda periyodik artımlı denetim noktası yöntemini kullanmanın geleneksel birincil-yedek replikasyon protokolüne göre ve diğer denetim noktası alma yöntemlerine göre en iyi sistem başarımı ve en düşük bekletme süresi yakalayabildiği gösterilmiştir. Bulgularımıza dayanarak, geniş ölçekli birincil-yedek replikasyon uygulamaları için yükbilinçli sıkıştırılmış artımlı (LACPB) denetim noktasını önermekteyiz. Geniş ölçekli ve karşılaştırmalı test sonuçları, LACPB yönteminin oldukça yüksek sistem başarımı, ayrıca düşük ve istikrarlı bekletme sürelerini hareketli iş yükleri için de sağladığı gösterilmiştir.","Several distributed services ranging from key-value stores to cloud storages require fault-tolerance and reliability features. For enabling fast recovery and seamless transition, primary-backup replication protocols are widely used in different application settings including distributed databases, web services and the Internet of Things. In this thesis, we address the communication cost of the primary-backup replication protocol, and propose utilizing the checkpointing concept for improving the efficiency and performance of primary-backup replication. We then develop a software framework by extending the open-source RocksDB key-value store of Facebook on the PlanetLab overlay network using a geographically replicated system setup, and evaluate various checkpointing algorithms including non-periodic, periodic, incremental and compressed checkpointing. Experimental scenarios utilize the well-known benchmarking tool YCSB to generate realistic query workloads. Using various metrics of interest including blocking time, checkpointing time, checkpoint size, compression ratio and throughput, and testing with realistic workloads, our findings indicate that incremental checkpointing combined with a periodic usage performs the best by providing better system throughput and decrease in average blocking times in comparison to the traditional primary-backup replication and other checkpointing algorithms. Based on our findings, we propose load-aware compressed incremental checkpointing method (LACPB) for large scale primary-backup replication. Large-scale and comparative experimental results indicate that LACPB maintains drastically higher system throughput as well as reduced and stable client blocking times even in the dynamic workload scenarios."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir kontrol sisteminin H ∞ -normu, sistemin girdi uzayını çıktı uzayına gönderen oper- atörün normundan daha fazlasını ifade eder; bu norm çoğu durumda sistemin güçlü kararlılığı ile ilgili bir ölçüdür. Mesela zaman içinde değişmeyen standart bir lineer sistem için bir nevi şekilli kararlılık yarıçapının tersine karşılık gelir. H ∞ -normu cinsin- den bu tip ifadeler, gecikmeli sistemlerin de dahil olduğu, diğer birçok kontrol sistem- leri için de geçerli. Bu tez çalışmasınin ilk kısmı büyük çaplı sistemler için H ∞ -normunun hesabını ilgilendiriyor. Bu doğrultuda model mertebesini indirgemek için kullanılan araçlardan faydalanarak bir alt uzay yöntem çıkarımı yapıyoruz. Özellikle durum uzayını ufak bir alt uzaya sınırlıyoruz, ve de Petrov-Galerkin koşullarının sistemin diferensiyel kısmı tarafından ikinci bir ufak alt uzay üzerinde sağlanmasını şart koşuyoruz. Sonuç olarak elde edilen ufak çaplı problemi varolan yöntemlerle çözüyoruz. Akabinde iki ufak alt uzay, transfer fonksiyonunun sanal eksen üzerinde belirli noktalardaki tekillik vek- törlerinin bu uzaylara eklenmesi ile büyütülüyor. Geliştirdiğimiz alt uzay büyütme yönteminin alt uzay boyutuna bağlı olarak süper-lineer hızda bir yakınsamaya yol açtığını kanıtlıyoruz. Bu, yöntemin hesaplama zamanının, varolan en iyi yöntemlerin hesaplama zamanlarına göre ciddi mertebede, mesela on defa ya da daha fazla, kısa olması ile sonuçlanıyor. Tezin ikinci kısmı, ilk kısımdaki fikirleri parametrelere bağlı büyük çaplı tanım- layıcı bir sistemin H ∞ -normunun kabul edilebilir parametre değerleri kümesi üz- erinde minimizayonu çerçevesine taşıyor. İlk kısımda olduğu gibi koşul uzayı ufak bir alt uzaya sınırlanıyor, ve ikinci bir alt uzay üzerinde Petrov-Galerkin koşulları şart koşuluyor. Bu ikinci kısımda ufak çaplı problemler, aynı parametre değerleri kümesi üzerinde, ufak mertebeli tanımlayıcı sistemlerinin H ∞ -normlarının minimiza-syonunu gerekli kılıyor. Yine süper-lineer hızda yakınsamaya yol açan bir alt uzay büyütme stratejisi tasarlıyoruz. H ∞ -normunun minimizasyonu hesabına göre ek zor- luklar içeriyor, çünkü bu minimum-maksimum şeklinde bir optimizasyon problemi; mesela süper-lineer yakınsama hızı kanıtı içteki problemi olabildiğince büyük yapan değişken değerini dolaylı olarak parametrelerin bir fonksiyonu olarak ele alıyor. Son kısımda dikkatimizi, port-Hamilton (PH) sistemleri olarak bilinen, özel bir yapıya sahip bir takım tanımlayıcı sisteme çeviriyoruz. Bu tip sistemler mesela devre simulasyonu modellerinde ya da ses çıkaran fren probleminde ortaya çıkıyor. Bu PH sistemleri için tanımlı özel bir kararlılık yarıçapının ilgili bir tanımlayıcı sis- temin H ∞ -normu ile bağlantısı var. Bu bağlantıdan faydalanarak tezin ilk bölümünde geliştirdiğimiz alt uzay çerçevisini büyük çaplı PH sistemlerinin kararlılık yarıçaplarının hesabı için uyarlıyoruz. Bu uyarlamayı yaparken, yapıyı koruyan model indirgeme yöntemlerini kullanıyoruz, dolayısıyla indirgenmiş ufak çaplı problemler de aynı PH yapısına sahip. Tez boyunca geliştirdiğimiz bütün yöntemleri hem sentetik, hem de gerçek uygulamalardan kaynaklanan birçok örnek üzerinde detaylı şekilde test ediy- oruz.","The H-infinity norm of the transfer function of a control system is more than a norm on the operator that maps the input of the system to its output; it is a metric that is often associated with the robust stability of the system. For instance, for a standard time-invariant linear system, it is the reciprocal of a structured stability radius, and such interpretations carry over to many other control systems including delay systems. The first part of this thesis work concerns the computation of the H-infinity norm for a large-scale system. We derive a subspace framework that is based on tools from model order reduction. In particular, we restrict the state space to a small subspace, and impose the Petrov-Galerkin conditions with respect to another small subspace on the differential part of the system. The resulting small-scale problem is solved by existing methods, then the two subspaces are expanded with the inclusion of certain singular vectors of the transfer function evaluated on the imaginary axis. We prove rigorously that our expansion strategy leads to a superlinear rate-of-convergence with respect to the dimensions of the subspaces. This results in significant run-time speed-ups compared to best existing methods, often ten times or more. The second part translates the ideas in the first part to the setting ofH-infinity norm minimization for a large-scale parametrized descriptor system over a set of admissible parameter values. As in the first part, state space is restricted to a small subspace and Petrov-Galerkin conditions are imposed. Now the small-scale problems involve H-infinity norm minimization over the same admissible values of the parameters. We again devise a subspace expansion strategy that leads to superlinear convergence, which we prove in theory and observe in practice. The H-infinity norm minimization problem comes with additional challenges, since this is a minmax problem; for instance the proof of superlinear convergence treats the maximizers of the inner problem implicitly as a function of the parameters. The last part turns attention to a special type of descriptor systems with a particular structure, namely port-Hamiltonian (PH) systems, which arise from various applications such as circuit simulation models and the brake squeal problem. A particular notion of a stability radius of a PH system is linked to the H-infinity norm of a descriptor system, so we adapt the subspace framework from the first part for the computation of this stability radius for a large-scale PH system. We employ structure-preserving model reduction techniques, meaning the subspace are designed in a way to ensure that small-scale problems also have PH structure. All of the approaches are tested extensively on synthetic examples, as well as several examples that originate from real applications."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, üç paylaşımlı bellek programlama modelinde yarış durumu tespit edebilmek için yöntemler sunuyor: (i) OpenMP görevleri, (ii) Atomik DataFlow (ADF) gibi veri-akışı programlama modelleri, ve (iii) gömülü sistemlerdeki POSIX iş iplikleri. Bu modellerde yarıs durumlarını tespit etme ihtiyacı HPC, paralel programlama ve koşut zamanlı programlama topluluklarında sıkca kullanılması gerçeğinden güç almıştır ama bu programlama modellerinde yarış durumu tespit edecek araçların eksikliği bulunmaktadır. Belirlilik yarışı, koşut yürüyen varlıklar (mesela görevler) aynı hafıza bölgesine aralarında belirli bir sıralama olmadan eriştiklerinde ve içlerinden en az bir erişim o bölgeye yazma olduğunda oluşan bir durumdur. Sonuç olarak, belirlilik yarışı içeren programlar aynı girdinin farklı koşumlarında farklı sonuç çıktısı üretebilirler. OpenMP ile paralel program yazarken potansiyel problemlerden biri belirlilik yarışı oluşmasıdır öyle ki verilen girdi için program farklı koşumlarında beklenmedik bir şekilde farklı son çıktılar üretebilir. Böyle şaşırtan davranışlar OpenMP görevlerinin yanlış sıralanmasından doğabilir. OpenMP görevlerindeki belirlilik yarışlarının işleyış süresi içinde tespiti için bir yöntem sunuyoruz. OpenMP programlarının anlamına dayanarak, önerdiğimiz çözüm, bir OpenMP programını aralarında çıkarım bağımlılıkları olan görev koleksiyonları olarak modelliyor, öyle ki her görev ya üstü kapalı olarak bir parallel bölge yapısı tarafından yaratılıyor ya da doğrudan bir görev yapısı tarafından yaratılıyor. Biz belirlilik yarışlarını tespit ederken yürütme sırasını belirlemek için böyle bağımlılıklara dayanan bir önce-olur ilişkisi tanımlıyoruz. Bu biçimselleştrimeye dayanarak görevlerin ortak bağımlılıklarının olmadığı koşut hafıza erişimlerini tespit eden ve raporlayan TaskSanitizer isimli aracı geliştirdik. Son olarak, TaskSanitizer işleyiş süresinde çalışıyor, mikro-değerlendirme deneylerinde hataları bulabildi ve çalışma ortamlarında faydalanılabilecek kadar etkili. Archer OpenMP programlarında koşut zamanlı iş iplikleri arasında veri yarış durumu tespiti için etkili bir araçtır. Aksine, biz koşut zamanlı bileşenler arasında sıralama olmadığı durumlarda oluşan belirlilik yarışlarını tespit ediyoruz. Archer bu durumları tespit etmekte başarısız olabilir ve aynı iş iplikleri tarafından çalıştırılan koşut görevleri kaçırıyor. İş iplikleri yerine görevler üzerinde önce-olur ilişkisi inşa ederek bu durumları yakalayabiliyoruz. ADF'deki belirlilik yarışlarını çıktı belirsizliği olarak adlandırmaya karar verdik çünkü bütün görevler atomiktir ve bu yüzden hatalı program ve girdinin farklı koşumlarında farklı çıktılar gözlemlenebilir. Eğer programcı aynı hafıza bölgelerine erişen görevler arasındakş gerekli bağımlılıkları belirlemezse, çıktı belirsizliği mümkün hale gelir. Bu durum gayet mümkündür çünkü yüksek seviyede koşut zamanlı programları gerçeklemek çetrefilli bir iştir ve programcılar kolayca program çıktısını etkileme potansiyeline sahip istemsiz belirsizlikler uygulamaya koyabilir. Böyle istemsiz belirsizlikler programcının niyeti dışında aynı girdi ile farklı koşumlarda farklı sonuçlar üreten özel belirlilik yarışlarıdır. Veri-akışı çalışma modelli paylaşımlı bellek sistemlerde geliştirilmiş uygulamalardaki çıktı belirsizliğini tespit etmek için bir teknik öneriyor ve gerçekliyoruz. Böyle belirsizlik hataları görevlerin bazı sıralamalarını sağlamak için kullanılan görev bağımlılıklarının eksik veya yanlış sıralanmasından dolayı oluşabilir. Önerilen yöntem, veri-akışı bağımlılık çizgesinde görevler üzerinde önce-olur ilişkisinin formüle edilmesine dayanmaktadır. Gerçeklemesi iki ana fazdan oluşur: günlük kayıtları ve tespiti. Yürütümden gerekli bilgiyi kaydetmek için, araç LLVM derleyici altyapısı üzerinde, veri-akışı çatısı ve uygulamalarını ölçüm araçlarıyla donatır. Sonra, yürütümde bulunan çıktı belirsizliklerinden toplanan günlük ve raporları işler. Programcıya olası belirsizlik hatalarına karşın bir test çatısı sağlamak için, araç geliştirme döngüsüne entegre edilebilir. Etkinliğini göstermek için, ADF modelinde yazılan bir değerlenmdirme deney kümesi ile çalıştık ve onlardaki gerçek belirsizlik hatalarını raporladık. Son olarak; 32-bit ARM tabanlı, çok iş örgülü, POSIX iş örgüleri kullanan C/C++ uygulamalarında koşut zamanlı veri yarışlarını tespit eden bir araç olan Embed-Sanitizer'ı öneriyoruz. Gömülü sistem yazılımlarında koşut zamanlı veri yarışlarını sanallaştırma, emülasyon ya da başka bir mimari kullanmadan yerel olarak tespit etme fikrini teşvik ediyoruz. Hedef donanınmda çalışan uygulamalardaki veri yarışlarını tespit etmek daha kesin sonuçlar, artan verimlilik sağlar ve böylece geliştiriciye yüksek üretkenlik sağlar. EmbedSanitizer, 64-bit uygulamalar için bir yarış algılama aracı olan ThreadSanitizer'ı 32-bit ARM uygulamalarında yarış tespiti yapacak şekilde geliştirir. EmbedSanitizer'ı 933MB RAM'i ve 4 mantıksal çekirdeği olan ARMv7 işlemcili bir makinede PARSEC değerlendirme deneylerini kullanarak değerlendiriyoruz. Yarış tespiti sonuçlarımız aynı değerlendirme deneyinin ThreadSanitizer kullanan 64-bit bir makinede verdiği sonuçlarla eksiksiz biçimde uyuşmakta. Ayrıca, EmbedSanitizer'ın başarım ek yükleri gömülü yazılım geliştirmek için yaygın bir platform olan bir emülatörde yarış algılamasını çalıştırmaya göre daha düşük. Bu tez, belirlilik yarışlarını tespit etmek için bir yöntem sunuyor ve OpenMP görevlerini kullanarak etkililiğini gösteriyor. Buna ek olarak, ADF gibi veri akışı programlama modellerinde belirlilik yarışlarını tespit etmek için bir yöntem sunuyor. Son olarak, bu tez gömülü sistemlerde 32-bit POSIX iş örgüleri uygulamalarındaki veri yarışlarını tespit etmek için bir araç sunuyor. Bu tezin hem endüstriye hem de araştırma topluluklarına faydalı olacağı bekleniyor. Ayrıca bu tez yarış algılamada daha iyi çözümler için ileri araştırmaların kapısını açıyor.","This dissertation proposes methods for detecting runtime data races in three shared memory programming models: (i) OpenMP tasks, (ii) dataflow programming models such as Atomic DataFlow (ADF), and (iii) the POSIX Threads in embedded systems. The need for methods of detecting races in these models is fueled by the fact that they are commonly used in the HPC, parallel programming, and concurrent programming communities but there is a lack of tools to detect races in these programming models. A determinacy race is a condition which occurs when concurrently executing entities (e.g; tasks) access the same memory location without specified ordering between them and at least one access is a write to that memory location. As a result, a program with determinacy races may produce different final output results at different runs on the same input. One potential problem when writing parallel programs with OpenMP is to introduce determinacy races where for a given input, the program may unexpectedly produce different final outputs at different runs. Such startling behavior can result from the incorrect ordering of OpenMP tasks. We present a method to detect determinacy races in OpenMP tasks at runtime. Based on OpenMP program semantics, our proposed solution models an OpenMP program as a collection of tasks with inferred dependencies among them where a task is implicitly created with a parallel region construct or explicitly created with a task construct. We define happens-before relation among tasks based on such dependencies for determining an execution order when detecting determinacy races. Based on this formalization, we developed a tool, TaskSanitizer, which detects and reports concurrent memory accesses whose tasks do not have common dependencies. Finally, TaskSanitizer works at runtime, has been able to find bugs in micro-benchmarks and it is reasonably efficient to be utilized in a working environment. Archer is an efficient tool for detecting data races in OpenMP programs between concurrent threads. In contrast, we detect determinacy races where ordering between concurrent components is missing. Archer may fail to detect such cases and it also misses concurrent tasks executed by the same thread. By building the happen-before relations on tasks rather than threads, we can catch these situations. We decided to call determinacy races in ADF as output nondeterminism because all tasks are atomic and therefore different final program outputs can be observed at different runs of the same buggy program and input. Output nondeterminism is possible if programmer does not specify necessary dependency between tasks which access the same memory locations. This is possible as implementing highly concurrent programs can be challenging because programmers can easily introduce unintended nondeterminism, which has the potential to affect the program output. Such unintended nondeterminism is output nondeterminism which is a special determinacy race where a program produces different final outputs at different runs on the same input, without such intention of the programmer. We propose and implement a technique for detecting output nondeterminism in applications developed on shared memory systems with dataflow execution model. Such nondeterminism bugs may be caused by missing or incorrect ordering of task dependencies that are used for ensuring certain ordering of tasks. The proposed method is based on the formulation of happens-before relation on tasks in a dataflow dependency graph. Its implementation is composed of two main phases; log recording and detection. For recording the necessary information from the execution, the tool instruments the dataflow framework and the applications, on top of the LLVM compiler infrastructure. Later it processes the collected log and reports on the found output nondeterminism in the execution. The tool can integrate well with the development cycle to provide the programmer with a testing framework against possible nondeterminism bugs. To demonstrate its effectiveness, we study a set of benchmark applications written in Atomic DataFlow programming model and report on real nondeterminism bugs in them. Lastly, we propose EmbedSanitizer, a tool for detecting concurrency data races in 32-bit ARM-based, multithreaded, POSIX Threads C/C++ applications. We motivate the idea of detecting data races in embedded systems software natively; without virtualization or emulation or use of alternative architecture. Detecting data races in applications on a target hardware provides more precise results and increased throughput and hence enhanced productivity of the developer. EmbedSanitizer extends ThreadSanitizer, a race detection tool for 64-bit applications, to do race detection for 32-bit ARM applications. We evaluate EmbedSanitizer using PARSEC benchmarks on an ARMv7 CPU with 4 logical cores and 933MB of RAM. Our race detection results precisely match with results when the same benchmarks run on 64-bit machine using ThreadSanitizer. Moreover, the performance overhead of EmbedSanitizer is relatively low as compared to running race detection on an emulator, which is a common platform for embedded software development. This dissertation proposes a method for detecting determinacy races and it demonstrates its effectiveness using the OpenMP tasks. Moreover, it presents a technique for detecting determinacy races, dubbed output nondeterminism, in dataflow programming models such as the ADF. Lastly, it proposes a tool for detecting data races in 32-bit POSIX Threads applications for embedded systems. It is with anticipation that this dissertation will benefit both the industry and research communities. It also opens doors for further research on race detection for better solutions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mevcut İnternet'in en iyi çabalı hizmeti doğası, bir video hizmetinde yer alan tüm taraflar, yani ağ hizmeti sağlayıcıları (NSP), üst düzey (OTT) video servis sağlayıcıları (VSP) ve kullanıcılar (istemciler), için verimsizliklere yol açmaktadır. NSP tarafından bakılırsa, NSP'ler genellikle en iyi çabalı İnternet hizmeti için aylık bir abonelik ücreti aldığından, video hizmetlerinin artan hacmi gelirlerine katkıda bulunmaz. OTT VSP'nin perspektifinden bakılırsa, özel bir IP ağı üzerinden yönetilen IPTV hizmetlerinden farklı olarak, kullanıcılarına belirli bir deneyim kalitesi (QoE) vaat edemezler çünkü NSP'ler tarafından sunulan en iyi çabalı İnternet üzerinden servis vermektedirler. Son kullanıcılar tarafından bakılır ise, ekstra bir ücret karşılığında bir miktar QoE ile video hizmetleri alma seçeneği mevcut sistemlerde mümkün değildir. Bu tezde, bu eksikliklerin farkında olarak, farklı hizmet düzeyleri ve hizmet seviyesinde kullanıcı farkındalığı da dahil olmak üzere, açık çoklu-hizmet arası ağ oluşturma vizyonunu uygulayarak, 5G ağları için anahtar bir teknoloji olan yazılım tanımlı ağlar (SDN) üzerinde yönetilebilen video hizmeti mimarileri önerilmektedir. Öncelikle, NSP, VSP ve kullanıcılar arasında, hizmet kalitesi (QoS) ayrılmış ağ dilimleriyle SDN üzerinden NSP tarafından yönetilen veya VSP tarafından yönetilen akış hizmetleri sağlamak için merkezi ve dağıtılmış mimariler öneriyoruz. QoS rezervasyonunun, kullanıcı başına QoE dalgalanmalarını ve heterojen video istemcileri arasındaki adaletsizliği gidermek için tek başına yeterli olmadığını ve son kullanıcıların adil paylaşım bit hızlarını bilerek TCP gelen veri penceresi uyarlamalarını kullanmaları gerektiğini gösteriyoruz. Bunun için NSP tarafından yönetilen (merkezi) ve VSP tarafından yönetilen (merkezi veya dağıtılmış) hizmet modellerini öneriyoruz. İkinci olarak, NSP'lerin birden fazla hizmet seviyesi ve ilgili iş modelleri de dahil olmak üzere tek operatörlü SDN üzerinde katma değerli video hizmetleri (VAVS) sunabilmelerini sağlamak için bir video hizmeti mimarisi ve yeni bir kaynak ayırma optimizasyonu çerçevesi öneriyoruz. Bu amaçla, küçük bir grup akış için (yeni servis talepleri ve bazı mevcut olanlardan oluşan) kaynakların (yol, bit hızı ve kabul kontrolü) tahsislerinin aynı anda gerçekleştirildiği yeni bir parti optimizasyon çerçevesini sunuyoruz. Dinamik kaynak tahsislerini çevrimiçi olarak hesaplamak için, toplam NSP gelirini en üst düzeye çıkarırken, aynı hizmet düzeyine sahip bir grup talepler arasında kaynakların adil bir şekilde tahsis edilmesini amaçlayan, sezgisel grup kısıtlamalı en kısa yol prosedürünü öneriyoruz. Üçüncü olarak, her bir NSP'nin kendi ağ kaynaklarını yönetirken bireysel olarak e2e QoS garantileriyle birlikte operatörler arası hizmetler sunmasını sağlayan, çoklu operatör ağları üzerinden uçtan uca (e2e) QoS yollarının dinamik optimizasyonu için SDN üzerinde dağıtımlı bir açık değişim çerçevesi öneriyoruz. Önerilen sistemde, tüm NSP'lerin SDN denetleyicileri, birbirleriyle iletişim kurarak, kendi ağları üzerinden bir dizi QoS yolunun satış fiyatlarının bildirilmesi için iletişim kurarlar. Her bir NSP, duyurusu yapılan QoS yollarının bir alt kümesine teklif vererek müşterileri için en iyi e2e fiyat performans yolunu dinamik olarak belirleyerek, NSP ağları üzerindeki bu yollar birleştirilir ve uçtan uca yollar oluşturulur.","The best effort nature of the current Internet leads to inefficiencies for all parties involved in a video service; namely, network service providers (NSP), over-the-top (OTT) video service providers (VSP), and users (clients). From the perspective of the NSP, increasing volume of video services does not contribute to their revenues since NSPs commonly charge a flat monthly subscription fee for the best-effort Internet access. From the perspective of OTT VSP, unlike IPTV services that are offered over a managed private IP network, they cannot promise specific quality of experience (QoE) to their users as they rely on the best-effort service offered by some NSP. From the perspective of end-users, they do not have a choice to receive video services with some level of QoE for an extra-cost. In this thesis, recognizing these shortcomings, we investigate the vision of open multi-service inter-networking, including different service-levels and service-level awareness of users, and propose managed video service architectures using software defined networking (SDN) that is a programmable networking paradigm and a key technology for 5G networks. First, we propose centralized and distributed architectures for collaboration between NSP, VSP and users to provide NSP-managed or VSP-managed streaming services over SDN with quality-of-service (QoS) reserved network slices. We show that QoS reservation alone is not sufficient to overcome QoE fluctuations per user and unfairness between heterogeneous video clients, and clients also need to employ TCP receive-window adaptation knowing their fair-share bitrate through collaborative streaming service models that are NSP-managed (centralized) and VSP-managed (centralized or distributed). Second, we propose a video service architecture and a novel resource allocation optimization framework to enable NSPs to offer value-added video services (VAVS) over single operator SDN including multiple service-levels and associated business models. To this effect, we introduce a new batch-optimization framework, where resource (path, bitrate and admission control) allocations for a small group of flows (consisting of new service requests and some existing ones) are performed simultaneously. In order to compute dynamic resource allocations online, we propose a heuristic group-constrained-shortest-path procedure that aims for a fair allocation of resources among a group of requests with the same service level, while maximizing the total NSP revenue. Third, we propose an SDN-enabled distributed open exchange framework for dynamic optimization of end-to-end (e2e) QoS paths over multi-operator networks, which enables individual NSPs to offer inter-operator services with e2e QoS guarantees while allowing each NSP to manage their own network resources. The SDN controllers of all NSPs communicate with each other to advertise a set of QoS-enabled paths across their own network with ask prices, and each NSP dynamically selects the best e2e price-performance path for its customers by bidding on a subset of these advertised paths, and form e2e paths by concatenating them."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde MorphNet adını verdiğim, dilden bağımsız, uçtan-uca çalışan bir nöral model tarif ettim. MorphNet Biçimbilimsel Cözümleme ve Belirsizlik Giderme uygulamalarını eş zamanlı bir şekilde yapabilmek için tasarlanmıştır. Geleneksel olarak, biçimbilimsel olarak karmaşık dillerin analizi iki aşamalı olarak yapılmaktadır: (i) Sonlu durumlu dönüştürücüler tabanlı Biçimbilimsel Cözümleme uygulaması hedef sözcüğün olası tüm görülüş biçimlerini listeler, (ii) İstatistiksel Biçimbilimsel belirsizlik giderme uygulaması da kelimenin içinde bulunduğu bağlamı dikkate alarak doğru görülüş biçmini seçer. MorphNet'te diziden diziye nöral ağlar mimarisi kullanılarak Biçimbilimsel Cözümleme ve Belirsizlik giderme uygulamaları bir araya getirilmiştir. Modelde üç farklı Geri Dönüşümlü Yapay Sinir Ağı girdi kelimelerin çesitli özelliklerinin vektör uzayında kodlanmasında ve bir adet iki katmanlı Geri Dönüşümlü Yapay Sinir Ağı doğru biçimsel görülüşün üretilmesi için kullanılmaktadır. Bu tezde, doğru biçimbilimsel analizler ile etiketlenmiş data ile eğitildiğinde, MorphNet'in yirmi altı dilde geçmiş sistemlerden daha iyi veya kıyaslanabilir sonuçlar elde ettiği gösterilmiştir","I describe and evaluate MorphNet, a language-independent, end-to-end model that is designed to combine morphological analysis and disambiguation. Traditionally, analysis of morphologically complex languages has been performed in two stages: (i) A morphological analyzer based on finite-state transducers produces all possible morphological analyses of a word, (ii) A statistical disambiguation model picks the correct analysis based on the context for each word. MorphNet uses a sequence-to-sequence recurrent neural network to combine analysis and disambiguation. The model consists of three LSTM encoders to create embeddings of various input features and a two layer LSTM decoder to predict the correct morphological analysis. When MorphNet is trained with text labeled with correct morphological analyses, the model is able to achieve state-of-the art or comparable results in twenty-six different languages."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Parolalar, çevrimici kullanıcı kimlik dogrulamada kullanılan en yaygın yöntemdir. Geleneksel parola sistemlerinde, bir kullanıcı bir giriş sunucusuna kimlik doğrulamasını düşük dağıntılı hatırlaması kolay bir parola ile yapar. Maalesef bu alandaki mevcut çözümler ne oltalama (phishing), aradaki adam (man-in-the-middle), balküpü (honeypot) ve çevrimdışı sözlük gibi saldırılarılara karşı güvenli ne de portatiftirler. Dağıtılmış tek parolalı sistemler yukarıda bahsi geçen saldırlara karşı güvensiz olan geleneksel parola protokollerinin zorluklarını gidermek amacıyla önerilmiştir. Dağıtılmış tek parolalı sistemler bu saldırılara kars güvenliği normal sistemlere ek bir depolama saglaycısı ekleyerek (ornegin; bir bulut depolama veya taşınabilir bir mobil cihaz) sağlar. Bu sistemler teorik olarak güvenlik ispatı sunmalarının yanı sıra, kullancının bütün hesapları için düşük dagıntııl tek bir parolayı güvenli bir sekilde kullanmasına olanak sağlarlar. Bu tezde, güvenli dağıtılmış tek parolalı sistemler için genel bir yapı (çerçeve) sunulmustur. Bu yapıdan ve bu yapıda sunulan özelliklere sahip farklı kriptografi k taslaklardan faydalanarak güvenli dağıtılmış tek parolalı sistem örnekleri sunulmuş ve bu örneklerin performans değerlendirmeleri sayısal olarak ortaya konmuştur. Bu yapıdan oluşturulan bir sistem örneği tanıtılmıştır. Genel yapının teorik ispatı için ideal ve reel dünya güvenlik tanımlamarı yapılmış ve bu örneğin teorik ispatını da bu ideal-reel simulasyon ile kanıtlanmıştır. Son olarak, sundugumuz örneğin ve daha önceden literaturde sunulmuş ancak kullanıcı deneyi yapılmamış olan dagıtılmış tek parolalı bir sistemin kullanıcı deney çalışmaları gercekleştirilmiştir. Bu sistemlerin kullanıcı deneylerini günümüzde yaygın olarak kullanlan ve bir çok saldırıya karşı guvensiz olan geleneksel parola ve iki faktörlü kimlik doğrulama sistemleriyle karşılaştırılmış ve dağıtılmış tek parolalı sistemlerin bu alternatiferine göre daha kullanılabilir olduğu gözlemlenmiştir.","Passwords are the most widely used factor in various areas such as secret sharing, key establishment, and user authentication. Single password protocols are proposed (starting with [Belenkiy et al., 2011]) to overcome the challenges of traditional password protocols and provide provable security against offline dictionary, man-in-the-middle, phishing, and honeypot attacks. While they ensure provable security, they allow a user securely to use a single low-entropy human-memorable password for all her accounts. They achieve this with the help of a cloud or mobile storage device. However, an attacker corrupting both the login server and storage can mount an online dictionary attack on user's single password. In this thesis, we introduce a framework for distributed single password protocols (DiSPP) that analyzes existing protocols, improves upon them regarding novel constructions and distributed schemes, and allows exploiting alternative cryptographic primitives to obtain secure distributed single password protocols with various tradeoffs. Previous single password solutions can be instantiated as part of our framework. We further introduce a secure DiSPP instantiation derived from our framework enforcing the adversary to corrupt several cloud and mobile storage devices in addition to the login server in order to perform a successful offline dictionary attack. We also provide a comparative analysis of different solutions derived from our framework. We de fine ideal and real world indistinguishability for DiSPP, and formally prove security of our proposed solution via ideal-real simulation. Finally, we implement two DiSPP instantiations (based on mobile and cloud) and assess their usability with their counterparts (traditional password and two-factor authentication). We conclude that DiSPP systems overall constitute a usable alternative to existing solutions that do not provide offline dictionary attack protection."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, ortak eylem psikolojisi literatüründen ilham alarak, ""iletişimsel ipuçları"" adı verilen, insan hareketinde ilk bakışta göze çarpmayan, sözsüz kinematik değişiklikleri robot hareketlerinde niyet belirtimini artırmak için kullanmaktadır. Bu doğrultuda bu çalışma, ilk olarak iki zamansal ve iki uzaysal iletişimsel ipucunun saptandığı bir insan-insan deneyi (n = 14) gerçekleştirir ve bir simüle robot deneyi (n = 30) vasıtasıyla bu iletişimsel ipuçlarının kullanışlılığını sınar. Çalışma, iletişimsel ipuçlarının, robot hareketlerinde niyet belirtimini artırdığını doğruladıktan sonra, iletişimsel ipuçlarını, ipucu parametreleri aracılığıyla kullanan, otomatikleştirilmiş bir niyet belirtici hareket yaratım çerçevesi geliştirir. Bu hareket yaratım çerçevesini kullanarak, iletişimsel ipuçlarının, gerçek dünyada insan-robot etkileşimi senaryolarında da kullanışlılığı bir insan-robot deneyi (n = 34) ile kanıtlanmış, farklı bakış açıları ve nesne düzenlerine karşı gürbüzlüğü de sınanmıştır. İnsan-robot deneyinde ipucu parametreleri elle seçilmiş, hareket yaratım çerçevesi bu eksikliği kapamak için bir aktif takviyeli öğrenme metoduyla güçlendirilmiştir. Bunu takiben, bir öğrenme deneyi (n = 30) ile önerilen yöntem incelenmiş ve robotun kullanıcıya özgü ipucu parametreleri öğrenebildiği görülmüştür. Son bir deney (n = 11), tüm kullanıcılarda niyet anlamayı güçlendiren, evrensel ipucu parametrelerinin varlığını ortaya koymuştur.","This thesis, inspired by the joint action literature, utilizes subtle, non-verbal kinematic changes that are not consciously recognizable in human motion called ""communicative cues"" to improve intent expressiveness of robot motion. Towards this end, this study fi rst identifi es two temporal and two spatial communicative cues through a human-human experiment (n = 14) and evaluates their utility in a simulated robot experiment (n = 30). After verifying the efficacy of communicative cues for generating intent expressive robot motion, an automated intent expressive motion generation framework is developed that handles the communicative cues through cue parameters. Via utilization of this motion generation framework, the communicative cues' efficacy in real world human-robot interaction scenarios is verifi ed and their robustness in different viewpoint and object confi gurations is evaluated in a human-robot experiment (n = 34). As the cue parameters are chosen manually in the human-robot experiment, an active reinforcement learning methodology is utilized to enhance the motion generation framework. Through a follow up human-robot learning experiment (n = 30), the proposed method is analyzed and it was seen that the robot can come up with user-speci fic cue parameters. Furthermore, a fi nal human-robot experiment (n = 11) revealed that there might also be universal cue parameters that improve intent understanding for all users. This thesis contributes to the body of literature through investigation of human motion to identify the communicative cues, verifi cation of the cues through a two-layer systematical analysis and evaluation of their robustness in different viewpoint and object con figurations. Furthermore, an intent expressive motion generation framework is developed and it is enhanced with active reinforcement learning to achieve an improved intent expressive motion."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan vücudundaki çeşitli ağlar arasında, sinir sistemi en geniş ve karmaşık nano-ağı oluşturmaktadır. Bu ağ, çeşitli iç ve dış uyaranlarla moleküler düzeyde iletişim kurmakta ve vücudun diğer çevresel sistemleri ile ara yüz oluşturarak normal olarak çalışmalarına yardım etmektedir. Doğal olarak mevcut moleküler iletişim sistemlerinin bilgi ve iletişim teknolojisi (BİT) perspektifiyle araştırılması, bu ağların anlayışında yeni bir yaklaşım sağlamaktadır. Bu nedenle, önerilen tezin temel motivasyonu sinir sistemi ve bu ağda gerçekleşen iletişimin BİT perspektifiyle analiz edilmesidir. Özellikle, sinir sisteminin ağ perspektifiyle modellenmesini, sinir sistemine dayalı moleküler iletişim için deneysel test düzeneklerini, sinir sisteminin çevresel sistemi olarak insülin-glikoz sistemini ve bu sistemlerin Biyo-Nano Nesnelerin İnternet'inde (BNNİ) uygulamalarının tanımlanmasını hedefledik. Sinir sisteminin araştırılması mevcut teşhis ve tedavi tekniklerinin artırılmasında yeni istikametler sağlayacak, aynı zamanda, sinir sistemine dayalı olabilecek yapay nano-ağlar için bir çatı sağlayacaktır.","Amongst the various networks inside the human body, the nervous system forms the largest and the most complex nanonetwork. This network communicates with several types of internal and external stimuli on a molecular scale and interfaces with other peripheral systems of the body to help it function normally. Study of naturally existing molecular communication systems from information and communication-theoretical (ICT) perspective provides a novel approach in understanding these networks. Therefore, the main motivation of the proposed thesis is to analyze the nervous system and the communication happening within this network from an ICT perspective. In particular, we target modeling of the nervous system from a network perspective, experimental testbeds for molecular communication based on nervous system, insulin-glucose system as a peripheral systems of the nervous system and decribe applications of these system in the Internet of Bio-Nano Things (IoBNT). The study of the nervous system will provide novel directions to augment the existing diagnostic and treatment techniques as well as provide a framework for artificial nanonetworks that may be based on the nervous system."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çok çekirdekli işlemciler, artan başarımları ve düşen fiyatları sebebiyle birçok teknolojik cihazda tercih edilmektedirler. Günlük işlerde insanlara yardım etmekten (cep telefonu uygulamaları ile olduğu gibi), hayati öneme haiz vazifelere (uçuş kontrol sistemleri, ameliyat yardım araçları, askeri veritabanları ve işletim sistemlerinde olduğu gibi) yardım etmeye kadar olan aralıkta birçok görev yerine getirirler. Bu cihazların tam gücünü kullanabilmek için, programcılar yüksek verimli, koşut-zamanlı yazılım kütüphaneleri geliştirmelidir. Fakat, koşut-zamanlı kütüphane yazımı zor ve hataya açık bir iştir. Kütüphanenin doğruluğunu sağlamak için, programcı olası bütün binişimleri dikkate almalı ve gerekli durumlarda istenmeyen davranışları önlemek için eşleme yöntemlerine başvurmalıdır. çoğu zaman test yöntemi doğruluğu sağlamak için sonuçsuz kalır ve standart hata ayıklama araçları hataları ortaya cıkartmakta yetersiz kalır. Özellikle yaşamsal yazılım parçaları için, biçimsel yaklaşım şarttır. Bu tezde, koşut-zamanlı kütüphanelerin emniyet özelliklerini doğrulamaya, yani kötü bir şey asla olmaz veya program kötü bir hale asla erişemezi göstermeye odaklandık. Fakat, herhangi bir program için sav doğrulama problemi, bizim ilgilendiğimiz koşut-zamanlı ortamlar için karar-verilemez bir problem. Bu ``karar-verilemezlik'' engelini aşmak icin, araştırmacılar hal-erişilebilirliği yerine programın yürütümlerine bağlı olan daha güçlü doğruluk kıstasları önermişlerdir. Temel olarak, geliştirme ispatları, hem belirtim programının hem de gerçekleme programının yürütümlerinin benzer özellikler gösterdiğini ispatlamakta kullanılır. Yürütümler üzerindeki ``özellik'' secçimine göre doğruluk ispatları etkin bir şekilde yapılabilir. Mesela, sıralanabilirlik, yığıt ve kuyruk gibi koşut-zamanlı veri yapıları için standart doğruluk kıstasıdır. Bizim koşut-zamanlı gerçekleme ile atomik belirtim arasında gözlemsel geliştirme ilişkisi kurmamıza olanak sağlar. Sıralanabilirlik, sistemler arasında ileri veya geri simülasyon ilişkisi kurularak gösterilebilir. Özellikle koşut-zamanlı veri yapıları için, bir geri simülasyon ilişkisi bulmak daha zordur. Ek olarak, bir geri veya ileri simülasyon ilişkisinin varlığı hiçbir zaman kesin değildir. Bu tezin bir parçası olarak, genel kanının aksine, Herlihy & Wing Kuyruğu ve Zaman-Damgalı Yığıtı da içeren birçok karmaşık gerçeklemenin ileri simülasyon ilişkisi bularak doğruluğunun ispatlanabileceğini gösteriyoruz. İleri simülasyonların varlığına dair sonucumuz otomatikleştirilebilir, doğal ve basit ispatlara yol açıyor. Sıralanabilirliği ispatlamak için başka yöntemler de mevcuttur. Soyutlama teknikleriyle birleştirilmiş Lipton'un indirgeme kuramı geliştirme ispatları için güçlü bir araçtır. Bu tezin başka bir kısmı, Ardışık Sıralı (SC) hafıza üzerinde çalışan Chase-Lev İş-çalan Kuyruğu (WSQ) gerçeklemesinin bir sıralanabilirlik ispatını tasvir ediyor. İspat mekanize edilmiştir ve CIVL ispat sistemi ile yapılmıştır. İspat doğal bir şekilde yapılandırılmış geliştirme katmanlarından oluşmaktadır. En alt seviye betimleme, atomikliği donanım tarafından garanti edilmiş en ince taneli işlemler ile WSQ gerçeklemesidir. Üst katmanlara doğru, indirgeme, soyutlama ve Owicki-Gries (OG) notlarıyla atomik bloklar daha iri taneli hale gelir. En üst katman, her kuyruk yöntemi için, sıralanabilirliği göstermeye yetecek kadar sıkı, basit atomik işlemlerden muteşekkildir. Tezin son bölümü ise Tam Saklama Sırası (TSO) hafıza modeli altında çalışan programların sağlamlığını, yani bütün TSO yürütmelerinin SC yürütmelerine eşdeğer olduğunu, göstermek için bir yöntem sunuyor. Bu yöntem TSO sağlamlık sorununa hitap etmek için SC indirgeme ve soyutlama tekniklerinden faydalanıyor. Ek olarak, sağlam olmayan programların TSO ve SC yürütmelerini fazla yaklaşık tahmin ederek sağlam programlar elde edebilmek için bir soyutlama tekniği takdim ediyoruz. Bu soyutlama TSO değişmezlerini ispatlamak için var olan SC akıl yürütme yöntemlerini kullanmanın yeterli olacaği sıkılıkta programlar hasıl ediyor. Bu yöntemleri geniş bir kıyaslama kümesinde CIVL ispat sistemini kullanarak değerlendirdik.","Multi-core processors are preferred in many technological devices due to their increasing performance and reduced costs. They perform a range of tasks from helping people in their daily routines (i.e., via apps in cell-phones) to assist vital missions (i.e., flight control systems, surgery assistant devices, military databases and operating system kernels). To harness full potential of these devices, programmers need to develop efficient concurrent libraries. However, writing a concurrent library is a difficult and error-prone task. To ensure correctness of the library, the programmer needs to consider all possible interleavings of the concurrent execution units and provide synchronization mechanisms whenever necessary to prevent undesired behaviors. Most of the time, testing is inconclusive and standard debugging tools are incapable of exposing bugs. Formal treatment is necessary, especially for mission critical software components. In this thesis, we focus on ensuring safety properties of concurrent libraries i.e., something bad never happens or the program never reaches to a bad state. However, assertion checking for arbitrary programs is undecidable for the concurrent settings we consider. To overcome this undecidability barrier, researchers propose stronger correctness criteria that depend on the executions of the program instead of reachable states. Basically, refinement proofs are used to demonstrate that executions of both the specification program and the implementation show similar properties. Based on the choice of the property on executions, correctness proofs can be efficiently performed. For instance, linearizability is the standard correctness criterion for concurrent data structures like stacks and queues. It allows us to establish observational refinement relation between the concurrent implementation and the atomic specification. Linearizability can be shown by establishing forward or backward simulations between systems. In particular, finding a backward simulation relation is more difficult for concurrent data structures. In addition, existence of a backward or forward simulation is never guaranteed. As a part of this thesis, we show that contrary to common belief, many complex implementations including the Herlihy & Wing Queue and the Time-Stamped Stack can be proven correct by finding forward simulations. Our result on existence of forward simulations leads to simple and natural correctness proofs that are amenable to automation. There are other ways of proving linearizability. Lipton's reduction theory combined with program abstraction techniques is a strong tool for performing refinement proofs. Another part of the thesis describes a linearizability proof for the concurrent Chase-Lev Work-Stealing Queue (WSQ) implementation on Sequentially Consistent (SC) memory. The proof is mechanized and performed by using CIVL proof system. The proof consists of naturally structured refinement layers. The lowest level description is the WSQ implementation with the finest grained actions of which atomicity is guaranteed by the hardware. Through the upper layers, atomic blocks get coarser by using reduction, abstraction techniques and Owicki-Gries (OG) annotations. The highest layer consists of a simple atomic action for each queue operation that is tight enough to show linearizability. The last part of the thesis presents a method for showing robustness of a program running under the Total Store Ordering (TSO) memory model, i.e., all of its TSO executions are equivalent to SC executions. This method utilizes SC reduction and abstraction techniques to address TSO robustness problem. In addition, we introduce an abstraction technique that allows us to obtain robust programs from non-robust programs by over-approximating both TSO and SC executions. This abstraction yields programs tight enough so that using existing reasoning methods for the SC semantics is sufficient for proving TSO invariants. We have evaluated these methods on a large set of benchmarks using CIVL proof system."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kolaylıkla bulunabilen derinlik sensörlerinden elde edilen, genellikle delikler ve boşluklar içeren, kısmi ve gürültülü yüzey bilgisinin problemlerinin üstesinden gelen ve yayılım uzaklığına dayanan, yineleyen izometrik nokta eşlemesi yöntemleri önermekteyiz. İki nokta kümesi arasında eşleme problemini, izometriden sapmayı en aza indiren en uygun kısmi eşleme bulma olarak formüle etmekteyiz. Algoritmamız, özellikli noktalar arasında herhangi bir nokta eşleme yöntemi ile elde edilen kabaca bir ilk eşleme ile başlar. Bu ilk eşleme, olabildiğince güvenilir bir eşleme kümesi bulunana kadar tekrarlayan tam eşleme algoritması ile budanıp güncellenir. Elde edilen seyrek ama güvenilir eşlemeler, daha sonra yoğun bir eşleme kümesi elde etmede taban olarak kullanılır. Ek olarak, bir nokta kümesini iki simetrik tarafa kümeleyen bir bütünsel içsel simetri algılama yöntemi sağlamaktayız. Bu tekniği, insan modelleri gibi içsel simetriler sergileyen şekiller üzerindeki sonuçlarımızın güvenilirliğini daha da arttıracak şekilde, nokta temelli eşleme yöntemlerimiz ile birleştirmekteyiz. Yapılan deneyler yöntemlerimizin tıkanma, büyük bozulumlar ve topolojik gürültü sergileyen derinlik verileri üzerinde karşılaştırmalı olarak en iyi performansı verdiğini göstermektedir.","We propose iterative isometric point correspondence methods that rely on diffusion distance to handle challenges posed by commodity depth cameras, which usually provide incomplete and noisy surface data exhibiting holes and gaps. We formulate the correspondence problem as finding an optimal partial mapping between two given point sets, minimizing deviation from isometry. Our algorithm starts with an initial rough correspondence between keypoints, obtained via any point matching technique. This initial correspondence is then pruned and updated by iterating a perfect matching algorithm until convergence to find as many reliable correspondences as possible. The resulting set of sparse but reliable correspondences then serves as a base matching from which a dense correspondence set is estimated. We additionally provide a global intrinsic symmetry detection technique which clusters a point cloud into symmetric sides. We incorporate this technique into our point-based correspondence methods, further improving the reliability of our results on shapes exhibiting intrinsic symmetries such as human models. The experiments show that our methods provide state of the art performance over depth frames exhibiting occlusions, large deformations, and topological noise."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Nesnelerin İnterneti (IoT), fiziksel ve dijital dünya arasındaki bağı güçlendirmeyi amaçlayan ve gelişmekte olan bir teknolojidir. IoT, modern dünyanın önemli bir parçası olan İnternet üzerinden gerçek dünyadaki fiziksel nesnelerle etkileşimin sorunsuz ve verimli bir şekilde sağlanmasını öngörmektedir. Kablosuz Algılayıcı Ağları (WSN) tüm insan gereksinimlerine akıllı çözümler sağlamak amacıyla fiziksel nesneleri akıllı, uzaktan erişilebilen, denetlenebilen ve IoT özellikleriyle iletişim kurabilen nesnelere dönüştürmekten sorumludur. IoT kavramını gerçekleştirmek için, etkili veri üretmek, veri toplamak ve kablosuz algılayıcı ağlar aracılığıyla toplanan verileri sunmak önemli konulardır. Bununla birlikte, WSN, düğümlerin kaynak kısıtlı ve güvenilmez yapısı nedeniyle değerli veri kaybına yol açabilecek birçok zorlukla karşılaşmaktadır. Veri replikasyonu, IoT tabanlı sensör sistemlerinde verimli veri yönetimini kolaylaştırmak için ümit vaat eden bir tekniktir. Bu tezde, öncelikle IoT tabanlı sensör sistemlerinin avantaj ve dezavantajlarını vurgulayarak son teknoloji veri replikasyon protokolleri için veri alma, sorgu dengeleme, sistem sağlamlığı, veri kullanılabilirliği gruplarından oluşan bir sınıflandırma önerilmektedir. Ardından, düğüm hataları nedeniyle oluşan veri kayıplarını önlemek ve ağdaki veri erişilebilirliğini artırmak amacıyla IoT tabanlı sensör sistemlerine özel, tamamen dağıtık ve atlamalı veri replikasyon tekniklerinden oluşan bir veri replikasyon ve toplama (DRACO) sistemi önermekteyiz. DRACO sisteminde önerilen veri replikasyon algoritması, bir mobil evre düğümünün, ağ verilerinin çoğunu toplamak için ağ düğümlerinin az bir kısmını ziyaret etmesini gerektiren verimli bir veri toplama mekanizması ile birleştirilmiştir. DRACO sisteminin en önemli özelliği, yönlendirme yükünü tamamen ortadan kaldırarak ağ düğümleri arasında yönlendirme iletişimine ihtiyaç duymamasıdır. Ağ simülatörü (NS-3) kullanılarak gerçekleştirilen kapsamlı simülasyon deneyleri, DRACO'nun, düğüm hatalarının varlığında yüksek veri erişilebilirliği ve veri toplama verimliliği sağladığını göstermektedir. Karşılaştırmalı simülasyon sonuçları, literatürdeki öncü veri replikasyon tekniklerine kıyasla, DRACO'nun veri erişilebilirliğini yaklaşık %15 ve %34'lük kazanımlarla artırdığını ortaya koymaktadır. Benzer şekilde, ağda oluşturulan veri kopyaları da yaklaşık %18 ve %40 oranında azaltılarak sistem verimliliği geliştirilmiştir. Ayrıca, DRACO, ağdaki veri dağıtım kalitesini belirleyen ve verimli veri toplanma işlemini kolaylaştıran etkin replika yayılımı sağlamaktadır.","Internet of Things (IoT) is an emerging technology aimed at bridging the gap between physical and digital worlds. IoT envisions to provide a seamless and efficient way of interacting with real-world physical objects through the Internet which highly impacts modern day life. Wireless Sensor Networks (WSNs) are primarily responsible for embedding intelligence and turning physical objects into IoT-enabled communicating entities that can be remotely accessed and controlled, thus, providing the basis for smart solutions to all the human needs. To realize the concept of IoT, efficient data generation, collection, and presentation through WSNs are crucial issues. However, WSNs face numerous challenges mainly because of the resource-constrained and unreliable nature of nodes which may result in loss of valuable sensed data. Data replication is a promising technique to facilitate efficient data management in IoT-based sensor systems. This thesis initially proposes a taxonomy of state-of-the-art data replication protocols for IoT-based sensor systems highlighting their pros and cons by classifying them into sub-categories of data retrieval, query balancing, system robustness, and data availability. Then, we propose a data replication and collection (DRACO) framework, composed of a fully distributed hop-by-hop data replication technique for IoT-based sensor systems to avoid data loss due to node failures and enhance data availability in the network. Additionally, the proposed data replication scheme in DRACO is coupled with an efficient data collection mechanism where a mobile sink node visits a relatively smaller percentage of network nodes to collect most of the network data. Thus, no routing structure is needed to enable communication among network nodes which completely eliminates the routing overhead which is a key feature of the proposed approach. Extensive simulation experiments using network simulator (NS-3) show that DRACO ensures high data availability in the presence of node failures as well as provides maximum data collection efficiency. Comparative simulation results reveal that in comparison to two other state-of-the-art approaches, namely, greedy and random replication techniques, DRACO improves data availability with maximum gains of about 15% and 34%, respectively. Similarly, it improves average replicas created in the network with maximum gains of about 18% and 40%, respectively. Furthermore, DRACO ensures a better replica spread which determines the quality of data dissemination in the network as well as facilitates efficient data collection."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapay sinir ağları kullanarak videoların semantik bölütlenmesi şu sıralar popüler bir görevdir, ancak bu alanda yapılan çalışmalar çoğunlukla RGB videolar üzerinedir. Bunun temel nedeni, piksel seviyesinde sınıf bilgisi içeren büyük RGBD video veri kümelerinin bulunmamasıdır. Bu tezde, tamamen konvolüsyonel ve özyinelemeli tamamen konvolüsyonel yapay sinir ağları mimarileriyle video semantik bölütlenmesine derinlik ve zaman bilgilerinin katkılarını araştırmak için bir sentetik ve bir gerçek RGBD video veri kümesi kullanıyoruz. Ek olarak, tamamen konvolüsyonel yapay sinir ağlarından özyinelemeli tamamen konvolüsyonel yapay sinir ağlarına ağırlık aktarımı yapıyoruz ve farklı derinlik kodlama metodları uyguluyoruz. Deneylerimiz, derinlik bilgisinin semantik bölütleme sonuçlarını geliştirdiğini ve zaman bilgisinden faydalanmanın daha yüksek kaliteli çıktı bölütlemeleriyle sonuçlandığını göstermektedir.","Semantic segmentation of videos using neural networks is currently a popular task, however the work done in this field is mostly on RGB videos. The main reason for this is the lack of large RGBD video datasets, annotated with ground truth information at the pixel level. In this work, we use a synthetic and a real RGBD video dataset to investigate the contribution of depth and temporal information to the video segmentation task using fully convolutional and recurrent fully convolutional neural network architectures. Additionally, we employ weight transfer from fully convolutional neural networks to recurrent fully convolutional neural networks and investigate different depth encoding schemes. Our experiments show that the addition of depth information improves semantic segmentation results and exploiting temporal information results in higher quality output segmentations."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İstatistiksel öğrenme alanındaki yöntemlerin anlamlı bir çoğunluğu veriyi çizgeler olarak modellemektedir. Yakınlık çizgeleri gözetimli ve gözetimsiz istatistiksel öğrenme alanlarındaki pek çok probleme çözümler sunmaktadırlar. Bu çizgeler arasında sınıf örtüsü yakalama yönlü çizgeleri (SÖYYÇ) sınıf örtüsü problemini (SÖP) çözmek için tanıtılmıştır. SÖYYÇ'ler sınıflama ve kümeleme için de kullanılabilir. Ancak, bu yönlü çizgeler daha iyi sınıflama ve kümeleme yöntemleri geliştirmek için de genelleştirilebilirler. Bu tezin amacı, istatistiksel öğrenme alanındaki popüler sorunlara yakınlık yakalama yönlü çizgeleri (YYYÇ) ile çözümler sunmaktır. Bu sorunlar arasında; gürbüzlük, prototip seçimi ve küme sayısının tespiti gibi sorunlar yer almaktadır. YYYÇ'ler esasında SÖYYÇ'lerin genelleştirilmiştir halleridir ve YYYÇ'ler daha önce uzaysal veri analizi problemlerinde de kullanılmışlardır. Biz SÖYYÇ'lerin ve YYYÇ'lerin gözetimli ve gözetimsiz istatistiksel öğrenme alanındaki performansını inceleyecek, bu çizgelerin gerçek yaşam problemlerin nasıl değinebileceğini tartışacağız. İlk olarak SÖYYÇ tabanlı sınıflayıcıların, veri setlerindeki sınıflardan herhangi birinin diğer sınıflardakinden daha çok gözleme sahip olduğunda, diğer sınıflayıcılara göre göreceli olarak iyi performans gösterdiğini vurgulayacağız. Bu probleme sınıf dengesizliği problemi ismi verilmektedir. Daha sonrasında, barisentrik koordinat sistemlerini kullanarak ve Delaunay mozaiklemelerini R^d yi mozaikleyecek şekilde genişleterek, YYYÇ tabanlı sınıflayıcılar ve kümeleme yöntemleri geliştireceğiz. Bu yöntemler, sınıf dengesizliklerine karşı gürbüz olacak ve hesapsal olarak takip edilebilen prototip setlerine sahip, cazip ve hızlı yöntemler olacaklardır. Özellikle kümeleme algoritmalarımız, parametrelerden bağımsız olarak tanımlanmış ve SÖYYÇ'lerin gözetimsiz halleri olan, küme yakalama yönlü çizgelerine (KYYÇ) dayalıdır. Biz veri setlerini, uzaysal veri analizinde kullanılan Ripley'nin K fonksiyonuna dayalı araçlar ile böleceğiz ve ayrıca YYYÇ'lere dayalı küme toplulukları tanımlayıp kümeleme yöntemlerini destekleyen algoritmalar geliştireceğiz. Bu tür yöntemler ise veri setlerine mahsus olan alan bilgisini elde etmenin zor olduğu gerçek yaşam problemlerinde önemini göstereceklerdir.","In the field of statistical learning, a significant portion of methods model data as graphs. Proximity graphs, in particular, offer solutions to many challenges in supervised and unsupervised statistical learning. Among these graphs, class cover catch digraphs (CCCDs) have been introduced first to investigate the class cover problem (CCP), and then employed in classification and clustering. However, this family of digraphs can be improved further to construct better classifiers and clustering algorithms. The purpose of this thesis is to tackle popular problems in statistical learning like robustness, prototype selection and determining the number of clusters with proximity catch digraphs (PCD). PCDs are generalized versions of CCCDs and have been proven useful in spatial data analysis. We will investigate the performance of CCCDs and PCDs in both supervised and unsupervised statistical learning, and discuss how these digraph families address real life challenges. We show that CCCD classifiers perform relatively well when one class is more frequent than the others, an example of the class imbalance problem. Later, by using barycentric coordinate system and by extending the Delaunay tessellations to partition R^d, we establish PCD based classifiers and clustering methods that are both robust to the class imbalance problem and have computationally tractable prototype sets, making them both appealing and fast. In addition, our clustering algorithms are parameter-free clustering adaptations of an unsupervised version of CCCDs, namely cluster catch digraphs (CCDs). We partition data sets by incorporating spatial data analysis tools based on Ripley's K function, and we also define cluster ensembles based on PCDs for boosting the performance. Such methods are crucial for real life practices where domain knowledge is often infeasible."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Alışılagelmiş insan-bilgisayar etkileşiminde, kullanıcılar niyetlerini geleneksel giriş cihazları (ör. klavye, fare, oyun çubuğu) ve bunlarla uyumlu standart grafiksel kullanıcı arayüzü elemanları ile iletmektedir. Yakın zamanda kalem-temelli etkileşim, bu geleneksel yöntemlere daha doğal bir alternatif olarak öne çıkmıştır. Ancak, mevcut kalem-temelli sistemler de etkileşim süresince yardımcı mod değiştirme mekanizmalarına (ör. fiziksel/sanal tuşlar, düğmeler, menüler) olan yüksek bağımlılıkları nedeniyle kısıtlı kalmaktadır. Bu tezde, kalem-temelli etkileşime doğal olarak eşlik eden göz bakış hareketlerinin, kalem-temelli sistemlerin yardımcı mod değiştirme mekanizmalarına olan bağımlılıklarını azaltmak için nasıl kullanılabileceği tarif edilmektedir. Özellikle, normalde yardımcı mod değiştirme elemanlarına gereksinim duyan bir takım sanal manipülasyon komutunun, kullanıcıların doğal göz bakış davranışları yardımıyla %88 başarı oranıyla iletilebildiği gösterilmektedir. Bakış kipinin müdahalesiz ve şeffaf bir şekilde tamamlayıcı bilgi kanalı olarak kullanımının bizi gerçek manada doğal kalem-temelli etkileşim hedefine yaklaştıracağına inanmaktayız. Çalışmamızda, (1) çeşitli kalem-temelli etkileşim senaryolarında bakış yönü davranışının doğası incelenmiş, (2) kullanıcılar normal alışkanlıklarını ve etkileşim yollarını devam ettirirken kullanışlı ve kullanılabilir bakış yönü davranış örüntüleri çıkarılmış ve istatistiksel yöntemlerle modellenmiş, (3) bu modeller, kullanıcının komutlarla alakalı anlık niyet ve hedeflerine dinamik olarak adapte olabilen tamamıyla entegre bakış yönü-temelli akıllı bilgi görselleştirme sistemleri yaratmak için kullanılmış, (4) bu sistemler, 19 katılımcı ve 5 farklı kullanıcı arayüzü senaryosu içeren kapsamlı bir kullanılabilirlik çalışması ile değerlendirilmiştir. Değerlendirme sonuçları, kullanıcıların doğal göz bakış davranışlarını kullanarak ve etkileşim akışını bozmadan kullanıcı ve adaptif arayüz arasında ortak bir anlayışı başarıyla sağlayabildiğimizi göstermiştir.","In typical human-computer interaction, users convey their intentions through traditional input devices (e.g. keyboards, mice, joysticks) coupled with standard graphical user interfaces elements. Recently, pen-based interaction has emerged as a more intuitive alternative to these traditional means. However, existing pen-based systems are limited by the fact that they rely heavily on auxiliary mode switching mechanisms during interaction (e.g. hard or soft modifier keys, buttons, menus). In this thesis, we describe how eye gaze movements that naturally occur during pen-based interaction can be used to reduce dependency on explicit mode selection mechanisms in pen-based systems. In particular, we show that a range of virtual manipulation commands, that would otherwise require auxiliary mode switching elements, can be issued with an 88% success rate with the aid of users' natural eye gaze behavior during pen-only interaction. We believe the non-intrusive and transparent use of gaze modality as a complementary information channel will bring us closer to the goal of truly intuitive pen-based interaction. To this end, we (1) investigate the nature of gaze behavior during various pen-based interaction scenarios, (2) mine for, extract, and create statistical models for useful and usable gaze behavior patterns while users keep their normal habits and ways to interact, (3) use these models to create fully integrated gaze-based intelligent information visualization systems that are able to dynamically adapt to user's spontaneous task-related intentions and goals, and (4) evaluate these systems via a thorough usability study involving 19 participants and 5 different user interface scenarios. Evaluation results demonstrate that we can successfully establish a shared understanding between the user and the adaptive interface based on users' natural eye gaze behavior, and without interrupting the interaction flow."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otonom taşıtlar bir çok sensörü birlikte kullanarak ileri yazılım sistemleri tarafından insan girdisi olmaksızın seyahat etmektedir. Otonom taşıt grupları otonom davranışın gelişmiş hali olup, taşıtların kablosuz iletişim vasıtası ile yakın mesafelerde trafik verimliliğini, seyahat güvenliğini, yakıt tüketimini ve gaz emisyonunu iyileştimeyi hedeflemektedir. Diğer taraftan, birbirini takip eden otonom taşıt grupları çoklu otonom taşıt grupları olarak bilinmektedir. Otonom taşıt grupları ve çoklu otonom taşıt grupları yürürlükte olan radyo frekans (RF) bazlı IEEE 802.11p'yi (DSRC) taşıtlar arası iletişim için kullanmaktadır. Fakat, DSRC tıkanıklık kaynaklı başarım verim kaybı, RF kıtlığı ve güvenlik gibi problemlerden olumsuz yönde etkilenmektedir. Görünür Işık ile İletişim (VLC) oldukça yeni bir alternatif teknoloji olup, ışık yönlülük ve geçirmezlik özellikleri ile DSRC problemlerini hedeflemekte ümit vadetmektedir. Fakat, otonom taşıt gruplarında VLC kullanımı VLC'nin dış ortam koşullarına duyarlılığı örneğin sis, taşıtlar arası uzaklık ve/veya virajlı yol kaynaklı kısa süreli iletişim kesintileri nedeni ile otonom taşıt grubu kararlılığını sekteye uğratmaktadır. Bu tez kapsamında melez DSRC ve VLC kullanım temelli güvenli ve verimli iletişim yapılarına odaklanmaktayız. İlk olarak, VLC iletişim sınırlarını belirleyebilmek için taşıtsal VLC iletişim nitelikleri deneysel olarak tekil ve ikili kanal bazlı veri iletimi halinde olacak şekilde değişken ışık karartma düzeyleri ve açılarında taşıtsal ortamda analiz edilmiştir. Yapılan deneysel çalışma Lambertian ışınım modelinin hızlı anahtarlama diyot içeren lamba (LED) ışınım modelini doğru bir şekilde yansıtmadığını, ikili kanal kullanımının açısal sınırı tekil kanal veri iletimine göre 10 derece arttırdığını ve ışık karartma seviyesinin VLC veri iletimini ve ölçümlenen sinyal gücünü etkilediği gösterilmiştir. İkinci olarak, veri çarpıtma, veri yeniden gönderme, sahte grup manevra paketi oluşturma ve sinyal boğma saldırıları altında otonom taşıt grubu kararlılığı ve güvenli grup manevraları sağlanması için melez DSRC ve VLC temelli, SP-VLC isimli güvenlik protokolü önermekteyiz. Farklı senaryolarda sahte grup isteği ve sahte grup yanıtı veri paketlerinin yol kenarında varsayılan saldırgan tarafından gönderildiği sahte otonom grup manevra saldırısı tanımlanmıştır. SP-VLC sadece hedeflenen taşıt ile iletişim için gizli anahtar oluşturumu ve VLC ile peryodik anahtar güncellemesi, mesaj aslıyla aynılık kodu kullanımı ile paket bozulmamışlığı ve aslıyla aynılık kanıtlanımı, ardışık taşıtlar arasında oluşturulan gizli anahtar ile şifrelenen ve çözülen verinin DSR ve VLC tümleyici nitelikleri kullanımı ile iletilmesi, paket alınım nitelikleri temelli sinyal boğma saldırısı saptanımı ve sadece VLC ile iletişime geçiş ve ortak DSRC ve VLC kullanımı ile güvenli otonom taşıt grubu manevra gerçeklenim süreçlerini içermektedir. Önerilen SP-VLC protokol işlevselliği detaylı analizler yapılarak var olan tüm saldırılar altında gösterilmiştir. Gerçekçi taşıt devinimlik, VLC ve DSRC kanal modelleri ve otonom taşıt grubu yönetimi kullanılarak simülasyon platformu geliştirilmiş, daha önce önerilen ve otonom taşıt grubu üyeleri arasında hız ve uzaklık değişiminin sırası ile 25% ve 10% olduğu DSRC ve DSRC-VLC melez yönetim protokollerine göre SP-VLC otonom grup üyeleri arasında 0.1%'den az hız ve uzaklık değişimi ile grup kararlılığını sağlamıştır. Üçüncü olarak, uygulama katmanı veri trafiği altında gecikme süresi ve veri paketi dağıtım yüzdesi gereksinimlerini sağlamak için DSRC ve VLC temelli çoklu otonom taşıt grubu veri iletim protokolü önermekteyiz. Taşıt yoğunluğun yüksek ve kanal tıkanıklığına neden olduğu çoklu otonom taşıt gruplarında grup üyeleri veri iletimi için VLC kullanmaktadır. Önerilen melez veri iletim protokolünün kanal erişim çekişmesini görüş açısında bulunan taşıtlara kısıtlayarak veri paketi dağıtım yüzdesini ve gecikme süresini iyileştirdiği gösterilmiştir.","Autonomous vehicles use a variety of sensors together with advanced software to drive without any human input. Autonomous vehicle platoon is an enhancement of autonomous behaviour where vehicles are organized into groups of close proximity through wireless communication with the goal of improving traffic throughput, transportation safety, fuel consumption and emissions. The chain of platoons that follow each other, on the other hand, refer to multiplatoon. Autonomous platoon and multiplatoon systems mostly adopt the current dominant vehicular radio frequency (RF) technology, IEEE 802.11p (DSRC), for communication among vehicles. However, DSRC suffers from problems of performance degradation due to congestion, the scarcity of RF and security. Visible Light Communication (VLC) is a recently proposed alternative communication technology with the potential of addressing problems by exploiting the directivity and impermeability of light. However, utilizing only VLC in vehicle platoon may degrade platoon stability since VLC is sensitive to environmental effects, i.e. fog, and might have short-term unreachability due to the increase in the inter-vehicle distance and/or loss of line-of-sight on a curvy road. In this thesis, hybrid usage of DSRC and VLC is investigated to achieve secure and efficient architecture for the vehicular platoons. First, we experimentally analyze the characteristics of vehicular VLC in different scenarios including single and dual channel data transmission considering various light dimming level and bearing angle of values with the goal of determining the usage limitation of VLC in the vehicular environment. We demonstrate that state of the art Lambertian radiation pattern does not represent the automotive light emitting diode (LED) radiation pattern accurately. Dual channel usage increases the angular limitation by up to 10\degree ~compared to the single channel VLC and dimming is a key parameter in VLC, which affects data dissemination and received power signal strength. Second, we propose an DSRC and VLC based hybrid security protocol for platoon communication, namely SP-VLC, with the goal of ensuring platoon stability and securing platoon maneuvers under data packet forgery, data packet replay, fake maneuver packet and jamming attacks. We define platoon maneuver attack based on the identification of various scenarios where a fake maneuver request packet or a fake maneuver response packet is transmitted by a malicious actor on the road side. SP-VLC includes mechanisms for secret key establishment and periodic update via VLC to ensure the participation of only the target vehicle in communication; authentication using of message authentication code to ensure the packet integrity; data transmission over both DSRC and VLC incorporating the encryption and decryption of the packets using the secret key generated between consecutive platoon members to exploit the complementary propagation characteristics of data transmission; jamming detection and reaction to switch to VLC only communication based on packet reception characteristics; and secure platoon maneuvering based on the joint usage of DSRC and VLC. We demonstrate the functionality of the proposed SP-VLC protocol under all possible security attacks by both providing a detailed analysis and performing extensive simulations. We develop a simulation platform combining realistic vehicle mobility model, realistic VLC and DSRC channel models and vehicle platoon management and demonstrate that SP-VLC protocol generates less than 0.1\% difference in the speed and distance variation of platoon members during attacks in comparison to 25\% and 10\% in that of previously proposed DSRC and DSRC-VLC hybrid protocols, respectively. Third, we propose a DSRC and VLC based safety message dissemination protocol for multiplatoon to satisfy the hard delay and high packet delivery ratio constraints of the safety application under application level data traffic. Vehicles utilize VLC for safety message dissemination within the platoon when the multiplatoon has high vehicle density leading to high medium contention. DSRC is adopted for platoon based data dissemination when VLC is disconnected within the dissemination distance. We demonstrate that the proposed hybrid protocol improves both packet delivery ratio and delay by limiting the contention to the line-of-sight vehicles."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Coğra olarak dağtılmış veri merkezleri Internet kullanıcılarınn veri depolama, veri işleme ve büyük ölçekli hesaplama taleplerini karşılamak için gerekli olanakları sağlamaktadır. Bu tip veri merkezlerinin hem kullanıcı hem de sağlayıcıyı etkileyen işletim maliyetinin önemli bir bölümü elektrik tüketiminden kaynaklanmaktadır. Bu tezde, bulut veri merkezi birimlerinin enerji tüketimi problemini dikkate almaktayız ve elektrik maliyeti, karbon emisyonu ve SLA ihlalleri durumunda ceza maliyetlerinin azaltılması için önerilen tekniklerin temel özelliklerini belirlemekteyiz. Tez kapsamnda ilk olarak veri merkezlerinin değişen konumu ve sıcaklık farklarından yararlanarak maliyet optimizasyonu üzerine odaklandık. Son teknoloji ürünü CRAC birimleri göz önünde tutularak, havayı serinletmek için dış ortamdaki üç farklı sıcaklık aralığnı dikkate alan bir maliyet modeli oluşturduk. Bu modele bağlı olarak, gelen iş yükünü en düşük SLA ihlali, soğutma maliyeti ve enerji tüketimiyle zamanlamak amacıyla iş yükünü yönetmek için elektrik fiyatlarındaki değişim, veri merkezinin iç ve dışındaki bölgesel sıcakılğı kullanarak mekan-ısı duyarlı deneyimsel algoritmalar tasarladık. Önerdiğimiz algoritmaların başarımını inceledik ve deneysel sonuçları SLA ihlalleri, soğutma maliyeti ve genel işletme maliyetleri açısından mevcut çözümlerle karşılaştırdık. CloudSim platformu üzerinde gerçekçi veri merkezi senaryoları ve iş yükü kayıtlarıyla yapılan benzetimler ve doğrulamalar sonucunda, önerdiğimiz zamanlama yöntemlerinin mevcut EIR+RIA, RIR+CIA, EES ve CADOS zamanlama yaklaşımlarıyla karşılaştırıldığnda soğutma maliyeti açısından %15 ile %75 ve genel işletme maliyeti açısından %3.89 ile %39 arasında tasarruf sağladığı gösterilmiştir.","Geographically distributed data centers provide facilities for the Internet users to fullfill the demand of storage, processing and large scale computations. Most of the operational cost of such data centers is due to the electricity consumption, which affects both service providers and consumers. In this thesis, we address the problem of energy consumption of cloud data center entities and identify key characteristics of state-of-the-art techniques proposed for reducing the electricity cost, carbon emission and financial penalties in case of SLA violations. We focus on cost optimization by taking advantage of temperature and space varying properties of distributed data centers. By considering the state-of-the-art CRAC units which utilize outside air for cooling purposes, we propose a cost model based on the outside cooling which takes into account three dierent temperature ranges for cooling purpose and operations of CRAC units. Based on the model, we propose spatio-thermal aware heuristic algorithms to manage workload using the variation of electricity price, locational outside and within the data center temperature, where the aim is to schedule the incoming workload requests with minimum SLA violations, cooling cost and energy consumption. We analysed the performance of our proposed algorithms and compared the experimental results against the existing solutions for SLA violation, cooling cost and overall operational cost. Experiments and verication conducted on the CloudSim platform with realistic data center scenarios and workload traces show that our proposed scheduling policies save between 15% to 75% in terms of cooling cost and between 3.89% to 39% of overall operational cost compared to the existing scheduling policies of EIR+RIA, RIR+CIA, EES and CADOS."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"El, kol ve baş ile içgüdüsel veya planlı yapılan vücut jestleri yüz-yüze iletişimin önemli öğelerinden bir tanesidir. Vücut jestleri ve konuşma, zamanda birlikte yer alan, konuşmacı tarafından bilişsel olarak duygu ve etkileşim içeriğine bağlı planlanan ve birlikte üretilen iletişim mekanizmalarıdır. Geçmiş çalışmalarımızda jest–konuşma ortak modellerinden ürettiğimiz konuşma ile sürülen jest animasyonu yöntemleri geliştirdik. Bu yöntemler ağırlıklı olarak Viterbi çözücüler kullanmaktadır ve gerçek zamanlı çalışmamaktadır. Bu tez çalışmasında, Viterbi algoritmasındaki parametreleri en uygun şekilde ayarlayarak, bu yöntemlerin gerçek zamanda çalışır hale getirilmesi üzerine incelemeler yaptık. Aynı zamanda ek bir girdiye ihtiyaç olmadan doğrudan konuşma sinyalleri ile üst–vücut jestlerini sentezlemeye odaklandık. Jest ifadeleri, hareket yakalama verilerinden elde edilen beden devinim örüntülerine karşılık gelir ve üst–vücut animasyonlarının oluşturulmasında kullanılır. Kullandığımız yöntemde jest ifadelerinin seçilmesi işlemi, konuşmadan çıkarılan bürün özniteliklerini kullanarak önceden eğitilmiş yarı saklı-Markov modeli ile yürütülür. Gerçek zamanda oluşturulan animasyonları özgün yöntemle elde edilen animasyonlarla karşılaştırmak üzere hem nesnel hem de öznel ölçütler kullandık. Nesnel değerlendirme için, gerçek zamanda oluşturulan animasyonlar ile özgün yöntemle oluşturulan animasyonların çerçeve bazında ne kadar çakıştığını ölçtük. Öznel değerlendirmelerde ise A/B ikili karşılaştırma sınamasını kullandık. Hem nesnel hem de öznel değerlendirme sonuçlarının gösterdiği üzere sistemimiz gerçek zamanda gerçekçi ve doğal konuşma sürümlü jest animasyonları üretebilmektedir.","Gesticulation, which includes instinctive or planned hand, arm and head body gestures, is an essential component of face-to-face communication. Gesture and speech co-exist in time with a tight synchrony, and they are planned and shaped by the emotional state and produced together. In our early studies we have developed joint gesture-speech models and proposed algorithms for speech driven gesture animation. These algorithms are mainly based on Viterbi decoders and can not run in realtime. In this thesis we investigate real-time implementation of these algorithms via optimal adjustment of the parameters in the Viterbi algorithm and focus on synthesizing upper body gestures in real-time, directly from speech signals without need for additional input. Our framework generates upper body gesture animations by selecting gesture phrases, which are defined in terms of body motion extracted from motion capture data. The selection is driven by a pre-trained hidden semi-Markov model (HSMM) which uses prosody features extracted from speech. Experimental evaluations are performed to compare realtime and non-realtime speech driven gesture animations using both objective and subjective evaluations. Objective evaluations quantify the similarity between gesture phrases over the frames of the corresponding animations. Subjective evaluations are performed with A/B pair comparison test. The experimental results confirm that our system is able to produce realistic and compelling speech-driven body gestures in real-time."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Benzerlik; nesne tanıma, sınıflandırma ve gruplama kavramlarının altında yatan psikolojideki temel kavramlardan biridir. Psikologlar, insan beyninin benzerliği algıla-ma yetisini açıklamak için bir çok teori ortaya atmışlardır. Fakat, kısmen benzerliğin öznel olması ve öznenin dikkat ettiği özelliklere göre değişmesi yüzünden, henüz üzerinde anlaşma sağlanan genelgeçer bir teori yoktur. Bu tezde, benzerlik kavramını elle çizilen çizimler için inceleyip, benzerlik ölçümü için bir altın standart oluşturma ve serbest formlu çizim sahnelerini algısal benzerlikleri üzerinden gruplama problemini ele aldık. Bu amaçla, elle çizilmiş 2400 çizim sahnesinden oluşan büyük bir veri kümesi derledik. Ayrıca, uzman insanların oluşturduğu gruplardan benzerlik değerlerini elde edebileceğimiz bir masa gruplama protokolü tasarladık. Oluşturduğumuz altın standardın doğruluğunu, kullanıcılar arasındaki uyumu ölçerek kanıtladık. Gruplama sisteminin performansını, oluşturulan altın standart ile uyum derecesini ölçerek değerlendirdik. Gruplama sisteminin insanların çizim sahnelerini gruplama şekline çok benzer çalıştığını gösteren, yüksek uyum skorları elde ettik.","Similarity is a fundamental concept in psychology that underlies object recognition, classification and clustering. Psychologists have theorized many explanations for human mind's ability to perceive similarity. Yet, there is still no agreed upon theory, partly because similarity is quite subjective and varies with the features attended by the subject. In this thesis, we explored the concept of similarity for hand-drawn sketches, and address the problem of building a gold standard for assessing similarity and clustering free-form sketch scenes through perceptual similarity. Toward this end, we collected a large dataset consisting of 2400 hand-drawn scenes. We further designed a table-grouping protocol for obtaining a measure of similarity through similarity ratings of human assessors. We verified the validity of the constructed gold standard through inter-rater agreement. We evaluated the performance of the clustering system by measuring the degree of agreement with the constructed gold standard. We obtained high agreement scores, showing that the clustering system operates very similar to human way of grouping sketch scenes."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Proteinler arasındaki fiziksel etileşimlerin haritaları olan protein-protein etkileşim ağları, biyolojik işleyişleri anlamak için önemli ve gerekli soyut yapılardır. Bir protein-protein etkileşim ağında, düğümler proteinleri, kenarlar ise ikili etkileşimleri temsil eder. Yüksek aradalık skoru olan proteinleri analiz etmek, proteinlern biyolojik işleyişlerdeki ve moleküler fonksiyonlardaki rolü açısından önemlidir. Protein–protein etkileşim ağları, etkileşimlerin yapısal ve kimyasal özellikleirni içermez. Biz, yapısal bir protein-protein etkileşim ağı modeli oluşturmayı ve bu ağ modeli üzerinde, kısa yol, akış ve rastgele yol aradalık algoritmalarıyla hesaplanan, yüksek aradalık scoruna sahip proteinleri incelemeyi amaçladık. Yapısal bir protein-protein etkileşim ağı oluşturmak amacıyla, ağın kenarları üzerinde, eşzamanlı ve dışlayan etkeleşimleri temsil edebilmek için ağırlık değerleri belirledik ve bu ağırlıkları herbir aradalık algoritması için düzenledik. Ayrıca, eşzamanlı gerçekleşebilen etkileşimler kümeleri oluşturarak, ağ örnekleri oluşturduk ve bu örnekler üzerinde proteinlerin aradalık değerlerini hesapladık. Kısa yol aradalık algoritmasıyla yapısal protein-protein etkileşim ağında tespit edilen darboğaz özelliği gösteren proteinlerin yüzde 37,52'inin yapısal olmayan etkileşim ağında tespit edilen proteinlerden farklı olduğunu gözlemledik. Bu fark oranı, akış aradalık algoritması için yüzde 18,75; rastgele yol aradalık algoritması için yüzde 31,25 olarak hesaplanmıştır. Yalnızca yapısal protein-protein etkileşim ağında darboğaz özelliği gösteren proteinlerin, DNA tamiri ve uyarılması gibi biyolojik işleyişlerde ve hormone alıcı bağlanması ve transkripsiyon aktivasyonu gibi moleküler fonksiyonlarda rolleri olduğu belirlenmiştir. Ayrıca, aradalık özelliğinin arayüzler üzerindeki mutasyonlarla pozitif korelasyonu olduğunu saptadık.","Protein-protein interaction networks (PPIN) that are maps of physical interactions between proteins are important and essential abstractions to understand biological functions and processes. Nodes represent proteins and edges represent pairwise interactions between proteins in a PPIN. Analysis of the proteins with high betweenness score is important in terms of their role in biological processes, molecular functions and cancer. PPINs do not indicate structural and chemical features of interactions. Our goal is to construct a structural PPI network (SPPIN) and analyze the proteins with high betweenness score found by shortest path, flow and random walk betweenness algorithms. To construct the SPPIN, we assigned weights on the edges of the network to reflect the effect of the simultaneous and mutually exclusive interactions and adjusted these weights for different betweenness algorithms separately. We also generated random samples of the network by selecting a set of interactions that can co-occur for each sample and calculated betweenness scores. We observed that %37.5 of bottleneck proteins of SPPIN are different from bottleneck proteins of PPIN found by shortest path betweenness centrality algorithm. This percentage is %18.75 for flow betweenness and % 31.25 for random walk betweenness algorithm. Bottleneck proteins of SPPIN which are different than bottleneck nodes of PPIN have roles in biological processes such as DNA repair and stimulus and have molecular functions such as hormone receptor binding and transcription activation. In addition, we observed that betweenness is positively correlated with number of mutations on interfaces."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"RGB-D sensörlerinin son zamanlarda kolay ve ucuz temin edilebilir hale gelmesi, nesne tanıma alanında renk ve derinlik bilgilerinin ortak kullanımının yaygınlık kazanmasına yol açmıştır. RGB-D verisi ile nesne tanıma özellikle robotik alanı için oldukça önemlidir, ve derinlik bilgisinin kulanımının başarımı arttırdığı bilinmektedir. Renk ve derinlik bilgilerinin nesne tanıma için bir arada kullanılması yaygın bir araştırma konusudur. Bu tez çalışmasında bu problemin çözümüne yönelik olarak, renk bilgisi için önceden eğitilmiş iki boyutlu derin evrişimsel sinir ağları, derinlik bilgisi için ise üç boyutlu evrişimsel sinir ağları kullanılmıştır. Ayrı ayrı eğitilmiş iki boyutlu ve üç boyutlu sinir ağlarından elde edilen özniteliklerin tümleştirilmesiyle çeşitli nesne gösterimleri oluşturulmuş, ve bu gösterimlerin RGB-D nesne tanımadaki başarımları Washington RGB-D veri seti üzerinde sınanmıştır. ImageNet veri kümesi üzerinde önceden eğitilmiş derin evrişimsel sinir ağları aracılığıyla, RGB verisinden anlamlı gösterimler elde edilebilmiştir. Diğer yandan derinlik bilgisi renk bilgisiyle birlikte üç boyutlu bir voksel gösterimi olarak kodlanmış, ve üç boyutlu evrişimsel bir ağ yapısının bu gösterimi kullanarak sıfırdan eğitilmesiyle anlamlı ortak öznitelikler öğrenilmiştir. Washington RGB-D veri seti üzerinde elde edilen RGB kategori bazlı tanıma başarımının, literatürde şu ana dek ulaşılmış en iyi başarım sonucundan daha iyi olduğu görülmüştür. RGB-D bazlı tanıma başarımı ise literatürde elde edilmiş en iyi sonuca denk bir başarım sağlamıştır. Tez çalışmasında son olarak, önceden eğitilmiş iki boyutlu evrişimli sinir ağlarından üç boyutlu evrişimli ağlara bilgi aktarımı, potansiyel bir öğrenme transferi konusu olarak ele alınmıştır.","Recent availability of low cost RGB-D sensors has led to an increased interest in object recognition combining both color and depth modalities. Object recognition from RGB-D images is particularly important in robotic tasks and the inclusion of depth has been proven to increase the performance. The problem of combining depth and color information is being widely researched. This thesis addresses this problem by initializing a 2-D Convolutional Neural Network (CNN) for RGB information via transfer learning and 3-D Convolutional Neural Network for encoding depth information. The obtained feature representations are fused to report performance over the RGB-D object recognition task. The transferred weights are from CNNs that are trained on large ImageNet classi cation challenge dataset and produces meaningful features. The depth information is encoded along with the color information in a 3-D voxel and learns joint features from scratch using a 3-D CNN. The approach is evaluated on the Washington RGB-D dataset and the performance for RGB category recognition exceeds the state-of-the-art, while the RGB-D performance is on par with it for category recognition. Due to good features learnt by the 3-D CNN, the potential of transfer learning from 2-D pre-trained CNN to 3-D CNN to include depth information is also addressed."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizim tanımanın ilgilendiği alan el ile çizilen dijital mürekkepleri sembolik bilgisayar ifadelerine çevirmektir. Çizim tanımanın ilk günlerinden beri alandaki çalışmaların çok büyük bir bölümü belirli ve iyi tanımlanmış veri tabanları için isabetli tanıyıcı modellerini geliştirmek üzerine odaklanmıştır. Bu gibi çalışmalar geleneksel makine öğrenmesi yaklaşımlarını kullanmaktadır. Bu yaklaşımlarda, önceden belirlenmiş sınıfların ve her sınıf için çokça etiketli örneğin bulunduğu varsayılır. Fakat pratikte bu varsayımların karşılanması güçtür. Gerçekte bir çizim tanıma sistemini geliştiren tasarımcının elinde hiç bir etiketli veri yoktur ve veri etiketleme külfeti ile karşı karşıyadır. Bu çalışmada etiketleme külfetini hafifletmek amacı ile geliştirilen, az sayıda etiketli ve çok sayıda etiketsiz veri kullanarak eğitilen sistemler sunmaktayız. Sunduğumuz sistemler çok küçük boyuttaki etiketlenmiş örnek kümesini öz-öğrenme yöntemleri ile etiketsiz örnekler arasından yeni örnekleri etiketleyerek otomatik olarak genişletmeyi hedefler. Otonom etiketlemenin sonucunda yeterli boyutta etiketli eğitim verisi ortaya çıkar ve bu veriler ile sınıflandırıcılar eğitilebilir. Bu çalışmada uygulaması ve çalışma zamanı karmaşıklığı birbirinden farklı dört öz-öğrenme yöntemi sunulmuştur. Sunulan yöntemlerden biri içeriksel eşdizimli örüntüleri kullanarak, tasdik edilebilir düzeyde daha çeşitli örnekler içeren eğitim örnek kümeleri elde etmektedir. Büyük veri setleri ile yürütülen titiz deneyler sonucu, sunulan içeriksel tabanlı özgün yöntemin tanıma performansını önemli ölçüde yükselttiği gözlemlenmiş ve raporlanmıştır.","Sketch recognition is the task of converting hand-drawn digital ink into symbolic computer representations. Since the early days of sketch recognition, the bulk of the work in the domain has focused on building accurate recognition algorithms for specific domains, and well defined databases. These lines of work adopt traditional machine learning approaches. They assume the presence of a fixed set of symbol classes, and availability plenty of annotated examples per class. However, in practice, these assumptions do not hold. In reality, the designer of a sketch recognition system starts with no labeled data at all, and faces the burden of data annotation. In this work, we propose to alleviate the burden of annotation by building systems that can learn from very few labeled examples, and large amounts of unlabeled data. Our systems perform self-learning by automatically extending a very small set of labeled examples with new examples extracted from unlabeled sketches. The end result is a sufficiently large set of labeled training data, which can subsequently be used to train classifiers. We present four self-learning methods with varying levels of implementation difficulty and runtime complexities. One of these methods leverages contextual co-occurrence patterns to build verifiably more diverse set of training instances. Rigorous experiments with large sets of data demonstrate that this novel approach based on exploiting contextual information leads to significant leaps in recognition performance."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Asenkron programlama, kullanıcı girdilerine hızlı yanıt vermenin önemli bir gereksinim olduğu olaya dayalı modern web ve mobil uygulamalarda yaygın olarak kullanılmaktadır. Bu programlar, asenkron program çalışmasının getirdiği farklı kontrol akışı ve çok iş parçacıklı programların doğasında olan zorluklardan ötürü eşzamanlılık hatalarına yatkındırlar. Yakın zamanda, olaya dayalı ve asenkron programlardaki program hatalarını yakalamayı amaçlayan çalışmalar yapılmış olsa da, bu programlar için çalışma platformundan ve uygulamadan bağımsız, üst seviyeli bir doğruluk kriteri henüz tanımlanmamıştır. Bu tezin amacı, olaya-dayalı asenkron programlar için bir doğruluk kriterinin tanımlanması ve bu programlardaki eşzamanlılık hatalarının belirlenmesinde kullanılacak yöntem ve araçlar geliştirilmesidir. Bu doğrultuda, asenkron programların eş zamanlılık davranışlarını inceleyen statik ve dinamik araçlar geliştirdik. Eşzamanlı bir asenkron programın çalışma davranışlarının bazı kısıtlama parametrelerine göre belirlenmiş bir alt kümesini, nondeterministik, tek iş parçacıklı bir programda kodlamayı sağlayan yöntemler üzerinde çalıştık. Geliştirdiğimiz yöntem, programda var olan senkronizasyonu göz önüne alarak verilen bir kısıtlama parametresi için daha çok eşzamanlı program davranışını inceleyebilmeye olanak sağlamaktadır. Bir başka çalışma olarak, Android uygulamalarındaki asenkroni hatalarını yakalayan dinamik bir araç geliştirdik. Bu araç, çok iş parçacıklı programların incelenmesinde bilinen bir teknik olan sistematik eşzamanlılık taramasını olaya-dayalı asenkron programlara uyarlamaktadır. Araç, verilen bir dizi kullanıcı girdisi için, uygulamadaki olay işleme prosedürlerinin oluşturduğu asenkron görevlerin farklı çalışma sıralarını sistematik olarak test etmektedir. Asenkron programlardaki eşzamanlılık hatalarını incelemede elde ettiğimiz deneyime dayanarak, asenkroniye karşı dayanıklılık olarak isimlendirdiğimiz bir doğruluk kriteri tanımladık. Bu kriter, programcının olaya-dayalı asenkron programın çalışma davranışları konusundaki beklentilerini karşılamakta ve programın bazı kısımları araşındaki etkileşimi göz ardı edebilmesine izin vermektedir. Kabaca asenkroniye karşı dayanıklılık, program içindeki asenkroniye ve programın farklı parçalarının paylaşılan bellek üzerinde girişikleme yaparak işlem yapmasına bakılmaksızın programın tüm olay işleme prosedürlerinin ve tüm asenkron görevlerinin izole bir şekilde çalıştığı soyut bir görünümü ile aynı davranışları sergilemesini gerektirmektedir. Asenkroniye karşı dayanıklılık, iki sezgisel özelliğin; olay serileştirilebilirliği ve olay determinizminin birleşimi olarak ifade edilebilir. Kriterin bu şekilde ifade edilebilmesi, algoritmik olarak verimli bir şekilde kontrol edilebilmesine olanak sağladığı için önemlidir. Olay serileştirilebilirliği, farklı ortamlarda da kullanılan serileştirilebilirlik kavramının olay işleme prosedürlerine uyarlamasıdır. Bu özellik, bir dizi olay işleme prosedürünün eşzamanlı çalışmasının, olay işleme prosedürlerinin seri olarak, bir olayın işlenmesi tamamen bittikten sonra bir başka olay işlenerek art arda çalışmasına denk olmasını gerektirmektedir. Olay determinizmi tek bir olay işleme prosedürüne odaklanır. Bir olay işleme prosedürünün, oluşturduğu asenkron görevlerin paylaşılmış bellek üzerinde girişikleme yaparak çalışmasına bakılmaksızın tek deterministik sonucu olmasını gerektirmektedir. Olay serileştirilebilirliği ve olay determinizmi özelliklerinin doğrulanması için yeni yöntemler geliştirdik. Sunduğumuz yöntemin dikkat çekici yanı, eşzamanlı çalışabilen sınırsız sayıda asenkron görev içeren çok iş parçacıklı programların analizini tek iş parçacıklı programlarda ulaşılabilirlik problemine indirgemesidir. Açık kaynaklı Android uygulamaları üzerinde yaptığımız deneysel çalışmalar, asenkroniye karşı dayanıklılık analizinin pratikte uygulanabilir olduğunu göstermektedir. Ayrıca, uygulamalardaki gerçek eşzamanlılık hatalarının asenkroniye karşı dayanıklılık ihlali olarak yakalanabilmesi ve önceden bilinmeyen bazı eşzamanlılık hatalarının da bulunması önerdiğimiz doğruluk kriterinin geçerliliğini göstermektedir.","Asynchronous programming is ubiquitous in modern event driven web and mobile applications for which responsiveness to user inputs is a key concern. These programs are prone to concurrency errors due to the different control flow of asynchronous execution and the inherent scheduling nondeterminism of multithreading. Though recent work focuses on detecting bugs in event driven and asynchronous programs, there is no high-level correctness criteria that is independent of the computing platform or the application logic. The goal of this dissertation is the characterization of the correctness of event-driven asynchronous programs and building techniques and tools for detecting the concurrency bugs in these programs. Towards formulating a correctness criteria, we build static and dynamic concurrency exploration tools for asynchronous programs. We devise a sequentialization-based technique which encodes a particular subset of possible concurrent program behaviors into a nondeterministic sequential program, where this subset of behaviors is specified by some bounding parameters. By taking the program synchronization into account, our method provides an ecient exploration that can capture a larger set of concurrent program behaviors for a given bounding parameter. We also develop a dynamic exploration tool to detect and reproduce asynchrony bugs in Android applications. It adopts systematic concurrency exploration that is well-known for multithreaded programs to event-driven asynchronous programs. Given a sequence of user inputs to an application, the tool systematically executes alternate schedules of tasks in the execution of this sequence of event handlers. Based on our experience on the analysis of concurrency bugs in asynchronous programs, we define a correctness criteria, robustness against asynchrony. It captures the programmer expectations about the behavior of an event-driven asynchronous program and enables the programmer to ignore certain interference among some parts of his program. Roughly, regardless of the asynchrony and the concurrent accesses to the memory, robustness requires a program to exhibit the same set of behaviors with an abstract view of its execution in which the event handlers and asynchronous invocations run in isolation. Robustness against asynchrony can be formulated as a conjunction of two intuitive properties: event serializability and event determinism. This formulation of robustness is important as it yields an efficient algorithmic checking method. Event serializability is an adaptation of the serializability property used in some other contexts to the event handlers in a program. It requires the executions of a sequence of event handlers to be equivalent to a serial execution of them in which an event handler is dispatched only after the completion of all asynchronous tasks of the previous event. Event determinism focuses on a single event handler and requires it to produce a deterministic outcome regardless of the interleaving execution of its asynchronous invocations. We present novel algorithms for the verification of event determinism and event serializability. Remarkably, our method reduces the analysis of the event-driven asynchronous programs with unbounded number of concurrent tasks to the reachability problems in sequential programs. Our experimental work on the open source Android applications shows that our robustness checking algorithm is viable and can be used in practice. Moreover, the real concurrency bugs in the applications can be captured as robustness violations and unknown bugs can be uncovered by checking the robustness of an application."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Asenkron programlama modelleri, mobil, masaüstü ve web uygulamaları gibi modern bilişim platformlarındaki eşzamanlı geliştirme uygulamalarında yaygın kullanımdadır. Bütün eşzamanlı programlarda olduğu gibi, asenkron programlarda da akış mantığını oturtmak geliştirici için zorlayıcıdır. Geçmişten günümüze gelmiş eşzamanlı yapılardan (örn. iş parçacıkları) farklı olarak, asenkron yapılarda (örn. geri çağırma) yürütmenin doğrusal bir sıralaması olmadığından ve bu sıralama asenkron olayların icrasına bağlı değişebildiğinden, bu yapılar kontrol ve veri akışını tersine çevirebilir. Bu durumda yakalaması güç eşzaman hataları ortaya çıkabilir ve bu hatalar farkedilmesi, yeniden üretilmesi ve tamir edilmesi zor olduğundan yazılım sürecinde ciddi etkilere sebep olabilir. Yarış durumları, eşzamanlı programlarda istemsiz belirsizliğin ana kaynağı olarak görülmekte ve yüksek seviyelerdeki eşzaman hatalarının göstergesi olarak görülmektedir. Geçmişten günümüze gelmiş eşzamanlı programlama modellerinde yarış durumlarını ortaya çıkarmak için hem araştırma çevrelerinden hem de sanayi çevrelerinden yoğun ilgi gösterilmiş olsa da, asenkron programlama modellerinde yarış durumlarını ortaya çıkarmak için kullanılacak metodlara kritik bir ihtiyaç vardır. Bu tez, bahsedilen ihtiyaçları karşılamak için, eşzaman methodu olarak asenkron programlama yapıları kullanan uygulamalarda yarış durumlarını ortaya çıkarmak için yeni teknikler sunmaktadır. Sunduğumuz bu yarış durumu belirleme tekniği farklı asenkron programlama modelleri üzerine uygulanabilir. Ana olarak, JavaScript web uygulamaları ve veriakışı programlama yapılarını paylaşımlı hafıza üzerine uygulayan melez programlama modelleri üzerinde odaklandık. Tekniğimiz; statik ve dinamik analiz yaklaşımlarının avantajlarını biraraya getirmeyi amaçlamaktadır. Yürütme kodu sadece bir kere çalıştırılır, fakat birçok yürütme sıralaması statik analiz yaklaşımı kullanılarak incelenir. Bu tekniğin temelindeki görüş, tekniğin paylaşımlı hafıza noktaları üzerindeki eşzamanlı değişimleri farklı sıralamalarda birbirinden bağımsız incelemek yerine birleştirilmiş bir yapıda saklamasından gelir. Tekniğimizi açıklamanın bir yolu, belirli bir yürütme için olası farklı sıralamaları gözönünde bulundurarak, komşu yürütme sıralamalarını tek bir yürütme üzerinden incelemektir. Çalışmamızın ilk adımı olarak, JavaScript web uygulamalarındaki asenkron yapıları kullanıcı etkileşimi ve sunucu yanıtları üzerinden inceledik. öncelikle, yarış durumlarının web uygulamalarındaki zararlarını belirleyebilmek için rastgele seçilmiş ve sistematik seçilmiş sıralamalı sunucu istemleri (XMLHttpRequest) kullanılarak deneysel çalışmalar yapılmıştır. Diğer uygulama çeşitlerine nazaran, web uygulamaları geçici durumlarda gözlenen yarış durumları konusunda, etkilerin kısa ömürlü olmasından dolayı daha affedici bir yapıya sahiptir. Bunun sebebi, bu yarış durumlarının sadece programın gözle görülmeyen kullanıcı kısımlarında oluşması ya da, en kötü ihtimalle, kullanıcı arayüzünde farkedilmeyen veya sayfayı yenileyerek yok edilebilen aksaklıklardan oluşmasındandır. Bu deneysel çalışmanın sonuçlarından yola çıkarak, web uygulamarındaki yarış durumlarında, kalıcı durum üzerinde odaklanmanın daha kullanışlı olacağını önerdik. Bu kalıcı durumlar kullanıcı tarafında saklanan çerezler, localStorage ve sessionStorage mekanizmaları olabileceği gibi sunucu tarafındaki yan etkiler de olabilir. Geliştirdiğimiz tekniği, asenkron yapıları yoğun şekilde kullanan 26 web sitesi üzerinde değerlendirdik ve zararsız yarışların (geçici durumlarda gözlenen) zararlı yarışlardan (kalıcı durumlarda gözlenen) çok daha yaygın olduğunu gözlemledik. Son olarak, JavaScript kütüphanelerinde yarış durumu ile ilgili hataları bildirmek için kullanılan yeni bir asenkron yapı -Promiseler- üzerinde bir araştırma çalışması hazırladık. Tezin ikinci kısmında, veriakışı yapılarını paylaşımlı hafıza programlama modelleriyle biraraya getiren melez programlama modelleri üzerinde çalıştık. Bu melez programlama modelleri, programcılara görevleri ana yürütme birimi olarak ve aralarındaki veri bağıntılarıyla birlikte tanımlamak için veriakışı soyutlaması sağlar. Saf veriakışı modelinin tersine, ki bu model görevlerin yan etki olmadan yürütüleceğini varsayar, melez modeller görevlerin iş parçacığı senkronizasyonu kullanarak (örn. kilit ve işlem belleği) veri paylaşmasına olanak verir. Olası yürütme sıralamaları ve uygulamayı oluşturan görevleri araştırmak için, olasılık garantili rastgeleleştirilmiş bir araştırma tekniği geliştirdik. Daha sonra, melez programlama modellerinde yarış durumlarını ortaya çıkarmak için kullanılacak modüler bir yarış belirleme tekniği geliştirdik. Farklı programlama modelleri tarafından -örneğin veri akışı ve paylaşımlı hafıza- dayatılan önce-olur ilişkileri (yürütme sırası) için modüler bir gösterim sunduk. Bu programlama modellerine örnek olarak veriakışı ve paylaşımlı hafıza verilebilir, ki bunlar belırleme mekanizmasını kilit bazlı ya da işlem hafızası bazlı senkronizasyon sistemleri gibi farklı tipteki paylaşımlı hafızalar için genişletmede kullanılabilir. Son olarak, geliştirdiğimiz yarış belirleme mekanizmasını genel bir prototip araç olarak kodladık, böylece geliştirdiğimiz bu araç farklı tiplerdeki melez programlama modelleri için de kolayca genişletilebilir.","Asynchronous programming models has become ubiquitous in concurrent application development for the modern computation platforms, i.e. mobile, web and desktop applications. Similar to all concurrent programs, reasoning about asynchronous programs is challenging for the developers. Different than legacy concurrency constructs (i.e. threads), asynchronous constructs (i.e. callbacks) can invert the control and data flow as the execution order is not linear and may change depending on the dispatch of asynchronous events. This may result in subtle concurrency bugs that can have a big impact on the development cycle as it is notoriously hard to detect, reproduce, and fix them. Data races have been considered as the main source of unintended non-determinism in concurrent programs and seen as a symptom of a higher-level concurrency error. Although race detection in legacy concurrent programming models have attracted a great deal of attention from both the research communities and commercial counterparts, there is a critical need for race detection techniques for asynchronous programs. To address these needs, this thesis introduces techniques for detecting data races on applications using asynchronous programming constructs as a way of concurrency. We introduce race detection technique that is applicable to different asynchronous programming models. We focused mainly on JavaScript web applications and applications using hybrid programming models that integrates dataflow programming construct into shared memory programming models. Our techniques attempts to combine advantages of both static and dynamic analysis. We executed the application code only once, yet we explore multiple execution order with the help of our static analysis approach. The key idea behind our analysis technique is that it encodes the effects of concurrent updates on the shared state by merging the values as opposed to separately considering different orderings. One way to see our approach is that we explore neighboring schedules for a particular runtime execution by analyzing possible re-orderings of the concurrent entities with a single pass over the observed execution. As the first part of our study, we explored JavaScript web applications using asynchronous constructs for user interaction and server requests. First, we conducted empirical studies for identifying the real damages of data races in the web applications by devising randomized and systematic exploration of server requests (XMLHttpRequests) orderings. As opposed to other types of applications, web applications have a more forgiving nature for data races on transient state, since their effects are ephemeral. This is because these races only occur in invisible to the user portions of the program state, or, at worst, UI glitches that are either unnoticed by the user, or disappear if the user reloads the page. In the light of our empirical study, we suggested that a more useful way to think about data races on the web is by focusing on persistent state, such as client-local cookies, localStorage and sessionStorage mechanisms, as well as server-based side effects. We evaluated our technique on 26 web sites that heavily uses asynchronous constructs, we demonstrate that benign races (on transient state) are considerably more common than harmful ones (on persistent state). Finally, we conducted an exploratory study on the reported race related bugs on JavaScript libraries that uses new asynchronous constructs: Promises. In the second part of the thesis, we worked on hybrid programming models combining dataflow constructs with share memory programming model. These hybrid programming models provide programmers with dataflow abstractions for defining tasks as the main execution unit with corresponding data dependencies between each other. Contrary to the pure dataflow model which assumes side-effect free execution of the tasks, these models allow tasks to share the data using some form of thread synchronization, such as locks or transactional memory (TM). We devised a randomized exploration technique with probabilistic guarantees for exploring possible execution orders of the tasks composing the application. Later, we devised a modular race detection technique which can be used for detecting data races in hybrid programming models. We presented a modular representation of happens before relations (execution orders) imposed by different programming models, i.e. dataflow and shared memory, which can be used for extending the detection mechanism for different type of shared memory synchronization systems, i.e. lock-based, transactional memory. Finally, we implemented our detection mechanism as a generic tool prototype that can be easily extended for different type of hybrid programming models."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde dilden bağımsız, karakter seviyesinde etiketleme yapan bir model tarif et- tim ve bu modeli Varlık İsmi Tanıma, Sözcük Türü Etiketleme ve Yüzeysel Çözümleme üzerinde değerlendirdim. Bu modelde bir cümle, kelimeler yerine karakter dizisi olarak temsil edildi. Model, karakterleri girdi olarak alan ve her karakter için etiket olasılık dağılımı üreten, üst üste yığılmış çift yönlü Geri Dönüşümlü Yapay Sinir Ağları'ndan oluşmaktadır. Daha sonra bu olasılık dağılımları, Viterbi algoritması kullanılarak kelime seviyesindeki etiketlere çevirildi. Model, sadece işaretli veri kümesi kullan- makta olup, elle belirlenmiş öznitekliklere ve harici kaynaklardan gelen bilgilere (diğer sözdizimsel işaretleyicilerin çıktısı, isim listeleri) ihtiyaç duymamaktadır. Bu tezde tarif edilen model, Varlık İsmi Tanima'da 7 dilde elde edilen en iyi sonuçlara yakın, Sözcük Türü Etiketleme'de 4 dilde geçmiş sistemlerden daha iyi ve İngilizce Yüzeysel Çözümleme'de rekabetçi sonuçlar vermiştir.","I describe and evaluate a language-independent character-level tagger for sequence labeling problems: Named Entity Recognition (NER), Part-of-Speech (POS) tagging and Chunking. Instead of words, a sentence is represented as a sequence of charac- ters. The model consists of stacked bidirectional LSTMs which input characters and output tag probabilities for each character. These probabilities are then converted to consistent word level phrase tags using a Viterbi decoder. The model uses only labeled data and does not rely on hand-engineered features or other external resources like syntactic taggers or Gazetteers. The model is able to achieve close to state-of-the-art NER performance in seven languages, performs as well as or better than previous work in four languages for POS tagging and yields competitive results for English Chunking dataset."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde akıllı telefonlar, tablet bilgisayarlar, hatta akıllı televizyonlar giderek daha güçlü işlemcilere ve hesaplama gücüne kavuşmaktadırlar. Bilgisayarlar tarafından gerçekleştirilen birçok görev, örneğin internet kullanımı bu cihazlarda da aktif olarak gerçekleştirilebilir duruma gelmiştir. Ancak mevcut internet sitelerinin büyük bir çoğunluğu masaüstü bilgisayarlarda görüntülenmek üzere tasarlanmıştır. Bu nedenle her cihazın donanımsal özelliklerine uygun web tasarımlarının oluşturulması bir zorunluluk haline gelmiştir. Ancak her internet sitesinin manuel olarak yeniden tasarlanması çok yüksek miktarda uzman analizi ve maliyet gerektirmektedir. Bu çalışmada, bahsi geçen problem için yeni bir çözüm olarak, yarı otomatik çok-kipli internet içeriği çeviri sistemi geliştirilmiştir. Geliştirdiğimiz sistem, tek bir web sayfasından, bu sayfanın farklı bir platform için nasıl yeniden tasarlanması gerektiğini, bir uzmanın sözlü tanımlamalarına dayanarak öğrenebilir ve öğrendiği bu modele göre sayfanın farklı örneklerinin yeniden dizaynını gerçekleştirebilir. Geliştirdiğimiz sistem literatüre üç temel katkı sağlamaktadır. İlk katkımız, makine öğrenmesi metotlarını kullanarak internet siteleri için çeviri modelleri öğrenebilen modüler bir sistem mimarisidir. İkinci katkımız, öğrendigi modelleri kullanarak daha önce görülmemiş sayfaları yeniden dizayn edebilen bir çeviri motorudur. Üçüncü katkımız ise, benzer sistemlerin değerlendirilmesinde kullanılabilecek, 10 farklı web portalının 300 örneği kullanılarak oluşturulmuş bir veri arşividir. Sistem üzerinde gerçekleştirdiğimiz nesnel değerlendirmelere göre, geliştirdiğimiz sistem çeviri modellerini yüksek bir başarı oranı ile öğrenebilen bir sistemdir. Nesnel değerlendirmelerin yani sıra, gerçek kullanıcılar ile yapmış olduğumuz kullanıcı deneyleri, sistemin kullanılabilir olduğunu göstermiştir.","Non-desktop platforms such as mobile phones, tablets, and connected TVs now serve as primary means for browsing the web. Unfortunately, much of the existing web content has been designed and optimized for desktop viewing. Hence, tailoring the content for viewing on multiple platforms with various constraints has become a necessity. However, manual customization for each platform is costly, and not always feasible. In this work, we introduce a semi-automatic multimodal web content retargeting system that works with minimal assistance of an expert. From a single page, our system learns how a web portal should be tailored based on verbal and gestural descriptions of a human operator, and converts previously unseen pages based on what it has learnt. We have three contributions. First, we introduce a modular system architecture with a flexible backend for learning content translation rules from examples using machine learning. Our second contribution is a translation engine capable of applying previously learned translation rules to translate unseen pages. Our third contribution is a large database consisting of 300 manually coded web pages from 10 popular portals, which will serve as a benchmark database for evaluating similar systems in the future. Our discussion is supported by objective evaluation of the system's ability to learn, and results from a carefully designed usability study demonstrating the effectiveness of the proposed system with real users. The results show that our system can learn conversion rules and it is a usable system."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Modern mobil aygıtlarda, 3G modemler en fazla enerji tüketimine neden olan bileşenlerdendir. Dolayısıyla, 3G modemlerin kullanım verimliliği batarya ömrünü önemli ölçüde etkilemektedir. Bu verimlilik, önemli ölçüde mobil uygulamaların 3G ağ kullanma karakteristiği ile ilintilidir. Buna karşın, birçok mobil uygulama, 3G modemin enerji tüketim karakteristiği göz önünde bulundurulmadan geliştirilmektedir. Mobil uygulamalar, genellikle küçük ve periyodik veri transferleri için 3G ağ bağlantısı talep etmektedirler. Her veri transferi 3G modemi uyandırmakta ve batarya tüketimini hızlandırmaktadır. Bu verimsiz bağlantı kullanımı, mobil aygıt arka plan kipindeyken (kullanıcı aygıtla etkileşim halinde değilken) de devam etmektedir. Bu tezde, 3G modemin arka plan kipinde harcadığı enerjiyi azaltmayı amaçlayan bir yöntem sunuyoruz. Yöntemimiz aygıttan çıkan ağ paketlerini belirli bir süre boyunca bir kuyrukta bekletmekte ve hepsini bir arada göndermektedir. Dolayısıyla, 3G modemin uyandırılma sayısını azaltarak enerji tüketimini düşürmektedir. Yöntemimizi Motorola marka akıllı telefon üzerinde gerçekleştirdik ve arka plan kipinde popüler mobil uygulamaları çalıştırarak ağ paket trafiği izlerini topladık. Bu izler üzerinden benzetim yaparak enerji tüketimi değerlerini çıkardık ve farklı koşullar için enerji kullanımında %39'a varan azalma gözlemledik.","In modern mobile devices, 3G modems are one of the most energy consuming components. Therefore, utilization efficiency of 3G modems have a significant impact on battery life. Utilization of 3G modems is mostly determined by the 3G network usage patterns of mobile applications. However, most of the mobile applications are developed being unaware of energy consumption characteristics of 3G modems. They request 3G network connection for mostly small and periodic data transmissions. Each data transmission awakens the 3G modem and speeds up the battery consumption. This inefficient connection behavior continues when the device is in the background mode (i.e., when the user is not interacting with the device). In this thesis, we propose a method to reduce energy consumption of 3G modems in background mode. Our method implements a queuing mechanism to delay outgoing packet traffic for a predefined time and send them in batches. Thus, it reduces the total number of 3G modem wake-up events and hence the energy consumption. We implement our method on a Motorola smartphone and collect background network traffic traces using well-known mobile applications. Using trace driven simulations, we demonstrate that our method can reduce 3G modem related energy consumption by up to 39% for different cases."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalem tabanlı etkileşimi destekleyen donanımlar uzun zamandır mevcut olmasına rağmen, bu sistemlerin girdilerinin etkin ve sağlıklı bir şekilde işlenmesi nispeten geri kalmış bir konudur. Bu durum kısmen çizim tanıma probleminin üssel karmaşıklığından kaynaklanmakta olup, bahse konu karmaşıklık orta büyüklükteki bir çizim için dahi bilinen yöntemler ile en iyi çözümü bulmayı imkansız hale getirmektedir. Mevcut çizim tanıma sistemleri, karmaşıklık problemini aşmak için ya basitleştirici varsayımlar kullanmakta, ya da en iyi olmayan çözümleri kullanıcıya sunmaktadır. Bu tezde, çizim tanıma problemi için, mevcut metodların aksine en iyi tanıma çözümünü çokterimli bir karmaşıklıkta bulabilen, eğitilebilir bir çerçeve yaklaşım sunulmaktadır. Sunulan yaklaşım, çevrimiçi çizimlerin yanısıra, çevrimdışı ve herhangi bir sıra izlemeyen çizimler için de kullanılabilmekte, kullanıcı girdisi hakkında herhangi bir varsayımda bulunmamaktadır. Önerilen yaklaşım, denetimli öğrenme teknikleri, çizge teorisi ve dinamik programlama metodlarına dayanmaktadır. Çalışmamızda sunulan çerçeve yaklaşım, aşağıdan yukarıya çizim tanıma probleminin iki temel alt problemi olan vuruşların parçalanması ve çizim bölütlenmesi problemlerine başarı ile uygulanmıştır. Ayrıca çevrimdışı ve belirli bir çizim sırası izlemeyen çizimler için, uzamsal sıralama metodu geliştirilmiş, uzamsal sıralama için birden çok çizge algoritması ve birden falza bağdaşım ölçüsü tanımlamıştır. Önerilen metodların doğruluğu ve çalışma süreleri birden çok veri setinde denenmiş, deneyler sonucunda, geliştirilen yaklaşımın, bazı veri setlerinde bilinen en iyi çizim tanıyıcılar ile başabaş sonuçlar verdiği, bazılarında ise farklı derecede iyi sonuçlar ürettiği tespit edilmiştir.","Hardware supporting pen-based interaction have been around for a long time, however progress in efficient and intelligent processing of input has been lagging far behind. This is partly due to the complicated nature of the sketch recognition problem. Optimal sketch recognition is intractable even for moderate-sized sketches. Recent methods deal with the problem either by making simplifying assumptions or by adopting sub-optimal methods. In this thesis, as an alternative to the sub-optimal methods, we describe an optimal and polynomial-time trainable framework for multi-domain sketch recognition. Our solution handles offline and interspersed sketches as well as online sketches, and it does not make assumptions about user input. Our unified framework is based on supervised machine learning techniques, graph theory, and dynamic programming. We apply the framework to two fundamental problems of bottom-up sketch recognition: stroke fragmentation and sketch segmentation. Dynamic programming approach is directly applicable to the fragmentation of strokes and segmentation of ordered primitives. For other cases, such as offline and interspersed sketches, we introduce the \textit{spatial serialization} concept to impose an order on the primitives. We propose different graph theoretic methods and coherence models to convert 2D points into an ordered set of primitives. We evaluate the accuracy and runtime of different serialization schemes on multiple datasets. For both fragmentation and segmentation problems, experiments show that the accuracy of the unified framework either matches with the state-of-the-art, or it surpasses them by a large margin."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Üveite bağlı vitröz bulanıklık, fundus fotoğrafında göz fundusunun ayrıntılarının seçilmesini zorlaştırır. Oftalmologlar fundus fotoğrafına bakarak, hastaların üveal ağındaki enflamasyonun şiddetinin bir göstergesi olarak, vitröz bulanıklık derecesini tespit ederler. Fundus fotoğraflarının manuel incelenmesi hem zaman kaybına yol açar hem de hatalara açıktır. Bu tezde, vitröz bulanıklığın otomatik fotoğrafik derecelendirilmesi için algoritmik bir metot öneriyoruz. Metodumuz iki kısımdan oluşmaktadır: öznitelik seti bulma ve sınıflandırma. Çeşitli sınıflandırma algoritmaları farklı derecede vitröz bulanıklığa sahip fundus resimleri ile eğitilmiştir. Doğrusal çekirdek fonksiyonuna sahip bir destek vektör makinesi ile sınıflandırılan kapsamlı bir öznitelik setinin sonuçları gösterilmiştir.","In the presence of vitreous haze due to uveitis, fundus details become less discernible in fundus photography. Ophthalmologists use fundus photographs in order to evaluate the vitreous haze level of patients as an indication of the severity of the inflammation in the uveal tract. Manual inspection of fundus photographs is time consuming, in addition to being error prone. In this thesis, we propose an algorithmic methodology for automated photographic grading of vitreous haze. The proposed algorithm is founded on automatic quality assessment and fundus image segmentation techniques. The method has two stages, namely feature set extraction and classification. Several classifiers are trained using a data set of fundus images with varying degrees of vitreous haze. Promising results are presented based on a comprehensive set of extracted features and a support vector machine based classifier with a linear kernel function."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Koşut-zamanlılık heryerde bulunmaktadır. Birçok internet, gerçek zamanlı ve gömülü yazılımlar doğası gereği koşut zamanlılık kullanıllarak tasarlanır. Fakat, koşut zamanlı yazılımları programlamak karmaşık ve hataya meyilli eylemlerdir. Doğasında bulunan tutarsızlık gereği, programcının yazdığı programın nasıl bir davranış göstereceğini anlaması zordur. Bir Swing dokümantasyonu der ki : ""Eğer kullanmaktan sakınabiliyorsanız, akışları kullanmaktan sakının. Akşları kullanması zor olabiliyor ve programınızın hata ayıklanma işlemini zorlaştırabiliyor. Genellikle, grafiksel bileşen özellikleri gibi kullanıcı arayüz ¸calışmalarında kesinlikle gerekli olmayabilirler."", [97]. Diğer bir dökümanda [98] ise : ""Aynı kümedeki kilitler için yarışan akışların neden olduğu kilitlenme akışlı programlarda çözülmesi en zor problemdir. O kadar zordur ki çözmeye bile teşebbüs etmeyiz. ... Buna rağmen, kilitlenmenin var olup olmadığını belirleyebilmek için tek seçenek kaynak kodun sıkı bir incelenmesidir..."". Sonuç olarak koşut-zamanlılıktan kaynaklananan hataları tespit etmek ve tekrar üretebilmek kaynak kodu gözden geçirme, tek akışlı bir program için geliştirilen sınama teknikleri gibi klasik yazılım mühendisliği teknikleri ile zordur. Çoğunlukla programlama dili gercekleştiricileri ve koşut-zamanlı kütüphane gerçekleştiricileri koşut-zamanlılık ile zorluk çekenlerdir. Bu temel olarak yazılım ve donanımın ilişkisini çözümlemenin zorluğundan kaynaklanır. S.Adve ve H-J.Boehm der ki: ""Donanım mimarları için zorluğun bir parçası programlama dilleri seviyesinde açık bir bellek modelinin olmamasıdır. Programcının donanımdan ne bekleyeceğini muallakta bırakmaktadır"", [31]. Bütün bu problemler, yazılım ve donanım ilişkisini formal bir şekilde belirtmeyi önemli kılmaktadır çünkü formal modeller açıkça belli olmayan tutarsızlıkları ifade edebilir. Bu formal tanımlama çabasının ana ürünü bellek modellerinin üzerinde gerçekleştirildi, örneklerden biri geleneksel paylaşımlı belleklerde ardışık tutarlılık, diğeri işlembilgisel belleklerde serileşebilmedir. Bu modelleme çalışmaları bir programın beklenen davranışlarını formal olarak tanımladığı için önemlidir. Bu modelleme çalışmaları geleneksel paylaşımlı belleklerde yarış-içermeyen (DRF) ve işlembilgisel belleklerde ¸celi¸ski-i¸cermeyen gibi s¨ozle¸smeler sa˘galyarak g¨uvenilirlik garantileri tanımlarlar. Çoğunlukla daha çok başarıma ihtiyaç duyulduğundan ardışık tutarlılık veya sıkı serileşebilme gibi sözleşmeleri zorunlu kılarak yazılım uygulamalar için işlemcileri yavaşlatmaya gerek yoktur. Bellek erişimini optimize etmek için, bazı bellek modelleri yazmaların atomikliğini gevşetebilir, makina komutlarının yerlerini değiştirebilir veya okumalardan sonra gerçekleşen yazmaların yarattığı çelişkileri göz ardı edebilir. Bu durumda, algoritma veya koşut-zamanlı veri yapısı tasarlarken bu tip güçlü modeller, örneğin, ardışık tutarlılık (SC) veya sıkı serileşebilme, açısından düşünmek doğru program üretmek için güvenilir olmayabilir. Sonuç olarak daha gevşek memory modelleri sunulmuştur. Bu tezde sırası ile geleneksel paylaşımlı bellek alanında ve işlembilgisel bellek alanında iki gevşetilmis bellek modeli olan yazmaların-tam-sıralaması (TSO) ve bellekkopyası-soyutlanma (SI) üzerinde çalışan programlar için durağan doğrulama yöntemi sunuyoruz. Kısım 2 içersinde, bellek-kopyası-soyutlama ve benzeri gevşek bellek modelleri üzerinde çalıştırılan programların doğrulanması için durağan doğrulama yöntemi sunuyoruz. Bellek-kopyası-soyutlama ve benzeri gevşek bellek modeller oldukça fazla kullanılan modellerdir. SI altında çalışan işlemler için ardışık olarak çalışıyormuş gibi yorum yapabilme imkanı kaybolur, bu işlemler artık serileşebilir işlemler değildir. Bu tezde önceki yapılan işten [23] farklı olarak, serileşemez programları da kotarıyoruz. Bir kaynak kodu diğer kaynak koda SI semantiğini içerecek şekilde dönüştürmeyi sunuyoruz. Dönüştürülmüş programın kullanıcı tarafından kaynak koda eklenen ek sözleşme dipnotları ile birlikte doğrulanması orjinal programın SI altında doğrulanmasına denktir. Bizim SI semantiği için önerdiğimiz dönüştürme tekniği VCC'nin modülerliğini ve ölçeklenirliğini korumaktadır. Yöntemimizi işlembilgisel bellek literatüründeki deneme programlarına uyguladık [95]. Kısım 3 içersinde, x86-TSO bellek modelleri üzerinde çalışan programlar ile ilgili kolay yorumlamayı sağlamak için program iyileştirmeyi sağlayan kanıt yöntemi sunuyoruz. 3.4 bölümünde kaynak kodun içine TSO semantiğini ek kod olarak yerleştiriyoruz ve programın yorumlanmasını kolaylaştırana kadar TSO'ya özgü iyileştirme adımlarını uyguluyoruz, [94]. TSO'ya özgü işlemci yerel yazma tamponlarını ve belleğe yazma işlemini soyutlanmış halını kaynak kodun içine yerleştiriyoruz. Bu yöntemde belirli proglamlama kalıplarının belirli iyileştirme kalıpları ile örtüşüp örtüşmediğini gözlemliyoruz. Bu yöntemin kullanılabilir olduğunu mekanik iyileştirmelerini engelsiz kilitleme yöntemi Spin-Lock ve acquire/release programlama kalıbı gösteren Send-Receive örneği için 3.4.7 kısımında 3.4 ile gösterdik.","Concurrency is everywhere. Many internet, real-time and embedded applications are most naturally designed using concurrency. However, programming conccurent software is complex, error-prone task. Because of inherent non-determinism, it is difficult for the programmer to understand the effect of his or her program. The Swing documentation states : ""If you can get away with it, avoid using threads. Threads can be difficult to use, and they make programs harder to debug. In general, they aren't necessary for strictly GUI work, such as component properties"", [97]. Another note in [98] states ""Deadlock between threads competing for the same set of locks is the hardest problem to solve in any threaded program. It is a hard enough problem, in fact, that we will not solve it or even attempt to solve it. .... Nontheless, a close examination of the source code is the only option available to determine if deadlock is a possibility ..."". As a result we can state that concurrency bugs are difficult to detect, reproduce and diagnose using conventional software engineering techniques such as code review and test-based techniques developed developed for sequential programs. Generally, language and concurrency library implementers are the ones who have iv difficulty with concurrency. This is mainly due to hardness of resolving the internals of interaction of hardware and software to build reliable software. S.Adve and H.-J.Boehm state: ""Part of challenge for hardware architects was the lack of clear memory models at the programming language level. It was unclear what programmers expect hardware to do"", [31]. All these problems make formalising HW/SW interaction crucially important because formal models can express aspects of non-determinism that are not apperent. The main product of this formalisation effort has been made on memory models; e.g sequential consistency (SC) in traditional shared memory, serializability in transactional memory. They are important because they define and formalize the expected behaviours of a program. They define safety guarantees for programs with providing contracts such as data-race-freeness (DRF) in traditional shared memory, conflict free in transactional memory between hardware and software; e.g. data-race-free program under SC model. Mostly due to performance need, we do not need to enforce sequential consistency or strict serializability in transactional memory and slow down the processors for some applications. To optimize memory access, some memory models may introduce relaxing atomicity of writes, allow instruction reordering or ignore write-after-read conflicts. In this case, thinking in terms of these strong models, e.g. SC or strict serializability, when designing concurrent data structures or algorithms cannot be reliable to produce correct programs. As a result, weaker memory models are proposed. In this thesis, we propose static verification methods for programs running on two weak memory models such as total-store-order (TSO) and snap-shot-isolation (SI) models respectively from traditional shared memory and transactional memory domains. In chapter 2, we present a static verification approach for programs running under snapshot isolation (SI) and similar relaxed transactional semantics. Relaxed conflict detection schemes such as snapshot isolation (SI) are used widely. Under SI, transactions are no longer guaranteed to be serializable, and the simplicity of reasoning sequentially within a transaction is lost. In this thesis, we present an approach for statically verifying properties of transactional programs operating under SI. Differently from earlier work [23], we handle transactional programs even when they are designed not to be serializable. We present a source-to-source transformation which augments the program with an encoding of the SI semantics. Verifying the resulting program with transformed user annotations and specifications is equivalent to verifying the original transactional program running under SI – a fact we prove formally. Our encoding preserves the modularity and scalability of VCC's verification approach. We applied our method successfully to benchmark programs from the transactional memory literature [95]. In chapter 3, we propose a proof method approach for refining programs to make them easy to reason under x86-TSO model. In this approach we introduce store buffers explicitly into source and try to refine program via applying TSO specific sound refinement steps [94]. We put the store buffers into source and abstract the TSO memory flush operation explicitly in the source. In this approach, we try to observe whether specific type of programming patterns result in specific type of refinement pattern. We show applicability of the approach with mechanized refinement of mechanized refinement of non-blocking Spin-Lock and acquire/release pattern SendReceive in Section 3.4.7 by approach 3.4."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir kelimenin anlamlarını elle hazırlanmış sözcüksel bir veritabanı ile temsil etmenin birden çok problemi bulunmaktadır. Bu tip sözlükler bir kelimeye ait tahmin ettiğimiz anlamı işaret edecek bir kayıt bulundurmayabilirler; zirâ sözlükler çok genel hazırlanmaktadırlar. Bu sözlükler bir kelimenin çok nâdir kullanılan anlamlarını içerirler fakat alana özgü anlamları barındırmayabilirler. Sözcük anlamı tümevarımı (word sense induction) kelimenin anlamlarını (kullanımlarını) ayırırken elle oluşturulmuş, durağan bir kaynak yardımı almadığından bu tip problemleri çözmektedir. Diğer popüler çözümlerin (muğlaklığı giderilmek istenen kelimenin birinci dereceden ya da ikinci dereceden birlikte görüldüğü diğer kelimeleri kullanarak elde edilen bir temsil üzerine kümeleme ya da çizge bölümlemesi gibi) aksine tezde takip ettiğimiz yöntem; muğlaklığını gidermek istediğimiz kelimeyle aynı bağlamda kullanılması ihtimâli yüksek olan kelimeleri ve bu kelimelerin olasılıklarını bir dil modeli yardımıyla bulup, bunları S-CODE adlı Öklitsel gömme (embedding) algoritması vasıtasıyla modellemektir. Bu algoritma ile kelime türü gömülerine (word embedding) ek olarak, kullandığımız yöntemle bağlam gömülerini (context embedding) de elde etmekteyiz. Bağlam gömüleri daha sonra $k$-means algoritması yardımıyla kümelenerek hedef kelimemize ait farklı kullanımlar ayrıştırılmaktadır. Bu yöntemin benzerleri önceki yıllarda gözetimsiz sözcük türü tümevarımı (unsupervised part-of-speech induction) ve gözetimli bağlılık ayrıştırması problemlerinde başarılı olmuşlardır. Yöntemimizi SemEval 2010, SemEval 2013 kelime anlamı tümevarımı yarışmalarında hazırlanmış veri kümelerinde ve OntoNotes projesinin (versiyon 5.0) işaretlediği cümleleri kullanarak bizim oluşturduğumuz, yüksek uzlaşmaya (>%90) ve en az 500 örneğe sahip kelimelerin bulunduğu bir sözcük veri kümesinde test ettik. Bu tezin literatüre katkısı şöyle özetlenebilir. (1) Sözcük anlamı tümevarımı problemi için bağlam temsillerinden yararlanan bir yöntem öneriyoruz. (2) (a) Literatürde önerilen kelime temsillerini, aynı bağlamda kullanılması ihtimâli yüksek olan kelimelerin olasılıklarını kullanarak bağlam temsillerine dönüstürüp kendi onerdigimiz bağlam temsilleriyle kıyasladık. (b) Farklı kümeleme algoritmalarını ($k$-means, Spectral Clustering, DBSCAN) ve bu algoritmaların Sözcük Anlamı Tümevarımı problemine özel farklı yaklaşımlarını (lokâl yaklaşım, sözcük türüne bağlı yaklaşım) kıyasladık. Son olarak, OntoNotes projesini kullanarak olusturduğumuz veri kümesini hazırlayan prosedürü, ilerde aynı problemle uğraşacak araştırmacılara yardımı dokunması ümidiyle paylaştık. Araştırmacılar aynı parametrelerle aynı veri kümesini üretip, bizim önerdiğimiz yöntem ile kendi sistemlerini karşılaştırabilir; yâhut farklı parametrelerle kendilerine uyacak bir veri kümesi üretebilirler. Yapılan çalışmayı ve alınan sonuçları tekrarlamak için gerekli kodlara https://github.com/osmanbaskaya/wsid adresinden ulaşılabilir.","There exist several drawbacks of representing the word senses with a fixed list of definitions of a manually constructed lexical database. There is no guarantee that they reflect the exact meaning of a target word in a given context, since they usually contain definitions that are too general. More so, lexical databases often include many rare senses while missing corpus/domain-specific senses. Word Sense Induction (WSI) focuses on discriminating the usages of a polysemous word with- out using a fixed list of definitions or any hand-crafted resources. In contrast to the most common approach in WSI, which is to apply clustering or graph partitioning on a representation of first- or second-order co-occurrences of a word, my method obtains a probability distribution for each context suggested by a statistical model. This distribution helps to create context embeddings us- ing the co-occurrence framework that represents the context with low-dimensional, dense vectors in Euclidean space. Then, these context embeddings are clustered by k-means clustering algorithm to discriminate usages (senses) of a word. This method proved its usefulness in Unsupervised Part-of-Speech Induction, and su- pervised tasks such as Multilingual Dependency Parsing. I examine this method on SemEval 2010 and SemEval 2013 Word Sense Induction lexical sample tasks, and the dataset I created using OntoNotes 5.0. This new lexical sample dataset has high inter-annotator agreement (IAA) (>90%) and number of instances for each word type is more than any previous lexical sample tasks (>500 instances). The contributions in this thesis are as follows: (1) I suggest a method to attack the Word Sense Induction problem. (2) I provide a comprehensive analysis (a) in embedding step by comparing other popular word embeddings by transforming each of them to context embeddings using substitute word distributions for each context, and (b) in clustering step by comparing different clustering algorithms (k- means, Spectral Clustering, DBSCAN) and different clustering approaches (local approach where instances of each word type clustered separately, and part-of-speech based approach where instances tagged with same-part-of-speech clusters indepen- dently). The code to replicate the results in this thesis can be found at https://github.com /osmanbaskaya/wsid."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hücredeki biyolojik süreçler çoğunlukla birçok protein - protein etkileşim ağları tarafından yürütülmektedir. Hesaplamalı biyolojinin amaçlarından biri proteinler arasındaki etkileşimlerin tahmini ve bir hücrenin nasıl çalıştığını anlamak için hesaplamalı yöntemleri kullanmaktır. Günümüzde, protein etkileşim verilerini oluşturmak için yaygın olarak kullanılan bir çok deneysel ve hesaplamalı yöntemler vardır. Öncü hesaplamalı biyoloji araçlarından biri de PRISM'dir. Geliştirilen ilk şablon tabanlı protein etkileşim tahmin aracı PRISM'dir. PRISM birçok yayın da başarısını kanıtlamış ve yaygın olarak protein-protein etkileşimlerin yapısal modellemelerin tahmini amacıyla kullanılmıştır. Ancak, protein-protein etkileşimleri ağları üzerinde kullanılmasının, hem performans hem de kullanılabilirlik açısından sınırlamaları vardı. Bu tezde, PRISM'in yeni bir sürümünün tasarlanması (PRISM2.0) ve protein etkileşim ağları üzerinde etkili ve kolay kullanımı olan bir araç olması amaçlanmıştır. PRISM2.0 birbirinden bağımsız modüllerden inşa edilmiştir, bu nedenle kullanılması ve daha da geliştirilmesi kolaylaştırılmıştır. PRISM2.0'nin web sunucu sürümü de bir protein-protein etkileşimleri havuzunu oluşturmak için geliştirilmiştir. Bu havuz büyüdükçe, protein-protein etkileşimleri için zengin bir kaynak olması amaçlanmaktadır. Kullanıcılar, herhangi bir kurulum ya da bilgisayar dünyası hakkında daha fazla bilgi sahibi olmadan web sunucusunu kullanarak PRISM 2.0 kullanabilmektedirler. Bu araçların biyolojik yolların yapısal modellemesi ile ilgilenen bilim insanları için faydalı olacağına inanmaktayız.","Biological processes in the cell mainly are carried out by protein – protein interaction networks among numerous proteins. One of the goals of the computational biology is to use mainly computational methods to predict interaction among proteins and understand how a cell functions. Today, there are many experimental and computational approaches used extensively to generate protein interaction data. One of the pioneer computational tools is PRISM. PRISM is the first template based protein – protein interaction prediction tool developed. PRISM has proven its success at many publications and used widely for prediction and structural modeling of protein-protein interactions as a stand-alone tool. However, its application to pathways, a network of protein-protein interactions, has both performance and usability limitations. In this thesis, a new version of PRISM (PRISM2.0) has been designed and implemented to make it efficient and easy-to-use for network of proteins predictions. PRISM2.0 is built from loosely coupled modules, therefore it is easy to use and develop further. A web server version of PRISM2.0 has been developed to create a protein–protein interaction repository. As the repository grows, it is expected to be a rich resource for protein-protein interactions. Users can enjoy using the web server without any installation or any further information about computer world rather than browsing the web page. We believe these tools will be beneficial for the community who is interested structural modeling of biological pathways."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Duygusal patlamaların, altında barınan duygulanımların anlaşılmasında önemli bir rol oynaması sebebiyle, bu barınan duyguların tanınması araştırmacıların önemli bir konusu olmuştur. Kaynaklarda, özellikle kahkalardan beslenen ve sadece sese dayanan girdileri kullanarak, duygusal patlamaların tanınmasında bir çok yaklaşım geliştirilmesine rağmen, çok kipli yaklaşımların duygusal patlamaları elde etmede kulanılması daha etkili sistemleri kullanılmasını gerektirdiği için çok yaygın değildir. Bu tezde, güncel olarak kullanılan ve bizim sunduğumuz yaklaşımları çeşitleri ile sunmaktadır. Bu yaklaşım, önce duygusal patlamaları açığa çıkaran ve sonra bu patlamaları gruplandıran iki katmanlı bir sıra düzeni kapsamaktadır. Bu aşamalı sınıflandırmada, işitsel ve yüzdeki belirli noktalarından elde edilen ipuçları birleştirilmiştir. Deneysel değerlendirmeler için, yüksek kaliteli ses ve yüzdeki belirlenen noktaların haraketlerinin düzgünleştirilmiş yakalanımını içeren, doğal ve gerçekçi konuşmaları içeren Interactive emotional dyadic motion capture database (IEMOCAP) kullanılmıştır. İlk önce duygusal patlamaların IEMOCAP veri tabanındaki etkileri belirtilip, önerdiğimiz yaklaşımı bu data üzerinde test edildi. Deneyler sonucunda yaklaşımımızın duygusal patlanımları elde etme ve sınıflandırmada sadece görsel veya işitsel tek kipli projelerden gözle görülebilir bir biçimde daha iyi çalıştığını görüldü. Ayrıca, yaklaşımımızın verimini kaynaklarda referans olarak alınan çeşitli sınıflandırma algoritmaları ile olan karşılaştırılması sunulmuştur.","Affect bursts play a critical role in recognizing underlying emotions and hence their detection is of particular interest to researchers. In the literature, there are a number of approaches for detecting affect bursts, particularly laughters, from audio-only inputs, but multimodal approaches are limited while they can provide more effective systems for the affect burst detection. This thesis presents the current state of the art as well as our approaches to detect affect bursts along with their type. This includes a two layer hierarchical classifier, first to detect affect burst events and then to classify these events into affect burst types. In the hierarchical classifier we combine cues from audio and facial landmark points. In experimental evaluations we use the Interactive emotional dyadic motion capture database(IEMOCAP), which contains realistic and natural dyadic conversations with high quality audio and normalized motion capture of facial landmark points. We firstly annotate affect bursts events in the IEMOCAP database and test our proposed approach over it. We observe significant performance improvements with the multimodal approach over audio-only and visual-only unimodal schemes in detecting affect bursts along with their types. We also show comparison among different types of classifiers which serve as baseline."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein yapısal hizalanması iki protein molekülü arasındaki mesafeyi en aza indirecek olan ideal dönüşüm matrisinin bulunması işlemidir. Proteinlerin 3 boyutlu yapılarının en önemli karakteristik özelliklerinden biri olması ve Protein Veri Bankası'na (PDB) eklenen yeni yapı sayısının üstel olarak artması nedeniyle proteinlerin yapısal hizalanması konusu oldukça kapsamlı olarak çalışılmaktadır. Yapısal hizalama işlemi proteinlerin evrensel bilgilerinin elde edilmesine, proteinlerin görevlerinin belirlenmesine, homolog proteinlerin tanımlanmasına ve proteinlerin yapısal olarak sınıflandırılmasına olanak sağlamaktadır. Bu tez kapsamında, iMatch adı verilen yeni bir yapısal hizalama yöntemi öne sürülmektedir. iMatch, MultiProt yazılımının ikili hizalama bileşeni temel alınarak ve bağlanma bölgesi – protein yüzeyi hizalaması probleminin yapısal bileşeni olacağı göz önüne alınarak tasarlanmıştır. iMatch bir nesne tanıma yöntemi olan geometrik karım tekniğini kullanarak proteinlerin bölgesel benzerliklerini tespit eder. Bu bölgesel eşleşmeler daha sonrasında dönüşüm matrislerinin benzerliklerine göre gruplandırılır ve uzatılarak küresel eşlenme elde edilir. Eşlenme sürecinde kullanılan bulgusal teknikleri ve iMatch'in çalışma mekanizması detaylı bir şekilde açıklanmıştır. iMatch parametrik yapısından dolayı kolaylıkla özelleştirilebilir bir yapıya sahiptir. Bu esneklik iMatch'in yapılması gereken görev türüne göre ayarlanabilmesine olanak sağlamaktadır. Ayrıca, bu parametrik yapı iMatch'e bağlanma bölgesi – protein yüzeyi hizalanmasına uyum sağlaması becerisini kazandırır. Varsayılan ideal parametreler iki adımlı bir optimizasyon süreci sonucunda belirlenmiştir ve bağlanma bölgesi – protein yüzeyi hizalanması alanında gelecekte yapılabilecek çalışmalar için her bir parametrenin hizalama doğruluğu ve süratine olan etkisi detaylı olarak incelenmiştir. iMatch'in genel performansı üç farklı tanınmış veri setinde ve yapısal hizalama alanında rüştünü ispat etmiş günümüzde kullanılan yöntemlere karşı değerlendirilmiştir.","Protein structure alignment is the task of finding an optimal transformation between two protein structures that minimizes the distances between two molecules. Since 3D protein structure is one of the most important characteristics of proteins and the number of deposited structures in the Protein Data Bank (PDB) is exponentially increasing, the structural alignment of proteins has been studied extensively. The structural alignment of proteins can provide evolutionary information, allow prediction of function, identify homologs and provide a classification mechanism. In this thesis, a new structural alignment method called iMatch has been proposed. iMatch is based on the pairwise structural alignment component of MultiProt and is designed as the structural alignment component for alignment of binding sites onto protein surfaces. iMatch uses the object recognition method geometric hashing to identify the similar local regions on the proteins. These local regions are then clustered according to their transformation similarities and extended further to obtain a final global alignment. The heuristics used throughout the alignment process and the working mechanism of iMatch has been explained in detail. iMatch is a highly customizable alignment method due to its parametric nature. This flexibility allows the configuration of iMatch according to the task in hand for better accuracy. Furthermore, this parametric nature grants iMatch the ability to be adjusted for the binding site – surface alignment. The default optimal parameters have been identified by a two-step optimization process and each parameter's effect to the alignment speed and accuracy has been inspected thoroughly for future studies on binding site – surface alignment. The overall performance of iMatch has been evaluated on three different well-known datasets against state-of-the-art structural alignment methods existing today."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sıralamayı öğrenme alanındaki araştırmalar eğitim veri kümesinin verildiğini farzeden gelişmiş öğrenme algoritmalarına dayanmaktadır. Ancak eğitim veri kümesinin kalitesi sıralamayı öğrenme başarımını etkiler. Sorgu ile sonuç arasındaki ilişki kararını elde etmenin maliyeti ve bütçe kısıtı göz önüne alındığında, bir eğitim veri kümesi ancak sınırlı sayıda ilişki kararı içerebilir. Bütçenin sınırlı olduğu bir durumda, kararlama için sarfedilen çabanın çeşitli sorgular için önceliklendirilmesi ve bilgi erişim sistemlerinin etkili ve verimli bir şekilde değerlendirilmesi konusunda birçok araştırma vardır. Ancak, tamamlanmamış kararların sıralamayı öğrenmedeki etkileri hakkında çok az araştırma yapılmıştır. Varolan araştırmalarda, herhangi bir amaç metriği ve tamamlanmamış kararları oluşturan örnekleme dağılımı için uygulanabilir çözüm bulunmamaktadır. Bu çalışmada, her sorgu için az sayıda karar barındıran eğitim veri kümelerinin daha iyi nasıl kullanılabileceğine ve bu kümeleri kullanarak nasıl dahha iyi sıralama başarımı elde edebileceğine odaklandık. Bu amaçla, katmanlama örnekleme yöntemi kullanarak tamamlanmamış kararlar oluşturduk. Bu kararları kullanarak, StatAP yöntemi ile amaç fonksiyonu olan ortalama kesinlik değerlendirme metriğinin tarafsız tahminini hesapladık. Ardından, gerçek ve tahmini amaç metriklerinin LambdaMART algoritması ile eniyi değerlerini bulduk. Tahmini ortalama kesinlik modelinin tamamlanmamış kararlar kullandığımızda, gerçek ortalama kesinlik modeline göre daha iyi başarım elde ettiğini gösterdik. Ortaya sunduğumuz yöntemin herhangi bir veri kümesi için uygulanabilir olduğunu gösterebilmek amacıyla, yöntemimizi iki farklı veri kümesinde test ettik.","Research in learning to rank has been placed on developing sophisticated learning algorithms mainly, assuming the training set as a given. However, the quality of the training set directly affects the quality of the learned ranking systems. Considering the expense of obtaining relevance judgements and the budget constraint, one can get limited number of judgements to construct a training set. Much research has been devoted to distribute the judgement effort across different queries, and efficient and effective evaluation of retrieval systems given a limited judgement budget. However, little research has been done regarding the effect of incomplete judgements in learning-to-rank and available studies do not propose a solution to the problem that can be applicable given any objective metric and any sampling distribution that used to generate the incomplete judgements. In this work, we focus on how to better utilize training sets with shallow (less) judgements per query and obtain better ranking performance using such training sets. For this aim, we generate the incomplete judgements using stratified sampling strategy and use StatAP method to compute unbiased estimates of the objective evaluation metric, i.e. mean average precision (MAP), given these judgements. Then, we use LambdaMART algorithm to train a ranking model by using the estimated and actual objective metrics. By using two different datasets, we show that estimated MAP performs significantly better than actual MAP as the training objective in learning to rank when judgements are incomplete."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Donanım ve akıllı mobil cihazların ilerlemesi, ve İnternet'in yaygınlanması, yeni BT hizmetleri'nin gelişmesine sebep olur. Dolayısıyla, daha fazla veri işletmeler ve bireyler tarafından oluşturulup ve kullanılır. Bu, verilerin toplumlarda ne kadar önemli bir rolü olduğunu göstermektedir. Veriler çağın en değerli varlıklarıdır. Çeşitli olaylar verileri tehdit etmektedir: Yangın ve sel gibi doğal afetler, bilgisayar virüsleri, güvenilmeyen çalışanlar, vs. Bulut depolama verileri tutmak için güvenilir bir platform sunuyor. Maliyet tasarrufu, veriye küresel erişim ve düşük yönetim yükü gibi avantajlar getiren bulut depolamanın, aynı zamanda bazı kötü yanları da bulunmaktadır: veri sahibi kendi verilerinin üzerine direk kontrolünü kaybedebilir, ve güvenilir olmayan başkaları verilere erişim hakkı kazanabilir. Bu tez güvenilir bulut depolama hizmeti sağlamayı hedeflemektedir. Burada güvenilirlik farklı güvenlik özellikleri için kullanılmaktadır. Erişilebilirlik istemcinin bulunduğu herhangi bir yerde ve herhangi bir zamanda, verilerine erişmesini sağlamak anlamına gelir, bu tanımın bir parçasıdır. Biz dağıtım ve replikasyon düzeyini kullanarak, önemli ölçüde arama ve güncelleme süresini azaltırken, erişile- bilirliği destekleyen bir düzeni öneriyoruz. Üstelik, silinti düzeltme kodları kullanmakla hazır bulunmayı sağlamayı hedefliyoruz. Veri bütünlüğü bulut hizmet sağlayıcı tarafından istemciye her cevap ile birlikte cevap doğruluğunu kontrol etmek için bir kriptograﬁk kanıt vermesi gerekir demektir. Önerdiğimiz tüm planlar kötü niyetli servis sağlayıcılar varlığında bütünlüğü sağlayabilir. Güvenilirliğin ayrı bir parçası olan veri gizliliği, önceden belirlenmiş olanlar harici hiç kimsenin verilere erişememesi güvencesini kapsar. Biz kötü niyetli ortamlarda dinamik veriler için gizliliğini ve bütünlüğünü destekleyen bir aranabilir simetrik şifreleme düzeni öneriyoruz.","As the hardware and smart mobile devices progress and Internet becomes pervasive, new sets of IT services appear. These services are fed in by data and generate new data as output. Therefore, more and more data is generated and used by enterprises and individuals which confirms the important role of data. Data is the most valuable asset of this era. There are several threats targeting data, ranging from natural disasters such as fire and flood to computer viruses and crashes, and from untrusted employees having access to data to outside adversaries. Cloud storage offers a trusted platform to host others' data, and brings advantages such as cost saving, global access to data, and reduced management overhead, while posing some disadvantages: The data owner looses the direct control over her data, and some other unauthorized people gain access to the data. This thesis targets providing reliable cloud storage services. The reliability is used here for different security properties. It includes availability which means that the client can access her data at any time and any location. We propose a scheme that supports availability through distribution and replication, while improving search and update time considerably. Moreover, we propose a generic framework for constructing proof of retrievability schemes that aim achieving availability through employing erasure-correcting codes. It also encompasses data integrity which means that the cloud service provider gives a cryptographic proof along with each answer he sends to the client, enabling her to check correctness of the answer. Our schemes provide integrity in the presence of malicious adversaries. We also investigate database outsourcing and its security requirements (in a unified security definition): completeness, correctness, and freshness. Moreover, it comprises data confidentiality, which gives the assurance that no one except those who were determined by the client already, can access the data. We propose a searchable symmetric encryption scheme that supports both confidentiality and integrity for dynamic data in malicious settings."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"PDZ domain'i (PSD-95/Discs-large/ZO-1 homology) tek ve çok hücreli organizmalarda evrimsel süreç boyunca oldukça korunmuş ve en bol miktarda bulunan domain ailelerinden biridir. Evrimsel korunmuşluğun ve birçok organizmada yaygın olarak bulunuyor olmasının işaret ettiği gibi PDZ domain'inin önemli sayıda hücresel fonksiyonun yerine getirilmesinde rolü büyüktür. Bu fonksiyonlar arasında vesiküler taşınım, nöronal sinaptik bağlantılar, nöron gelişim ve dağılım örnek gösterilebilir. Dolayısıyla, PDZ domain işlevinde meydana gelecek bir aksama, Usher's sendromu, epilepsi, şizofreni ve bazı kanser türleri gibi oldukça ciddi hastalıklara sebebiyet verebilir. PDZ domainleri 80-100 amino asit uzunluğunda, iki α-helix (αA- αB) ve six β-sheet (βA to βF) ten meydana gelir. PDZ domaininin en yaygın bağlanma şekli olarak bilinen 'canonical' etkileşimi, domain üzerinde bulunan bir bağlanma oluğu ile hedef proteinin C-terminal ucundaki 5 amino asitlik peptidler ile gerçekleşir. PDZ domainleri etkileşimlerindeni ligand seçimlerindeki özelleşmeye dayanarak Tip I, Tip II ve Tip III olmak üzere 3 sınıfa ayrılırlar. Genel olarak PDZ domainleri belirli bir sınıf peptide bağlansalar da, aralarında hem Tip I hem de Tip II ile etkileşebilen omainler olduğu bilinmektedir ve bunlar Tip I-II olarak sınıflandırılmaktadır. Bu çalışma, PDZ domainlerinin yapısal özelliklerinden faydalanarak, onlara yeni ligandlar bulunması ve sınıflandırılması üzerine yoğunlaşmaktadır. Deneysel olarak etkileşim bilgileri bulunan domainlerin ve ligandlarının özelliklerinden faydalanarak, yapay öğrenme ile etkileşim tahmini ve sınıflandırma için modeller geliştirilmiştir. Bu modelleri geliştirmek için en güvenilir yapay öğrenme algoritmalarından biri olan 'support vector machine (SVM)' kullanılmıştır. Bu modellerin performans analizleri çapraz validasyon ('cross-validation') testlerinin yanı sıra insan proteomunun taranıp, doğru tahmin edilen etkileşimlerin istatistikleri kullanılarak yapılmıştır. Sonuç olarak, sırasıyla 0.99 ve 0.91 AUC değerlerine sahip etkileşim tahmin eden model ve sınıflandırma modeli geliştirilmiştir. Insan proteomu tarama sonuçlarına göre de etkileşim tahmin eden model, PDZ domainleri aracılığıyla gerçekleşen etkileşimleri 17 % lik doğruluk oranı ile tahmin etmiştir. Ayırca, PDZ domaini genel-geçer sınıflandırma sisteminin doğruluğunu da güçlendirmiştir. Bu modeller ileride PDZ domaini barındıran proteinlerin yeni ligandlarını bulmak ya da bu proteinleri hedefleyen ilaç molekülleri tasarlamak amacıyla yapılacak olan çalışmalarda, aday olacak peptidlerin sayıca aza indirgenmesinde kullanılabilir.","PDZ domains (PSD-95/Discs-large/ZO-1 homology) are one of the most abundant and evolutionary conserved domain families through uni- and multi-cellular organisms. As its abundance and high evolutinary conservation indicates, PDZ domains mediate a number of distinct functions in the cell including vesicular sorting, neuronal synaptic plasticity, development and neural guidance. Therefore, malfunction of the PDZ domain causes several crucial diseases such as Usher's syndrome, epilepsy, schizophrenia and types of cancer. PDZ domains are 80-100 residues long and consist of two α-helices (αA- αB) and six β-sheets (βA to βF). The canonical interaction is the most common interaction type of PDZ domain where the PDZ domain binds the C-terminal of the target protein via the binding cavity. PDZ domains are categorized into three classes according to the motif of their binding partners as Class I, Class II and Class III. Altough, PDZ domains prefer to bind a particular class of peptides there are cases where the PDZ domain can interact with both Class I and Class II peptides, classified as Class I-II. This study focuses on building prediction models for PDZ domain mediated interactions and PDZ domain classification by using their structural features. By utilizing the properties of the PDZ domains and their ligands, those have the available interaction experimental data, an interaction prediction and classification model was built via machine learning approaches. One of the most robust machine learning approach, support vector machine (SVM) algorithm, was selected to train the models. The interaction prediction and the classification models performances were evaluated by cross-fold validation test and validation of human proteome scanning results on experimentally known interactions data. The interaction prediction model and the classification model have area under ROC curve with a number of 0.99 and 0.91, respectively. Moreover, the human proteome scanning results showed that the interaction prediction model was able to predict the known PDZ domain mediated interactions correctly with a TP rate of 17 %. Additionally, the general knowledge of classification of PDZ domains were supported by our results. These models could be utilized by future experimental studies to narrow the search space of the novel binding partners of PDZ domains as well as drug discovery studies that target PDZ domain containing proteins."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalem temelli donanımlar yaygınlaştıkça, çizim temelli akıllı arayüzler de daha popüler hale gelmektedir. Bu arayüzler çizim tanıma teknolojisinden faydalanarak doğal ve verimli insan-bilgisayar etkileşimi sağlamaktadır. Fakat bütün çizim tanıma sistemleri aynı zamanda çizim tanıma hataları da yapmaktadır ve bu hataları kullanıcılar düzeltmek durumundadırlar. Bu durum kullanıcının üzerine bir düzeltme yükü bindirmektedir. Bir çizim yanlış tanındığı zaman, kullanıcı hatayı düzeltme niyetini açıkça ya da üstü kapalı bir şekilde belli etmektedir. Ardından ya çizimini yeniden yaparak ya da bir sahneye yerleştirmek istediği sembolü bir sembol listesinden seçerek hatayı düzeltmektedir. Bu çalışmada çizim tanıma hatalarını düzeltmenin yükünü hafifletmek için, kullanıcının çizim tanıma hatasını düzeltme niyetini bakış verisinden faydalanarak belirleyebilen bir sistem sunuyoruz. Kullanıcıların çizimlerin yanlış tanınması halinde gösterdikleri tepkilerin karakteristik göz hareketlerine yol açtığını gösteriyoruz. Ayrıca bu göz hareketlerinin kullanıcılar henüz düzeltmeye başlamadan önce onların düzeltme niyetlerini okumak için kullanabileceğini gösteriyoruz. Bu çalışma 3 temel katkı sağlamaktadır. İlk olarak iki farklı çizim temelli etkileşim senaryosunda göz hareketlerinin kaydedildiği ve özenle kurgulanmış bir ""Wizard of Oz"" (Oz büyücüsü) deney düzeneği sunuyoruz. Daha sonra kullanıcıların göz hareketlerinin nitel karakteristiklerini ifade eden bir öznitelik seti sunuyoruz. Son olarak kullanıcıların çizim tanıma hatalarını düzeltme niyetlerini %86 doğrulukla tespit edebilen bir makine öğrenmesi taslağı sunuyoruz. Bulgularımızı kalem temelli etkileşim sırasında oluşan doğal göz hareketlerinden ne kadar anlam çıkarılabileceğini gösteren ayrıntılı deneyler ve istatistik analizler ile destekliyoruz.","Sketch based intelligent interfaces are gaining popularity as pen based hardware becomes more widespread. These interfaces make use of sketch recognition technology to facilitate natural and efficient interaction. Nevertheless all sketch recognition systems suffer from misrecognitions, which inflicts a correction cost on to the user. Every time a symbol gets misrecognized, the user explicitly or implicitly signals his intention to correct the error, and does so by either redrawing the symbol or selecting it from a list of alternatives. We propose a system for alleviating the cost of this two-step process by detecting users' intention to fix misrecognitions based on their eye gaze activity. In particular, we show that users' natural reaction to misrecognitions manifests itself in the form of characteristic eye gaze movement patterns. Furthermore, these patterns can be used to read users' intention to fix errors before they initiate such action. We have three main contributions. First, we present a carefully constructed Wizard of Oz setup for recording eye gaze patterns under two sketch-based interaction conditions. Then, we present a set of gaze-based features, which were designed to capture qualitative characteristics of users' eye gaze behavior. Finally, we present a framework for recognizing users' intention to fix errors, which achieves an 86% prediction accuracy. We support our findings through detailed experiments and statistical analyses, which provide further insight into how much can be inferred from eye gaze patterns that naturally emerge during pen-based interaction."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalemli tablet bilgisayar ve kalemli bilgisayar arayüzlerinin yaygınlaşması, birkaç çeşit otomatik çizim işleme çalışmalarına yol açtı. Bu çalışmalardan bazıları çizim tanıma ve taslak çizimleri güzelleştirme yöntemlerini araştırır. Bu tezde, özen gösterilerek çizilmiş görünen çizimleri özensiz çizimlerden ayırmayı, çözülmesi gereken yeni bir problem olarak ortaya koyup, çizimlerin ne kadar özenli göründüğünü örneklerden otomatik olarak öğrenen bir sistem sunuyoruz. Sistemimiz alan, nesne ya da bağlam bilgisi kullanmamaktadır. Çözüm yolumuz, insan algısından esinlenerek, çizimlerin basit parçalarının yön, uzunluk ve uzaklık bilgilerini kullanarak ne kadar özenli çizildiklerini ölçebilmektedir. Yöntemimizin başarısını ölçebilmek için, aynı zamanda insanlar tarafından ne kadar özenli çizildikleri değerlendirilmiş, 32 farklı sembolün 1535 çiziminden oluşan bir çizim veri kümesi derledik. Değerlendirmemiz gösteriyor ki, problem iki sınıflı sınıflandırma problemi olarak ele alındığında, sistemimiz %90'a varan bir başarıyla özenli çizimleri özensiz olanlardan ayırabilmektedir. Ayrıca problemin zor olmasına sebep olan etkenler ve sistemin başarısı ile çizimlerin ne kadar özenli göründükleri hakkında insanların ne kadar mutabık olduğu arasındaki bağlantısı üzerine analizlerimizi de bildiriyoruz.","The increasing availability of pen-based tablets and pen-based interfaces resulted in several lines of work that attempt to build computational tools for processing sketches. These include computational methods for classifying sketches and methods for beautifying rough drawings. In this thesis, we pose discrimination of clean sketches from messy ones as a new problem, and propose a computational framework for learning messiness from examples. Our system does not use domain, object or context specific knowledge. Our approach is inspired by human perception and can assess the messiness of hand-drawn sketches using features extracted from relative orientation, length and distance properties of primitives. In order to evaluate our method, we collected a sketch dataset that consists of 1535 drawings of 32 different symbols, which were also coded with subjective messiness information obtained from human raters. Our evaluation shows that we can discriminate messy and clean drawings with up to 90% accuracy when the problem is treated as a 2-class classification problem. We report detailed analysis on what makes this problem hard, and how the system accuracy relates to the inter-rater agreement on the messiness ratings of sketches."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada doğal dilin 3 boyutlu sahnelere dönüştürülmesini sağlayan yeni bir sistem tasarımı sunulmaktadır. Dilin görselleştirilmesini sağlayan bu sistem, soyut olmayan isimleri, bu isimleri niteleyen görünür etkisi olan sıfatları ve konum belirten sözcükleri doğal dil cümlelerinden anlayıp dinamik olmayan 3 boyutlu sahnelere çevirmekte ve bunları doğal dil işleme araçlarını, 3 boyutlu model galerilerini ve dil ile ilgili bilgi kaynaklarını kullanarak, kurallara dayalı olarak yapmaktadır. Bu sistemin anlatımında aynı zamanda dilin genelliği ve belirsizliğini gidererek onu 3 boyutlu olarak görselleştirebilecek teknikler sunulmaktadır. Bu sahne yaratma işlemi sonrasında bilgisayarın sahne ile ilgili bazı çıkarım sorularını cevaplayabileceği bir arayüz de tanıtılmaktadır. Sistemin, sadece dil işleme teknikleri kullanılarak zor cevaplanabilecek bazı çıkarım sorularına, gerçek dünyayı canlandırarak sadece dilden elde edebileceği bilgilerin yanında 3 boyutlu dünya ile ilgili fazladan edindiği bilgileri kullanarak cevap vermesini sağlayan yöntemler anlatılmaktadır.","In this thesis, a novel language visualization system is presented that converts natural language text into 3D scenes. The system is capable of understanding some concrete nouns, visualizable adjectives and spatial prepositions from full natural language sentences and generating 3D static scenes using these sentences. It is a rule based system that uses natural language processing tools, 3D model galleries and language resources during the process. Several techniques are shown that deals with the generality and ambiguity of the language in order to visualize the natural language text. A question answering module is built as well to answer certain types of spatial inference questions after the scene generation process is completed. The system demonstrates a new way of solving spatial inference problems by not only using the language itself but with the extra information provided by the visualization process."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğal dil işleme ile ilgilenen bilim insanlarının ilgi alanlarından biri de kelimeleri temsil edebilecekleri modeller bulmaktır. Kelime gömüleri her kelimeyi düşük boyutlarda, gerçel değerli, yoğun vektörler olarak gösterir ve en yaygın yöntemlerden biri haline gelmiştir. Kelime gömüleri klasik kategoriksel gösterimlerine yerine bir seçenek olarak görülmektedir. Kelime gömüleri bir kelimenin sözdizimsel ve anlamsal özelliklerini bir vektörün boyutlarında temsil eder. Bu temsillerin Varlık İsmi Tanımlama, Sözdizimsel Analiz gibi gözetimli doğal dil işleme görevlerinde başarılı olduğu gösterilmiştir. Bu çalışmada, bir kelime gömü yöntemini gözetimli doğal dil işleme görevlerinde inceliyorum. Yöntem, kelimeleri bir küre üzerinde temsil eder, öyle ki aynı bağlamda gözlenen kelimeler bu küre üzerinde yakın şekilde konumlandırılır. Bağlamların benzerliği ise o bağlamda gözlenebilecek kelimelerin olasılıksal dağılımları kullanılarak yapılır. Kelime gömülerini Varlık İsmi Tanımlama, Gruplama ve Bağlılık Ayrıştırması görevlerinde karşılaştırdım. İncelediğim yöntem en az diğerleri kadar başarılı ya da onlardan daha başarılı sonuçlar almıştır. Yöntem başarını çok dilli incelemelerde de sürdürmüştür ve Bağlılık Ayrıştırması görevinde bilinen en iyi sonuçları elde etmiştir.","One of the interests of the Natural Language Processing (NLP) community is to find representations for lexical items using large amount of unlabeled data. Inducing low-dimensional, continuous, dense word vectors, or word embeddings, have become the principal technique to find representations for words. Word embeddings address the issues of the classical categorical representation of words by capturing syntactic and semantic information of words in the dimensions of a vector. These representations are shown to be successful across NLP tasks including Named Entity Recognition, Part-of-speech Tagging, Parsing, and Semantic Role Labeling. In this work, I analyze a word embedding method in supervised Natural Language Processing (NLP) tasks. The framework maps words on a sphere such that words co-occurring in similar contexts lie closely. The similarity of contexts is measured by the distribution of substitutes that can fill them. I compared word embeddings, including more recent representations, in Named Entity Recognition (NER), Chunking, and Dependency Parsing. I examine the framework in a multilingual setup as well. The results show that the examined method achieves as good as or better results compared to the other word embeddings. The framework is consistent in improving the baseline systems across languages and achieves state-of-the-art results in multilingual dependency parsing."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Normalde, iki ve çok kişili ortaklaşa hesaplama protokollerinde güvenilir üçüncü bir şahıs olmadan veya çok maliyetli olan kademeli olarak bırakma (gradual-release) protokolleri kullanılmadan adalet sağlamaz. Güvenilir üçüncü şahıs kullanan iki veya çok kişili adil ortaklaşa hesaplama protokollerinde ya güvenilir kişiye çok iş düşüyor ya da elektronik ödeme ile adalet sağlanıyor. Ayrıca bu protokollerin çoğunda adil ve güvenli oldukları ayrı ayrı ispatlanmış. Bu yüzden bu protokoller bazı güvenlik açıklarına yol açabilirler. Bu sorunları çözmek için devre kullanan, iki veya çok sayıda ikisi içeren ortaklasa hesaplama protokollerini adil yapan iki yeni yapı oluşturduk. Bu iki yeni yapı iki veya çok kişili ortaklaşa hesaplama protokollerine çok az ek yük getiriyor. Güvenilir üçüncü şahıs hesaplamanın girdi ve çıktılarıyla ilgili hiç bir bilgi öğrenmiyor. Ayrıca ikili ortaklaşa hesaplamayı adil yapan yapımız, partiler arasında eşit yük vererek de adaleti sağlamış oluyor. Her iki yapıyla verdiğimiz adil ve güvenli ortaklaşa hesaplama protokollerinin adil ve güvenli olduklarını birlikte göstererek ispatladık (simülatör kullanarak) ve böylece hiç bir güvenlik açığının olmadığını göstermiş olduk. Çok kişili ortaklasa hesaplamayı adil yapan yapımız yeni oluşturduğumuz çok kişili adil takas protokolünü kullanmaktadır. Çok kişili adil takas (MFE), çalışılan bir araştırma alandır ve pratikte önemli bir yere sahiptir. Bizim araştırdığımız senaryo her partinin takas etmek istediği bir nesnesinin olduğu ve takas sonunda her partinin istediği nesneyi aldığı ya da hiç bir partinin hiçbir şey alamadığı senaryodur. Güvenilir üçüncü şahısın (TTP) adaleti sağlamak için zorunlu olduğu durumlarda takas protokolüne girdiği durumu tüm protokollerimizde kullandık. Oluşturduğumuz protokoller güvenilir üçüncü şahısa karsı da gizliliği sağlamaktadır. Sabit sayıda O(n^2) mesaj gerektiren (n kisi sayısı) iki çok kişili adil takas protokolü oluşturduk. Protokollerin birinde, kisiler sadece etkin olarak doğruluğunu gösterebildikleri nesneleri takas edebilmektedirler (örn. bir kontratı ortaklasa imzalama). Diğer protokolümüzde, kisiler etkin olarak ya da etkin olmadan doğruluklarını gösterebildikleri herhangi bir nesneyi takas edebilirler. Bunu elektronik ödeme sistemlerinden yaralanarak basardık. Burada eğer bir kisi istediği nesneyi her hangi bir kişiden alamazsa, onun yerine karşı taraftan bir ödeme yapılıyor. Daha sonra bu iki takas protokolünün herhangi bir takas topolojisine adapte edilebileceğini gösterdik. Takas protokollerimiz n - 1 tane kisi kötü niyetli olsa bile adil olma özelliğini koruyorlar.","Secure computation cannot be fair in general against malicious adversaries, unless a trusted third party (TTP) is involved, or gradual-release type of costly protocols with super-constant rounds are employed. Existing optimistic fair and secure computation protocols with constant rounds are either too costly to arbitrate (e.g., the TTP may need to re-do almost the whole computation), or require the use of electronic payments or bitcoins. Furthermore, most of the existing solutions were proven secure and fair separately, which, we show, may lead to insecurity overall. Therefore, we propose two new framework for secure two-party and multi-party computation that can be applied on top of both circuit based secure computation protocols to make them fair. We show that our fairness overhead is minimal with these frameworks, compared to all known existing work, and the TTP never learns the inputs or outputs of the computation. Furthermore, our framework for the two party computation makes a protocol fair even in terms of the work performed by the two parties Alice and Bob. We also prove that the frameworks makes a circuit based secure computation protocol fair and secure simultaneously, through one simulator, which guarantees that our fairness extensions do not leak any information. The framework for fair and secure multi-party computation includes multi-party fair exchange protocol that we designed. Multi-party fair exchange (MFE) is understudied field of research, with practical importance. We examine MFE scenarios where every participant has some item, and at the end of the protocol, either every participant receives every other participant's item, or no participant receives anything. This is a particularly hard scenario, even though it is directly applicable to protocols such as fair SMPC or multi-party contract signing. We analyze the case where a trusted third party (TTP) is optimistically available, although we emphasize that the trust put on the TTP is only regarding the fairness, and our protocols preserve the privacy of the exchanged items against the TTP. We construct two asymptotically optimal multi-party fair exchange protocols that require a constant number of rounds, in comparison to linear, and O(n^2) messages, in comparison to cubic, where n is the number of participating parties. In one protocol, we enable the parties to efficiently exchange any item that can be efficiently put into a verifiable escrow (e.g., signatures on a contract). In our other protocol, we let the parties exchange any verifiable item, without the constraint that it must be efficiently put into a verifiable escrow (e.g., a file cannot be efficiently verifiably escrowed, but if its hash is known, once obtained, the file can be verified). We achieve this via use of electronic payments, where if an item is not obtained, the payment of its owner will be obtained in return of the item that we sent. We then generalize our protocols to handle any exchange topology efficiently. Our protocols guarantee fairness in its strongest sense: even if all n-1 other participants are malicious and colluding, fairness will hold."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalem ile kontrol edilebilen tablet ve mobil aygıtların ortaya çıkması ile birlikte, dijital kalem kullanılan ara-yüzler ve insan ile olan etkileşimi artarak önem kazanmaktadır. Ancak, mevcut dijital kalemler pasif aygıtlar olup yalnızca yazma ve seçme gibi aksiyomlar için kullanılmaktadır. Bu araştırmada, kullanıcılara titreşim ve eylemsizlik prensibi kullanılarak dokunsal geri bildirimler verebilen yeni bir dijital kalem sunulmaktadır. Tasarlamış olduğumuz bu dijital kalem, uçlara yakın olarak yerleştirilmiş iki adet titreşim motoru sayesinde kalemin uzun ekseninde aşağı ve yukarı yönde akma efekti oluşturabilmektedir. Ayrıca, gövdeye entegre edilmiş olan DC motor sayesinde kalemin uzun ekseni etrafında her iki yöne dönme efekti oluşturabilmektedir. Önerilen dokunsal hisler, iki ayrı psiko-fizik deneyi ile test edilmiş ve denekler belirlenmiş olan parametrelerde yüksek başarı oranları ile verilen hisleri algılamışlardır. Bir başka psiko-fizik deneyinin sonuçları ile DC motorun çalıştırılma şeklinin dönme hissinin algılanması üzerinde etkisi olduğunu gösterilmiştir. Son olarak, tasarlamış olduğumuz dijital kalem, kullanıcının görsel ve dokunsal algılar ile etkileşim halinde olduğu kalem bazlı bir oyunda test edilmiş ve deneyin sonuçları da verilen dokunsal his duyusunun kullanıcılar tarafından etkin bir şekilde algılandığını göstermektedir.","With the emergence of pen-enabled tablets and mobile devices, stylus-based interaction has been receiving increasing attention. Unfortunately, styluses available in the market are all passive instruments that are primarily used for writing and pointing. In this research, we describe a novel stylus capable of displaying certain vibro-tactile and inertial haptic effects to the user. Our stylus is equipped with two vibration actuators at the ends, which are used to create a sensation of up and down flow along the stylus. The stylus is also embedded with a DC motor, which is used to create a sense of bidirectional rotation about the long axis of the pen. Through two psychophysical experiments, we show that, when driven with carefully selected timing and actuation patterns, our haptic stylus can convey flow and rotation information with high accuracy. Results from a further psychophysical experiment provide insight on how the shape of the actuation patterns effects the perception of rotation. Finally, experimental and subjective results from our interactive pen-based game show that our haptic stylus is effective in practical settings."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez kelime bağlamlarını temsil etmek için yeni bir düşey bağıntı tanımlamaktadır. Bir kelimenin düşey bağıntısı kelimenin bağlamında değiştirim sonucu onun yerine gelebilen olası kelimelerin oluşturduğu bağıntıdır. Öte yandan yatay bağıntı bir kelimenin öncesindeki ya da sonrasındaki kelimeler arasında kurulan bağıntıdır. Bir kelimenin yerini alabilecek olası kelimeler işlenmemiş veri üzerinde eğitilmiş bir istatistiksel dil modeli ile hesaplanmaktadır. Sonuç olarak kelime bağlamları, o bağlamda görülebilecek olası kelime dağılımları ile temsil edilmektedir. Bu tez bahsi geçen yeni düşey bağıntıyı kullanabilen farklı doğal dil işleme modelleri tanımlamakta ve bu modellerin doğal dil işlemede kullanılan farklı dizisel etiketleme problemleri üzerindeki uygulamalarını göstermektedir. Doğal dil işleme problemlerindeki dizisel etiketlemenin temel amacı verilen bir kelime dizisine birebir denk gelen dizisel etiketileri bulamaktadır. Bu nedenle modeller girdi olarak kelime dizisi almakta ve çıktı olarak her kelimeye bir etiket gelecek şekilde bir etiket dizisi vermektedir. Öğreticisiz modellerde çıktı dizisi her kelimeye ait küme isimleri iken öğreticili modellerde çıktı dizisi her kelimeye ait önceden tanımlanmış etiketlerdir. Bu tezde 5 farklı model tanımlanmaktadır. İlk model öğreticisiz bir modeldir ve olası kelime dağılımlarını kullanarak kelimeleri kümelemeyi amaçlamaktadır. İkinci model verilen bir kelime ile o kelimeye ait olası kelimelerin birlikte görülme sıklıklarını modelliyen öğreticisiz bir modeldir. Üçünci model kelimenin yerini alabilecek kelimeleri kullanarak olasılıksal oylama yapan bir modeldir. Bu model ilk iki modelin aksine, her kelimenin olası etiketlerine ihtiyaç duyan öğreticili bir modeldir. Dördüncü model dizisel etiketleme probleminde sıklıkla kullanılan saklı Markof modelleriyle birlikte kullanılabilen 2 yöntem önermektedir. Bir önceki model gibi bu model de her kelimeye ait olası etiketlere ihtiyaç duyar. Tezdeki son model gürültülü kanal modelidir ve bu model gürültülü kanal ve alınan mesajı kullanarak esas gönderilmek istenen mesajı bulmayı amaçlar. Bu modelde her bağlam bir kanal, her kelime alınan mesaj ve kelimeye ait etiket ise gönderilmek istenen esas mesajdır. Tezin son kısmında yukarıda bahsi geçen modeller farklı özelliklerdeki etikeleme problemlerine uygulanmıştır. İlk iki model öğreticisiz sözcük türü bulma problemine uygulanmıştır. Olasılıksal oylama modeli ise Türkçe ekbiçim belirsizliği giderme problemi üzerinde denenmiştir. Saklı Markof modeline dayanan yöntemler ise öğreticili sözcük türü bulma problemine uygulanmıştır. Son olarak gürültülü kanal modeli kelime anlam belirsizliği giderme problemi üzerinde denenmiştir.","This thesis introduces a new paradigmatic representation of word contexts. Paradigmatic representations of word context are constructed from the potential substitutes of a word, in contrast to syntagmatic representations, which are constructed from the properties of neighboring words. The potential substitutes are calculated by using a statistical language model that is trained on raw text without any annotation or supervision. Thus, each context is represented as a distribution of substitute words. This thesis introduces models with different properties that can incorporate the new paradigmatic representation, and dis- cusses the applications of these models to the tagging task in natural language processing (NLP). In a standard NLP tagging task, the goal is to build a model in which the input is a sequence of observed words, and the output, depending on the level of supervision, is a sequence of cluster-ids or predefined tags. We define 5 different models with different properties and supervision requirements. The first model ignores the identity of the word, and clusters the substitute distributions without requiring supervision at any level. The second model represents the co-occurrences of words with their substitute words, and thus incorporates the word identity and context information at the same time. To construct the co-occurrence representation, this model discretizes the substitute distribution. The third model uses probabilistic voting to estimate the distribution of tags in a given context. Unlike the first and second models, this model requires the availability of a word-tag dictionary which can provide all possible tags of each given word. The fourth model proposes two extensions to the standard HMM-based tagging models in which both the word identity and the dependence between consecutive tags are taken into consideration. The last one introduces a generative probabilistic model, the noisy channel model, for the tagging tasks in which the word-tag frequencies are available. In this model, each context C is modeled as a distinct channel through which the speaker intends to transmit a particular tag T using a possibly ambiguous word W . To reconstruct the intended message (T ), the hearer uses the distribution of possible tags in the given context Pr(T|C) and the possible words that can express each tag Pr(W |T ). The models are applied and analyzed on NLP tagging tasks with different characteristics. The first two models are tested on unsupervised part-of-speech (POS) induction in which the objective is to cluster syntactically similar words into the same group. The probabilistic voting model is tested on the morphological disambiguation of Turkish, with the objective of disambiguating the correct morphological parse of a word, given the available parses. The HMM-based model is applied to the part-of-speech tagging of English, with the objective of determining the correct POS tag of a word, given the available tags. Finally, the last model is tested on the word-sense disambiguation of English, with the objective of determining the correct sense of a word, given the word-sense frequencies."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sıcak noktalar protein-protein ara yüzlerindeki aminoasitlerin sadece küçük bir alt kümesidir ama bağlanma serbest enerjisine büyük katkı sağlarlar. Sıcak noktalara deneysel olarak karar vermek için aminoasitlerin alanin aminoasidine mutasyonuna bağlı bağlanma serbest enerjilerinin değişimini ölçülür. Eğer aminoasidin mutasyonu bağlanma serbest enerjisinde çok büyük bir değişime yol açıyorsa bu aminoasit sıcak nokta olarak tanımlanır. Sıcak noktalara deneysel olarak karar vermek zaman alıcı, emek yoğundur ve ekonomik maliyeti yüksektir. Bu sebeple, sıcak nokta tahmini için hesaplamalı yöntemler geliştirilmiştir. Bu yöntemler eğitim ve test setleri kullanır. Ancak, sıcak nokta tahmini için standart değerlendirme (benchmark) seti yoktur. Biz hesaplamalı sıcak nokta tahmini için 13 veri setinin birleşiminden oluşan ve 79 protein kompleksinin 1203 aminoasidi için verileri içeren yeni bir değerlendirme veri setini sunuyoruz. Makine öğrenme tabanlı metotlar sıcak nokta tahminleri için sıklıkla kullanılan yöntemlerdir ve bu yöntemlerde çeşitli özellikler birbirleriyle kombine edilirler. Biz literatürü taradık, değişik özellikler topladık ve bu özelliklerin sonuçlar üzerine etkisini eleştirel olarak değerlendirdik. Sonuç olarak güçlü etkisi olan yetmiş özellik tespit edildi. Belirlenen özellikler kullanılarak çeşitli makine öğrenme tabanlı metotların, sunucuların ve bir programın eklentisinin değerlendirme seti üzerindeki performansları kıyaslandı. Sonuçlara göre random forest sınıflayıcı en yüksek kesinliğe (%80) sahiptir ve KFC2_A var olan diğer metotlar arasında en yüksek F-ölçü'süne (0.49) sahip olmasına rağmen naïve Bayes metodunun F-ölçü'sünü geçmez (0.50). Değerlendirme veri seti, güçlü özelliklerin değerleri ve dört sunucunun ve bir eklentinin tahmin sonuçları http://prism.ccbb.ku.edu.tr/hotbase adresinde yer alan HotBase internet ara yüzü aracılığıyla görülebilir ve indirilebilir.","Hot spots are only a small subset of protein-protein interface residues but they account for the majority of the binding free energy. Measuring the binding free energy change upon mutating residues to alanine is an experimental way to determine hot spots. If mutation of a residue gives rise to a significantly large change in the binding free energy then this residue is defined as a hot spot. Experimental determination of hot spots is time-consuming, labor intensive and has high economic costs. Therefore, computational methods have been developed for hot spot prediction. These methods use training and testing data sets. However, there are no standard benchmark data sets for hot spot prediction. We present a new benchmark data set that is combination of 13 data sets and includes data of 1203 residues of 79 protein-protein complexes for computational hot spot prediction. The frequently used methods for hot spot prediction are machine-learning based and several features are combined in these methods. We reviewed literature, collected different features and critically assessed the effect of these features on results. As a result, seventy features that have strong effects are determined. The performances of different machine-learning methods, four servers and a plugin for a program using determined features on benchmark data set are compared. The results reveal that random forest classifier has the highest accuracy (80%) and although KFC2_A has the highest F-measure (0.49) among existing methods, but it does not exceed the F-measure of naïve Bayes method (0.50). The benchmark data set, values of powerful features, and prediction results of four servers and a plugin can be viewed and downloaded via HotBase web interface located at http://prism.ccbb.ku.edu.tr/hotbase"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek hızlardaki AKM taramalarında görüntü kalitesini artırmak için, LQG ve tekrarlı kontrolcü (TK) kullanan tümleşik bir yapı önerilmiştir. Önerilen LQG+TK tasarımında kullanılan kontrolcü değişkenlerinin, tarama hatasını en aza indirecek şekilde, en iyi değerlerini hesaplamak için, önce tarama işleminin bir MATLAB/Simulink modeli oluşturulmuş ve sonra bu model üzerinde genetik algoritmalar çalıştırılmıştır. Tasarımımızda kullandığımız LQG kontrolcü geleneksel PI kontrolcüsüne göre daha gürbüzdür ve daha iyi izleme başarımı sağlar. Ayrıca, bizim yaklaşımımızda, her tarama satırının profili sistemi bozucu bir etki olarak kabul edildiği için, önerilen LQG kontrolcüsü tarama profilindeki büyük değişimlerin geleneksel PI kontrolcüsüne göre daha iyi üstesinden gelir. Öte yandan TK, kontrolcü bant genişliğini artırarak izleme başarımını daha da iyileştirir. Simulasyon ve deneysel sonuçlarımız önerilen LQG+TK tasarımının daha önceki çalışmalarımızda kullanılan PI+TK yapısından daha iyi olduğunu gösteriyor.","In order to improve the image quality at high scan speeds in AFM imaging, a combined controller architecture employing a linear quadratic Gaussian (LQG) controller and a repetitive controller (RC) is proposed. To calculate the optimal values of the parameters used in the proposed LQG+RC design minimizing the scan error, a Matlab/SIMULINK model of the whole scan process is developed first and then genetic algorithms are employed on the simulation model. In our design, the LQG controller provides more robustness and better tracking performance than a traditional PI controller used for the same purpose. Moreover, since the profile of a scan line is treated as a disturbance in our approach, the proposed LQG controller handles the large variations in scan profile better than the PI controller. The RC on the other hand extends the controller bandwidth and further improves the tracking performance. Our simulations and the experimental results show that the scan performance of the proposed LQG+RC design is better than that of the PI+RC design implemented in our earlier studies."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar sistemlerinde veri depolama eğilimi veriyi lokal olarak tutmaktan dış kaynak kullanımına kaymış. Kullanıcıların veriyi bulut veya görevdeş ağlarda tutma eğilimi yanında, önemli bir gereksinim de bilginin güvenliğidir. Başlıca güvenlik gereksinimlerinden birisi dosyaların tamamını kullanıcı tarafına almadan tutarlılığının sağlanması ve güncellemelerin yapılabilmesidir. Görevdeş depolama sistemlerinde sağlanması önemli diğer bir özellik tepki süresi ve hızlı erişim acısından dosyaların veri sahibine yakın düğümlerde tutulmasıdır. Tez çalışmasının birinci kısmında, di- namik ispatlanabilir veri saklama adlı model önerilip, başarım analizi yapılmış. Bu modelde, atlamalı liste yapılı optimize edilmiş FlexList adlı bir veri yapısı sunup, bu yapıyı temel alan FlexDPDP adlı bütün dinamik ispatlanabilir veri saklama sistemi önerilmiştir. Ayrıca, FlexDPDP işlemleri için optimize algoritmalar önerilip, bun- ların zaman, enerji ve depolama boyutu bakımından kazanımları analiz edilmiştir. ? Ikinci kısımda ise, InterLocal adlı yeni bir tutarlılık ve replikasyon garantili yerel görevdeş depolama sistemi önerilmektedir. InterLocal, her düğümde bilgi tutarlılığını sağlayabilmek amacıyla atlamalı grafik veri yapısı tabanlı FlexDPDP kullanıp, dönüm noktalı çok boyutlu ölçeklenebilir algoritmalar ile düğüm yer hesaplaması yapmak- tadır. Hem normal atlamalı grafik tabanlı depolama sistemi hem de InterLocal depo- lama sisteminin gerçekleştirimi yapılmış ve başarımları çesitli ağ senaryolarında Plan- etLab ortamındaki deneylerde karsılaştırılmış. Dosya erişim süresinde tutarlılığı sağlama koşulu ile üç kat kadar hızlanma elde edilmiş ve en kötü senaryoda bile nor- mal atlamalı grafik depolama sisteminin erişim süresinin sağlandığı gözlenmiştir.","Trend in computer storage flows from possessing data locally to data outsourcing. Although users tend to store data at cloud or peer-to-peer storage systems, they also require guarantees about the security of data. A key requirement is the ability to check integrity of the files without downloading them and make necessary updates. In case of peer-to-peer storage systems, it is also desirable to place files at the nodes physically close to the data owner for minimal response time and efficient access. In the first part of this thesis, we implement and examine a system based on Dynamic Provable Data Possession (DPDP) model. We present an optimized data structure based on skip lists called FlexList and its advantages over other data structures. We then propose FlexDPDP: a complete dynamic provable data possession system employing FlexList. Furthermore, we develop optimized algorithms for FlexDPDP operations and analyze the efficiency gains in terms of time, size and energy. In the second part of this thesis, we propose and evaluate InterLocal, a novel integrity and replication guaranteed locality-based peer-to-peer storage system. We employ a skip graph as the underlying overlay structure, and use landmark multidimensional scaling for peer locality calculation, on the top of FlexDPDP at each node to provide data integrity. We implement both a regular skip graph based storage system and InterLocal, and evaluate their performance on the PlanetLab under various scenarios. We obtain 3x speed up in terms of le access by providing InterLocal, and a gradual performance decrease in case of replica failures, having a worst-case performance that is equal to that of a regular skip graph based storage system."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ağ açıklamaları ve analizleri sistem biyolojisi için önemli araçlardır ; hücre içindeki ve hücreler arasındaki karmaşık ilişkileri özetlemekte güçlüdürler ve genellikle ilaç keşfi için ipuçları sağlayabilirler. Bu tezin ilk bölümünde, ""Protein Arayüzey ve Etkileşim Ağı (P2IN)"" olarak isimlendirdiğimiz yapısal bir ağ modelini tanıttık. Bu ağ protein-protein arayüzey yapılarının ve protein etkileşim ağlarının entegrasyonundan oluşmaktadır. Bu arayüzey bilgisine bağlı ağ organizasyonu hangi protein çiftlerinin yapısal olarak benzer arayüzeylere sahip olduğunu ve hangi proteinlerin aynı yüzey bölgesine bağlanmak için rekabet ettiğine açıklık getirmektedir. Daha sonra, protein-protein arayüzey motiflerine dayanan yeni bir ağ saldırı stratejisi önerdik, ""Arayüzey Saldırısı"". Benzer arayüzey mimarileri ilintisiz protein çiftleri arasında oluşabilirler. Bu nedenle, prensip olarak, birine bağlanan bir ilacın belli bir oranda diğerlerine de bağlanma olasılığı vardır. Arayüzey Saldırısı, benzer arayüzey motiflerinden oluşan tüm etkileşimleri ağdan aynı anda kaldırır. Bu strateji ağ farmakolojisinden ilham almıştır ve potansiyel ""dış-hedefler"" 'in tahminine izin verir. Biz p53 sinyal ağının P2IN'ini inşa ettik ve ağda sağlamlık analizleri gerçekleştirdik. Biz (1) sıkça gözlemlenen arayüzeylerin (ağın çeşitli yerlerine dağılmış kenarlar) saldırılara hedef alınmasının, yüksek dereceli proteinlerin (hub düğümleri) ortadan kaldırılması kadar yıkıcı olabileceğini (2) sıkça gözlemlenen arayüzeylerin ağda her zaman topolojik olarak kritik noktalarda bulunmadığını (3) Arayüzey Saldırısının sistemdeki fonksiyonel değişiklikleri, tek tek proteinleri hedef alan saldırılardan daha iyi ortaya çıkarabildiğini gösterdik. ""Dış-hedef"" tespiti örnek çalışmasında, CDK6 ve CDKN2D arasındaki arayüzeyi engelleyen ilaçların, CDK4 ve CDKN2D arasındaki etkileşimi de etkileyebileceğini bulduk. Son olarak da, genotip-fenotip ilişkisinin tahmini için, protein etkileşimleri ve bu etkileşimlerin üç-boyutlu yapısının nasıl kullanıldığını açıkladık. Meme kanserinde primer tümörün akciğer ve beyin metastazına yol açması ile ilintili fenotipe özel protein etkileşim alt-ağları inşa ettik. İlk olarak, ""işbirliği-ile-suçluluk"" prensibini kullanarak, metastaza neden olan genlerle (tohum gen) en çok ilişkide bulunan protein etkileşimlerini seçtik. Daha sonra, kompleks halleri Protein Bilgi Bankası' nda bulunmayan etkileşimlerin yapılarını modelledik. Son olarak, mutasyonlar tarafından manipüle edilmiş olabilecek etkileşimleri bulmak için, arayüzey yapıları üzerinde mutasyonları işaretledik. Bu alt ağlarda yapılan fonksiyonel analizler bağışıklık sistemi, enfeksiyon hastalıkları ve akciğer metastazı arasındaki potansiyel ilişkiyi ortaya çıkarmıştır, ama bu bağlantı beyin metastazı için kayda değer bir şekilde gözlenmemiştir. Bunun yanı sıra, yapısal analizler her iki metastaz alt-ağı içindeki protein etkileşim arayüzlerinin mikrobiyal protein kaynaklı olduğunu gösterdi. Bahsi geçen bu protein etkileşimlerinin hücre yapışması ile ilintili olduğu gözlemlendi. Hücre yapışması metastaz için önemli bir mekanizmadır; bu nedenle bu protein etkileşimleri bulaşıcı hastalık ve metastaz tarafından paylaşılan benzer moleküler yolaklarla ilgili olabilirler. Son olarak da, metastaz alt-ağlarındaki proteinlerin arayüzey bölgelerine amino asit varyasyonlarını eşleyerek, bazı mutasyonların metastaz türünün ayırt mekanizmalarına dahil olduğuna dair ipuçları bulduk.","Network descriptions and analyses are important tools in systems biology; they are powerful in abstracting the complex relationships inside cells and between them, and they often provide clues for drug discovery. In the first part of this dissertation, we introduce a structural network model that we call ""Protein Interface and Interaction Network (P2IN)"", which is the integration of protein?protein interface structures and protein interaction networks. This interface-based network organization clarifies which protein pairs have structurally similar interfaces and which proteins may compete to bind the same surface region. Next, we propose a new network attack strategy, ""The Interface Attack"", based on protein?protein interface motifs. Similar interface architectures can occur between unrelated proteins. Consequently, in principle, a drug that binds to one has a certain probability of binding to others. The interface attack strategy simultaneously removes from the network all interactions that consist of similar interface motifs. This strategy is inspired by network pharmacology and allows inferring potential off-targets. We built the P2IN with the p53 signaling network and performed network robustness analysis. We show that (1) ""hitting"" frequent interfaces (a set of edges distributed around the network) might be as destructive as eliminating high degree proteins (hub nodes), (2) frequent interfaces are not always topologically critical elements in the network, and (3) interface attack may reveal functional changes in the system better than the attack of single proteins. As a case study, we tried to detect the off-targets of some CDK6 binding drugs. We found that drugs blocking the interface between CDK6 and CDKN2D may also affect the interaction between CDK4 and CDKN2D. Lastly, we describe how we use protein interactions and the structural knowledge on interacting surfaces of proteins (interfaces) in predicting the genotype-phenotype relationship. We built the phenotype specific sub-networks of protein-protein interactions (PPIs) involving the relevant genes responsible for lung and brain metastasis from primary tumor in breast cancer. First, we selected the PPIs most relevant to metastasis causing genes (seed genes), by using the ""guilt-by-association"" principle. Then, we modeled structures of the interactions whose complex forms are not available in Protein Databank. Finally, we mapped mutations to interface structures (real and modeled), in order to spot the interactions that might be manipulated by these mutations. Functional analyses performed on these sub-networks revealed the potential relationship between immune system, infectious diseases and lung metastasis progression, but this connection was not observed significantly in the brain metastasis. Besides, structural analyses showed that some PPI interfaces in both metastasis sub-networks are originating from microbial proteins, which in turn were mostly related with cell adhesion. Cell adhesion is a key mechanism in metastasis; therefore these PPIs may be involved in similar molecular pathways that are shared by infectious disease and metastasis. Finally, by mapping the mutations and amino acid variations on the interface regions of the proteins in the metastasis sub-networks we found evidence for some mutations to be involved in the mechanisms differentiating the type of the metastasis."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada, çok izlekli C/C++ uygulamalarındaki eş zamanlı programlama hatalarının tespitini geliştirmek ve bu uygulamaların hareket belleği (TM) teknolojisi kullanılarak kurtarılmasını sağlamak için bir yöntem öne sürüyoruz. Hareket Belleği, koşut program tasarımını ve uygulamasını basitleştiren, başarımı arttıran ve uygulamaları çoğu eş zamanlı programlama hatalarından koruyan bir koşut programlama modelidir. Bizim yaklaşımımız, eş zamanlı programlama hatalarının tespit edilme başarımını arttırmak için TM kullanmakta ve kalıt C/C++ uygulamalarının eş zamanlı programlama hatasızlıktan faydalanabileceği bir çerçeve sağlamaktadır. Mevcut eş zamanlı hatası tespit eden yaklaşımlar ya çok yavaş ya da mevcut işlemci mimarisinde fazladan değişikliklere ihtiyaç duymaktadırlar. Bu yöntemlerin uygulamayı yavaşlatması birkaç sebepten kaynaklanabilir: bunları uygulamaya eklemek için kullanılan araçlar, hataların tespiti için gerekli hesaplamalar ve bu teknikler tarafından kullanılan hata tespitiyle alakalı verinin uygun biçimde korunması. Çok izlekli bir uygulamadaki her izleğin komut akışını küçük hareketlere bölmek için bir yol sunmaktayız. Daha sonra, eş zamanlı hata tespit verisinin iyi taneli korunmasını elde etmek için çakışma tespitini kullanmaktayız. Eş zamanlı hatalarından, özel olarak veri yarışlarından, kurtulmak için hareket yazma arabelleklerini ve geri dönüş mekanizmalarını kullanmaktayız ve hatalı veriye veya eş zamanlı programın parçalarına fazladan koruma dayatmaktayız. Yaklaşımımız birçok çok çekirdekli denektaşı uygulamada iyi çalışmakta ve uzlaşılmış yollara nazaran eşzamanlı hata tespitinde belirgin bir başarım artışı göstermektedir. Bu gelişmeler önerilen yaklaşımın endüstride kullanımı için cesaret verici öncül sonuçlardır.","We propose a technique to improve detection of concurrency errors of multi-threaded C/C++ applications and recovery of these applications from the errors using transactional memory (TM) technology. Transactional memory is an emerging parallel programming model which simplifies parallel program design and implementation, improves performance and protects applications from most concurrency bugs. Our approach uses TM to improve performance of detection of concurrency errors and provides a framework by which legacy C/C++ applications can benefit from concurrency error-freedom. The current concurrent error detection approaches are either too slow or need extra modification to current processor architecture. The slowdown of these techniques stems from a number of reasons: instrumentation used to add them into application, necessary computations needed to detect errors, and cost of proper protection of error-detection-related data used by these techniques. We propose a way to divide the instruction stream of each thread in a multi-threaded application into small transactions. We then use conflict detection to get fine-grain protection of concurrency error detection data to improve performance. We use transaction write buffers and rollback mechanism to recover from concurrency errors, data races in particular, and impose extra protection on erroneous data or portions of concurrent program. Our approach works well on a number of multi-core benchmark applications and shows a significant performance improvement of concurrent error detection over conventional means. These improvements are encouraging initial results for the industrial usage of the proposed approach."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bulut depolama sistemleri gittikçe ucuzluyor ve yaygınlaşıyor. Bu sistemlere olan ilgi arttıkça günlük yaşamlarımızda da, endüstride de insanlar verilerinin güvenliğini daha çok önemsemeye başlıyorlar. Bu tezde biz, bir sunucunun, müşterinin verisinin istediği kısımlarını yazma okuma şeklinde değiştirmesine izin verirken aynı zamanda verinin tamamını indirmeden bütünlüğünü ispatlayabileceği, kullanılabilir tam bir sistem öneriyoruz. Bu sisteme, Erway et al. ın ismini koyduğu dinamik ispatlanabilir veri saklama deniliyor. Önce bulut sistemleri için en verimli duruma getirilmiş FlexList veri yapısını (Esnek Uzunluk-tabanlı doğrulanabilir atlamalı liste) ve eski veri yapılarından farklarını anlatıyoruz. Devamında bu veri yapısını dinamik ispatlanabilir veri saklama sistemindeki kullanımını gösteriyor ve yeni oluşan plana (scheme) FlexDPDP diyoruz. FlexDPDP planını paralelleştirme yöntemleri kullanarak ve verimli algoritmalar sağlayarak daha da iyi kullanılabilecek bir hale getiriyoruz. Her bölümün sonunda o bölümde tavsiye ettiklerimizin verimlilik incelemesini sunuyoruz. Ayrıca, iyileştirilmiş FlexDPDP planını geniş çaplı ağ deneme yatağı olan PlanetLabda çalışır hale getirdik ve FlexDPDPnin dinamik veri özelliğini sağlamasına rağmen, en verimli statik veri saklama sistemiyle (PDP) kıyaslanabilir olduğunu gösteriyoruz. Son olarak da kurduğumuz planı gerçeğe yakın ortamlar ve gerçek dosya su rümleme sistemlerinden alınmış iş yükleriyle test ederek geliştirilmiş algoritmalarımızın verimliliğini gösteriyoruz.","Cloud storage systems are becoming cheaper and more available. With the increase in popularity of the cloud storage systems both in industry and our personal lives people have started to care about the security of their data on the clouds. In this thesis, we develop and test a complete system for a server able to prove integrity of the client?s data without her downloading the whole data, and still letting the client interact with her data in a read/write manner. This system is called Dynamic Provable Data Possession by Erway et al. . We first show the FlexList: Flexible Length-Based Authenticated Skip List, a data structure optimized for secure cloud storage systems, and its differences from previous data structures. Then we demonstrate its utilization on the dynamic provable data possession system and we call the new scheme FlexDPDP. We further optimize the FlexDPDP scheme using parallelization techniques, and provide optimized algorithms to reduce the time complexity of the protocol. We provide an analysis on all of our proposals at the end of each chapter. We also deployed the optimized FlexDPDP scheme on large-scale network test-bed PlanetLab, demonstrating that FlexDPDP performs comparably to the most efficient static storage scheme (PDP), while providing dynamic data support. Finally, we demonstrate the efficiency of our proposed optimizations on multi-client scenarios according to real workloads based on real version control system traces."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizim insanların doğal iletişim araçlarından biridir. Kalem temelli cihazların son zamanlardaki artışı ile birlikte, insan-bilgisayar etkileşimi alanında çizim arayüzleri ve çizim tanıma sistemlerine olan ilgi büyüyen bir eğilim göstermektedir. Çizim tanıma sistemlerine yönelik güncel yaklaşımlar, çizimlerin tanınma oranlarını artırmak adına makine öğrenimi teknolojilerini fazlaca kullanmaktadırlar. Makine öğreniminde sistem var olan örneklerin üzerinden bir oğrenim yapabilmektedir. Her ne kadar bu örneklerin sayısının fazla olması çizim tanıma sisteminin tanıma performansı icin önemli bir koşul olsa da, makine öğrenim teknolojilerinin pratik kullanımı hususunda bazı konular dikkat çekmeye başlamıştır. Çizim tanıma sistemlerinin gelişimini aksatan konulardan biri de aşırı veri işleme sorunudur. Bir çizim tanıma sisteminin gözetimli öğrenimi sırasında, eğer sistem büyük bir veri grubu kullanarak eğitilmeye çalışılırsa, bu durum uzun öğrenme zamanına ve sınıflandırma performansı düşük olan hantal bir sistem modeline neden olur. Bu çalışmada biz pratik, basit, uygulaması kolay ve herhangi bir çizim tanıma sistemiyle ilgilenen kişinin eğitim veri setini daha ufak stil gruplarına bölümlendirmesi ve bir kullanıcının çizim stilinin tayin edilmesi icin başvurabileceği bir yöntem amaçlanmaktadır. Bizim yöntemimiz belirli insanların çizim stillerinin birbirine benzediği gözleminden faydalanarak, stilleri birbirine benzeyen küçük insan toplulukları için modeller üretip bu sayede bir taraftan öğrenme ve sınıflandırma zamanlarında azalmaya yol açarken, diğer taraftan çizim tanıma oranlarında önemli bir düşüş yaşamamayı amaçlamaktadır. Genel olarak sistemimiz iki temel kısımdan oluşmaktadır. İlk kısımda, insanların çizim stil farklılıklarından faydalanalarak var olan tüm eğitim veri setinin daha küçük stil gruplarına bölümlenmesi amaçlanırken, ikinci kısımda, sisteme yeni gelen bir kullanıcının stilinin belirlenip ilk kısımda üretilen stil gruplarından hangisine ait olduğunun tayin edilmesi amaçlanmıştır.","Sketching is one of the natural mode of communication among humans. With the recent increase in the availability of pen-based devices, a growing trend towards sketch-based interfaces and sketch recognition systems have emerged in Human Computer Interaction. Modern approaches to sketch recognition make heavy use of machine learning technology to maximize recognition accuracies by learning from examples. Although having more training examples is key to the performance of any sketch recognition framework, certain aspects related to the practical use of machine learning technology have surfaced as real issues that need attention. One of these practical issues that hinders the development and deployment of sketch recognition systems is the excessive computational resources.During supervised learning of a sketch recognition system, if a large training dataset is used to train a system model, it costs more training time and results in a bulky model with poor classi cation performance. In this thesis, we propose a practical, simple, and easy to implement method that sketch recognition practitioners can resort to for partitioning their training data by based on sketching styles of users. Our method leverages the observation that certain groups of people have similar sketching styles, and generating models for smaller groups of people with similar styles reduces training and classi cation times without a signi - cant sacri ce in recognition accuracies.Our overall system is consisted of two main parts such that in the rst part, we partition the all available training data into style sub-groups and in the next part, we designed a system to identify sketching style of an incoming user to assign the user into one of the style groups generated in the rst part. We demonstrate the utility of our approach with empirical results obtained from databases of various sizes and characteristics."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezin amacı, insan ve robotların dahil olduğu fiziksel işbirliği süreçleri için doğal bir ortak kontrol sistemi kurulmasını sağlayacak bir bakış açısı geliştirmektir. Robotların insanlarla beraber tamamlamaya çalıştığı görevler karmaşık ve dinamik hale geldikçe, insan-insan iletişimine benzer mekanizmalar ile iletişim sağlama ihtiyacı doğmaktadır. Ancak, varolan sistemlerin çoğu, çok kipli iletişimin zenginliğinden çok performansı iyileştirmeyi hedeflemektedir. Bu tezde, insan ve robot arasındaki iletişimin, robotun, insanın niyetini algılayıp kendi kontrol seviyesini dinamik olarak ayarlayabildiği bir karar verme süreci sayesinde geliştirilebileceği öne sürülmektedir. Bu amaçla, lider ve takipçi rolleri tanımlanmış ve partnerlerin sadece kuvvet kanalı aracılığıyla anlaşarak rollerini dinamik olarak değiştirmeleri sağlanmıştır. Amacımız, insan-robot işbirliğinin kuvvet tabanlı bir rol değişim mekanizması ile sanal ve fiziksel dünyada nasıl geliştirilebileceğini araştırmaktır. Bulgularımız, rol değişimi içermeyen bir","This dissertation aims to present a perspective to build more natural shared control systems for physical human-robot cooperation. As the tasks become more complex and more dynamic, many shared control schemes fail to meet the expectation of an effortless interaction that resembles human-human sensory communication. Since such systems are mainly built to improve task performance, the richness of sensory communication is of secondary concern. We suggest that effective cooperation can be achieved when the human?s and the robot?s roles within the task are dynamically updated during the execution of the task. These roles define states for the system, in which the robot?s control leads or follows the human?s actions. In such a system, a state transition can occur at certain times if the robot can determine the user?s intention for gaining/relinquishing control. Specifically, with these state transitions we assign certain roles to the human and the robot. We believe that only by employing the robot with tools to change its behavior during collaboration, we can improve the collaboration experience. We explore how human-robot cooperation in virtual and physical worlds can be improved using a force-based role-exchange mechanism. Our findings indicate that the proposed role exchange framework is beneficial in a sense that it can improve task performance and the efficiency of the partners during the task, and decrease the energy requirement of the human. Moreover, the results imply that the subjective acceptability of the proposed model is attained only when role exchanges are performed in a smooth and transparent fashion. Finally, we illustrate that adding extra sensory cues on top of a role exchange scheme is useful for improving the sense of interaction during the task, as well as making the system more comfortable and easier to use, and the task more enjoyable."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çok sayıda protein arasındaki etkileşimler biyolojik ağların yapıtaşlarını oluşturur. Bir hücrenin fonksiyonunu anlamak için, hücre içinde gerçekleşen biyolojik süreçleri incelemek gerekir. Hücredeki birçok biyolojik süreç de biyolojik ağlara bağlıdır. Bu yüzden biyoloji alanındaki araştırmacılar her zaman bu ağlara ve etkileşimlere önem vermiştir. Proteinler arasındaki etkileşimleri öngören birçok deneysel yöntem mevcuttur. Ayrıca özellikle son yıllarda, bu amaç için birçok hesaplamalı yöntem de geliştirilmiştir. Bu projede, protein etkileşimlerini tahmin etmek için kullanılan bir algoritma bir internet sunucusu aracılığıyla otomatik hale getirilmiştir. PRISM algoritması, bilinen şablon protein arayüzlerine yapısal ve evrimsel benzerlik kullanarak çeşitli proteinler arasındaki etkileşimleri tahmin eder. Bizim yazılımsal sistemimiz, önceden hesaplanmış protein etkileşimlerinden ve bu etkileşimleri hesaplamak için kullanılan şablon ve hedef yapılarından oluşan ilişkisel bir veritabanı oluşturur. Yazılımsal sistem; şablonlar, hedefler ve tahminler olmak üzere üç bölümden oluşur. Veritabanının içeriği uygun PRISM internet sayfası kullanılarak sorgulanabilir. Ayrıca, JMol eklentisi kullanılarak şablon ve hedef yapıların görselleştirilmesi sağlanmıştır. İnternet sunucumuzun bir başka önemli amacı, mevcut protein yapıları ilişkisel veritabanımızda bulunmadığı zaman PRISM algoritmasını en baştan çalıştırmaktır. Bu durumda, algoritmanın adımları sırasıyla uygulanır ve bu adımlar arasındaki elle yapılan bütün işlemler elenmiş olur. PRISM algoritmasının hesaplaması fazla zaman alabilir ve çaba gerektirebilir. Dört katmanlı bir internet veritabanı uygulama mimarisi ve mesaj sıra sistemi kullanılarak birçok hesaplama aynı anda çalışabilir ve kullanıcılar protein yapılarını girmek harici elle yapılan herhangi bir işlem yapmak zorunda kalmazlar. Elle yapılan işlemler elendiği için, kullanıcılar algoritmanın bir sonraki aşamasına geçmek için önceki aşamanın bitmesini beklemek zorunda kalmazlar.","Protein - protein interactions among numerous proteins are the building blocks of the biological networks. To understand the function of a cell, it is necessary to examine biological processes occuring in the cell. Many biological processes in the cell depend on these biological networks, thus researchers in the field of biology always give attention to these networks and interactions. There are many experimental methods available to predict protein-protein interactions. Also, many computational methods have been developed for this aim, especially in recent years. In this project, we present a web server for the automation of a protein-protein interaction prediction algorithm. PRISM (Protein Interactions by Structural Matching) algorithm predicts interactions among various proteins by using structural and evolutionary similarity to known template interfaces. Our software system constructs a relational database of pre-calculated protein-protein interaction predictions, and the template and target structures used to calculate these predictions. It involves three sections which are templates, targets and predictions. Contents of the database can be queried using the appropriate PRISM web page. Also, visualization of these template and target structures are generated using Jmol plug-in in our web server. Also, another important aim of our web server is to run the PRISM algorithm from scratch when the protein structures in hand are not present in the relational database. In the case of such a situation, the steps of the algorithm are executed in an orderly fashion and all manual operations between these steps are eliminated. Computation of PRISM algorithm can take a lot time and effort. Using a 4-tier web database application architecture and a message queue system, many computations can run concurrently and users do not have to do any manual operations during the computation aside from input their protein structures. Since the manual operations are eliminated, users do not have to wait one step to finish to continue to the next step of the algorithm."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizim, fikirlerin ifade edilmesi ve paylaşılması için doğal ve etkili bir araçtır. Bu nitelikler çizimin kalem tabanlı sistemler için yeni bir etkileşim kipi olmasını sağlıyor. Çizim tabanlı arayüzlerin kullanılabilirliği başarı çizim tanıma sistemlerinin varlığına dayanır ki bu da çok sayıda etiketlenmiş verinin model eğitimi için kullanılmasını gerektirir. Ne yazık ki, çizim verisini etiketlemek zaman alıcı ve masraflıdır. Çünkü etiketleme için insanların katılımı gereklidir. Bu çalışmada, hedeflenen tanıma başarısı için gerekli manuel etiketleme yükünün azaltılmasında","Sketching is a natural and effective means for expressing and sharing ideas. These qualities have made sketching an emerging interaction modality in pen-based systems. Sketch-based interfaces rely on the availability of accurate sketch recognition engines, which in turn require large amounts of labeled data for training. Unfortunately, labeling sketch data is time consuming and expensive, because it requires the involvement of human annotators. We demonstrate the utility of the active learning technology in reducing the amount of manual annotation required to achieve target recognition accuracy. The first part of our work presents the first comprehensive study on the use of active learning for isolated sketch recognition. We present results from an extensive analysis which shows that the utility of active learning depends on a number of practical factors that require careful consideration. These factors include the choices of batch selection strategies, informativeness measures, seed set size, and domain-specific factors such as feature representation and the choice of database. Since active learning community lacks such factor based analysis, our empirical analysis is examplary. Our results imply that the Margin-based informativeness measure consistently outperforms other measures. We also show that the use of active learning brings definitive advantages in challenging databases when accompanied with powerful feature representations. The second part of our work deals with active learning on sketches containing more than one object, the so-called \scenes"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Geçici araç ağları (GAA) güvenlik mesajları dağıtımı, dinamik rota keşfi, oyun ve eğlence imkanları sunan ümit verici bir Akıllı Taşıma Sistemleri teknolojisidir. Bu tez çalışmasının ilk bölümünde en az haberleşme ek yükü ile benzer hareket modellerine göre hareketli taşıtlar arasında kararlı gruplar kurup dayanıklı iletişim ağı oluşturmaya odaklanılmıştır. Bu bağlamda VMaSC isimli, taşıtsal çok kararlı gruplar bazlı en az hereketli taşıtın çok sekmeli olarak seçildiği yeni bir gruplama algoritması tasarladık. Ağ Simülatörü (ns-3) ortamında hareketlilik modelinin SUMO ile yapıldığı geniş çaplı simülasyonlar VMaSC adlı algoritmamızın grup liderlik süresini %25 arttırdığını, grup liderlik değişimini de %10 azalttığını ortaya çıkarmıştır. Tez çalışmasının ikinci bölümü IEEE Taşıtsal Kablosuz Erişim Ortamları (TKEO) ile 3GPP (LTE) ağları entegrasyonunu üzerinedir. TKEO, IEEE 802.11p protokolü bazlı geçici modda çalışmakta olup taşıtlar arası ve taşıt - yol kenarı baz istasyonları arasında haberleşmeyi sağlamaktadır. LTE ise mobil haberleşme son teknoloji ürünü olup hücresel yapıda baz istasyonsal çözümler sunmaktadır. Bu çalışmada, yüksek veri hızlı IEE 802.11p bazlı GAA lar ile geniş yayın alanlı 3GPP (LTE) ağlarının birlikte çalıştığı bir yapı sunuyoruz. Bu yapıda taşıtlar gruplama algoritmamız olan VMaSC ile gruplanıyor ve seçilen lider IEEE 802.11p ve LTE arabirim özelliği ile çalışmaktadır. Ns-3 ortamında, hareketlilik modelinin SUMO ile yapıldığı geniş çaplı simülasyonlarda çok sekmeli gruplanmış GAA-LTE entagrasyonunun %90 veri paketi dağıtımını maximum gecikmenin 1 saniyenin altında gerçekleştirdiğini göstermiştir .","Vehicular Ad-Hoc Network (VANET) is a promising Intelligent Transportation System (ITS) technology that enables numerous applications such as safety message dissemination, dynamic route discovery, gaming and entertainment. First part of the thesis focuses on constructing stable clusters by determining the vehicles sharing similar mobility pattern to provide robust communication with minimum overhead in the presence of highly mobile vehicles. In this context, we propose VMaSC: Vehicular Multi-hop algorithm for Stable Clustering, a novel clustering technique based on choosing the node with the least mobility through multiple hops. Extensive simulation experiments performed using Network Simulator (ns-3) with the vehicle mobility input from the Simulation of Urban Mobility (SUMO) reveal that VMaSC increases cluster head duration by 25\% while decreasing the number of cluster head changes by 10%. Second part of the thesis considers the integration of IEEE Wireless Access in Vehicular Environments (WAVE) and 3GPP networks (LTE). WAVE operates based on ad-hoc mode with IEEE 802.11p protocol and enables vehicle-to-x (V2X) communication with vehicles and roadside infrastructures. LTE is a state-of-the art technology for mobile communication and provides a cellular infrastructure based solution. We propose an architecture combining these two technologies to achieve the high data rates of IEEE 802.11p-based VANETs and wide coverage of 3GPP (LTE) technology simultaneously. In this architecture, vehicles are clustered based on our approach VMaSC, and elected heads operate as dual-interface node with the functionality of IEEE 802.11p and LTE interface. By performing extensive simulation experiments in ns-3 with the vehicle mobility input from the Simulation of Urban Mobility (SUMO), multi-hop clustered VANET-LTE integrated architecture has been demonstrated to achieve over 90\% data packet delivery ratio with maximum delay below 1 second."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kelimelerin Öklit uzayında gerçek yoğun vektörler tarafından temsili kelimeler arasındaki ilgililiğin uzaklık ve açı cinsinden tanımlanmasına olanak sağlamaktadır. Kelime temsilleri tarafından işgal edilen bölgeler kelimelerin sözdizimsel ve anlamsal özelliklerini yansıtmaktadırlar. Bunlara ek olarak, kelime temsilleri doğal dil işleme algoritmalarına öznitelik olarak eklenebilmektedirler. Bu tez içinde, kelime temsillerini denetimsiz olarak, örneksel ilişkilerini yani kelimelerin değiştirilebilirliğini kullanarak üretiyoruz. S-CODE isimli Öklitsel gömme algorıtmasını çalıştırarak kelime türü temsillerine ek olarak, kelime bağlamı ve kelime andacı temsilleri elde ediyoruz. Kelime bağlamı ve kelime andacı temsilleri her kelime turu için sadece bir temsille kısıtlanmadıkları için çok sözdizimsel kategorili kelimelerle başa çıkma yeteneğine sahiptirler. Kelime türü, kelime bağlamı ve kelime andacı temsillerini k-means algorıtmasını kullanarak kümeleyip sözcük türü tümevarımı (part-of-speech induction) problemine uyguluyoruz. Penn Treebank bütüncesinin 45 sözcük türü etiketli Wall Street Journal kısımı için tür ve andaç temelli sözcük türü tümevarımları elde ediyoruz. Sözcük türü tümevarımlarımız ile tür temelliler için 0.8025 ve andaç temelliler için 0.8039 Çoktan-Bire eşleme kesinlikleri elde ediyoruz. Bildiğimiz kadarıyla tekniklerimiz bu sonuçlarla alandaki en gelişmiş teknikler olmuşlardır. Bununla beraber, çok anlamlılığı ölçmek için 'Altın Standart Etiket Treddütü' ölçüsünü takdim ederek andaç temelli sözcük turu tümevarımlarımızın çok sözdizimsel kategorili kelimelerde başarılı olduğunu gösteriyoruz.","Representation of words as dense real vectors in the Euclidean space provides an intuitive definition of relatedness in terms of the distance or the angle between one another. Regions occupied by these word representations reveal syntactic and semantic traits of the words. On top of that, word representations can be incorporated in other natural language processing algorithms as features. In this thesis, we generate word representations in an unsupervised manner by utilizing paradigmatic relations which are concerned with substitutability of words. We employ an Euclidean embedding algorithm (S-CODE) to generate word context and word token representations from the substitute word distributions, in addition to word type representations. Word context and word token representations are capable of handling syntactic category ambiguities of word types because they are not restricted to a single representation for each word type. We apply the word type, word context and word token representations to the part-of-speech induction problem by clustering the representations with k-means algorithm and obtain type and token based part-of-speech induction for Wall Street Journal section of Penn Treebank with 45 gold-standard tags. To the best of our knowledge, these part-of-speech induction results are the state-of-the-art for both type based and token based part-of-speech induction with Many-To-One mapping accuracies of 0.8025 and 0.8039, respectively. We also introduce a measure of ambiguity, Gold-standard-tag Perplexity, which we use to show that our token based part-of-speech induction is indeed successful at inducing part-of-speech categories of ambiguous word types."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Araçlar arası ağlar sürücünün erişimi dışındaki bilgileri kullanarak yolcuların güvenliğini arttırmayı amaçlamaktadır. Fakat etkili ve güvenilir bir ağ kon- trol protokolü tasarlamak için devamlı değişen ağ yapısı ve sürekli olmayan bağlantılar dikkate alınmalıdır. Bu çalışmada araçlar arası ağlar için, yeni geliştirilmiş olasılık işlevleri kullanarak karşı şeritteki araçlardan yararlanan, yoğunluk uyarlamalı, yayılımcı bilgi dağıtma protokolü (EpiDOL) öneriyoruz. EpiDol?un verimliliğini, SUMO ortamında yaratılmış gerçekçi trafik izler- iyle ile ns-3 benzetimcisinde farklı yogunluktaki ağlarda benzetimledik. Bu benzetimleri verimlilik, uçtan uca gecikme, maliyet ve karşı şeritin kullanım oranı ölçütlerini kullanarak inceledik. Sonuç olarak EpiDOL?un az yogunluklu ortamlarda ,%90?dan fazla verimliliğe, yüksek yoğunluklu ağlarda ise herhangi bir ilave maliyet olmaksızın %75 verimliliğe ulaştığını gördük. Verimlilik bakımından karşılaştırıldığında EpiDOL daha önce önerilen Edge-Aware pro- tokolünden %10 , DV-CAST protokolünden %40 ve DAZL protokolünden %50 daha başarılı olmuştur. EpiDOL?ün performansını attırmak için Kanal Yoğunluk Oranını ve paket alış hızını paratmetre olarak kullanan erim uyar- lama özelliği ekledik. Bu özellik yüksek yoğunluklardaki verimliliğimizi %25 oranında iyileştirdi.","Vehicular ad-hoc networks (VANETs) aim to increase the safety of passen- gers by making information available beyond the driver?s knowledge. The challenging properties of VANETs such as their dynamic behavior and inter- mittently connected feature need to be considered when designing a reliable communication protocol in a VANET. In this thesis, we propose an epidemic and density adaptive protocol for data dissemination in vehicular networks, namely EpiDOL, which utilizes the opposite lane capacity with novel prob- ability functions. We evaluate the performance in terms of end-to-end delay, throughput, overhead and usage ratio of the opposite lane under different vehicular traffic densities via realistic simulations based on SUMO traces in ns-3 simulator. We found out that EpiDOL achieves more than 90% through- put in low densities, and without any additional load to the network 75% throughput in high densities. In terms of throughput EpiDOL outperforms the Edge-Aware, DV-CAST and DAZL protocols 10% , 40%, 50% respect- ively. To achieve high throughput performance regardless to density level, we proposed a range adaptivity feature which utilize two channel statistics Channel Busy Ratio (CBR) and reception rate. This feature improved our throughput by 25% in higher densities."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"3B video teknolojinin bir sonraki adımının çoklu-görüntülü video formatı olması beklenmektedir. Günümüzde stereo 3B yayını dijital platformlardan yapılabilse de, Internet üzerinden yapılacak bir yayın daha esnek bir yapı ile serbest görüş teknolojisinin evlerimize veya mobil platformlara gelmesi mümkündür. Burada esneklikten kastedilen, kalitenin ve/veya görüntü sayısının Internet üzerinden aktarım sırasında ölçeklenebilmesidir. IP ağları üzerinde çoklu görüntü iletilirken yaşanması muhtemel sorunların ölçeklendirme ile çözülebilmesi mümkündür. Dolayısı ile kaliteli bir servis ölçeklendirme için önem arz eder. Ancak söz konusu çok-görüntülü video olunca, ölçeklendirmeye rağmen sunucudaki kapasite gereksinimi kritik seviyelere ulaşabilir. Bu durum fizibilite açısından sorun teşkil eder. P2P ile video aktarımı, sunucu tarafındaki yüksek kapasite gereksinimi sorununu hafifletmek için kullanılabilir. Ancak öncelikle P2P teknolojisi ile birlikte gelen peerların çıkması gibi sorunların çözülmesi gerekmektedir. Bu çalışmaya temel video iletim kavramlarını ile en güncel P2P video iletim tekniklerini inceleyerek başlıyoruz. Sonrasında güncelin daha ötesine geçerek ölçeklendirilebilir P2P görüntü iletimi konusunu sunuyoruz. Son olarak da böylesi bir sistemin çoklu-görüntülü video desteğini nasıl elde edeceğini belirtiyoruz. Tüm bunlara ek olarak, elde edilen sistemin günümüz sistemleri ile karşılaştırmasına da bu çalışmada yer veriyoruz.","Multi-view three-dimensional (3D) video is the next natural step in the evolution of digital media technologies. Recent 3D auto-stereoscopic displays can display multi-view video with up to 200 views. While it is possible to broadcast 3D stereo video (two-views) over digital TV platforms today, streaming over IP provides a more flexible approach for distribution of stereo and free-view 3D media to home and mobile with different connection bandwidth and different 3D displays. Here, flexible transport refers to quality-scalable and view-scalable transport over the Internet. These scalability options are the key to deal with the biggest challenge, which is the scarcity of bandwidth in IP networks, in the delivery of multi-view video. However, even with the scalability options at hand, it is very possible that the bandwidth requirement of the sender side can reach to critical levels and render such a service infeasible. P2P video streaming is a promising approach and has received significant attention recently and can be used to alleviate the problem of bandwidth scarcity in server-client based applications. Unfortunately, P2P also introduces new challenges such as handling unstable peer connections and peers? limited upload capacity. In this thesis, we provide an adaptive P2P video streaming solution that addresses the challenges of multi-view video streaming over P2P networks. We start with reviewing fundamental video transmission concepts and the state of the art P2P video streaming solutions. We then take a look at beyond the state of the art, and introduce the methods for enabling adaptive video streaming for P2P network to distribute legacy monoscopic video. Finally, we move to modifications that are needed to deliver multi-view video in an adaptive manner over the Internet. We provide benchmark test results against the state of the P2P video streaming solutions to prove the superiority of the proposed approach in adaptive video transmission."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Geçici Araç Ağları (GAA), yol trafik durumunu iyileştirme ve yolcuların güvenliğini sağlama özellikleriyle ümit veren bir Akıllı Taşıma Sistemleri teknolojisidir. Bu tez çalışmasının ilk bölümü, çeşitli temel başarım ölçütleri kullanarak GAA?ların zaman ve konum tabanlı topoloji özelliklerinin gerçekçi analizinin sağlanmasıyla ilgilidir. Bu analizde, gerçek yol topolojileri ve PeMS veritabanından alınan gerçek zamanlı veriler, otoyollarda gerçekçi trafik akışları üretebilmek için mikroskobik hareketlilik modeliyle birleştirilmektedir. Ayrıca, daha gerçekçi, yakın zamanda önerilmiş engel-tabanlı kanal modeli kullanılmış ve bu karmaşık sistemin başarımı en çok kullanılan sabit-disk ve log-normal gibi daha basit kanal modelleriyle karşılaştırılmıştır. Temel başarım ölçütleri üzerindeki araştırmamız sabit-disk ve log-normal kanal modellerinin gerçekçi GAA topoloji özellikleri sağlamada yetersiz olduğunu açığa çıkarmıştır. Bu nedenle, link özelliklerinin zamana bağlı değişimlerini hesaba katmak amacıyla, araç yoğunluğu ve bir korelasyon modeline göre log-normal modelin parametrelerini adapte eden bir eşleştirme mekanizması önermekteyiz. Önerilen yöntemin karmaşık, hesaplama açısından pahalı ve gerçekleştirme açısından zor olan engel-tabanlı kanal modeliyle iyi bir eşleme sağladığı gösterilmiş ve modelin işlevselliği Kaliforniya?da bulunan iki farklı otoyoldan alınan gerçek verilerle doğrulanmıştır. Çalışmamızın ikinci bölümü, GAA?larda yoğunluk hesaplaması için dağıtık algoritmalarla ilgilidir. Araç yoğunluğu yol trafik durumunun gözlemlenmesinde kullanılan önemli bir sistem ölçütüdür. Araç yoğunluk hesaplaması için önerilmiş algoritmaların çoğu ya belli bir altyapıya dayanmakta ya da sisteme genel araç yoğunluğunu hesaplamak için yerel komşu bilgisini kullanmaktadır. Ancak, bu algoritmalar yüksek yerleştirme ve bakım masraflarının yanı sıra düşük güvenilirlik ve kısıtlı kapsamadan dolayı dezavantajlıdırlar. Çalışmamızda, görevdeş ağlarda sistem boyutunu hesaplamak için önerilmiş mekanizmalardan esinlenerek üç farklı tamamen dağıtık algoritma tasarladık. Sonuçlarımız sistem büyüklüğü hesaplama tekniği GAA?larda trafik yoğunluğunu hesaplamak için de kullanılabileceğini gösterdi. Buna ek olarak, GAA?lar için özel olarak tasarlanmış tamamen dağıtık CluSampling algoritmasını önerdik. Hem otoyollarda hem de şehir-içi bölgelerde, farklı bölge genişlikleri ve trafik yoğunluklarında yapılan kapsamlı benzetimler CluSampling algoritmasının ağdaki değişikliklere dayanıklı, yüksek kesinlikte ve en az zaman gerektiren çözüm sunan bir algoritma olduğunu ve bunları sağlarken ağ ve öncü araç üzerinde daha az yük oluşturduğunu göstermiştir.","Vehicular Ad-Hoc Network (VANET) is a promising Intelligent Transportation System (ITS) technology that aims to improve road traffic conditions and safety of passengers. First part of our work deals with providing a realistic analysis of the VANET topology characteristics over time and space using various key metrics of interest. In this analysis, we integrate real-world road topology and real-time data extracted from the Freeway Performance Measurement System (PeMS) database into a microscopic mobility model to generate realistic traffic flows along the highway. Moreover, we use a more realistic, recently proposed, obstacle-based channel model and compare the performance of this sophisticated model to the most commonly used more simplistic channel models including the unit disc and log-normal shadowing models. Our investigation on the key metrics reveals that both log normal and unit disc models fail to provide realistic VANET topology characteristics. We therefore propose a matching mechanism to tune the parameters of the lognormal model according to the vehicle density and a correlation model to take into account the evolution of the link characteristics over time. The proposed method has been demonstrated to provide a good match with more sophisticated but computationally expensive and difficult to implement obstacle based model and validated over the real data of two different highways in California. Second part of our work deals with distributed algorithms for density estimation in VANETs. Vehicle density is an important system metric used in monitoring road traffic conditions. Most of the existing methods for vehicular density estimation either use infrastructure, or use local neighbor information to estimate global vehicle density. These techniques however suffer from low reliability and limited coverage as well as high deployment and maintenance cost. We adapted and implemented three fully distributed algorithms for density estimation, inspired by the mechanisms proposed for system size estimation in peer-to-peer networks. Results show that system size estimation technique can be used for density estimation in VANETs. Moreover, we proposed a completely distributed algorithm CluSampling which has been specifically tailored for VANETs. The extensive simulations of these algorithms at different vehicle traffic densities and area sizes for both highways and urban areas reveal that CluSampling is robust to changes in the network and it provides high accuracy in least convergence time and introduces less overhead on the network and the initiator node."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada, çok izlekli C/C++ uygulamalarındaki eş zamanlı programlama hatalarının tespitini geliştirmek ve bu uygulamaların hareket belleǧi (TM) teknolojisi kullanılarak kurtarılmasını saǧlamak için bir yöntem öne sürüyoruz. Hareket Belleǧi, koşut program tasarımını ve uygulamasını basitleştiren, başarımı arttıran ve uygulamaları çoǧu eş zamanlı programlama hatalarından koruyan bir koşut programlama modelidir. Bizim yaklaşımımız, eş zamanlı programlama hatalarının tespit edilme başarımını arttırmak için TM kullanmakta ve kalıt C/C++ uygulamalarının eş zamanlı programlama hatasızlıktan faydalanabileceǧi bir çerçeve saǧlamaktadır. Mevcut eş zamanlı hatası tespit eden yaklaşımlar ya çok yavaş ya da mevcut işlemci mimarisinde fazladan deǧişikliklere ihtiyaç duymaktadırlar. Bu yöntemlerin uygulamayı yavaşlatması birkaç sebepten kaynaklanabilir: bunları uygulamaya eklemek için kullanılan araçlar, hataların tespiti için gerekli hesaplamalar ve bu teknikler tarafından kullanılan hata tespitiyle alakalı verinin uygun biçimde korunması. Çok izlekli bir uygulamadaki her izleǧin komut akışını küçük hareketlere bölmek için bir yol sunmaktayız. Daha sonra, eş zamanlı hata tespit verisinin iyi taneli korunmasını elde etmek için çakışma tespitini kullanmaktayız. Eş zamanlı hatalarından, özel olarak veri yarışlarından, kurtulmak için hareket yazma arabelleklerini ve geri dönüş mekanizmalarını kullanmaktayız ve hatalı veriye veya eş zamanlı programın parçalarına fazladan koruma dayatmaktayız. Yaklaşımımız birçok çok çekirdekli denektaşı uygulamada iyi çalışmakta ve uzlaşılmış yollara nazaran eşzamanlı hata tespitinde belirgin bir başarım artışı göstermektedir. Bu gelişmeler önerilen yaklaşımın endüstride kullanımı için cesaret verici öncül sonuçlardır.","We propose a technique to improve detection of concurrency errors of multithreaded C/C++ applications and recovery of these applications from the errors using transactional memory (TM) technology. Transactional memory is an emerging parallel programming model which simplifies parallel program design and implementation, improves performance and protects applications from most concurrency bugs. Our approach uses TM to improve performance of detection of concurrency errors and provides a framework by which legacy C/C++ applications can benefit from concurrency error-freedom. The current concurrent error detection approaches are either too slow or need extra modification to current processor architecture. The slowdown of these techniques stems from a number of reasons: instrumentation used to add them into application, necessary computations needed to detect errors, and cost of proper protection of error-detection-related data used by these techniques. We propose a way to divide the instruction stream of each thread in a multithreaded application into small transactions. We then use conflict detection to get fine-grain protection of concurrency error detection data to improve performance. We use transaction write buffers and rollback mechanism to recover from concurrency errors, data races in particular, and impose extra protection on erroneous data or portions of concurrent program. Our approach works well on a number of multi-core benchmark applications and shows a significant performance improvement of concurrent error detection over conventional means. These improvements are encouraging initial results for the industrial usage of the proposed approach."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Büyük ölçekli dağıtılmıs sistemler güvenilir servis sağlamak ve karmaşık problemlerin çözümü için yüksek miktarlarda hesaplama gücüne ihtiyaç duyarlar. Bu bağlamda, bu tarz sistemlerin enerji ihtiyaçları hızlı bir şekilde artmakta ve bu da beraberinde çok yüksek maliyetler getirmektedir. Bu alanlarda daha ileri noktalara gelebilmek icin bu maliyetleri düsürmek bu organizasyonlar için hayati önem taşımaktadır. Kullanılan toplam enerji miktarını düşürmenin yanı sıra, elektrik fiyatlarında görülen coğrafi ve zamana bağlı değişimlerden faydalanarak elektrik faturalarını düşürmek de mümkün olmaktadır. Tezin ilk kısmında, kullanıcılarn fi nansal bütçeler belirleyebildiği ve bu sayede kendi kaynaklarının kullanımından doğan fi nansal yükü sınırlayabilecekleri bir gönüllü işlem ağı sunuyoruz. Kullanıcıların tükettikleri elektrik fiyatının zamana bağlı olarak değiştiği varsayımı altında, yaklaşımımızın ilginç bir görev atama problemi oluşturduğunu gösterdik. Burada amaç kullanıcıların belirledikleri bütçeleri aşmayacak şekilde işlem ağında yapılan toplam işi maksimuma taşımak. NP zorlukta olan bu probleme çözüm olarak polinom zamanda çalışan sezgisel algoritmalar sunduk ve detaylı simulasyonlarımız sonucunda gönüllü işlem ağında işlenen toplam iş miktarının şu anda kullanılmakta olan tekniklere oranla %35 arttırılabileceğini gösterdik. İkinci kısımda, toplu işlerin çözümü için özelleşmiş coğrafi olarak dağıtımlı veri merkezleri modelliyoruz. Elektrik fiyatının mekana ve zamana bağlı olarak değiştiği varsayımını ve dışardaki havayı kullanan soğutma fırsatlarını göz önüne alarak, enerji masrafını azaltmayı lineer programlama problemi olarak modelliyoruz. İki sezgisel iş planlama algoritması sunuyoruz ve gerçek sistem kayıtları ve elektrik fiyatlarını kullandığımız simülasyonların sonucuna göre sunduğumuz algoritmalar toplam enerji masraflarını iş dengeleme amaçlı algoritmalara oranla %6'ya kadar azaltmaktadır.","Large scale distributed systems require massive amount of computing power to provide reliable services and to solve computationally complex problems. In that regard, energy needs in these systems are increasing rapidly and that brings substantial costs. Thus, reducing energy costs of such organizations is crucial to advance in these fields. In addition to explicitly reducing energy consumption, it is also possible to cut down the total electricity bill by exploiting spatial and temporal variations in electricity prices. In the first part of the thesis, we propose a volunteer computing network where peers can set monetary budgets, limiting the financial burden incurred on them due the usage of their computational resources. Assuming that the price of the electricity consumed by the peers has temporal variation, we show that our approach leads to an interesting task allocation problem, where the goal is to maximize the amount of work done by the peers without violating the monetary budget constraints set by the peers. We propose various polynomial time heuristic algorithms to the problem, which is NP-hard, and our extensive simulations show that our approach can increase the total amount of work done up to 35% compared to an existing baseline. In the second part, we consider a geographically distributed data center network that is specialized to run batch jobs with previously determined Service Level Agreements (SLAs). Taking into account the spatial and temporal variations in the electricity prices and free cooling opportunities by utilizing the outside weather, we model the problem of minimizing the energy cost as a linear programming problem. We propose two job scheduling heuristic algorithms and our simulations using real-life workload traces and electricity prices demonstrate that the proposed heuristics can decrease the total energy cost up to 6% compared to a load balancing baseline solution."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde özgün bir çalışma zamanı doğrulama çerçevesini sunuyoruz. Yaklaşımımızdaki ana amaç, geleneksel yarış durumu denetleyicilerindeki işletim yüklerini ayırıp, bilgisayarlarımızda bulunan donanımsal olanakları kullanarak, halihazırdaki çalışma zamanına koşut çalışan, koşutlu doğrulama yapmaktır. Bu yüzden çerçevemizi çok çekirdekli işlemcilere (CPU) ve grafik işlemcisine (GPU) sahip kişisel bilgisayarlara herhangi bir donanım eklemesi gerekmeksizin gerçekleştirdik. Çalışmamızdaki ana yenilik, koşutzamanlı bir programın güvenilirlik özelliklerinin grafik işlemcideki iş parçacıklarında denetlenmesinin ilk olarak öne sürülmesi ve bunun için gerekli tekniklerin ve algoritmaların tasarlanmasıdır. Daha önce bu denetlemenin tamamı merkezi işlem ünitesi üzerinde gerçekleşmekteydi, ve denetleyicinin iş parçacıklarının denetlenen programın iş parçacıklarıyla aynı işlemci üzerinde koşması programın başarımını önemli ölçüde düşürmekteydi. Denetleyicilerdeki işletim yükünü ikiye ayırıyoruz: (i) gözlemleme ve (ii) denetleme yükleri. Detaylı inceleyeceğimiz bu yazılım çerçevesi, ayırdığımız iki işletim yükünü farklı işlemcilere paylaştırmaktadır. Sonuç olarak, denetlenen koşutzamanlı programın başarımı, yalnızca gözlemleme ve bu gözlemin öteki işlemciye aktarımından kaynaklanan işletim yüklerinden dolayı etkilenir. Bazı ek donanım destekleriyle gözlemleme yükünü hafifleten benzer çalışmalar da literatürde bulunmaktadır. Sunacağımız çerçevenin ön ürünü olan KUDA birimlerimizle yaptığımız deneylerimiz, farklı işlemcideki iş parçalarında koşut zamanda yarış durumlarını (data race) denetlemek, denetlenen koşutzamanlı program ile çok yakın zamanlı (birkaç milisaniye mertebesinde gecikmeyle) çalıştığını göstermektedir. Bu sonuç ile, çerçevemizin yarış durumlarını denetlemekten de öte, hatadan geri dönmek için kullanılabilmesinin mümkün olduğunu anlıyoruz. KUDA?da henüz yarış durumlarına odaklandık, ancak ileride koşut zamanlı programların diğer koşut zaman hataları ve bellek hataları gibi farklı güvenilirlik özelliklerini de denetlemesini sağlayabiliriz. Çerçevemizde merkezi işlem ünitesinde çalışan koşut zamanlı programın gözlenmesi ile iş parçacıklarından toplanan çalışma izi bloklamayan bir veri yapısında biriktirilip grafik işlemcisine aktaran bir iş parçası çalıştırıp, çok sayıda görüntü işlemcisi çekirdeği kullanan koşut zamanlı denetleme birimlerimizde denetliyoruz. KUDA, koşutzamanlı çalışan 2 popüler denektaşı takımlarındaki tüm uygulamalarda yaşanmakta olan başarım kaybını (yalnızca bir denektaşı programı hariç tutulması kaydıyla) en az 3 kat azaltmaktadır.","We propose a novel technique by introducing a coprocessor to runtime verifica- tion, ergo reducing the cost of race detection without any hardware extension to mainstream PC environment. The goal of our approach is to offload the high com- putational overhead of traditional race detection to hundreds of cores available at modern GPUs. Existing runtime verification frameworks have been designed to run on the same processing units as the code being monitored and (i) instrumentation and (ii) analysis costs contribute to the slowdown of the program being monitored. The framework we propose allows us to carry out (ii) on separate, dedicated cores. As a result, the program being monitored experiences slowdown due to bookkeeping of events, bottleneck is not caused by race detection. An orthogonal line of work shows that with some inexpensive hardware support, monitoring costs can be reduced to negligible levels. By parallelizing the offloaded work, our experiments show that they run as fast as the program being monitored, on separate computational resources. As a demonstration of concept, we investigate runtime monitoring for concurrency bugs, in particular, data race detection. We use a few CPU threads and a large number of cores on a GPU to minimize the slowdown of the application on which race detection is being run."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, çoklu-iş parçacıklı ve koşut-zamanlı programların doğrulanması ile ilgili iki önemli probleme değinmekteyiz. Önce koşut-zamanlı bir uygulamanın doğrusallaştırılabilirliğini ispatlamak problemini ele alıyoruz. Geçerli bir ispat sistemine dayanarak doğrusallaştırılabilirlik ispatlarının mümkün kılındığı bir metod sunuyoruz. Metodumuz koşut-zamanlı uygulamanın amaçlanan tanımlamalarına dönüştürülmesine dayanmaktadır. Bu dönüştürülmeler ispat sistemin kuralları tarafından yönetilmektedir. Her dönüşüm adımı programın doğruluk ile ilgili davranışlarını korumaktadır. Limitte programın doğruluğunu gösteren tanımına ulaşılır. Yaklaşımımızda, doğrusallaştırılabilirlik kavramını, koşut-zamanlı programları ve ispat sisteminin kurallarını tanımlamaktayız. Ardından teorik bulgularımıza yer vermekteyiz.İkinci olarak, programcı tarafından tanımlanan çakışmaları bulan işlemsel programların doğrulanması problemini ele alıyoruz. Performans açısından bu t¨ur sistemler istenilse de bu sistemler kendilerini kullanan işlemsel programların doğrulanmasını zorlaştırmaktadır. Özellikle de bu sistemler dizisel kanıtların yapılmasını engellemektedir. Yaklaşımımızda, önce bu tip programları modelliyoruz.Sonra, dizisel kanıtların yapılmasını sağlayan bir reçete sunuyoruz. Bu reçetede program üzerinde soyutlamalar yapılmaktadır. Soyutlamalar yapıldıktan sonra, doğrulama işi otomatik dizisel doğrulamaya dönüşmektedir. Bu da VCC ve HAVOC gibi dizisel doğrulama araçlarıyla yapılabilir. Ana teoremimiz ise, soyutlanmış program doğrulamasının ilk orijinal işlemsel programın doğrulaması anlamına geldiğini ispatlamaktadır.","In this thesis, we consider two significant software verification problem regarding concurrent, multi-threaded programs. First, we consider the problem of proving the linearizability of a concurrent implementation. We suggest a sound method for verifying a concurrent implementation is linearizable based on a sound proof system. Our method is based on transforming the concurrent implementation into the specification aimed for the implementation. The transformation is governed by proof rules of the proof system. Each transformation step preserves certain behaviors of the program that are relevant to the specification of the program. At the limit, we obtain a program that is being the sequential specification considered for the correctness of the original concurrent program. In our approach, we provide the formalization of the linearizability notion, concurrent programs as well as the proof system and its rules. We then state our theoretical findings.Second, we study the verification of transactional programs with programmer-defined conflict detection. While programmer-defined conflict detection is desirable in terms of performance issues of the transactional memory systems, such relaxed conflict detection complicates the verification of the programs that use these transactional systems. In particular, the ability to use sequential reasoning provided by conventional transactional memories is lost when the relaxed conflict detection is introduced. In our approach, we first model and formalize such transactional programs. Then, we provide a recipe for the verification process in which we regain the ability to use sequential reasoning. This recipe includes abstractions provided by the programmer on the original program. After the abstractions are introduced, the verification problem becomes the sequential verification problem automated by sequential verification tools such as VCC and HAVOC. Our soundness theorem guarantees that once the abstracted program is verified, so is the original transactional program."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sayısal dünyada, aralarında herhangi bir karşılaştırma, aktarım, veya analiz yapabilmek için ilişkilendirilmesi gereken çok sayıda nesne modeli vardır. Şekil eşleme algoritmaları, buprobleme çözüm olarak verili iki nesne modeli arasında benzer veya anlamsal olarak denk yüzey noktalarını eşleştirmeyi hedeflerler.Bu çalışmada anlamsal olarak yakın ve yüzeyleri tamamen veya kısmi olarak örtüşen iki 3B şeklin öznitelik noktaları veya tüm noktaları arasında eşleştirme hesaplama problemineodaklanıyoruz. Bu örtüşmenin izometrik, yani uzaklık-koruyan, deformasyonlara ve ölçeklemeye karşı değişimsiz olduğunu varsayıyoruz. Bir başka deyişle, bizim geliştirdiğimizizometrik şekil eşleme algoritmaları şekil eşleme probleminin, şekiller arasındaki benzerlik miktarı, örtüşmenin kısmi ya da tam olması, istenilen eşlemenin çözünürlüğü gibi çeşitlietkenlere bağlı olarak ayrışan birçok farklı durumun üstesinden gelirler.Verilen iki şekil arasında çoğu zaman tatmin edici 3B eşlemeler bulabilen yöntemler olsa da bu yöntemlerin yüksek hesap yükü, hem kısmi hem yoğun eşleme yapamama, yaklaşıklık ve gömme hataları, simetrik parçaların karıştırılması gibi çeşitli sorunları vardır. Mevcut yöntemler şekil eşleme problemi için sağlam bir temel ve iyi bir başlangıç noktası oluştururken, bu çalışmada, verilen senaryoya göre tasarlanan yeni çözümler bu temel problemin ele alınmasında belirgin gelişmeler ve katkılar sağlamaktadır.3B şekil eşleme problemini tam ve kısmi eşleme olarak iki ana grupta inceliyoruz, ve ilk grubu çıktı çözünürlüğüne göre kaba ve yoğun eşlemeler olarak kendi içinde ikiye ayırıyoruz.Kaba çözünürlükteki tam eşleme problemi için, eşit uzaklıklarla ayrılan öznitelik noktalarını iki şekil yüzeyi üzerinden ortaklaşa örnekledikten sonra, problemi kaynak ve hedefşekillerdeki örnekler arasında olası tüm gönderimler üzerinden tanımlanan bir kombinatoryal eniyileme olarak formule ediyoruz ve bunu, olasılıksal bir yaklaşımla EM algoritmasi kullanarak çözebileceğimiz bir olasılıksal çatı içindeki log-olabilirlik enbüyütme problemine dönüştürüyoruz. Bu yöntem yüksek hesap yükü nedeniyle ancak kaba çözünürlükte görece az sayıda nokta arasında eşleme yapabilir. Tez çalışmasının bir sonraki aşamasında, şekil modellerindeki bütün noktalar arasında yoğun eşleme yapabilen hızlı, kabadan-inceye (çoklu çözünürlüklü), ve simetrik flip problemini de dikkate alan yeni bir algoritma tasarlıyoruz. Ölçek-değişimsiz ölçütümüz üzerine dayalı şekil ölçek düzgeleme yöntemimiz, diğer yandan, kısmi eşleme probleminin özel ve kısıtlı bir halinin üstesinden gelirken, diz-oyla-ve-birleştir (RAVAC) algoritmamız en genel kısmi eşleme durumunu ele alır. Bu iki yöntem de hem kısmi hem yoğun eşlemeler üretir.Bu çalışmada geliştirdiğimiz bütün yöntemleri, gerçek ve sentetik veriye dayalı çeşitli 3B şekil vertabanları üzerinde, literatürde mevcut diğer yöntemlerle karşılaştırmalı olaraksınıyoruz.","There are many pairs of objects in the digital world that need to be related before performing any comparison, transfer, or analysis in between. The shape correspondence algorithms essentially address this problem by taking two shapes as input with the aim of finding a mapping that couples similar or semantically equivalent surface points of the given shapes.We focus on computing correspondences between some featured or all present points of two semantically similar 3D shapes whose surfaces overlap completely or partially up to isometric, i.e., distance-preserving, deformations and scaling. Differently put, our isometric shape correspondence algorithms handle several different cases for the shape correspondence problem that can be differentiated based on how similar the shape pairs are, whether they are partially overlapped, the resolution of the desired mapping, etc.Although there exist methods that can, in most cases, satisfactorily establish 3D correspondences between two given shapes, these methods commonly suffer from certain drawbacks such as high computational load, incapability of establishing a correspondence which is partial and dense at the same time, approximation and embedding errors, and confusion of symmetricalparts of the shapes. While the existing methods constitute a solid foundation and a good starting point for the shape correspondence problem, our novel solutions designed for a given scenario achieve significant improvements as well as contributions.We specifically explore the 3D shape correspondence problem under two categories as complete and partial correspondences where the former is categorized further according to the output resolution as coarse and dense correspondences. For complete correspondence at coarse resolution, after jointly sampling evenly-spaced feature vertices on shapes, we formulate the problem as combinatorial optimization over the domain of all possible mappings between source and target features, which then reduces within a probabilistic framework to a log-likelihood maximization problem that we solve via EM (Expectation Maximization) algorithm. Due to computational limitations of this approach, we design a fast coarse-to-fine algorithm to achieve dense correspondence between all vertices of complete models with specific care on the symmetric flip issue. Our scale normalization method based on a novel scale-invariant isometric distortion measure, on the other hand, handles a particular and rather restricted setting of partial matching whereas our rank-and-vote-and-combine (RAVAC) algorithm deals with the most general matching setting, where both two solutions produce correspondences that are partial and dense at the same time.In comparison with many state-of-the-art methods, our algorithms are tested by a variety of two-manifold meshes representing 3D shape models based on real and synthetic data."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein dinamiği analizlerinde yapısal ve dalgalanma özellikleri kullanan metodlar uygulanmaktadır. Dalgalanma analizi çalışmalarının sonuçları buradan elde edilecek bilgilerin ümit verici olduğunu göstermektedir. Bundan önceki çalışmalarda kütle ve yay sistemleri sıklıkla kullanılmaktadır. Ancak dalgalanma analizlerinde kullanılan modellerde sistemdeki titreşimlerim tamamen harmonik olmaları nedeniyle deneysel verilerle arasında önemli bir fark oluşmaktadır. Harmonik davranıştan farklılıklar daha çok yavaş ve kollektif modlarda meydana gelmektedir. Anharmonik mod ayrışımı yöntemi bu farkı kapatabilecek düzeltmelerin ilk aşamasıdır. Daha yüksek mertebeden düzeltmeler modların kendi aralarında olan etkileşimlerden dolayı limitlidir. Mode eşlenimi düzeltmeleri ise enerji transferi ve allosterik mekanizmaya yönelik önemli bilgiler içermektedir.Bu çalışmada, moleküler dinamik sonuçları Dictyostelium discoideum myosin II motor yapısını bir test ortamı olarak kabul edip incelenmiştir. Mod dalgalanma dağılımları MD kullanılarak oluşturulup, tamamen harmonik olan sistemlerin üzerine anharmonik düzeltmeler yapılması amacıyla kullanılmıştır. Hermite tensör polinomları ise mod dalgalanma dağılımlarının elde edilmesinde kullanılmıştır. Mod uzayındaki dalgalanmalar daha sonrasında gerçek uzay koordinatlarına dönüştürülüp KL metriği ile karşılaştırılmıştır. Ligand bağlı ve serbest protein durumlarına ait sonuçlar mod çiftlenmi etkisinin tekbaşına proteinin işlevsel olarak önemli kısımlarını gösterdiği görülmüştür.","Analysis of protein dynamics uses structural and ? uctuation based methods. Fluctuation analysis of protein dynamics has proven to be a rewarding venue of research. Mass and spring models are used in previous research commonly. However, ? uctuations of this models are based on purely harmonic which has signi ? cant gap between the experimental results. Deviations from harmonicity mostly observe in slow, collective modes. Corrections like anharmonic modal decomposition are ? rst step in order to minimize this gap. The contribution of the higher-order corrections is limited because of the interacting modes.Mode-coupling corrections which yield valuable information on means of energy transfer and allostery.In this work, molecular dynamic results of Dictyostelium discoideum myosin II motor domain is used as test ground. Mode ? uctuation distributions produced using MD results, fully harmonic models and a model with anharmonic corrections. Tensorial hermite polynomials are used in order to obtain distributions of modal ? uctuations. Fluctuations onmodal space are transformed back into real space and distribution of residual ? uctuations is compared using KL divergence. Analysis results for ligand-bound and free myosin dynamics are used in order to demonstrate that the mode-coupling contributions alone highlight functionally important sites."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz iletişim teknolojileri için artan talep spektrum kıtlığına neden olmakta ve spektrumun etkin kullanımı için dinamik spektrum erişim tekniklerine, bir diğer değişle bilişsel radyoya ihtiyaç duymaktadır. Kablosuz algılayıcı ağları (KAA) haberleşme için lisanssız bantları kullanmaktadır, fakat bilişsel radyo yeteneğiyle algılayıcı dügümler lisanslı tayfa fırsatçı bir şekilde erişebilirler. Bilişsel radyo ile donatılmış algılayıcı düğümler yeni bir dağıtık algılama paradigmasını, Bilişsel Radyo Algılayıcı Ağlarının (BRAA) ortaya çıkarmaktadır. BRAA lisanslı bantlara erişimi ve bu bantlar üzerinden dağıtık algılamayı mümkün kılarak tayf kıtlığını azaltmaktadır. BRAA'nın eşsiz karakteristikleri ve zorlukları güvenilir ve enerji verimli dağıtık algılamanın gerçekleştirilebilmesi için yenilikçi ağ çözümlerini gerektirmektedir. Bu araştırmanın amacı BRAA'da güvenilir ve enerji verimli dağıtık algılama için yeni gelişmiş haberleşme algoritmalarının tasarımı, geliştirilmesi ve analizidir. Detaylı olarak,BRAA'da güvenilirlik ve tıkanıklık kontrolü için literatürdeki taşıma katmanı protokollerinin detaylı performans degerlendirmesi yapılmıştır. Çeşitli akıllı şebeke çevrelerinde gecikme-duyarlı ve çoklu-ortam haberleşmesi incelenmiştir. BRAA'da olay aktarımı için ilk defa tayf-bilinçli ve enerji-uyumlu güvenilir taşıma protokolü önerilmiştir. Sonrasında, aktörde güvenilir yerel kestirim sağlanması ve globalde aktörler arasında konsensus ulaşılması için yeni bir güvenilir kestirim için tayf boşluğu ataması algoritması önerilmiştir. Tayf bırakma ve verimlilik için ayrı radyo kullanımı analiz edilmiştir. Akıllı şebeke ortamlarının dağıtık algılama için zorluklarının üstesinden gelmek için tayf-bilinçli ve bilişsel algılayıcı ağlar.tanıtılmıştır. Son olarak, limitli ve zorlu su altı akustik tayf koşullarında ulasılabilir kapasiteyi arttırmak için tayf-bilinçli su altı ağları incelenmiştir.","The ever increasing demand for wireless communication technologies causes spectrum scarcity and inspires researchers to envision the dynamic spectrum access techniques, i.e., cognitive radio, for efficient utilization of spectrum. Wireless sensor networks (WSN) have been considered as operating at unlicensed bands, however, sensor nodes can access licensed spectrum opportunistically with incorporation of cognitive radio capability. Sensor nodes equipped with cognitive radio emerges a new distributed sensing paradigm, i.e., Cognitive Radio Sensor Networks (CRSN). CRSN alleviates spectrum scarcity via enabling access to and distributed sensing operation over licensed bands. The unique characteristics and research challenges posed by CRSN call for novel networking solutions tailored to realize reliable and energy-efficient distributed sensing.The objective of this research is to design, develop and analyze new advanced communication schemes for reliable and energy-efficient distributed sensing in CRSN.More specifically, a comprehensive performance evaluation of existing transport protocols is performed for reliability and congestion control in CRSN. Delay-sensitive and multimedia communication in CRSN is investigated for various smart grid environments. Furthermore, the spectrum-aware and energy-adaptive reliable transport (SERT) protocol for event delivery is first proposed for CRSN. Next, a new spectrum hole assignment for reliable estimation (SHARE) scheme is proposed to achieve reliable local estimation at actor and reach global consensus among actor nodes. The analysis of dedicated radio utilization for spectrum handoff and efficiency is presented for the cognitive radio networks. Moreover, spectrum-aware and cognitive sensor networks (SCSN) are introduced to address the unique challenges of the smart grid environments for distributed sensing. Finally, Spectrum-aware Underwater Networks are investigated to increase achievable capacity under the limited availability and harsh conditions of the underwater acoustic spectrum."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışmasında, çok kipli beden hareketi verisi üzerinde istatistiksel öğrenme teknikleri kullanarak, konuşma ile eşzamanlı, doğal ve inandırıcı üst beden hareketleri sentezi için yeni bir çatı yapısı ve sayısal model önerilmektedir. Önerilen çatı yapısı 4 ana kısımdan oluşmaktadır: i) üst beden hareketi ve prozodik bölütler üzerinde tek kipli kümeleme, ii) jest ve prozodik bölütler üzerinde çok kipli analiz, iii) konuşma güdümlü jest sentezi ve iv) beden jest animasyonu. İlk kısım, jestlerin ve konuşma prozodisinin zamansal orüntülerini öğrenmek için konuşma ve beden hareketlerinin tek kipli analizinden oluşmaktadır. Jest örüntülerinin belirlenmesi çok kanallı ve eş zamanlı video kayıtlarından çıkarılan beden hareketlerinin yarı denetlemeli zamansal kümelenmesi ile sağlanmıştır. Buna karşılık prozodi örüntüleri ise konuşma girdisinden çıkarılan prozodi özniteliklerinin denetimsiz zamansal kümelenmesiyle tanımlanmıştır. İkinci kısım, konuşma ve jestler arasındaki bağıntıları öğrenmek için gizli yarı Markov modellerine dayalı çok kipli bir analiz yöntemi kullanmaktadır. Üçüncü kısım beden hareketi sentezi problemini ele alır; bu da konuşma girdisi verildiğinde jest sekansının ve jest sürelerinin oluşturulmasına karşılık gelir. Son kısımda ise, sentezlenmiş hareket dizisinden doğal görünümlü bir üst beden hareketi animasyonunun oluşturulması hedeflenir. Önerdiğimiz konuşma güdümlü jest animasyon sisteminin başarımını oluşturmuş olduğumuz MVGL-MUB veritabanı üzerinde ölçüyoruz. Elde ettiğimiz deney sonuçları, önerdiğimiz sentez sisteminin, konuşma ile beden hareketleri arasındaki işitsel-görsel bağıntıyı uygun şekilde modellediğini ve böylece gerçekçi ve doğal üç boyutlu insan modeli animasyonları üretebildiğini göstermektedir.","In this thesis we present a new computational model for natural and believable upper-body gesture synthesis in synchrony with speech using statistical learning techniques over multimodal gesticulation data. The framework consists of four main tasks for: i) unimodal clustering of gesture and intonational phrases, ii) multimodal analysis of gesture and intonational phrases, iii) speech driven gesture synthesis, and iv) gesture animation. The first task consists of unimodal analysis of speech and upper body motion to learn temporal patterns of gesture and speech prosody. Body motion features, which are extracted from multi-channel synchronous video recordings, are used to define gesture phrases with a semi-supervised temporal clustering scheme. On the other hand prosody features, which are extracted from speech input, are used to define intonational phrases with an unsupervised temporal clustering scheme. The second task performs multimodal analysis to learn dependencies between gesture and intonational phrases by utilizing a hidden semi-Markov model (HSMM). Third, we perform gesture synthesis, that is extraction of gesture sequence and gesture durations, given the speech input. The final task is to perform gesture animation, where the synthesized gesture sequence is mapped into body motion sequences to maintain a natural looking animation. The performance of the proposed speech driven gesture synthesis system is tested over our MVGL-MUB Database. Experimental results demonstrate that our system is able to properly discover audiovisual correlations between speech and gesture thus it can synthesize realistic and natural body gestures along with 3D human model animation."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez video ve onunla piksel bazında ilişkilendirilmiş derinlik bilgisini, video içerisindeki cisim ve bu cisimle etkileşmek için kullanılan dokunsal arayüz noktası (HIP) arasındaki bağıl devinim sonucu oluşan dinamik kuvvetleri göz önünde bulundurarak haptic etkileşim alanına yeni bir yaklaşım sunmaktadır.Video içeriğine haptic (dokunsal) bilgi tümleşimi, cisimin geometrisi ve dokusuna bağlı haptic (dokunsal) giydirilmesi konusunda mevcut çalışmalar olmasına rağmen, bildigimiz dahilinde bağıl devinim kaynaklı dinamik kuvvetlerin haptic giydirilmesiyle ilgili bir çalışma bulunmamaktadır. Bizim yaklaşımımızda, video içeriği değitirilmesede, kullanıcı tarafından hissedilen kuvvet HIP'in görüntüdeki cisime göre yaptığı harekete bağlı olarak değişir. Bu amaçla HIP ivmesi sensörlerden elde edilen pozisyon bilgisinin numerik türevlemesiyle hesaplanırken, videodaki derinlik bilgisi ve devinim kestirim teknikleri kullanılarak da cisimin ivmesi kestirilmektedir. Daha sonra HIP ve görüntüdeki cisime kütle ataması yapılır. Bu atama yapılırken hesaplanacak kuvvetin haptic cihaz tarafından izin verilen azami kuvveti geçmemesi göz önünde bulundurulur. Son olarak bağıl devinim kaynaklı dinamik kuvvet Newton'un ikinci yasası kullanılarak hesaplanır ve haptik cihaz aracılığıyla kullanıcılara cisimin geometrik özelliklerinden kaynaklanan statik kuvvete ek olarak geri bildirilir. Dinamik kuvvetlerin eklendiği ve eklenmeden önceki haptic giydirilmesi sonucu hesaplanan kuvvetlerin deneysel sonuçları sağlanmıştır.","This thesis presents a new approach for haptic interaction with video and associated pixel-based depth data, including rendering of dynamic force due to relative motion between an object in a video and the haptic interface point (HIP) of the user. While the concept of haptic video, that is, haptic rendering of forces due to geometry and texture of objects in a video, has already been proposed, haptic rendering of force due to relative motion between a video object and the HIP has not been studied. We propose that the force experienced by a user should vary according to the movement of a video object relative to the HIP, even though the content of the video shall not be altered by this interaction. To this effect, the acceleration of a video object is estimated using video motion estimation techniques, while the acceleration of the HIP is estimated from the HIP position data provided by the haptic device. We assign mass values to the object and the HIP such that the maximum force rendered will be in the range that can be displayed by the haptic device. Then, the dynamic force due to the relative motion is computed by using Newton?s second law and displayed to the user through the haptic device in addition to the static forces due to the geometry and texture of the object. Experimental results are provided to demonstrate haptic rendering of forces calculated with and without including dynamic forces."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Regresyon tabanlı otomatik çeviri (RegMT) yaklaşımı otomatik çeviriye öğrenme modellerini, öğrenme örnekleri seçimini, özellik gösterimini, ve çeviriyi yaratmayı ayıran bir ögrenme platformu sağlar. Transdüktif öğrenme platformu RegMT yaklaşımını sayısal olarak daha hesaplanabilir yapar ve her test cümlesi için bağımsız olarak model kurar. Geliştirdiğimiz öğrenme örnekleri seçim algoritmaları RegMT yaklaşımını sayısal olarak daha hesaplanabilir yapmanın yanında standart SMT sistemlerinin performansını arttırır. Paralel öğrenme cümlelerinden önceki işlerden daha iyi cümle seçme metodları geliştirerek daha doğru RegMT modellerini daha az öğrenme cümlesi kullanarak elde edebiliyoruz.Otomatik çeviri için L1 düzenli regresyon tekniğini L2 düzenli regresyon tekniğinden daha iyi bir model olarak sunuyoruz. Elde ettiğimiz sonuçlar seyrek regresyon modellerinin L2 düzenli regresyon modelinden hedef özellikleri tahmin ederken, kelime eşleşmelerini bulurken, kelime dizimi tabloları oluştururken, ve çeviri yaratırken daha iyi olduğunu göstermektedir. RegMT modelinin performansını ve çevirilerin kalitesini ölçmek için iyi ölçüm teknikleri gelistirdik. İngilizceye çevirileri ölçerken insanlar tarafından performansı iyi bulunan F1 ölçüsünü kullanıyoruz. F1 bizim RegMT modellerinin performansını hedef özellik tahmin vektörlerini veya öğrenilen katsayı matrislerini veya verilen bir SMT modelini kendi kelime dizimi tablolarını kullanarak, hesaplaması pahalı olabilen çeviri adımını uygulamadan ölçmemize olanak sağlar.Seyrek L1 düzenli regresyonun L2 düzenli regresyondan Almanca-İngilizce ve küçük öğrenme kümeleri kullanırken İspanyolca-İngilizce çevirisinde daha iyi olduğunu gösteriyoruz. Grafik tabanlı çeviri kelime dizimi tabanlı çeviriye az kelime hazineli çeviri işlerinde alternatif olabilir.","Regression based machine translation (RegMT) approach provides a learning framework for machine translation, separating learning models for training, training instance selection, feature representation, and decoding. We use transductive learning framework for making RegMT computationally more scalable and consider model building step independently for each test sentence. We develop better training instance selection techniques than previous work from given parallel training sentences for achieving more accurate RegMT models using less training instances.We introduce L1 regularized regression as a better model than L2 regularized regression for statistical machine translation. Our results demonstrate that sparse regression models are better than L2 regularized regression for statistical machine translation in predicting target features, estimating word alignments, creating phrase tables, and generating translation outputs. We develop good evaluation techniques for measuring the performance of the RegMT model and the quality of the translations. F1 allows us to evaluate the performance of RegMT models without performing the decoding step, which can be computationally expensive.We use graph decoding on the prediction vectors represented in n-gram counts space or we decode using Moses after transforming the learned weight matrix representing the mappings between the source and target features to a phrase table that can be used by Moses during decoding. We demonstrate that sparse L1 regularized regression performs better than L2 regularized regression in German-English translation task and in Spanish-English translation task when using small sized training sets. Graph based decoding can provide an alternative to phrase-based decoding in translation domains having low vocabulary."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"3 boyutlu (3B) görüntü teknolojilerinin bir sonraki adımı Çoklu-Görüntülü Videodur (ÇGV).Görsel deneyim kalitesine duyarlı ÇGV iletimi günümüzün güncel bir araştırma konularında birtanesidir. ÇGV iletiminde gereken bant genişliği gönderilen görüntü sayısına bağlıdır, bu yüzdendaha önceki video standartlarından farklı olarak, sabit bant genişliğine sahip kanalları kullanmakzordur. Dolayısı ile 3B video iletimini değişken bağlantı kapasitesi sağlayabilen İnternet protokolü(IP) üzerinden yapılması makul bir tercihtir. IP sayesinde kullanıcılar bağlantı hızlarına göre farklısayıda görüntüyü alabilirler. Bu çalışmada IP'nin iki temel problemine değindik: i) ölçeklenebilirlikproblemi, ii) değişken miktarda mevcut bit hızı problemi.Yukarıdaki problemlere çözüm olarak eşten eşe (P2P) ağlar üzerinden ÇGV iletimi uygulanabilir.P2P ağlarını kullanmak, 3B video gönderme görevini eşlere dağıttarak ölçeklenebilir bir iletimsisteminin oluşmasına imkan sağlar. Ayrıca, iletim sisteminin uyarlanabilir olması, içeriğin mevcutkanal özelliklerine göre şekillenmesini mümkün kılarak, değişken kanal kapasitesine uygun akıtımyapılmasına olanak verir. Gene de, başarılı ÇGV iletim hizmeti sağlanabilmesi için algılanandeneyim kalitesinin detaylıca incelenmesi gerekmektedir. Bu çalışmada paket kaybı ve değişkenkanal kapasitesi gibi tipik IP ağ hatalarından etkilenen ÇGKların deneyim kalitesi üzerineodaklandık. İlk amacımız, transfer sırasındaki paket kayıplarından kaynaklanan hataların, ön hatadüzeltme (FEC) algoritmalarıyla ve hata gizleme methotlarıyla etkisini azaltmaktı. Daha iyi birÇGV algısı için FEC paketlerinin görüntüler arasında optimum paylaştırılması incelendir. Görseltest sonuçları bu paketlerin asimetrik paylaştırılmasının daha kaliteli bir algı sağladığını ortayaçıkardı. Sonrasında, 3B görüntünün kanalın değişken veri hızına uyarlanması için farklı adaptasyonmetotları önerdik. Ayrıca bu metotları izleyici tarafından algılanan kalite üzerinden değerlendirdik.Bu amaçla içerik boyutunu ölçeklemekten kaynaklanan görsel bozuklulukları görsel testlerledeğerlendirdik. Bu testler, iyi bir video kalitesi elde etmek için ara görüntülerin mümkün olduğukadar asimetrik olarak ölçeklendirilmesi gerektiğini gösterdi. Eğer yeterli olmaz ise, asimetrikkodlama eşiğine (~32dB) ulaşıldığında, tüm ara görüntülerin iletiminin durdurulmasının ve sadeceilk ve son görüntüye ek olarak pixel derinlik bilgisinin gönderilmesinin en yüksek kaliteyi sağladığısaptandı. Yukarıdaki sonuçlara ek olarak, bu çalışma ÇGV'nun iletimi için Torrent-tabanlı biriletim sistemi önermektedir.","Multi-view Video (MVV) is the next step in 3D evolution and quality of experience (QoE) awareMVV transmission is a hot research topic. In MVV transmission, the required bit rate depends onthe number of views transmitted therefore unlike the previous video standards, it is difficult tooperate over channels with fixed bandwidth capacity; making the Internet Protocol (IP) the naturalchoice for 3D video transmission. IP provides varying channel capacity and allows users to receivecontent at different bit rates according to their 3D displays setup. We addressed two majorproblems of IP: i) scalability problem, ii) varying amount of available bit-rate problem.In this thesis, we have adopted an adaptive MVV streaming mechanism over Peer-to-peer (P2P)networks to address both of these challenges. The P2P approach addresses the scalability problemby distributing the task for delivery 3D media among users; allowing a more scalable solution. Theadaptive streaming approach addresses the unreliable channel problem by adjusting the content tothe available channel properties. However, properly addressing the perceived quality of experience(QoE) of MVV is very crucial in order to achieve a successful adaptive P2P MVV service.Therefore, we have particularly focused on QoE of MVV that is subject to typical IP networkfailures such as packet loss and varying channel capacity. Our first goal is to reduce the effect of theerrors generated due to the packet losses during transmission by utilizing forward error correction(FEC) algorithms and concealment methods. The optimal allocation of FEC packets among viewsis investigated in order to achieve a better MVV perception. The subjective test results indicate thatasymmetric allocation of FEC packets results in better perceived quality. Next, we evaluated theperceived quality of MVV, when various adaptation methods are adopted to match MVV bit-rate toa given rate. For this purpose, we have conducted subjective tests to evaluate the visual distortionscaused by scaling the content rate (adaptation methods). These tests showed that in order to obtainthe best perceived video quality, intermediate views should be scaled asymmetrically as much aspossible. Once asymmetric coding threshold (~32dB) has been reached then it is possible to drop allthe views between the first and the last one and interpolate them using depth image-based rendering(DIBR) technique. Finally, we propose a mesh-based P2P streaming architecture that employs rateadaptation according to the findings to deliver the best QoE under diverse network conditions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tümleşik devre (yonga) teknolojisi, devrelerde kullanılan transistörlerin boyutu bakımından mikron altı rejime indikçe tümleşik devre üretim işlemi, yongaların hız performanslarında belirsizliğe sebep olan devre parametreleri değişkenliklerinden muzdarip hale gelmektedir. Üretim işlemindeki istatistiksel değişkenliklerin göz ardı edilemez seviyelere ulaşması bu değişkenlikleri hesaba katan istatistiksel zamanlama analizini zorunlu kılmıştır. Parametre değişkenliklerinin bir sonucu olarak aynı devreye ait olan üretilmiş her yonga farklı parametre değerlerine ve dolayısıyla farklı bir hız performansına sahiptir. Hız testinde başarılı olan yongalar satış için paketlenirken başarısız olanlar atılır. İstatiksel zamanlama analizinin ana amaçlarından birisi hız testlerini geçecek yongaların oranı olan zamanlama verimini tahmin etmektir. Sayısal devreler için önerilmiş olan istatiksel zamanlama analizlerinin neredeyse hepsi blok (mantık geçidi) düzeyinde çalışan metotlardır ve bunlara istatiksel statik zamanlama analizi ismi verilir, çünkü bu metotlar istatiksel olmayan statik zamanlama analizinin istatiksel duruma doğrudan genellemeleridir. Ancak blok düzeyi istatiksel zamanlama analizi birçok yaklaşım ve tahmin içermesi sebebiyle doğruluktan yoksundur. Bu tezde, transistör düzeyinde devre simülasyonlarına dayalı doğru istatistiksel zamanlama analizi boşluğunu doldurmaya çalışıyoruz. Bu amaçla, ilk olarak, bir devre içindeki değişkenlikleri modellemek ve istatistiksel olarak kritik olan yolları belirlemek için literatürdeki farklı teknikleri birleştiren yeni ve kapsamlı bir istatistiksel zamanlama analizi aracı öneriyoruz. Ama bizim esas orijinal katkımız, zamanlama verimini doğru ve hızlı bir şekilde tahmin eden bir metot elde etmek için önem örneklemesi yöntemini farklı bir şekilde transistör düzeyi Monte Carlo istatiksel zamanlama analizinin hızını arttırmak için kullanmaktır. Metodumuzu ISCAS'85 değerlendirme devrelerinde test ettik ve sonuçlar bizim önem örneklemesi tabanlı zamanlama verimi tahmin metodumuzun hızı ortalama 150 kat arttırdığını gösterdi.","As the Integrated Circuit (IC) technology scales down to deep sub-micron regime in terms of sizes of transistors used inside circuits, the IC manufacturing process suffers from circuit parameter variations, which cause uncertainties in the speed of the chips. The statistical variations of manufacturing process have increased to a non-negligible level, which necessitates statistical timing analysis considering the variations. As a result of the parameter variations, each manufactured chip of the same circuit has different parameter values and thus a different speed performance. The manufactured chips, which pass the speed tests, are packaged for marketing and others that fail the tests are discarded. One of the main aims of statistical timing analysis is to estimate timing yield, which is simply the fraction of chips that pass the speed tests. Almost all proposed statistical timing analysis methods for digital circuits are block (gate) level methods and they are called statistical static timing analysis (SSTA) methods, as they are direct generalizations of deterministic static timing analysis (DSTA) to the statistical case. However, block level statistical timing analysis lacks accuracy as it contains many approximations. In this thesis, we try to fill the gap for accurate statistical timing analysis based on transistor level circuit simulations. For this purpose, we first propose a new comprehensive statistical timing analysis tool that combines different techniques in the literature for modeling variations and extracting the statistically critical paths in a circuit. But our main novel contribution is timing yield estimation using importance sampling in a novel manner in order to speed up transistor level Monte Carlo (TL-MC) statistical timing analysis for obtaining both an accurate and efficient timing yield estimation method. We test our method on ISCAS?85 circuits and the results show that our IS based yield estimation method improves the speed performance two orders of magnitude on the average."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde doğal konuşmadan duygu tanıma problemi için biçimlendirici konumu ağırlıklı Mel frekans kepstral katsayısı (AMFKK) özniteliklerini sunuyoruz ve başarım sonuçlarını sıkça kullanılan Mel frekans kepstral katsayıları (MFKK), Doğru Spektral frekans (DSF) katsayıları, biçimlendiriciler ve bürün öznitelikleri başarımları ile karşılaştırıyoruz. DSF öznitelikleri biçimlendirici frekansları çevresinde birbirine yakın konumlandığından, MFKK özniteliklerinin çıkarımında kritik bant enerji değerlerini normalleştirilmiş ters harmonik ortalama fonksiyonu ile ağırlıklandırıyoruz. Beş sınıflı duygu tanıma problemi için hem standart hem de ağırlıklı MFKK öznitelik vektörlerini sol-sağ yapılı saklı Markov modeller (SMM) ile eğitiyoruz. FAU Aibo duygu yüklü konuşma veritabanı üzerindeki deney sonuçları AMFKK özniteliklerinin standart spektral özniteliklerden daha iyi başarım sağladığını ortaya koyuyor. Standart MFKK öznitelikleri % 39.43 başarım sağlarken, AMFKK özniteliklerinin SMM ile sınıflandırılması başarımda % 1.92 değerinde bir artış sağlıyor. Bu tezde ayrıca AMFKK, MFKK ve DSF öznitelikleri kullanılarak eğitilen farklı SMM sınıflandırıcılarının karar kaynaşımı da inceleniyor.","In this thesis, we propose formant position based weighted Mel Frequency Cepstral Coefficient (WMFCC) features for spontaneous emotion recognition from speech problem and compare performance results with commonly used feature sets such as Mel FrequencyCepstral Coefficients (MFCC), Line Spectral Frequency (LSF) features, formants and prosody. Since, the LSF features are positioned close to each other around formant frequencies, we propose normalized inverse harmonic mean function to weight critical band energies for the extraction of MFCC features. We evaluate both the standard and weighted MFCC feature sets with left-to-right Hidden Markov Model (HMM) structures for the five class emotion recognition task. Experimental results on the spontaneous FAU Aibo emotional corpus indicate that WMFCC features perform significantly better than standard spectral features. The HMM classifier with the standard MFCC features attain 39.43 % unweighted recall rate, whereas proposed WMFCC features based HMM classification brings 1.92 % improvement. Another contribution of this thesis is the fusion of classifiers using WMFCC, MFCC and LSF features."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"3B biçimlerin kesimlemesi, 3B nesnelerin sayısal olarak elde edilmiş örgü modellerinin anlaşılması ve işlenmesinde önemli bir basamak olmakla beraber, iskelet özütleme, model deformasyonu, örgülerin şeklini değiştirme, hareket tanıma, biçimlerin geri kazanımı, sıkıştırma ve çarpışma sezimi gibi bilgisayar grafiği ve bilgisayarla görme konularında da birçok uygulamaya sahiptir. Mevcut 3B biçim kesimleme yöntemlerinin büyük çoğunluğunun yalnızca durağan örgüleri göz önünde bulundurmasına karşın, hareketli nesnelerin devingen biçim modelleri, 3B edinim yöntemlerinin yakın zamandaki gelişimiyle beraber hızla yaygınlaşmaktadır. Bu akımın tipik bir örneği de, insan aktörlerin eklemli hareketini gösteren, değişmez bağlanırlık sahibi örgü dizilerinin kullanımıdır.Bu tezde, hareket bilgisi ekleyerek devingen örgü dizisi olarak verilmiş 3B eklemli şekillerin kesimlemesinde kullanılmak üzere ve durağan kesimleme yöntemleriyle elde edilen sonuçları iyileştirecek bir kesimleme yöntemi tanıtılmaktadır. Eklemli şeklin değişmez bağlanırlığa sahip bir örgü dizisi olarak verildiği, bu yüzden tepe noktalarının çerçeveler arası karşılıklarının ve dolayısıyla hareketlerinin önceden bilindiği varsayılmaktadır. Tepe noktaları arasındaki hem zamansal hem de uzamsal benzerlikleri kodlayan bir ilginlik matrisi oluşturmak için eklemli şeklin birden fazla çerçevedeki duruşu kullanılmaktadır. Daha sonra biçim, standart K-means ya da merge-cluster gruplandırma algoritmalarından birisi kullanılarak ilginlik matrisine dayalı olarak spektral alanda kesimlere ayrılır. İleri sürülen kesimleme yönteminin başarımı çeşitli örgü dizileri üzerinde gösterilmiştir.","3D shape segmentation is a key step for processing and understanding of digitally acquired mesh models of 3D objects and has numerous applications in computer graphics and vision such as skeleton extraction and model deformation, mesh morphing, gesture recognition, shape retrieval, compression and collision detection. While most of the existing 3D shape segmentation methods consider only static meshes, dynamic shape models of moving objects are becoming more and more commonplace with the recent advances in 3D acquisition techniques. A typical example of this trend is the use of ? xed connectivity mesh sequences to represent human actors with articulated motion.In this thesis, we present a method to segment an articulated shape given in the form of a dynamic mesh sequence by incorporating motion information so as to further improve the segmentation results obtained by static segmentation methods. We assume that the articulated shape is given in the form of a mesh sequence with ? xed connectivity so that the interframe vertex correspondences, hence the vertex movements, are known a priori. We use different postures of an articulated shape in multiple frames to constitute an af ? nity matrix which encodes both temporal and spatial similarities between surface points. The shape is then decomposed into segments in spectral domain based on the af ? nity matrix using one of the merge-cluster algorithm or the standard K-means clustering algorithm. The performance of the proposed segmentation method is demonstrated on various mesh sequences."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir çok görevdeş ağ uygulaması sistemdeki düğümlerde kısmi olarak bulunan veri erişim sıklığı, öğe sıklığı, sorgu ve olay sayısı gibi sistem genelindeki bilgiye ihtiyaç duyarlar. Dağıtık bir sistemde sık bulunan öğelerin belirlenmesi problemi sistem genelindeki bir bilginin hesaplanmasını gerektiren yaygın bir problemdir. Sistem genelindeki sıklığı belirli bir eşik değerinin üstünde bulunan öğelere sık bulunan veya yaygın öğeler denir. Günümüz P2P ağlarında, sistem genelindeki sık öğelerin bilgisine ihtiyaç duyan uygulamaların sayısı hızla artmaktadır. Bu yüzden, sık bulunan öğelerin verimli bir şekilde tespit edilmesi, bir çok görevdeş ağ uygulamalarında, düğümler için değerli bir servistir. Ayrıca, bu servis dağıtık veri tabanı uygulamalarında, önbellek yönetiminde, veri yineleme yöntemlerinde, algılayıcı ağlarda ve güvenlik mekanizmalarının önemli olduğu sistemlerde de kullanılabilir.Bu tezde, ProFID olarak adlandırdığımız, yapılandırılmamış görevdeş ağlarda sık bulu-nan öğelerin belirlenmesi için epidemik tabanlı yöntemi kullanan dağıtık bir yöntem sunulmuştur. Ortalama yönteminin epidemik tabanlı yöntem ile birlikte sık bulunan öğelerin tespit edilmesinde kullanılması ve pratik yakınsama yöntemi önerdiğimiz yöntemin yenilikçi ve yararlı özellikleridir. Bu konudaki çalışmalara katkılarımızı şu şekilde sıralayabiliriz. İlk olarak, sık bulunan öğe setinin bütün düğümlerde hesaplandığı tam dağıtık bir yöntem önerisi sunulmuştur. Bu yöntem yapılandırılmamış ağlarda ikili gruplar halinde ortalama yöntemi ile ağ boyutu tahminini kullanarak sık öğeleri tespit eden ilk yöntemdir. İkinci olarak, algoritmanın sonlandırılması için pratik yakınsama yöntemi sunulmuştur. Bu yöntem ile düğümler yerel durumlarındaki değişimleri değerlendirerek, diğer düğümlerden bağımsız olarak yakınsama kararı alırlar. Ayrıca, PeerSim simülatörü ile elde edilen kapsamlı sonuçlar kullanılarak önerilen yöntemin verimliliği, ölçeklenebilirliği ve uygulanabilirliği ölçülmüştür. Son olarak, ProFID ile uyarlanmış Push-Sum yöntemleri karşılaştırılmıştır. Bu karşılaştırma sonuçlarında, ProFID hem doğruluk hem de ölçeklenebilirlik açısındanuyarlanmış Push-Sum yöntemine göre daha iyi sonuçlar vermiştir.","Several P2P applications require a global view of system information such as data access frequencies, item frequencies, query and event counts, that are available locally and partially at peers. Frequent item set discovery (FID) in a distributed environment is a common problem requiring global information computation. Items that globally occur more than a threshold value are referred as frequent or popular and the number of diverse applications that need globally frequent items is increasing expeditiously in today's P2P networks. Therefore, efficiently discovering frequent items would be a valuable service for peers. Being significant for P2P systems, FID problem is also applicable to distributed database applications, cache management, data replication, sensor networks, and security mechanisms in which identifying frequently occurring items in the entire system is useful.In this thesis, we propose and develop a gossip-based distributed approach, namely ProFID, for discovering frequent items in unstructured P2P networks. In contrast to the prior studies, our solution progresses in a fully distributed manner using an atomic averaging function to discover frequent items. Utilizing averaging function with gossip-based aggregation in frequent item set discovery problem and a practical convergence rule are novel and beneficial features of our approach. We make the following contributions to the current state of the art. First, we propose a fully distributed Protocol for Frequent Item Discovery (ProFID) where the result is produced at every peer. ProFID uses a novel pairwise averaging function and network size estimation together to discover frequent items in an unstructured P2P network. We also propose a practical rule for convergence of the algorithm. In contrast to previous works, each peer gives local decision for convergence based on the change of updated local state. Moreover, we developed a model of ProFID in PeerSim and performed various experiments to compare and evaluate its efficiency, scalability, applicability. Finally, we compared the accuracy and scalability of ProFID with adaptive Push-Sum algorithm. The comparison results show that ProFID outperforms adaptive Push-sum in terms of accuracy, convergence speed and message overhead."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışmasında müzikle sürülen dans hareketi sentezi için çeşitli dans performanslarını inceleyen işitsel-görsel bir çatı yapısı önerilmektedir. Önerilen çatı yapısı öncelikle yinelenen temel dans ve müzik örüntülerini çıkarmayı hedefler. Daha sonra çıkarılan dans ve müzik örüntüleri arasındaki ilintiler incelenerek müzikten dans hareketlerine giden çoktan çoğa bağıntılar oluşturulur. Bu bağıntı ise müzikle sürülen dans koreografisi sentezinde ve kişiye özgü dans performansı animasyonu oluşturulmasında kullanılır. Bu çatı yapısını baz alarak ilk önce yinelenen temel dans ve müzik örüntülerinin güdümsüz çıkarımı ve ilinti analizini ele alan bir sistem sunmaktayız. Daha sonra ikinci bir sistemle önerilen çatı yapısının hem analiz hem de sentez kısımlarını görece basit bir senaryoda, dans performanslarının bir müzik için bir dans figüründen oluştuğu durumda, ele almaktayız. Son olarak, daha karmaşık bir senaryo için gerekli modelleme, analiz ve sentezi topyekün yapabilecek tam donanımlı bir sistem sunmaktayız. Bu sistem verilen müziğin içeriğine ve yapısına uygun alternatif dans koreografilerini otomatik olarak sentezlemektedir. Her bir sistem için deneysel sonuçlar göstermiştir ki; önerilen çatı yapısı müzik ve dans hareket örüntülerini belirlemede, belirlenen örüntüler arasında bağıntı modelleri oluşturmada, oluşturulan bağıntı modelleri ile müziğe uygun dans hareketleri sentezlemede başarılıdır.","In this thesis, we propose an audiovisual framework for analysis of dance performances towards music-driven dance motion synthesis. The proposed framework first aims to extract elementary recurrent music and dance motion patterns. Then the analyses of the correlations between the elementary music and dance motion patterns are used to construct many-to-many statistical mappings from music to dance motions. These many-to-many mappings are then used for music-driven dance choreography synthesis and personalized dance performance animations. Based on this audiovisual framework, we first present a system that deals primarily with the unsupervised correlation analysis of elementary recurrent music and dance motion patterns. Later we present a second system that considers both analysis and synthesis parts of the proposed framework in a rather simplified context where a dance performance is assumed to have only a single dance motion pattern which is to be synchronized with the musical beat. Finally, we present a complete system for modeling, analysis, and synthesis of audiovisual dance performances that handles more complex and realistic scenarios. The third system automatically synthesizes a variety of synchronized dance performances that perceptually match the emotions and contents of the accompanying music; as if they were arranged by a choreographer. Experimental results for each system demonstrate that the proposed framework is able to extract and utilize from audiovisual correlations between music and dance motion patterns for synthesis of compelling music-driven dance performances."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Geleneksel olarak video aktarımı, bant genişliğinin sabit olduğu karasal, uydu veya kablolu yayınlar üzerinden yapılmaktadır. Bu yüzden, MPEG-2 ve MPEG-4 AVC gibi video kodlama standartları istenen çözünürlük ve uygulamaya bağlı olarak görüntüyü sabit bir oranda sıkıştırırlar. Gelişmekte olan IP ve WebTV üzerinden video aktarımı çözümlerinde ise, aktarımın birbirinden farklı ve değişen kapasitelere sahip, kablolu veya kablosuz IP ağları üzerinden yapılması gerekmektedir. Bilindiği üzere, bu tarz değişen kapasitelere sahip hatlar üzerinden yapılan yayınlarda en iyi görüntü kalitesine ulaşabilmek için, videonun sıkıştırma oranını hat kapasitesine göre düzenleyen uyarlanabilir video aktarım çözümleri kullanılmalıdır.Uyarlanabilir aktarım çözümleri temelde üç parçadan oluşur: Videoyu farklı oranlarda sıkıştırabilen bir kodlayıcı, hatta oluşabilecek tıkanıklığa duyarlı bir iletim protokolü ve bu ikisini kullanacak olan bir uyarlama motoru. Bu çalışma, hem iletim hem de kodlama sırasında uygun tekniklerle nasıl bir uyarlama motoru yapılabileceğini açıklamaktadır. Bu amaçla çalışmada, ölçeklenebilir SVC video kodlayıcısı ile DCCP ve TCP iletim protokolleri kullanılmıştır. Ayrıca sistem, DCCP protokolünün kablosuz ağlardaki performansını arttırmak için kablosuz paket kayıplarıyla tıkanıklık kayıplarını birbirinden ayıran bir mekanizma barındırmaktadır.","Traditionally, video transport has been realized over dedicated, fixed bandwidth channels, such as terrestrial, satellite, or cable. Hence, video coding standards, such as MPEG-2 and MPEG-4 AVC, encode video at a fixed target rate for a given resolution and application. With the advent of video over IP and WebTV, video transport must now be achieved over heterogeneous IP networks, including a variety of fixed and wireless links. It is well-known that achieving the best video quality in this heterogeneous environment requires an adaptive streaming framework that can most efficiently adapt the source video rate to the available network throughput.Fundamental blocks of an adaptive streaming framework are a codec that can output video at multiple rates, a transport protocol that employs effective rate/congestion control, and an adaptation engine built on top of these. This work is on realizing the adaptation engine by supplying it with proper adaptation strategies to be used both in coding and transport blocks. For that purpose, an adaptive video streaming system, which employs SVC as the video codec and DCCP or TCP as the transport protocol, is implemented to evaluate various adaptation strategies in streaming scalable video. The system is also extended to wireless domain, proposing a solution to differentiate between wireless and congestion losses and therefore improve the performance of DCCP in wireless networks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri yapılarının ve servislerin koşut-zamanlı gerçekleştirmeleri, veritabanları, Internet sunucuları ve dosya sistemleri gibi geniş kullanım alanına sahip pek çok sistemin omurgasını oluşturmaktadır. Bu yazılımlar, koşut-zamanlı olarak çalışan çok sayıda istemciye verimli şekilde cevap verebilmek için küçük parçalı kilitleme ve bloklanmayan operasyonlar gibi eşleme teknikleri kullanırlar. Bu tekniklerin yanlış kullanımı, veri bozulması ve işletim sisteminin çökmesi, ve hatta bir uçuş kontrol sisteminin arızası gibi ciddi sonuçları olan koşut-zaman hatalarına yol açar. Koşut-zaman hatalarının tespiti, yeniden oluşturulması ve düzeltilmesi aslen ardışık yazılımlar için geliştirilen tekniklerle zor olmaktadır. Bu zorluk, koşut-zamanlı yazılımları kesin olarak, ve koşularının büyük kısmını kapsayacak şekilde sınayabilecek yeni program analizlerine ihtiyaç oluşturmaktadır. Bu tezde, bu ihtiyaca cevap veren ve farklı yollar izleyen iki teknik sunulmaktadır.Yarış durumları çoğu kez yüksek seviyede hataların belirtisidir ve istenmeyen koşulara yol açmaktadır. Bu tezin ilk kısmı, programın koşularını izleyerek, bir yarış durumunun hemen öncesinde bir DataRaceException fırlatan çalıştırma ortamı olan Goldilocks'ı sunmaktadır. Böylece, yarış durumu oluşturacak veri erişimlerinin oluşması engellenmekte, ve bu yarış durumlarının, teşhisinin zor olacağı hatalara yol açmadan işlenmesine izin verilmektedir. Sunulan çalışma ortamında çok-örgülü koşuların yarışsız, ve Java Bellek Modeli'ne göre ardışık tutarlı olması garantilenmektedir. Bu teminat, programcılara kolay kullanılabilir ve net bir semantik sağlamakta, ve hata ayıklama sırasında koşut-zamanla ilişkili pek çok olasılığın ortadan kaldırılmasına yardımcı olmaktadır. Böylece DataRaceException, değerli bir hata ayıklama aracı olmakta, aynı zamanda eğer makul bir hesaplama masrafıyla desteklenirse, yazılım için dağıtımı sonrası önemli bir güvenlik mekanizması olabilecektir. Ayrıca bu tezde, DataRaceException'ı desteklemek için Goldilocks adlı dinamik ve kesin bir yarış durumu tespit algoritması gerçekleştirilmiştir. Goldilocks algoritması genel ve sezgisel olmakla birlikte, farklı eşleme desenlerini ve yazılım işlem belleğini kullanan programları aynı biçimde işleyebilmektedir. Algoritmamız ve DataRaceException, Kaffe adlı bir Java sanal makinesinde gerçeklenmiş, ve sistemimiz çeşitli açık erişimli Java programları kullanılarak değerlendirilmiştir. Deneylerimiz, bu gerçekleştirmenin makul masrafları olduğunu ve başarımının literatürdeki diğer algoritmalarla rekabetçi olduğunu göstermektedir.Goldilocks gibi çalışma-zamanı izleme teknikleri hataların yokluğunu tamamen garanti edemez. Koşut-zamanlı yazılım, örneğin bir dosya sistemi ya da standart kütüphanenin bir parçası ise, hiç bir koşusunun hataya yol açmayacağının doğrulanması gerekmektedir. Bu, programın kaynak kodu üzerinde yapılacak durağan bir biçimsel ispat ile başarılabilir. Örgülerin paylaşımlı bellek üzerindeki olan küçük parçalı etkileşimleri, biçimsel ispatların kullanıcının ek girdileri açısından pahalı olmasına yol açmaktadır. Tezin ikinci kısmında, koşut-zamanlı programların durağan doğrulanması için geliştirdiğimiz bir ispat sistemi olan QED sunulmaktadır. Yaklaşımımızın kilit noktası, atomikliğin, güvenlik özelliklerinin ispatındaki zorlukların üstesinden gelmek amacıyla merkezi ispat aracı olarak kullanılmasıdır. İddialar ve sıralanabilirlik gibi geniş çapta kabul gören iki güvenlik özelliği desteklenmektedir. İddialar, programın yerel özelliklerini belirtmek için kullanılırken, sıralanabililik, veri yapıları için daha global ve zorlu ölçütler tanımlamaktadır. İspatlar, programın daha geniş atomik bloklarla adım adım yeniden yazılması ile yapılmaktadır. Buradaki yenilik, indirme ve soyutlama tekniklerini uygulayan ispat adımlarının, birbirinin bir sonraki çıktısını geliştirecek şekilde değişimli olarak kullanılmasıdır. İstenilen atomiklik seviyesine ulaşılınca, iddialar, ortaya çıkan programdaki atomik blokların içerisinde ardışık olarak denetlenmekte, iddiaların hepsi ispatlanınca orijinal programın doğruluğu ilan edilmektedir. Bu strateji, kullanıcının tasarım amacını açıkça ifade etmesini sağlayarak, ve program içerisindeki ardışık özellikler ve koşut-zaman kontrol mekanizmalarina olan ilginin net bir şekilde ayrımını kolaylaştırarak, ispatları önemli ölçüde basitleştirmektedir. İspat sistemimiz, var olan yöntemleri tamamlayıcı olup, bunların daha uysal bir şekilde uygulanmasına imkan vermektedir. Açık erişimli yazılım aracımız QED-Verifier, ispat kurallarının gerek duyduğu alt-seviye mantıksal çıkarım işlemlerini Z3 SMT çözücünün otomatik olarak denetleyebileceği doğrulama şartları olarak ifade ederek, ispatları mekanize etmektedir. QED'deki stratejimizin basit ve pratikliği, literatürde iyi bilinen programlar doğrulanarak ispatlanmış, atomikliğin, küçük parçalı kilitleme ve bloklanmayan algoritmalar gibi karışık koşut-zaman teknikleri kullanan programlar hakkında çıkarım yapmak için güçlü bir araç olduğu gösterilmiştir.","Concurrent implementations of data structures and services form the backbone of many widely-used systems such as databases, Internet servers, and file systems. In order to respond efficiently to a large number of clients accessing concurrently, such software makes use of sophisticated synchronization techniques including fine-grained locking and non-blocking operations. Incorrect use of these techniques makes software prone to concurrency bugs, which can have serious consequences, such as data corruption, operating system crash, or even more catastrophic results, e.g., failure of an aircraft flight control system. Concurrency-related bugs are notoriously difficult to detect, reproduce, and diagnose using code review and testing-based techniques originally developed for sequential programs. This difficulty creates demand for new program analyses that are capable of examining concurrent software precisely with high coverage of its executions. In this thesis, we present two techniques, each following a different direction to respond to this demand.Data races are often symptomatic of higher-level logical errors and may have unintended consequences. The first part of this thesis presents Goldilocks, a Java runtime that monitors program executions and throws a DataRaceException when a data race is about to occur. This prevents racy accesses from taking place, and allows race conditions to be handled before they cause errors that may be difficult to diagnose later. Multithreaded executions in our runtime are guaranteed to be race free and thus sequentially consistent as per the Java Memory Model. This strong guarantee provides an easy-to-use, clean semantics to programmers, and helps to rule out many concurrency-related possibilities as the cause of errors. Therefore, the DataRaceException is a valuable debugging tool, and, if supported with reasonable computational overhead, can be an important safety mechanism for deployed programs. To support the DataRaceException, we developed a novel, precise data-race detection algorithm called Goldilocks. The Goldilocks algorithm is general, intuitive, and can handle different synchronization patterns and software transactional memory uniformly. We have implemented our algorithm and the DataRaceException in the Kaffe Java Virtual Machine and evaluated our system on a variety of publicly available Java benchmarks. Our experiments indicate that our implementation has reasonable overhead and its performance is competitive with those of other algorithms in the literature.Runtime monitoring as in Goldilocks cannot fully guarantee absence of bugs. When the concurrent software is a part of, for example, a file system or a standard library, one has to verify that no execution of the software leads to an error. This can be accomplished by doing a static, formal proof on the program text. The interaction between threads over shared memory and at fine levels of concurrency make formal proofs expensive in terms of manual effort, requiring complex annotations and invariants, along with many auxiliary variables. In the second part of the thesis, we present QED, a proof system for static verification of concurrent programs. The key feature of our approach in QED is the use of atomicity as the central proof tool to overcome the challenges in proofs of safety properties. We support two widely-accepted safety specification, assertions and linearizability. While assertions specify local properties of the program, linearizability provides more global and stringent criteria for data structure implementations. A proof is done by rewriting the program with larger atomic blocks in a number of steps. A novel feature of our approach is alternating proof steps that apply reduction and abstraction, each of which improves the outcome of the other in a following step. After reaching a desired level of atomicity, we check assertions sequentially within the atomic blocks of the resulting program and declare the original program correct when we discharge all the assertions. This strategy leads to significantly simplified proofs by allowing the user the express the design intent in a clear way and facilitating a clean separation of concerns about the sequential properties and the concurrency control mechanisms in the program. Our proof system is orthogonal and complementary to existing methods and enables more tractable application of them. Our publicly available software tool, QED-Verifier, mechanizes proofs by encoding the low-level logical reasoning required by the proof rules as verification conditions automatically checked by the Z3 SMT solver. We demonstrated the simplicity and practicality of our proof strategy by verifying well-known programs from the literature and showed that atomicity is a powerful tool for reasoning about programs using a wide range of intricate concurrency techniques including fine-grained locking and non-blocking algorithms."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein katlanması lineer bir amino asit zincirinin 3 boyutlu işlevsel öz haline katlanması sürecidir. Proteinin 3 boyutlu öz yapısını yalnızca amino asit serisini kullanarak bulmak şu anda hesaplamalı biyoloji dalındaki en zor problemlerden biri olmakla birlikte çok yoğun şekilde araştırılmaya devam edilmektedir. 2 boyutlu HP modeli protein katlanması problemi için basitleştirilmiş bir modeldir. Bu modelde indirgemeler, protein katlanması sürecindeki baskın gücün hidrofobisite olduğu varsayımı üzerine yapılmaktadır.Bu tezde, protein katlanması problemini 2 boyutlu HP modelinde çözmek için Genetik Algortima ve Benzetilmiş Tavlama algoritmalarının birleşiminden oluşan üstsezgisel bir metod sunuyoruz. Yöntemimiz literatürde tanımlanmış yerel arama metodlarından bazı bölümler kullanmaktadır. Tabu Arama' da kullanılan hafıza bölümü, yeni bir 1-nokta çaprazlama işlemcisi ve probleme özel bir diversifikasyon bölümü algoritmamızda bulunan parçaları oluşturmaktadır. Algoritmamıza ek olarak, HP modelin içerdiği bazı bilgiler ve bu bilgilerin bir algoritmada nasıl kullanılabileceğine dair bazı tartışmalar da tezin son kısmında sunulmaktadır.","Protein folding is the process of a linear chain of amino acids folding into a functional 3D native structure. The problem of predicting this 3D native structure given only the amino acids of a protein is one of the most challenging problems in computational biology and is still rigorously investigated. The Hydrophobic-Polar model is a simplified model for the protein folding process. This model is based on the assumption that hydrophobicity is the dominant force that drives the protein folding process.In this thesis we propose a meta-heuristic algorithm (GAOSA) that combines the well known Genetic Algorithm approach with Oscillating Simulated Annealing to address the protein folding problem in the simplified 2D Hydrophobic-Polar Model. Our approach makes use of the pull move neighborhood for the mutation operator, a brute force 1-point crossover operator, a memory component borrowed from Tabu Search and a problem specific diversification phase. We also provide some insights about the implications of the Hydrophobic-Polar Model and how these implications can be utilized in an algorithm."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Proteinler hücresel fonksiyonların vazgeçilmez elemanlarıdırlar. Çoğu protein yapıolarak durağan bir resme sahip olmasına rağmen, protein dinamiği, bir başka deyişlezamana bağlı protein davranışı protein fonksiyonuna önemli derecede katkıda bulunur.Liganda bağlı, yapısal hareketlerin dışında, proteinler içerdikleri termal enerji sayesindekendilerine özgü hareketler gerçekleştirebilirler. Liganda bağlı olmadan proteinde yeralan bu yapısal hareketler fonksiyonel olarak önem taşımaktadırlar ve farklı proteinleriçin yapılan deneysel çalışmalara gore bu hareketler ile fonksiyonel hareketler arasındayapısal ve hareketin süreci bakımından ilişki bulunmaktadır. Bu içsel salınımlar, normalmod analizinin basitleştirilmiş bir versiyonu olan ANM ile belirlenen düşük frekanslı,protein modları tarafından açığa çıkarılabilmektedir. Biz bu çalışmada içerisinde enzim,antikor ve sinyal proteinlerinin bulunduğu 11 farklı proteine mod analizi uyguladık vemod analizi ile protein hareketleri arasında kinetik olarak herhangi bir bağlantı olupolmadığını irdeledik. Bu amaçla Hessian matriksinden elde ettiğimiz özdeğerlerikullandık. Bu özdeğerler modların titreşim frekanslarına dair bilgi içermekte ve ayrıcaANM çalışmalarında özdeğerler düşük ve yüksek frekanslı modları belirlemektedir.Bizim elde ettiğimiz sonuçlar özdeğerler ile protein hareketlerinin kinetik vetermodinamik özellikleri arasında bir bağlantıyı işaret etmektedir. Önceden debelirtildiği gibi bu içsel hareketler, proteine ait farklı yapılar arasında dinamik bir dengeoluşturmaktadır ve çalışmamızda önerdiğimiz gibi Hessian matriksinin özdeğerleri,protein hareketlerinin zaman periyodu ve termodinamik özellikleriyle bağdaşmaktadır.","Proteins are indispensible components of cellular functions. Although proteinstructure is determined as a static picture for most of the proteins, the dynamics or thetime-dependent behavior of proteins act as main contributors to protein function. Inaddition to ligand induced structural motions proteins also bear intrinsic motions arisingfrom thermal energy they contain. These ligand-independent motions possess functionalimportance according to experimental evidences for a large number of proteins and alink between these motions and functional motions are established both in terms ofstructure and timescale. These intrinsic fluctuations are revealed by low-frequencyindividual modes of proteins which are determined using a simplified version of normalmode analysis termed as Anisotropic Network Model (ANM). In this study, we applymodal analysis to eleven proteins including enzymes, antibodies and signal proteins.We investigate a kinetic relation between modal analysis and protein motions. For thispurpose we employ eigenvalues of Hessian matrix which carry information about thevibrational frequencies of these modes. In ANM studies these eigenvalues are used todetermine the low and high frequency modes of protein. Our findings imply acorrespondence between eigenvalues and kinetic/thermodynamic properties of proteinmotions. These intrinsic motions establish a dynamic equilibrium between distinctconformers of the protein and as we have proposed eigenvalues of Hessian matrixcorrelate well both with the timescales of these motions and thermodynamics of thesemotions."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein-protein etkileşimleri (PPE) biyolojik süreçlerin her seviyesinde çok önemlidir. Deneysel olarak kanıtlanmış PPE farklı veritabanlarına koyulmaktadır. Bu veritabanları PPE hakkında çeşitli bilgiler içermektedir, fakat hücrelerdeki tüm süreçler göz önüne alındığında, kapsamları düşüktür. Bu yüzden, PPE kapsamını genişletmek için güvenilir, daha doğru hesaplamalı metotlar gerekmektedir. Birçok araştırma grubu farklı bilgi ve metotlara dayanan çeşitli doğrulukta PPE tahmin algoritmaları geliştirmiştir. Ancak, yüksek doğrulukta bir PPE tahmin etme metodu geliştirmek ilgi çekicidir.Bu çalışma, var olan dizilim tabanlı PPE tahmin etme metotlarını değerlendirmeyi ve doğruluk oranları geliştirilmiş yeni bir metot önermeyi hedeflemektedir. Tahminler bir makine öğrenimi algoritması olan Destek Vektör Makineleri (DVM) ile yapılmaktadır. DVM, öğrenim etkileşim veri kümelerine göre kalıplar oluşturur ve etkileşimleri bu kalıplar ile tahmin eder. Bu çalışmada, pozitif öğrenim veri kümeleri deneysel PPE'leri, negatif öğrenim veri kümeleri hesaplanmış etkileşmeyen proteinleri içermektedir. Etkileşim bilgisini DVM'de betimlemek için, proteinlerin amino asit dizilim sıralarına göre n-gram frekansları hesaplanmıştır. DVM performansının, öğrenim veri kümelerindeki etkileşimlerden, farklı amino asit sınıflandırması tekniklerinden, n-gram frekanslarından ve ? değerlerinden fazlaca etkilendiği gösterilmiştir. Sekiz öğrenim veri kümesi için DVM kalıpları oluşturulmuştur ve DVM skorları ile detaylı karşılaştırmaları yapılmıştır. Bu skorlara göre, her veri kümesindeki etkileşimleri iyi tahmin eden birleştirilmiş öğrenim veri kümeleri oluşturulur. Daha sonra, en yüksek DVM skorunu elde etmeyi sağlayan en belirleyici nitelikler kümesi bulunur. Son olarak, en iyi DVM kalıpları, YUPE (Yapısal Uyumlu Protein Etkileşimleri) algoritması tarafından tahmin edilen PPE içindeki yanlış pozitiflerin elenmesi için kullanılır.","Protein-protein interactions (PPI) are of crucial importance at all levels of biological processes. The experimentally identified PPI are deposited in several databases. These databases contain diverse information about PPI; but their coverage is low when we consider full processes in cells. Thus, reliable, accurate computational methods are needed to improve the coverage. Many research groups have developed PPI prediction algorithms with varying accuracies based on different data and methods. However, to develop a new PPI prediction method with high accuracy is challenging.This study aims to assess existing sequence based PPI prediction methods and to propose a new algorithm with improved accuracies. The predictions are made via Support Vector Machines (SVM), which is a machine learning algorithm. SVM creates models based on training sets and predicts interactions via those models. In this study, positive training sets contain experimental PPI and negative training sets contain computational non-interacting proteins. In order to represent interaction data in SVM, n-gram frequencies of proteins are calculated according to their amino acid sequences. It is shown that SVM performance is strongly affected by interactions in training datasets, amino acid categorization techniques, n-gram frequencies, and ? values used. SVM models are created for eight datasets and the critical assessment of those datasets is made via their SVM scores. Based on those scores, combined training datasets are created that make accurate prediction of interactions in every dataset. Then, the best feature set that leads to the highest SVM scores is found. Finally, the best SVM models are utilized to eliminate false positives in putative protein interactions predicted by PRISM (Protein Interactions by Structural Matching) algorithm."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Zamanla değişen nesnelerin çok-bakışlı video dizilerinden 3B geriçatımı için hızlı ve verimli bir yüzey izleme yöntemi tanıtılmaktadır. Dinamik bir nesne örgü modelinin geometrisi ve bağlanırlığı, çok-bakışlı silüet ve 3B sahne akış bilgisine dayalı bir yöntemle, bir ilk gösterimden yola çıkarak, zaman içinde izlenir. Her çerçeveye ait örgü gösterimi, bir önceki çerçevenin örgü gösterimini silüet bilgisi ile belirlenen optimal yüzeye doğru deforme ederek elde edilir. Bu deformasyon süreci, örgü yeniden-yapılandırma işlemleri ve 3B sahne akış bilgisi ile desteklenir. Elde edilen uzay-zamanda pürüzsüz örgü dizisi, örgü yeniden-yapılandırma işlemleri ve tepe noktalarının küçük ölçekli yerdeğiştirme vektörleri cinsinden, ilk çerçevenin örgü gösterimi ile birlikte, verimli bir şekilde kodlanabilir. Önerilen yöntemin hızlı olması ve gösterim maliyeti düşük örgü modelleri üretmesinin yanısıra, bir diğer avantajı da devinimi katı olmayan dinamik nesnelerin modellenmesi için kullanılabilmesidir. Devinimi katı olmayan bir nesneyi temsil eden örgü gösteriminin zamanla değişebilen bağlanırlığı, deformasyon şırasında kullanılan örgü yeniden-yapılandırma işlemleri sayesinde başarı ile izlenebilmektedir. Yöntemin başarımı, hem gerçek hem de sentetik video dizileri üzerinde sınanmıştır.","We present a fast and efficient surface tracking method for modeling dynamic objects from multiview video streams. Starting from an initial mesh representation, the surface of a dynamic object is tracked over time, both in geometry and connectivity, based on multiview silhouette and 3D scene flow information. The mesh representation of each frame is obtained by deforming the mesh representation of the previous frame towards the optimal surface defined by the time-varying multiview silhouette information, using mesh restructuring operations and vertex displacements assisted by 3D scene flow vectors. The whole time-varying surface is then represented as a mesh sequence that can efficiently be encoded in terms of restructuring operations and small-scale vertex displacements along with the initial model. Our reconstruction method hence yields a compact time-varying mesh representation of the dynamic object, which is smooth both in time and space. The proposed method is not only fast and produces storage efficient mesh representations, but it also has the ability to deal with dynamic objects that may undergo nonrigid transformation. The time-varying mesh structure of such nonrigid surfaces, which is not necessarily of fixed connectivity, can also successfully be tracked thanks to the restructuring operations employed in our deformation scheme. We demonstrate the performance of the proposed method both on real and synthetic sequences."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Enerjik ve hızlı değişen ağ ortamlarında, yeni gelen her bir bilgi hakkında, ağdaki kullanıcıları bilgilendirme işlemi, çeşitli bilgi yayılımı algoritmalarıyla sağlanır. Güvenilirliği ve sağlamlığı ile, dedikodu yayılımı algoritmaları, bu algoritmalar arasında en yaygın olanlardan biridir. İçerik dağıtımı sırasındaki güvenilirliğin sağlanması için, herhangi bir mesaj kaybı durumunda, kaybolan mesajların yeniden temini, çesitli ara bellek yönetimi yöntemleri ile sağlanır. Sınırlı bellek kaynaklarının etkili bir şekilde kullanımı, ara bellek yönetiminin en temel amaçlarından biridir.Bu tez çalışmasında, bilgi yayılımı ve ara bellek yönetimi olmak üzere, görevdeş ağlardaki iki temel fenomen, topolojik yönlerden analiz edilmektedir. Bilgi yayılımı için, görevdeş ağlarda, verilen herhangi bir topoloji ile, komşuluk bilgisine bağlı, entropi-önler algoritmalarıyla yayılım incelenmektedir. Ağdaki bütün düğümlerin, her bir salgın döngüsü esnasındaki enfekte olma olasılıkları bulunmaktadır. Ara bellek yönetimi için, sistemdeki bellek kaynaklarını etkin bir biçimde kullanan ve bellek yükünü sistem üzerindeki kullanıcılar üzerine dengeli bir biçimde dağıtan bir algoritma olan, Adımsal Eşit Dağılımlı Ara Bellek algoritması ile bellek dağıtımı incelenmektedir. İnterneti örnekleyen sıra-düzensel ve üs kanunu temel topolojilerinin, ara bellek yönetimine olan farklı etkileri incelenmektedir.Topolojik özelliklerin bilgi yayılımına olan etkileri, sayısal hesaplamalarla incelenmiştir. Yayılım hızının, komşuluk matrisine, doğrusal olmayan bir yolla bağlı olduğu bulunmuştur. Ara bellek modelinin, sıra-düzensel ve üs kanunu topolojilerdeki başarım hesaplamaları benzetim sonuçlarıyla bulunmuştur. Temel başarım parametreleri olarak, ölçeklenebilirlik, güvenilirlik, yayılım gecikme zamanları ve dengeli dağılım dikkate alınmıştır. Adımsal Eşit Dağılımlı Ara Bellek algoritmasının, bellek yükünü sistem üzerindeki kullanıcılara dengeli bir biçimde dağıttığı benzetim sonuçlarıyla gösterilmiştir. Ara bellek seçimindeki karar verme sürecinin gecikmeye sebep olmasınıbeklediğimiz halde, üs kanunu topolojileri ele alındığında, bu gecikmenin büyük ölçüde giderildiği görülmüştür.","Keeping every node updated about the newly generated data in the dynamic and rapidly changing environment of a network is achieved via different data dissemination algorithms. Epidemics is one of the widely accepted algorithms because of its reliability and robustness. Message loss recovery for maintaining the reliability of the content delivery in case of message losses is achieved via several buffer management techniques. Efficient usage of limited memory resources is the basic deal for buffer management.In this thesis, we present our analysis of peer-to-peer (P2P) networking phenomena, namely data dissemination and buffer management, focusing on topological perspectives. For data dissemination, we examine spreading of epidemics for anti-entropy algorithms on several overlay network topologies, considering peer proximity. We derive nodes' exact probability distributions of being infected in each epidemic cycle of data dissemination. For buffer management, we examine buffering with an efficient algorithm, Stepwise Fair-share Buffering, that uses memory resources effectively and distributes the buffering load uniformly throughout the system. We analyze the effect of different topologies on buffer management, using hierarchical and power-law topologies, two basic types of topology modeling the Internet.For data dissemination, the effect of topological properties is studied using numerical evaluations. The rate of dissemination is found to be related to the adjacency matrix in a nonlinear way. For buffering, performance evaluations of various models with hierarchical and power-law topologies are conducted. Scalability, reliability, dissemination delays and uniformity are considered as basic performance parameters. We have shown that Stepwise Fair-share Buffering method facilitate better uniformity in distribution of buffering load, in view of our simulations. We expect to have higher delays due to decision process performed for bufferer selection; however, it is also shown that dissemination delay performance drawback is eliminated when power-law topologies are considered."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"In the last two decades, the idea of an emerging and evolving language has been studiedthoroughly. The main question behind this kind of studies is how a group of humans reachesan agreement on the phonology, lexicon and syntax. The improvements in computationaltools led the researchers build and test models that have been ran computer simulations toanswer the question. Although the models are mere re°ections of the reality, the resultshave been often useful and insightful. This dissertation follows the same line and proposesa new model, tested in a game based simulation methodology. Besides, this work tries to ¯llthe gap in the studies of lexicon compositionality and proposes a plausible explanation forthe transition from single word naming to multi word naming. The direction of the resultsis in line with the previous research such as the emergence of a stable and communicativelanguage. Moreover compositionality in lexicon is observed with a very simple bag of wordssyntax. The parameters in°uencing the results are analyzed in depth. Even though themodel does not meet the standards of the real world, future work hints insightful factsabout the transition from single word naming to syntax.","Bir dilin hiç bir öncülü olmadan ortaya çıkması ve evrim geçirmesi bilimadamlarınca son20 yıldır detaylıca araştırıldı. Bu araştırmaların arkasındaki esas soru ise bir grup insanınnasıl olup da ortak bir ses sistemi, kelime haznesi ve dilbilgisi üzerinde uzlaşması ve buuzlaşmaya göre iletişim kurması oldu. Gelişen hesaplama teknikleri ve bilgisayar araçlarısayesinde bu soruya cevap olabilecek sistemleri bilgisayarlarında modelleyip sonuçlarını al-abildiler. Bu mezuniyet tezi de bu araştırma çizgisi dahilinde yeni bir model önermektedir.Bu yeni modelin cevaplamak istedi¸gi soru ise, tek kelimelik dillerden, çok kelimeli dilleregeçişin nasıl gerekleşti¸gidir. Sonuçlar, tutarlı ve düzenli bir dil ortaya çıkması açısındanönceki sonuçları desteklemektedir. Yeni bir sonuç olarak da anlamlı parçalardan oluşan çokkelimeli ve basit bir dilbilgisine sahip olan bir dil ortaya çıkmıştır. Tezde ayrıca bahsedilensonuca etki eden parametreler incelenmiştir. Model ve deneyler olgunlaşmamış ve ba-sit gerçeklemeler olmalarına ra¸gmen, geliştirilmeleri durumunda tek kelimeden dilbilgisinegeçişin nasıl oldu¸gu konusunda daha fazla bilgi ve öngörü verebilecektir."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet protokolü (IP) üzerinden ses, video ve IPTV gibi uygulamaların başarısı göz önüne alındığında, 3BTV için iletiminin de IP üzerinden yapılması en makul seçimdir. Ancak bu seçimi yapmış olmak, iletimi gerçekleştirecek olan uygulamanın hazırlanmasını basit bir hale getirmez. Zira IP sadece ağ katmanında paket anahtarlamasına dayalı bir iletimin olmasını zorunlu kılar. Bunun dışındaki pek çok kararın, üstteki katmanlar tarafından alınmasına olanak sağlayan esnek bir yapıya sahiptir. Bu sebepten ötürü, eldeki kanalı en verimli şekilde kullanacak yapıyı oluşturmak uygulamanın sorumluluğundadır. Bu da ancak tüm katmanların özelliklerini göz önünde tutan bir yaklaşım ile mümkündür.Bu tez de çoklu-görüntülü tasfirlere dayalı 3B videonun akıtılması için yeni bir yapı önerilmektedir. Önerilen bu yapı şimdiki standardlara dayalı, esnek ve önceki sistemlerle uyumlu olacak şekilde hazırlanmıştır. Bu sadece şu an kullanımda olan tek görüntülü aktarımlarla birlikte çalışabilmektedir. Çoklu-Görüntülü videonun (ÇGV) kodlaması sırasında basitleştirilmiş Çoklu-Görüntülü-Kodlama (ÇGK) sistemi kullanılmıştır ve basitleştirilme işlemi yüzünden kaybedilen kalite gözardı edilebilecek düzeydedir. Akıtım sırasında Gerçek Zamanlı Akıtım Prokolu (Real-Time Streaming Protocol ) (RTSP), Oturum Betimleme Protokolü (Session Description Protokol) (SDP) ve Gerçek Zamanlı Taşıma Protokolü (Real-Time Transport Protocol) (RTP) kullanılmıştır. Ayrıca kaybedilen paketlerin yeniden oluşturulması için Raptor Kodlarından yararlanılmıştır. Buna ek olarak, geri kalan kayıpların neden olduğu görüntü bozulmalarının etkisini azaltabilmek için hata gizleme teknikleri kullanılmıştır. Sistem önceden ÇGK ile kodlanmış bir görüntünün, kod çözülmesi işlemini gerçek zamanda tamamlayıp, oluşan görüntüyü pek çok farklı ekranda gösterebilmektedir.Bu tez ile aşağıdaki sorulara cevap vermeyi amaçlıyoruz. i) 3B görüntünün aktarılması için ne kadarlık bir bantgenişliğine ihtiyaç duyulur? Bunu etkileyen değişkenler nelerdir? ii) Dilimlemeyi aktif hale getirmenin getirdiği külfetleri, kazançları göz önüne alındığında gözardı edilebilir mi? iii) Ne kadarlık paket kayıp yüzdesi için, ne kadarlık kanal kodlamasına ihtiyaç duyulur? iv) Raptor kodlarının kullanılmasının gerektirdiği yük ile alternatif çözümlerinki kıyaslandığında hangisi daha iyidir. v) 3B görüntünün aktarılması için şu an kullanılan protokollere ne gibi değişiklikler gereklidir? vi) Kod-çözme ve görüntünün gösterilmesi işlemlerinde çok çekirdekli sistemlerden faydalanılabilir mi? vii) Gönderilmek istenen bilginin büyüklüğünden başka paket kayıplarını gözlemlenebilir ölçüde etkileyen bir değişen var mıdır? Eğer varsa bu paketleme seçimlerimizi etkiler mi?","The Internet Protocol (IP) is the natural choice for 3D video transmission, if we considering the recent success of many voice over IP (VoIP), video over IP, and IPTV applications. However, the choice of IP for the network layer does not make the transmission application design straightforward, since IP only dictates the use of unreliable packet switched networks. Its flexible architecture allows use of various transmission protocols and policies over IP. Therefore, it is the task of the application to implement an efficient scheme that will most effectively manage the available channel throughput. This can only be achieved by understanding the characteristics of each layer in the protocol stack and implementing a cross-layer design approach.This thesis proposes a novel framework for streaming of 3D video based on multi-view video (MVV) representations. The proposed streaming platform is completely open standards based, flexible, and backwards compatible for supporting monoscopic streaming to legacy clients. The MVV in the server is compressed using a simplified form of MVC with negligible loss of compression efficiency and streamed using RTSP, SDP and RTP to clients. Raptor Codes are used for fighting with the possible packet loses at the network layer. The clients can perform basic error concealment to reduce the effects of remaining packet losses and can decode MVC in real-time. The modular client can display decoded 3D content on a multitude of 3D display systems.The proposed streaming platform has been extensively tested over the Internet to find answers to the following questions related to 3DTV transmission over IP: i) What is the required bandwidth for 3D transmission, and what are the parameters that affact this ratio? ii) Is the cost of enabling slice mode justified for achieving better channel throughput? iii) What is the required rate of channel coding against what percentage of packet losses? iv) How does Raptor Codes perform when we compare its redundancy level against alternative strategies such as Multiple Description Coding? v) What are the required modifications over the standard streaming protocols for 3DTV transmission? vi) How can the design take advantage of the multi-core processors? vii) Are there any other parameters that have significant influence on packet loss ratio beside bit-rate of the stream? If there is, do they affect the packetization strategy?"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sanal gerçeklik kullanılarak oluşturulan simülasyon bazlı eğitim, gelenekselminimal invasiv ameliyat eğitim tekniklerine alternatif olacaktır. Cerrahi simülatörler,stajyerlerin, gerçek laporoskopik operasyonlarda olduğu gibi, sanal doku ve organlaradokunmasına, hissetmesine ve hareket ettirmesine, doku ile cerrahi aletler arasındakietkileşimleri gözlemlemesine, imkan sağlayacaktır. Yumuşak doku davranışlarını iyi taklitedebilmek için, gerçeğe çok yakın doku modelleri geliştirilmesi gerekmektedir.Parçacık modelleri, yumuşak cisimlerin simulasyonun da oldukça kullanılan biryaklaşımdır. Bu teknik, cerrahi kesim ve yırtılma gibi biçimsel değişimlerin olduğukonularda ağ bazlı metotlara (örnek: sonlu eleman methodu) göre daha iyi bir yaklaşımsağlar. Ayrıca parçacık metodları, ağ bazlı olanlara göre hesaplama açısından daha basittirve uygulanması daha kolaydır. Yine de, fiziksel sistemin tepkisini belirlemek için her bir ağelemanının malzeme katsayıları hesaplanmalıdır. Bu nedenle, gerçekçi doku özelliklerininparçacık modeline entegre edilmesi kolay değildir.Bu tezde, bir robot kolu aracılığıyla toplanan deney verilerini kullanan, doğrusalolmayan viskoelastik doku davranışını simule eden bir parçacık modeli geliştirdik. İlkolarak, yumuşak doku özelliği gösteren bir silikon örneğin gerilime bağlı doğrusal olmayanelastik tepkisi ve zamana bağlı viskoelastik tepkisi ölçüm cihazı ile yapılan statik yüklemeve makaslama gevşemesi deneyleriyle elde edilmiştir. Toplanan veriler yardımıyla, silikonörnek doğrusal olmayan viskoelastik Maxwell katısı kullanılarak modellenmiştir. Dahasonra, bu Maxwell katısı ile aynı davranışı gösteren, doğrusal olmayan viskoelastikMaxwell katılarından oluşan 3 boyutlu parçacık bazlı bir ağ kurulmuştur. Parçacıklarıbirleştiren bireysel Maxwell katılarının malzeme özellikleri, geliştirilen bir dizi özgünoptimizasyon algoritması ile belirlenmiştir.","Simulation-based training using Virtual Reality techniques is a promisingalternative to traditional training in minimally invasive surgery. Surgical simulators let thetrainee touch, feel, and manipulate virtual tissues and organs through the same surgical toolhandles used in actual minimally invasive surgery while viewing images of tool-tissueinteractions on a monitor as in real laparoscopic procedures. Developing realistic organforcemodels for simulating soft-tissue behavior is an integral part of a surgical simulator.The particle system approach provides a better solution than mesh-based methods tothe topological changes encountered in simulation of surgical cutting and tearing. Inaddition, they are computationally less expensive and easier to implement than the meshbasedmethods. However, the material coefficients of each individual mesh element shouldbe calculated and fine tuned to integrate the realistic tissue properties into particle models,which is not trivial.This thesis presents an end-to-end solution to realistic particle-based simulation ofnonlinear viscoelastic tissue behavior based on the experimental data collected by a roboticindenter. First, the strain-dependent nonlinear elastic response and time-dependentviscoelastic response of a tissue-like silicon phantom is measured via static loading andstress relaxation experiments performed by a robotic indenter. The collected experimentaldata is used to construct a lumped model of the tissue phantom represented by a nonlinearviscoelastic Maxwell Solid. Then, a 3-dimensional particle-based network is developed tomimic the behavior of the lumped Maxwell model. The material coefficients of theindividual Maxwell elements connecting the particles are estimated through a set of noveloptimization algorithms."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"En yakın k komşu algoritması (EKK) uzun süreli çalışılmış parametresiz sınıflandırmaalgoritmalarındandır. EKK sınıflandırılmış örnek verilerin dağılımının altında yatan birleşik olasılıkyoğunluk fonksiyonunun bilinmediğini kabul eder ve bu fonksiyonu sınıflandırılmış örnek verilerikullanarak ölçümler. Her ne kadar bu varsayım pratikte karşılaşılan problemler açısından gerçekçi biryaklaşım olsa da EKK'nin sınıflandırma doğruluğu, veri depolama miktarı ve hesaplama zamanıüzerinde olumsuz etkilere sebep olur.Bu tezin amacı EKK algoritmasının anlaşılması ve EKK'nin sınıflandırma doğruluğununarttırılması için kullanılan yöntemlerin incelenmesidir. Bu tez esas olarak verilerin sahip olduğu ilgisizözelliklerin EKK algoritmasının sinıflandırma doğruluğuna olan etkisi üzerine yoğunlaşmıs ve busorunu çözmek amaçlı Stretch adında yeni bir yöntem önermiştir. Bu yöntem sınıflandırma öncesindeörnek veriler üzerinde dogrusal dönüşumler uygulayarak EKK'nin sınıflandırma doğruluğunuarttırmayı amaçlar. Başka bir değişle, Stretch örnek verileri kullanarak EKK algoritmasınınsınıflandırma doğrulu ğunu en yuksek büyüklüğe çıkartacak doğrusal dönüşümleri adım adım hesaplar.Bu yöntem her adımda örnek sınıflandırılmış veriler arasından bir veri seçer ve bu veriyi kendi ile aynısınıftaki verilerle yakınlaştıracak ve/veya kendisi ile farklı sınıflardaki verilerle uzaklaştıracak olandoğrusal dönüşümü hesaplar. Farklı adımlarda oluşturulmuş bu doğrusal dönuşümlerin bileşimi olansonuç doğrusal dönüşümü EKK algoritmasının sınıflandırma doğruluğu üzerinde istatiksel olarakkayda değer bir artış gösterir.","The k nearest neighbor learning algorithm (kNN) is one of the well studied nonparametriclearning algorithms. kNN assumes that the underlying joint probability density function of the trainingset is unknown and it estimates the underlying joint probability density functions using the labeled dataset (training set). Although this is a realistic assumption in terms of the real world problems, itintroduces some limitations on the predictive accuracy, the storage complexity and computationalcomplexity of the kNN.The goal of this thesis is to understand kNN and techniques that are used to increase thepredictive accuracy of kNN. This thesis mainly focuses on the effect of the irrelevant features on thepredictive accuracy of the kNN and introduces the Stretch method, a new preprocessing method toincrease the predictive accuracy of kNN by doing linear transformation on the training data matrix. Themethod incrementally constructs a linear transformation that maximizes the nearest neighborclassification accuracy on the training set. At each iteration the method picks an instance from the dataset, and computes a transformation that moves the instance closer to the instances with the samecategory and/or away from the instances in other categories. The composition of these iterative lineartransformations can lead to statistically significant improvements in kNN learning algorithms."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uç noktalarda katmanlı video isleme yöntemi ile iki katman olarak islenen video görüntüleri kullanan yeni bir esler arası çok noktalı video konferans mimarisi sunulmaktadır. Bu mimari, düsük bant genisliğine (bir video alıs ve gönderis) sahip kullanıcılara, bir noktadan bir noktaya konferans için gerekli ağ ve islemci gücünden daha fazlasına gerek duymadan çok noktalı konferans yetisi vermektedir. Daha önceki çalısmadan farklı olarak, bazı katılımcıların taban kalitede video izlemesi karsılığında, her katılımcının, istediği katılımcıyı herhangi bir zaman ve düzenlesimde izleyebilmesine olanak sağlamaktadır. Kullanılabilecek katmanlı video isleme yaklasımları betimlenmis, esler arası video gönderimi için bir protokol gelistirilmis ve benzetimler yapılmıstır. Protokolun basarımı incelenmistir. Prototip bir uygulama gelistirilmis ve basarılı bir sekilde gerçek hayata geçirilmistir. Birden fazla gönderimi destekleyen bant genisliklerinin etkileri incelenmistir. Ayrıca, taban katman video izleyenlerin sayısını ve eslerin gözlemlediği gecikmeyi azaltmayı ve birden fazla alıs bant genisliğine sahip kullanıcıları desteklemek üzere, karsılanan ek video taleplerinin sayısını arttırmayı hedefleyen bir çoklu eniyileme yaklasımı gelistirilmistir. Bu hedeflerin her birine önem katsayıları atayan bir yöntem sunulmus ve açıklanmıstır. Sunulan çoklu eniyileme yaklasımının, sistem içindeki kullanımı, örnek senaryolarla betimlenmistir. Sistem özellikleri tanımlanmıs ve doğrulanmıstır. Videonun güvenli bir sekilde iletimi için, kimlik doğrulama, bütünlük kontrolü ve inkar önleme destekleyen bir çözüm sunulmus ve etkinliği benzetimlerle gösterilmistir.","A new peer-to-peer architecture for multipoint video conferencing using layered video coding with two layers at the end hosts is presented. The system targets end points with low bandwidth network connections (single video in and out) and enables them to create a multipoint conference without any additional networking and computing resources than what is needed for a point-to-point conference. In contrast to prior work, it allows each conference participant to see any other participant at any given time under all multipoint configurations of any number of users, with a caveat that some participants may have to receive only the base layer video. Layered video encoding techniques usable within this architecture are described. A protocol for the peer-to-peer video transmission approach has been developed and simulated. Its performance is analyzed. A prototype has been implemented and successfully deployed. The effects of upload bandwidths that can support multiple video signals are investigated. Furthermore, a multi-objective optimization approach to minimize the number of base layer receivers, to minimize the delay experienced by the peers and to maximize the granted additional requests to support peers having multiple input bandwidths has been developed. A technique assigning importance weights to each of the objectives is proposed and explained. The use of the proposed multiobjective optimization scheme is demonstrated through example scenarios. The system has been fully specified and verified. To ensure secure transmission of video, a scheme providing authentication, integrity and non-repudiation is presented and its effectiveness is shown through simulations."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde koşut zamanlı çalışan programların test programı kapsam ölçümlerini yaparak, kullanıcının test programını geliştirmesi için kullanılabilecek konum çiftleri (Location Pairs) ölçütü sunulmaktadır. Location Pairs (LP) ölçütü akademi ve endüstride kullanılan koşut zamanlı çalışan programlarda sıklıkla karşılaşılan bir yüksek düzey hata örüntüsünden esinlenerek yaratılmıştır. Yüksek düzey hatalar koşut zamanlı çalışan bir programın bileşenlerinin arasında paylaşılan ve atomik olarak değiştirilmesi gereken değişkenlerin atomik olarak değiştirilmemesi sonucu ortaya çıkar. Bu tür hatalar alt düzey hatalardan farklıdır çünkü bu tür hataları önlemek için paylaşılan değişkenlerin tek tek korunması yerine birden fazla değişken birlikte korunmalıdır. LP ölçütü bir Java sanal makinesi ve statik yarış tespit aracı kullanılarak kodlanmıştır. İlk olarak statik yarış tespit aracı kullanılarak paylaşılan değişkenler tespit edilmektedir. İkinci adımda konum çiftleri paylaşılan değişkenler taranarak tespit edilmektedir. Daha sonra test programının yürütümü testin çalıştırılması esnasında sanal makine aracılığı ile gözlemlenmektetir. Son adımda gözlemler sonucu elde edilen ölçüm sonuçları kullanıcıya rapor edilmektedir. Kullanıcı bu sonuçlara göre daha fazla konum çiftini kapsayacak test senaryoları yaratabilir. Yaptığımız deneylerde de gösterdiğimiz gibi koşut zamanlı çalışan programların testlerinde LP ölçümü yüksek düzey hataların bulanmasında kullanıcılara yardımcı olmaktadır.","In this thesis, we present a coverage metric, namely the Location Pairs (LP) coverage, for improving the testing of programs which have concurrently running components. The LP coverage metric is inspired by an execution pattern, encountered in practically used software, that leads to high-level concurrency errors. A high-level concurrency error occurs if a set of thread-shared variables are needed to be accessed atomically, but some threads access them non-atomically. These errors are different from low-level data races, because to avoid data races, protecting single variables is sufficient. The LP coverage metric aims to detect if a pattern of two consecutive actions is executed. The measurement of the LP coverage metric is implemented using a Java virtual machine and a static race detection tool. First, we detect accesses to shared-variables using static analysis of the race detection tool. Second, location pairs are extracted using this information. Then, we monitor the execution of the tests and record any consequent accesses to locations which accesses the same shared data. At the end, we report covered and uncovered pairs to the developer. The developer can create more interesting scenarios using coverage report and can potentially prevent high level concurrency errors by testing rarely executed scenarios. We run our tool on several Java benchmarks to show that the tool can practically be used on large-scale programs and the LP coverage metric helps better to find concurrency errors compared to existing coverage techniques for sequential programs."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoğu bilimsel uygulama iki ayarlı veya uzatılmış iki ayarlı kayan-noktalı aritmetikten daha hassas hesaplamalara ihtiyaç duyar. Bu tez çift-kipli dört ayarlı veya aynı anda iki paralel iki ayarlı kayan-noktalı bölme ve karekök işlemleri yapabilen ünitelerin tasarımını sunar. Radix-4 SRT bölme ve karekök algoritmaları çift-kipli dört ayarlı kayan-noktalı bölme ve karekök unitelerinin uygulanmasında kullanılmıştır. Bu tezde sunulan bölme ünitesinin tasarım detayları geleneksel bir dört ayarlı bölüm ünitesinin nasıl geliştirilip bir tane dört ayarlı veya iki tane paralel iki ayarlı bölme işlemini desteklediğini gösterir. Alan ve en kötü ihtimalli gecikmeyi hesaplamak için geleneksel iki, geleneksel dört, çift-kipli iki ve çift-kipli dört ayarlı kayan-noktalı bölme üniteleri VHDL de oluşturuldu ve sentezlendi. Çift-kipli bölme ünitesine benzer olarak, tezin ikinci kısmı nasıl geleneksel bir dört ayarlı karekök ünitesinin geliştirilip bir tane dört ayarlı veya iki tane paralel iki-ayarlı karekök işlemini desteklediğini gösterir. Tüm tasarımların dogrulukları kapsamlı simulasyon ve testler sonucu tasdik edilmiştir. Sentezleme sonuçları çift-kipli dört ayarlı bölme ünitesinin kapladığı alanın geleneksel dört ayarlı bölme ünitesinden %22 ve gecikmesinin de %1 daha fazla olduğunu gösteriyor. Çift-kipli dört ayarlı karekök ünitesinin kapladığı alan ise geleneksel dört ayarlı karekök ünitesinden %22 ve gecikmesi de %2 daha fazla. Dört ayarlı bölme ve karekök işlemlerinin tamamlanması ellidokuzar ve iki tane paralel iki ayarlı bölme ve karekök işlemlerinin de yirmidokuzar tur gerektirir.","Many scientific applications require more accurate computations than double precision or double-extended precision floating-point arithmetic. This thesis presents the design of dual-mode quadruple precision floating-point division and square-root units that also supports two parallel double precision operations. A radix-4 SRT division and square-root algorithms are used to implement the dual-mode quadruple precision floating-point division and square-root units. The implementation details of the divider presented in this thesis show how a conventional quadruple precision divider is modified and the datapath can be divided into two parts to support both a quadruple precision and two parallel double precision operations. To estimate area and worst case delay, a double, a quadruple, a dual-mode double, and a dual-mode quadruple precision floating-point division units are implemented in VHDL and synthesized. Similar to the dual-mode division unit, it is shown that how the datapath of conventional quadruple precision square-root unit is modified and can be divided into two parts to support both a quadruple precision and two parallel double precision operations. The correctness of all the designs was tested and verified through extensive simulation. The synthesis results show that the dual-mode quadruple precision divider requires 22% more area than the quadruple precision divider and the worst case delay is 1% longer. Also the dual-mode quadruple precision square-root unit requires 22% more area than the conventional quadruple precision square-root unit and the worst case delay is 2% longer. A quadruple precision division and square-root operations take fifty nine cycles and two parallel double precision division and square-root operations take twenty nine cycles."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dağıtık içerik dağıtım servislerinde güvenilirliğin sağlanması için, etkin bir ara bellek yönetimi yoluyla başarılmış bir kayıp mesaj kurtarım mekanizması vazgeçilmez bir bileşendir. Ara bellek yönetimi konusundaki mevcut yaklaşımlar, akış kontrolü, bellek kullanımının azaltılması, mesaj dengesinin sağlanması ve bellek parçalarının yer değiştirmesi gibi çok sayıda problem bileşeni üzerinde yoğunlaşmaktadır. Bu tez çalışmasında, geniş ölçekli biyolojiden esinlenen eşler arası veri dağıtım servislerine yönelik ara bellek yönetimi problemi ele alınmıştır. Biyolojiden esinlenen epidemik protokoller; ağ hatalarına karşı dayanıklı ve ölçeklenir olmaları ve olasılıksal güvenilirlik garantisi sağlamaları açısından kayda değer avantajlara sahiptir. Bu tip protokollerde güvenilirlik ve ölçeklenirlik sağlamasının yanı sıra, etkin bir ara bellek mekanizması ile birleştirildiğinde, sistem genelindeki bellek kullanımı da iyileştirilebilir. Önceki yaklaşımlarla karşılaştırıldığında tekdüze bir yük dağılımı sağladığı kanıtlanan ve eşlerin her birinin sistemin kısmi bir görünümüne sahip olduğu bir koşulda genel ara bellek kullanımını azaltan ve Adımsal Eşit Dağılımlı Ara Bellek olarak adlandırılan yeni bir algoritma önermekteyiz. Bu yaklaşımın başlıca hedeflerinden biri; sistem içerisindeki ara bellek tutucularının ara bellek yükü mevcut eşler arasında dengelenecek şekilde seçilmesi ve bunun sonucunda içerik dağıtımının etkinliğinin arttırılmasıdır. Bu yaklaşım; her bir mesajın ara bellek tutucusu olarak yalnızca eşlerin küçük bir alt kümesi seçildiğinden, bellek kullanımını da azaltmaktadır. Aynı zamanda, geniş ölçekli senaryolara uygulanabilir, güvenilir bir dağıtım sağlar ve dinamik sistem giriş ve çıkışlarına adapte olabilir. Ara bellek boyutunu ayarlayarak, yüksek olasılıkla mesaj dengesini sağlar. Ara bellek modelinin başarım değerlendirmesi ve önceki yaklaşımlarla kapsamlı bir karşılaştırması gerçekleştirilmiştir. Değerlendirmeler; ölçeklenirlik, güvenilirlik, hatalara uyumluluk ve tekdüzelik analizlerini içermektedir. Ara bellek düzeylerinin bir fonksiyonu olarak dağıtım güvenilirliğine ilişkin analitik sonuçlar da türetilmiştir. İlgili sonuçlarda Markov zincir analizi temel alınmıştır ve bu sonuçlar sayısal olarak değerlendirilmiştir. Benzetimlerle gerçekleştirilen karşılaştırmalar, sonuçların güvenilirlik açısından iyi bir alt sınır oluşturduğunu göstermektedir. Yüksek düzeyli güvenilirlik değerleri için, elde edilen analitik sınırların benzetim sonuçları ile tutarlılığı gösterilmiştir.","For supporting reliability in distributed content dissemination services, message loss recovery mechanism achieved via efficient buffer management is an indispensable component. The available approaches for buffer management concentrate on several aspects of the problem such as flow control, reducing the memory usage, providing message stability and the replacement of buffer items. In this thesis study, we consider buffer management problem in support of large-scale bio-inspired peer-to-peer data dissemination services. Bio-inspired epidemic protocols have considerable benefits as they are robust against network failures, scalable and provide probabilistic reliability guarantees. Coupled with an efficient buffering mechanism, system wide buffer usage can be optimized while providing reliability and scalability in such protocols. We propose a novel algorithm, Stepwise Fair-share Buffering, that is shown to provide uniform load distribution in comparison to earlier approaches and reduces the overall buffer usage where every peer has the partial view of the system. A major aim of our approach is to be able to choose bufferers uniformly throughout the system so that the load of buffering will be well balanced among participating peers and the efficiency of content dissemination will be improved as a result. This also reduces the memory usage since only a small subset of the peers is chosen as bufferers for each message. Furthermore, it is applicable to large-scale scenarios, provides reliable delivery and is adaptable to dynamic join and leaves to the system. It adjusts the buffer size to achieve message stability with a high probability. Performance evaluation of the buffering model and extensive comparisons with earlier approaches are performed. The evaluations include scalability, reliability, adaptivity to failures and uniformity analysis. We also derive analytical results for reliability of dissemination as a function of buffer levels. These results are based on a Markov chain analysis and are evaluated numerically. Comparison with simulations shows that they provide a good lower bound for reliability. For high level of reliability values, the bounds are very close to the simulation results."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hücrelerdeki karmaşık biyolojik işlevler, protein-protein etkileşimlerini de kapsıyan biyolojik moleküllerin birleşmesi ile yürütülür. Proteinlerin birleşmesini açıklamak için türlü yöntemler geliştirilmiştir. Buna rağmen, protein-protein etkileşimlerinin mekanizması hâlâ yeterli olarak aydınlatılmamıştır. Protein-protein arayüzlerini ?iki tek zincirli protein arasındaki mimari bağlanma yüzeyi elemanlarını? geniş ölçekte karakterize etmek amacıyla, burada, yapısal biyolojinin iki önemli problemi olan arayüz sıcak noktalarının tahminine ve arayüzdeki uzaysal desenlerinin keşfedilmesine yönelik yeni hesaplamalı teknikler sunuyoruz.Sıcak noktalar , arayüzlerin cok ufak bir kısmını oluşturmalarına rağmen bağlanma enerjisinin büyük coğunluğuna katkı sağlıyan amino asitlerdir. Hesaplamalı sıcak noktalara karar vermek için, dizilimsel korunmuşluğuna ve çözücü erişilebilirliğine dayalı yeni ve verimli bir yöntem sunuyoruz. Tahmin edilen sıcak noktaların, deneysel sıcak noktalarla oldukça karşılıklı ilişkili olduğu gözlenmiştir. Sonuçlar göstermiştir ki; kullanılabilir deneysel verinin noksanlığından dolayı makine öğrenme yaklaşımları, önerilen gözlemsel yaklaşımdan daha başarılı değildir. Protein arayüzleri uzelerinde tahmin edilen sıcak noktalar, http://prism.ccbb.ku.edu.tr/hotsprint adresinde yer alan HotSprint internet arayüzü aracılığı ile sorgulanıp, görüntülenebilir.Protein birleşmeleri, yapısal olarak protein bağlanma kısımlarının uzerlerindeki etkileşim halindeki örüntülerden kaynaklanıyor olabilirler (uzaysal arayüz desenleri ). Bu tez calışmasının ikinci kısmında, çizge didikleme kullanan yeni bir sıklıkla tekrar eden arayüz uzaysal örüntü keşif yöntemi geliştirilmiştir. Önerilen yöntem, sPpprint, protein arayüzlerindeki dizilimde sıralı olmak zorunda olmayan, önceden bilinmeyen ortak atom kümelerini (altyapılarını) bulur. Alınan ilk sonuçlar, arayüzün tipine karar vermek için kullanılabilecek uzaysal protein arayüz desenleri var olduğunu önermektedir.","Complex biological processes in cells are carried out by association of biological molecules including protein-protein interactions. Many diverse approaches have been developed to explain association of proteins. Notwithstanding, mechanisms of protein-protein interactions are still not adequately elucidated. In order to characterize protein-protein interfaces --the architectural binding site elements in between two monomers-- on a large scale, here, we present novel computational techniques addressing two important structural biology problems: prediction of interface hot spots and discovery of spatial interface motifs.Hot spots are residues comprising only a small fraction of interfaces yet accounting for the majority of the binding energy. We present a new efficient method to determine computational hot spots on protein interfaces based on sequence conservation and solvent accessibility of interface residues. The predicted hot spots are observed to correlate considerably with the experimental hot spots. The results reveal, due to lack of available experimental data, machine learning approaches do not overperform proposed empirical approach. Predicted computational hot spots on protein interfaces can be queried and visualized via HotSprint web interface located at http://prism.ccbb.ku.edu.tr/hotsprint .Protein associations might be structurally mediated by interacting patterns on the protein binding sites (spatial interface motifs). In the second part of this thesis study, a new frequently reoccurring interface spatial pattern discovery method employing graph mining is developed. Proposed method, sPpprint, finds not necessarily sequence-contiguous and a priori unknown common set of atoms (substructures) on the protein interfaces. Initial results suggest that there exist discriminative spatial protein interface motifs that may be used to determine type of the interface."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez raporunda verimli video akıtımı için Çok Hedef-İşlevli Eniyileme (MOO)şemaları sunulmaktadır.İlk olarak, düşük ve sabit kapasiteli ağlarda kesintisiz içerik uyarlamalı video akımı içingecikme-bozunum eniyilemesi metodu sunulmaktadır. Giriş videosu içerik analizi yapılarakçeşitli ilgililik seviyelerine ayrıştırılmaktadır. Video en küçük gecikme ve bozunum hedef-lenerek uyarlamalı olarak her sahne için en iyi uzaysal ve zamansal çözünürlük venicemlemeyle kodlanmakta ve akıtılmaktadır. Arabellek ve bozunum sınırlamalarıyla birlikte,önemsiz sahnelerin bit hızı düşürülmekte ve önemli kısımların kalitesi arttırılmaktadır.Sonra en yüksek ?uygulama katmanı servis kalitesinde?, en yüksek video kapasitesinde(zaman sekmesi başına video saniyesi) ve servis kalitesinde adil kablosuz video akıtımı içinçapraz-katmanlı eniyilenmiş bir video bit hızı uyarlama ve kullanıcı çizelgeleme şemasısunulmaktadır. Bu hedefler her zaman sekmesinde i) en küçük oynatma zamanına, ii) enyüksek video kapasitesine ve iii) en yüksek video kalitesine ulaşan kullanıcı seçilerek MOOile eniyilenmektedir.Son olarak, var olan ağlarda stereo videolarin kodlanması ve akıtımı için uyarlamalı birmetod önerilmektedir. Fazladan sıkıştırmanın uzaysal ve zamansal ölçeklemeyle sağlandığıiçerik uyarlamalı stereo video kodlama uygulanmaktadır. Kullanıcıların gösterim imkanlarıdahilinde mono veya stereo video izleyebildikleri uçtan uca bir akıtım sistemi tanıtılmakta,MOO problem formüllemeleri önerilmektedir.Ulaşılan kazanımlar deneysel sonuçlarla gösterilmektedir.","In this thesis, we propose Multiple Objective Optimization (MOO) frameworks for efficient videostreaming.Firstly, we introduce pre-roll delay-distortion optimization (DDO) for uninterrupted content-adaptive video streaming over low capacity, constant bitrate (CBR) channels using MOO. Contentanalysis is used to divide the input video into shots with assigned relevance levels. The video isadaptively encoded and streamed aiming minimum pre-roll delay and distortion with the optimalspatial and temporal resolutions and quantization parameters for each shot. With buffer and distortionconstraints, the bitrate of unimportant shots is reduced to achieve an acceptable quality in importantshots.Secondly, we introduce a cross-layer optimized video rate adaptation and scheduling schemeto achieve maximum ``application layer"" Quality-of-Service (QoS), maximum video throughput (videoseconds per transmission slot), and QoS fairness for wireless video streaming. Using the MOOframework, these objectives are jointly optimized such that the user with i) the least remainingplayback time, ii) highest available video throughput and iii) maximum video quality is served.Finally, we propose an adaptive framework for compression and streaming of stereo videousing the existing network infrastructure. We employ content-adaptive stereo video coding (CA-SC),where additional compression is achieved by spatial and/or temporal downsampling depending on thecontent. An end-to-end streaming system where the end-users can view the video in mono or stereomode depending on their display capabilities is implemented and MOO formulations are proposed.The improvements achieved are demonstrated with experimental results."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyircinin konumuna güre seşilmis birkaş bakışı yollayarak kullanıcıların 3-B materyal-o c c sleri etkileşimli ve düşuk bant genişliği ile izlemesini sağlayacak yeni bir 3-B cok bakışlıs usü sg g ş süvideo iletimi yüntemi sunuyoruz. Onerilen yüntem, hem noktadan noktaya tek alıcıyao ogünderim işin hem de cok kullanıcıya coğagünderim ile calışabilir. Dahası sistemimizo c ş şg o şshem daha yoğun ışık alanı tü rü nde cok bakışlı videolarin hem de derinlik bilgisi ileg s uu ş sdaha seyrek cok bakışlı videolarin iletiminde kullanılabilir.ş süOnerilen yüntemde gerekli bakışlari seşmek icin kullanıcının kafa konumu sü reklio s c uolarak takip edilir. Kullanıcının güreceği gürü ntü yü yü ksek kalitede gerişatabilmeko g ou u u u cişin sistem seşilen bakışlara daha fazla bant genişliği ayırır. Ağdaki gecikmelerininc c s sg gve akış değişimlerinin istenmeyen etkilerini azaltmak işin kullanıcının bakışı üncedens gs c sotahmin edilir ve ihtiyaş duyulan akışlar ünceden istenir. Buna ek olarak, tahmin hata-c solarından kaynaklanabilecek duraksamaları engellemek işin yü ksek derecede sıkıstırılmışc u skomşu bakışlar da ihtiyaş duyulan bakışlara ek olarak yollanır. Yollanacak komşus s c s sbakış sayısını belirlemek icin kafa hareketlerinin aniliğini dikkate alan yeni bir metriks gü ückullanılmıştır. Onerilen sistem, Cok Bakışlı Video Sıkıştırma (MVC) ve Olşeklendirilebilirs ş s sVideo Sıkıştırma (SVC) ï¬kirlerinden faydalanarak hem daha verimli sıkıştırma hems sde daha esnek bakış seşimi sağlamayı amaşlar.sc g cCok bakışlı videolar ünerilen yüntemle cok sayıda kullanıcıya günderileceği zaman,ş s o o ş o guygulama katmanı coğagonderim kullanılır. NICE olarak adlandırılan bir uygulamaşgkatmanı coğagonderim protokolu, ünerilen sisteme uyarlanmıştır.şg o s1","We present a novel 3-D multi-view video delivery scheme that allows users watch3-D content interactively with signiï¬cantly reduced bandwidth requirements by trans-mission of small number of views selected according to viewer?s head position. Theproposed scheme can be used for point-to-point delivery to a single user and/or multi-cast delivery to many users. Furthermore, the scheme can transport dense light-ï¬eld-like multi-view sequences or multi-view sequences together with depth information.In the proposed scheme, the user?s head motion is tracked to select the requiredviews dynamically. The system then allocates more bandwidth to these required viewsto render the user?s current viewing angle. In order to reduce the adverse eï¬ectsof delays due to network and stream switching, required view points are predictedand pre-fetched. Highly compressed lower quality views are also sent alongside theselected high quality views to provide protection against stoppage due to erroneousviewpoint predictions. A novel metric based on the abruptness of the head movementsand delays in the system is introduced to help in determining the number of theadditional lower quality views to be transmitted. The proposed system makes useof Multi-View Coding (MVC) and Scalable Video Coding (SVC) concepts togetherto obtain improved compression eï¬ciency while providing ï¬exibility in bandwidthallocation to the selected views. Rate-distortion performance of the proposed systemis reported under diï¬erent conditions. Use of the introduced metric in determiningthe optimal operating point, i.e., the number of lower quality views that need to betransported, is demonstrated.Overlay multicast is employed to reduce the bandwidth requirements when theproposed scheme is used for transporting 3-D content to several users. An application-layer multicast protocol, called NICE, was modiï¬ed for this purpose. NICE is chosendue to its lower control overhead and short join-latency when compared to other1application-layer multicast protocols; reducing the network delay for the proposedselective multi-view video delivery system.2"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışması üç boyutlu şekil yakalama problemi için yüzey deformasyonuna dayalıbir çatı yöntem sunmaktadır. Bu yöntem, bir başlangıç modelinden yola çıkarak, yüzeyevrimine dayalı bir teknikle uzamsal olarak yumuşak ve topolojisi doğru bir yüzey örgügösterimi oluşturur. Şekli kuşatan yüzeyi temsil eden başlangıç örgü modeli, yüzey evrimisırasında yerel örgü dönüşüm işlemleri ile gerektiği şekilde inceltilir ya da sadeleştirilir,böylelikle modelin nesne yüzeyinin yerel özelliklerine uyum göstermesi sağlanır.Yakınsama durumunda elde edilen son örgü modeli, çatallanma, çıkıntı ve kovuk gibikarmaşık yüzey detaylarını uygun bir şekilde ifade edebilir. Önerdiğimiz deformasyonyöntemini, statik (devinimsiz) nesnelerin modellenmesi amacıyla, silüetten şekil geriçatmave bunun optik üçgenleştirme ile birleştirilmesi problemlerine uyguladık. Elde edilen deneysonuçları tezde sunulmaktadır. Önerdiğimiz yöntem aslında genel bir deformasyon çatısıtarif eder ve üç boyut bilgisi veren her türlü veriye uygulanabilir. Deformasyonumodellemek için benimsediğimiz Lagrange yaklaşımı ile örgü modelin yüzey evrimisırasında değişebilen bağlanırlığı ve geometrisi izlenebilir, ve dolayısıyla önerdiğimiz çatı,dinamik sahnelerin zamanla değişen gösterimlerinin verimli bir şekilde oluşturulması içinde kullanılabilir.Anahtar Sözcükler: 3B geriçatma, silüetten şekil, yapısal ışıktan şekil, optiküçgenleştirme ile şekil, yüzey deformasyonu, yüzey evrimi.","We present a generic surface deformation framework for the problem of 3D shaperecovery. A spatially smooth and topologically plausible surface mesh representation isconstructed via a surface evolution based technique, starting from an initial model. Theinitial mesh, representing the bounding surface, is refined or simplified where necessaryduring surface evolution using a set of local mesh transform operations so as to adapt it tothe local properties of the object surface. The final mesh obtained at convergence canadequately represent the complex surface details such as bifurcations, protrusions and largeconcavities. We demonstrate the performance of our deformation framework on theproblem of shape from silhouette and its fusion with shape from optical triangulation for3D reconstruction of static objects. The framework is in fact very general and applicable toany kind of data that can be used to infer 3D geometry. Since the approach we take forsurface deformation is Lagrangian, that can track changes in connectivity and geometry ofthe deformable mesh during surface evolution, the proposed framework can also be used tobuild efficient time-varying representations of dynamic scenes.Keywords: 3D reconstruction, shape from silhouette, shape from structured light, shapefrom optical triangulation, surface deformation, surface evolution."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hücrede meydana gelen bir çok biyolojik süreçte, proteinler arasındaki karmaşık etkileşim ağlarıgözlemlenir. Hesaplamalı biyolojinin en büyük hedefi, hücre denen bu son derece karışık yapıdakimekanizmanın bilgisayar ortamında benzetimi olarak özetlemek mümkündür. Bu bağlamda, proteinlerarasında atom seviyesinde meydana gelen etkileşimler, biyolojik ağların yapıtaşlarıdır. Günümüzde,bir çok deneysel ve işlemsel yöntem yaygın bir şekilde kullanılarak protein etkileşim verisi eldeedilmektedir. Ancak, önemli bir etkileşim veri kaynağı olan protein bileşikleri çoğu zaman göz ardıedilir. Bu tezde, protein bileşiklerinden elde edilmiş arayüzlerin sorgulanması, incelenmesi, vegörselleştirilmesi için geliştirmiş olduğumuz yazılımsal sistem (PRISM) ve onun sonuçlarısunulmuştur. Protein arayüz verikümesi ve onlara benzer proteinlerin listesini kullanarak, tüm ilgilibilgiler bir ilişkisel veritabanında saklanmıştır. Veritabanının içeriği Internet sayfamız aracılığıylakullanıma sunulmuştur. Sitemizin tasarımında protein arayüzlerinin araştırılmasında kullanılabilecekçevrimiçi araçlara yer verilmiştir. İkili benzerlik verisi kullanılarak bir etkileşim haritası çıkartılıp,buna ilaveten, etkileşim verisinin görselleştirilmesi ve grafiksel ortamda araştırılmasına yönelik biraraç geliştirilmiştir. Veritabanımızdaki mevcut bilgiler, protein arayüzleri hakkında biyolojik açıdananlamlı bazı çıkarımlarda ve protein etkileşim verisi üzerinde grafik kuramı tabanlı incelemelerdebulunmada kullanılmıştır.","Many biological processes in the cell are carried out by a complex network of interactions amongnumerous proteins. The ultimate goal of computational biology would be to simulate this extremelycomplex machinery called the cell, using in silico methods. In that sense, binary interactions occurringat the atomic level between individual proteins are the building blocks of biological networks. Today,there are many experimental and computational methods used extensively for the generation of proteininteraction data. However, one important source of interactions is often overlooked: proteincomplexes. Here, we present PRISM, our software system used for the query, visualization andanalysis of protein interfaces obtained from protein complexes of known structures. Starting with thedataset of protein interfaces and a list of similar proteins to those interfaces, all relevant informationabout them are collected in a relational database. The content of the database is made availablethrough our website. The design of the website also incorporates additional on-line tools used for thepurpose of exploring protein interfaces. An interaction map is generated using the pairwise similaritydata. In addition, a graphical tool is developed which can be used for the visualization and explorationof interaction data. We have used the data stored in our database to extract some biologically relevantinferences about protein interfaces, and to perform a graph theoretic analysis of protein interactionnetworks."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Devingen plansız ağların (mobile ad hoc networks) kendine özgü karakteristiği güvenilir(reliable) grup iletişimini gerçeklemede yeni yaklaşımlar gerektirmektedir. Mevcutyaklaşımlar genelde kablolu ağlara özgü tekniklerle çözüm yoluna gittiklerinden plansızağların devingen ortamına uyum sağlayamamaktadırlar. Diğer yandan, yaygın-tabanlı(epidemic-based) yaklaşımların durumsuz (stateless) yapısı plansız ağların kararsız ortamınaoldukça uyum sağlamaktadır.Bu tez çalışması, EraMobile (Epidemic-based Reliable and Adaptive Multicast for Mobilead hoc networks) adında, devingen plansız ağlar için yeni bir çoğa gönderim (multicast)protokolü sunmaktadır. EraMobile'ın hedefi yüksek düzeyde güvenilirlik gerektiren grupuygulamalarında güvenilir çoğa gönderimi en düşük seviyede ağ ek yükü (overhead) ileelverişsiz ağ şartlarında dahi gerçekleştirmektir. EraMobile, devingenliğin neden olduğudinamik ve kestirilemez ağ düzeni değişimleri ile başa çıkabilmek için yaygın-tabanlı birteknik kullanmaktadır. Önceki çalışmaların aksine bu teknik hem çoğa gönderim dağıtımınıhem de kayıp verilerin telafisini gossip adı verilen özel mesajlarla alt katmanda herhangi biryol atama (routing) protokoluna gerek duymadan yapabilmektedir. Gossip mesajlar periyodikolarak telsiz kapsama alanı içindeki diğer katılımcılara telsiz ortamın doğal yayın özelliğindenyararlanılarak yayınlanır (broadcast). Ardından çoğa gönderim veri dağıtımı eşler arası (peer-to-peer) iletişimlerle gerçekleştirilir. Kullandığımız yaygın-tabanlı teknik çoğa gönderim içinherhangi bir ağaç benzeri yapı kullanmamaktadır. Ayrıca, yöntemimiz ağın genel veya özelbilgisi, telsiz kapsama alanında olan diğer katılımcıların bilgisi ve çoğa dağıtım grupüyelerinin bilgisine gereksinim duymamaktadır. Buna ek olarak, kullanmış olduğumuz teknikfazladan veri iletimini engelleyerek ağın üzerindeki ek yükü önemli ölçüde azaltmaktadır.EraMobile'ın önemli diğer özelliği de değişken katılımcı yoğunluğuna uyarlanarak hem ağulaşılabilirliğinin zayıf olduğu seyrek ortamlarda hem de sıkışıklığa açık yoğun ortamlardagüvenilir çoğa gönderim dağıtımını gerçekleştirebilmesidir. Ns-2 ağ benzeticisi ilegerçekleştirilen geniş kapsamlı benzetim çalışmalarının sonucunda, EraMobile diğer çoğagönderim dağıtım protokollerinden daha iyi paket dağıtımı, güvenilirlik ve ek yük verimliliğisağlayarak denenen çoğu senaryo için tam güvenilir çoğa gönderim dağıtımını başarmıştır.","The characteristic of mobile ad hoc networks demands new set of network strategies inorder to provide reliable group communication. The techniques used by majority of theexisting solutions are inherited from wired networks and can not adapt well to the dynamicenvironment of mobile ad hoc networks. On the other hand, the stateless character ofepidemic-based approaches is well suited to the non-deterministic nature of such networks.This thesis presents a novel protocol, namely EraMobile, offering Epidemic-based Reliableand Adaptive Multicast for Mobile ad hoc networks. EraMobile?s target is group applicationsrequiring high-level of reliability, and the protocol aims to provide fully reliable multicastdata delivery with minimal network overhead even in the adverse network conditions.EraMobile utilizes an epidemic-based method in multicast operation to cope with dynamicand unpredictable topology changes arising from the mobility. In contrast to prior studies, thismethod carries out both multicast delivery and recovery of missing data by use of gossipmessages without needing any underlying routing protocol. The gossip messages areperiodically disseminated by exploiting the broadcast nature of wireless medium. Themulticast data delivery is then performed through peer-to-peer communications succeedingthe gossip broadcasts. Our epidemic mechanism does not require the maintenance of any tree-or mesh-like structure for multicasting. It also needs neither having global or partial view ofthe network nor having information of neighboring nodes and group members. Besides, itsubstantially minimizes the overhead incurred by eliminating redundant data transmissions.Another distinguishing feature of EraMobile is its capability of adapting to varying nodedensities in order to provide reliable data delivery in both sparse networks, where the networkconnectivity is prone to interruptions, and dense networks, where congestion is likely tooccur. EraMobile is shown to achieve fully reliable multicast data delivery for most of thescenarios studied through extensive simulations on ns-2 network simulator by outperformingthe other protocols compared, especially in terms of both packet delivery ratio and overheadefficiency."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Duraksız çoğul ortam iletimi aktif araştırma alanlarından biri olmuştur. Bu konudakiaraştırmalar doksanlı yılların başlarından beri etkin ve kullanışlı ortam kodlama teknikleri,eşzamanlama, bant genişliği, paket kaybı ve paket gecikme sorunları, teslim protokolleri,gelişmiş ortam oynatıcıları ve görüntüleme sistemleri gibi birçok konuda devametmektedir. Günümüzde, aktarım sistemlerini oluşturmak için değişik audio ve videoçeşitlerini destekleyen birçok açık kaynak kodlu veya ticari ürün mevcuttur. Klasik ikiboyutlu video iletimini sağlayan duraksız iletim sistemlerinin geliştirilmesinden sonra ilgiüç boyutlu sistemler üzerinde yoğunlaşmıştır. Bu ilgi üç boyutlu görüntülemecihazlarındaki ve çok görüşlü video kodlama tekniklerindeki ümit verici gelişmelersonucunda doğmuştur.Çok görüşlü video, aynı sahnenin birkaç kamera tarafından farklı perspektiflerdenyakalanan görüşlerini içerir. Herhangi bir zamanda bu görüşlerin sadece iki tanesi birizleyici tarafından izlenebilir. Bu durum stereo videoların duraksız aktarımını sağlayan veçok görüşlü videolara adapte edilebilen sistemlerin geliştirilmesi fikrini akla getirmiştir. Busistemlerin adaptasyonu izleyicinin o andaki perspektifine bağlı olarak ilgili iki görüşünseçilmesi ve gösterilmesiyle sağlanır. Bu tez çalışması dizayn edilen bir uçtan uca duraksızstereo video aktarım sisteminin gerçekleşmesini içermektedir. Sistem gerçek zamanlıduraksız aktarım protokolü(RTSP), gerçek zamanlı ağ protokolü(RTP), oturum duyuruprotokolü (SAP) ve oturum tanımlama protokolü (SDP) gibi iyi bilinen ve aynı zamandaticari ve açık kaynak kodlu mono duraksız aktarım sistemleri tarafından da kullanılanstandart protokoller kullanılarak kurulmuştur. Sistem mimarisi mono ve stereo videotransferi için mevcut bant genişliği veya kullanıcının alıcı ve görüntüleme kapasitesine görekendini ayarlayabilecek şekilde düşünülmüştür. Bunu başarabilmek için de sistem ikigörüşün farklı kanallar üzerinden bağımsız transferini sağlayacak şekilde kurulmuştur.1Çoğul ortam sunucusu bir RTSP sunucusunun gerektirdiği ana fonksiyonlarla dizaynedilmiş ve geliştirilmiştir. Alıcı tarafta ise iki kanal üzerinden alınan stereo videodizilimlerinin işlenmesi ve gösterilmesi için açık kaynak kodlu VideoLAN Client monoortam oynatıcısı genişletilmiştir. Bunun yanında, Live555 RTSP kütüphanesi ve FFMPEGkodlayıcı-kodçözücü kütüphanesi stereo bit dizgilerini işleyebilecek hale getirilmiş veortam oynatıcısına uygun olarak kaynaştırılmıştır. Sonuç olarak geliştirilen uçtan ucaaktarım platformu daha önceden kodlanmış stereo videoları aktaran sunucudan ve RTPüzerinden alınan ortamların istemci tarafında senkronize olarak gösterimini sağlayan birortam oynatıcısından oluşturulmuştur. Sistem stereo için genişletilmiş H.264 videokodlayıcı-kodçözücüsü kullanılarak sıkıştırılan stereo video dizileri kullanılarak,izleyicilerin stereo videoları projektörler ve polarize edilmiş gözlükler vasıtasıylagörebildikleri bir görüntüleme sistemiyle test edilmiştir.Ekteki Dosya İsimleri:Selen_Pehlivan_StereoVLC.zipSelen_Pehlivan_StereoRTSPServer.zip2","Media streaming has been an active research area where efficient and network friendlymedia coding techniques, synchronization, bandwidth, packet loss and delay issues,delivery protocols, and, integrated media players and display systems have all beenaddressed since early 90?s. Today, there exist several open source and commercial productsthat can be used to build an integrated streaming environment supporting various audio andvideo formats. After the developments of streaming systems that feature classical, 2Dvideo, the attention now is focused on the third dimension; mostly because of the promisingprogresses in 3D displays and multi-view video coding techniques.Multi-view video contains views of the same scene from multiple perspectives capturedby several cameras. However, only two of these views can be watched by a human viewerat any given time. This brings the idea of developing streaming system delivering stereovideo that can be adapted to multi-view by selecting the two views displayed based on theuser?s current perspective. This thesis presents design and implementation of an end-to-endstereoscopic streaming system. The system has been constructed using the standardprotocols: Real Time Streaming Protocol (RTSP), Real-time Transport Protocol (RTP),Session Announcement Protocol (SAP) and Session Description Protocol (SDP) that arealso used in well known commercial and open source monoscopic streaming systems. Thesystem architecture is based on independent transmission of two channels of the stereovideo in order to achieve selective transmission of mono or stereo video depending on theavailable bandwidth or the user?s receiver and display equipment. The server side designedand developed to implement the basic functionalities required from an RTSP server. On thereceiver side, open source VideoLAN Client media player has been extended in order toprocess and display received stereo video sequences over two logical channels. Also, opensource Live555 RTSP library and FFMPEG codec library are integrated into the media1player for extending it to handle stereo streams. The resulting end-to-end platform consistsof a pre-encoded stereo-video-streaming server and a media player providing synchronizedstereo display on the client side with media delivery over RTP. The system is tested usingstereo video sequences compressed using H.264 Extension video codec and a displaywhere end users can view the stereo video using two projectors and polarized glasses.Attached File Names:Selen_Pehlivan_StereoVLC.zipSelen_Pehlivan_StereoRTSPServer.zip2"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışmasında, moleküler etkileşimlerin sanal ortamlarda haptik geri-beslemeliolarak gösterilip simülasyonunun yapılması amacıyla verimli hesaplama yöntemlerigeliştirilmiştir. Simülasyonlarımızda, haptik cihaz kılavuzluğunda katı ilaç molekülü proteinyüzeyindeki kenetlenme bölgesine götürülür, ilaç molekülü üzerinde oluşan kuvvetlerhesaplanıp ölçeklenerek kullanıcıya gerçek zamanlı olarak sanal ortamlarda yansıtılır. Buçalışmada, haptik arayüzün, bağlanma bölgesindeki ilaç molekülünün başlangıç konumununtespitinde bir ön aşama olarak kullanıldığında, bağlanma işlemini hızlandırdığını vebağlanma hatalarını azalttığını gösteriyoruz. İlaç molekülünün haptik cihaz yardımıylakenetlenme bölgesine yerleşimi ve kabaca hizalanmasını takiben, ilacın bağlantı bölgesindekison konumunu yeni bir hesaplama yöntemi ile buluyoruz. Bu yaklaşımda, ilaç molekülünü endüşük potansiyel enerji seviyesini arayan katı bir cisim gibi görüyoruz. İlaç molekülünün katıcisim konumlarının hesaplanması moleküler etkileşim kuvvetlerinin etkisi altında kalarakhareket eden atomlarının yeni pozisyonları kullanılarak en küçük kareler yaklaşımıylayapılmıştır. Önerilen yaklaşım standart katı cisim formülasyonlarını kullanan molekülersimülasyon yöntemlerine göre hesaplamalı açıdan daha verimlidir. Ek olarak, proteinyüzeylerinin etkileşimli olarak görselleştirilmesi ve olası bağlanma bölgelerinin tespitiamacıyla yeni bir haptik görselleştirme yöntemi önerilmiştir. 6 denekle yürüttüğümüzdeneysel çalışmalarımız göstermiştir ki, görsel ve haptik öğelerle beslenen denekler gerçekbağlanma bölgesini yanlış bölgelerden başarı ile ayırt etmişlerdir. Bunun yanında,önerdiğimiz hesaplamalı yöntemi kullanarak ilaç molekülünün bağlanma bölgesinde ilkhizalanmasını takiben molekülün en son konumu da başarıyla bulmuşlardır.","In this thesis, we present computationally efficient methods for visualization andsimulation of molecular interactions in virtual environments with haptic feedback. In oursimulations, the haptic device is used to guide a rigid ligand molecule into a receptor sitewhile the molecular forces acting on the ligand molecule are scaled and reflected to the userin real-time. We demonstrate that the presence of a haptic interface accelerates the bindingprocess and reduce the binding errors if it is used as a precursor to estimate the initialconfiguration of the ligand molecule at the binding site. After placement and rough alignmentof the ligand molecule inside the binding cavity with the help of a haptic device, we use anovel computational approach to determine the final binding configuration of the ligandmolecule. In this approach, the ligand molecule is treated as a rigid body seeking for thelowest potential energy configuration. The rigid body configurations of the ligand moleculeis calculated in a least square sense using the new positions of its atoms moving under theinfluence of molecular interaction forces. The proposed approach is computationally moreefficient than the molecular simulation methods that utilize the standard rigid-bodyformulations. We also present new methods for haptic visualization of a protein surfaceinteractively to search for potential binding sites. Our experimental studies with 6 subjectsshow that subjects can successfully identify the true binding site among the 5 potentialbinding sites using visual and haptic cues. In addition, we show that the proposed distanceminimization approach can be used to find the final configuration of a ligand molecule insidethe binding cavity after it is initially aligned by the subject."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uygulama alanlarının artması, eşler arası yardımlaşmalı sistemler üzerine olan ilgininartmasına neden olmaktadır. Eşler arası yardımlaşmalı sistemlerin ana uygulama alanlarındanbiri de içerik dosyalarının geniş boyutlu ağlar üzerinde dağıtılmasıdır. İnternet kullanımınınartması, yazılım yamaları veya filmler gibi büyük boyutlu verilerin ve bu verilere olan ilgininhızlı bir şekilde artmasına yol açmaktadır.Bu tez çalışmasında, büyük boyutlu içeriğin çok sayıda son kullanıcıya verimli ve etkinbir biçimde dağıtılmasını hedefleyen bir eşler arası sistem önerilip, SeCond: Eşler arasıepidemik içerik dağıtım protokolü, tasarımı gerçekleştirilmiştir. Önceki çalışmalardan farklıolarak, SeCond diğer eşlerin erişilebilir bloklardan haberdar edilmesi ve blok aktarımlarınınbaşlatılması için epidemik dağıtım mekanizmasını kullanmaktadır. Eşlerin farklılığı,uygulanma kolaylığı, ölçeklenebilirlik ve dinamik eş varış/ayrılışları destekleyebilmek gibiözelliklerin yanı sıra, sistem kaynaklarından yararlanmayı artırmak için de bant genişliğininkullanımına göre protokol parametrelerini ayarlayan mekanizmalar geliştirilmiştir. SeCondprotokolü ve onun ayrık durum simülasyon modeli tanımlanmıştır. Birçok senaryo için,protokolün kapsamlı bir performans değerlendirilmesi yapılmıştır. Modellediğimiz ve SeCondile karşılaştırdığımız BitTorrent protokolü, tanınan ve geniş bir kullanıcı kitlesine sahip olanbir eşler arası içerik dağıtım sistemidir. Performans sonuçları farklı geliş/ayrılış kalıpları içinölçeklenebilirlik analizini, ani yoğunluk senaryosunu, ek yük analizini ve eşitlik oranınıiçermektedir. Kullandığımız ana başarım ölçütleri ortalama içerik indirim zamanı, orijinaliçerik yayınlayıcısı üzerindeki yük, indirme/yükleme bant genişliği yararlanma oranı, eşitlikoranı ve iletişim ek yükünden oluşmaktadır. SeCond eşlerinin gerçekleştirilen senaryolarınbirçoğu için dosyayı BitTorrent eşlerine göre daha hızlı indirdiği ve protokolün özel birmekanizma önermediği halde BitTorrent kadar adil dağıtım yaptığı görülmüştür. SeCond'uneşlerin farklılığını göze alan, ölçeklenebilir ve uyarlanabilen bir protokol olduğu ortayakonulmuştur. Ayrıca analitik bir akış modelinin, SeCond protokolünün davranışınauygulanabilirliği gösterilmiştir.","Peer-to-peer (P2P) cooperative systems are becoming extremely popular as they finddiverse applications. One major application area is the content distribution over large-scale networks. As the usage of the Internet grows up, the number of large contents suchas software packages and popular movie files, and also the user population retrievingthese contents increase exponentially.In this thesis, we propose and design a peer-to-peer system, SeCond, addressing thedistribution of large sized content to a large number of end systems in an efficient andeffective manner. In contrast to prior work, it employs an epidemic dissemination schemefor state propagation of available blocks and initiation of block transmissions. In order tosupport heterogeneity of peers, ease of deployment, scalability, and adaptivity to dynamicpeer arrivals/departures, and also to increase the utilization of the system resources, wepropose mechanisms such as adjusting protocol parameters according to the bandwidthusages dynamically. We describe our protocol SeCond and its discrete event simulationmodel. A comprehensive performance evaluation has been accomplished for a wide rangeof scenarios. A well known and widely used P2P content distribution system isBitTorrent which we also model and compare as a benchmark. Performance resultsinclude scalability analysis for different arrival/departure patterns, flash-crowd scenario,overhead analysis, and fairness ratio. The major metrics we study include the average filedownload time, load on the primary seed, uplink/downlink utilization, communicationoverhead, and the fairness ratio. SeCond peers download the file faster compared toBitTorrent peers for most of the scenarios and the protocol is as fair as BitTorrentalthough it has no explicit strategy addressing free-riding. We show that SeCond is ascalable and adaptive protocol which takes the heterogeneity of the peers into account.We also illustrate the applicability of an analytical fluid model to the behavior ofSeCond."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETÇE Hesaplamalı Biyolojinin ve Biyobilişimin en büyük hedeflerinden biri biyolojik sistemlerin ve süreçlerinin daha iyi anlaşılabilmesini sağlamaktır. Tüm proteinlerin oluşturduğu etkileşim ağının aydınlığa kavuşturulması bu hedefe yönelik çalışmaların önemli bir parçasıdır. Dolayısıyla, protein-protein etkileşimlerini hızlı ve güvenilir bir şekilde kestirebilecek bilgisayar programlarına duyulan gereksinim gün geçtikçe artmaktadır. Bu tezde protein- protein etkileşimlerini yüksek başarılı bir şekilde kestirebilmek için tasarlanan bir algoritma sunulmaktadır. Bu algoritmanın tasarımında protein arayüzeylerindeki yapısal ve dizilimsel korunma görüngüsünü birleştiren yeni bir ""aşağıdan yukarıya yaklaşım"" kullanılmıştır. Algoritmayı 67 elemanlı bir şablon arayüzey ve 6170 elemanlı bir hedef pro tein veritabanı üzerinde çalıştırarak 62616 farklı protein-protein etkileşimi kestirilmiştir. Bu kestirimlerin daha sonra 3 farklı etkileşim veritabanıda harşılıklarının bulunup bulunmadığı denetlenmiştir. Ayrıca, bazı ilginç kestirimler yazında da taranmıştır. Sonuçlarda doğrulanan ile doğrulanmayan kestirimler arasında iyi bir denge olduğu görülmüştür. Doğrulanan kestirimler algoritmamızın güvenilirliğini gösterirken doğrulanmayan kestirimler doğada bulunan ama henüz gözlenmemiş olan veya laboratuar ortamlarında gerçekleştirilebilecek etkileşimlere işaret ediyor olabilirler. Bu doğrulanmamış etkileşim kestirimlerinin ilaç tasarımı alanında önemli etkilerinin olabileceğini düşünmekteyiz. Kestirim ve doğrulama sürelerini haftalar mertebesinden günler mertebesine indirebilmek için algoritmaları paralelleştirilmiş, kestirim algoritmasının 32lik bir Beowulf bilgisayar yığınında 29.39 kat hızlandığı gözlemlenmiştir.","ABSTRACT The major goal of Computational Biology and Bioinformatics is to achieve a better un derstanding of the principles of biological systems and processes using informatics tools. Elucidation of the full network of protein-protein interactions is a crucial part of this chal lenge. Thus, there is a growing need for fast and reliable in silico methods for predicting protein-protein interactions. Here, we present a high-performance algorithm for automated prediction of protein-protein interactions. We adopt a novel bottom-up approach that com bines structure and sequence conservation in protein interfaces. Starting with 67 known structures of protein interfaces and 6170 protein structures, we predicted 62616 distinct in teractions. We then checked whether these interactions existed in three different interaction databases. We also searched literature for some interesting cases. The results displayed a good balance of verified and unverified predictions. Verified interactions prove the relia bility of our algorithm whereas unverified ones may correspond to unobserved interactions that actually occur in nature or may synthetically be realized in laboratory conditions. We believe these unverified predictions may have important implications regarding drug design. We parallelized the algorithms to reduce execution times from the order of months to days: parallelized prediction algorithm demonstrated a speed up of 29.39 on a 32 node Beowulf cluster. IV"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETÇE Günümüzde elektronik aletlerin çoğu güçlü mikrodenetleyiciler içermektedir. Söz konusu cihazlara web sunucuları ekleyerek, web sayfalan şeklinde kullanıcı arayüzleri oluşturmak hem düşük maliyetli olacaktır hem de İnternet üzerinden cihaz kontrolü sağlanacaktır. Üreticiler maliyetleri düşürmek için kullanıcı arayüzlerinde bir düğmeye birden fazla fonksiyon yüklemektedirler ve bu nedenle arayüzler karmaşıklaşmaktadır. Web sayfalan şeklinde oluşturulan arayüzlerin maliyetleri düşük olduğu için kolay anlaşılır arayüzler oluşturulacaktır. Aynı zamanda, donanımsal kullanıcı arayüzleriyle kıyaslandığında, web sayfası tabanlı arayüzlerde gerektiğinde değişiklik yapmak çok daha kolaydır. Bu tezde yeni bir teknik yaklaşımla akıllı cihazların Internet üzerinden kontrol edilmesi ve cihazların bir ağ yapısı altında toplanarak birbirlerinin fonksiyonlanndan faydalarımdan anlatılmaktadır.","ABSTRACT Powerful microcontrollers are used as parts of most home and office appliances of today. Integrating web servers to these intelligent devices will aid in controlling such devices over the Internet and also creating user interfaces for them in the form of web pages. Assigning multiple functionalities to a single button help manufacturers economize user interfaces but, this makes them more complicated. Since the cost of web-based interfaces is considerably low, they can be used to provide the infrastructure for the design of simple and more user-friendly interfaces for household appliances. Also, a web page based interface is much easier to change, when needed, as compared to a hardware interface. This paper presents a novel approach to control devices over the Internet and to form device networks such that their components can make use of one another's services and functions while improving on user interfaces. The approach is used to create our prototype system, in which the main contributions are its lightweight design, automatic configuration and utilization of widely available network protocols of TCP/IP and HTTP. IV"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETÇE Kayan noktalı hesaplamalar, yuvarlama ve iptallenmeler sonucu ortaya çıkan hatalardan ötürü olumsuz etkilenmektedir. Hızlı bilgisayarlar programcılara yoğun sayısal hesaplamalar içeren programlar yazmalarına müsade etmekte, fakat aritmetik işlemler deki hataların toplanması sonucu hesaplanan sonuçlar doğru sonuçlardan çok farklı olabilmektedir. Aralıklı aritmetik, sayısal hesaplamalarda ortaya çıkan hataların izlenmesi ve kontrol edilmesi için etkin bir yöntemdir. Aralıklı aritmetik ile her veri değeri, aralığın son noktalarım oluşturan iki kayan noktalı sayıdan oluşur ve doğru sonucun bu aralıkta bulunacağı garanti edilir. Aralıklı aritmetiği destekleyen birçok yazılım aracı geliştirilmiştir. Bunların başlıcaları; aralıklı aritmetik program kitaplıkları, genişletilmiş bilimsel programlama dilleri ve aralıklı aritmetiği destekleyen derleyicilerdir. Aralıklı aritmetik işlemleri işlevsel (fonksiyonel) çağrılar ile gerçekleştirildiği için, bu yazılım araçlarının temel götürüşü yavaş olmalarıdır. Aralıklı aritmetiği hızlandırmak için, aralıklı aritmetik işlemleri (toplama/çıkarma, çarpma, bölme ve karşılaştırma/seçme) için donanımsal destekler geliştirilmiştir. Bu araştırmada, aralıklı bir sayının tersini almak için donanımsal destek incelenmiştir. Aralıklı ve kayan nokta sayılarının her ikisinin de tersini hesaplayabilen birleşik bir ünite tasarlanmıştır. Aralıklı ve kayan noktalı sayılarının her ikisinin de tersini hesaplayabilen bu ünite, geçit (kapı) seviyesinde gerçekleştirilmiş ve test edilmiştir. Daha sonra, yaklaşık olarak geçit sayısını ve gecikmeyi tahmin edebilmek için unite sentez edilmiştir. vi","ABSTRACT Floating-point computations suffer from undetected errors due to rounding and catastrophic cancellation. Fast computers let programmers write numerically inten sive programs, but computed results can be far from the true results due to the accu mulation of errors in arithmetic operations. Interval arithmetic provides an efficient method for monitoring and controlling errors in numerical computations. With inter val arithmetic, each data value is represented by two floating-point numbers which correspond to the endpoints of an interval, such that the true result is guaranteed to lie on this interval. To support interval arithmetic, several software tools have been developed including interval arithmetic libraries, extended scientific program ming languages, and interval enhanced compilers. The main disadvantage of these software tools is their speed, since interval operations are implemented using function calls. To speed up interval arithmetic, hardware support for interval arithmetic oper ations (addition/subtraction, multiplication, division, and comparison/selection) has been developed. In this research, hardware support for interval reciprocal operation is investigated. The combined interval and floating-point reciprocal unit is designed to support both interval and floating-point reciprocal operations. The unit that supports reciprocal operation for interval and floating-point arithmetic is implemented at gate level and tested. Furthermore, the unit is synthesized to estimate the number of gates and delays."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,"ABSTRACT Cell activity is carried out by the interaction of various proteins. Complex interactions among proteins constitute molecular pathways, which are the mechanisms by which the living cells perform biological processes. Understanding pathways is crucial in revealing mechanisms of cellular activity, thus understanding the reasons behind genetic disorders. Domains, which are independent subunits of proteins, play an important role in protein interactions. The first method presented in this thesis uses association rule mining on protein interaction data to extract domain-domain interaction rules. The method was applied on a database of protein interactions, which resulted in rules, some of which are supported by biological knowledge. Microarray expression data is another data source to study protein interactions. Most microarray data analysis methods are based on clustering genes that show similar expression patterns. However, clustering results often need to be refined, which can be done either by using biological expertise or by integrating other biological data. The second proposed method integrates domain-domain interaction rules with microarray data. The method is based on a previously developed probabilistic model which unifies protein interaction and microarray data. Results show that integrating domain-domain interaction rules produces gene clusters of higher coherence. Finally, a paxallelization of the second proposed method and its implementation, together with performance results are presented. m"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Silüet ve Optik Üçgenleştirme ile Yüzey Geriçatımıiçin Hacimsel Bir Tümleştirme TekniğiÖzetEtkin bir geriçatım tekniği olan optik üçgenleştirme, hassas bir yöntem olarak bilinmeklebirlikte, kimi sorunlar ve eksiklikler de içerir. Örneğin yüzeyin lazer ışığını yansıtmaözelliklerinden ya da örtmeden kaynaklanan sorunlar, geriçatılan modelin yüzeyinde hatalarave delikler oluşmasına neden olabilmektedir. Öte yandan edilgen bir geriçatım yöntemi olansilüetten şekil tekniği ile nesnenin görsel zarfının, üzerinde herhangi bir deliğingözlemlenmediği tam ve gürbüz bir geriçatımını elde etmek mümkündür. Bu tezçalışmasında, silüet imgelerinden ve optik üçgenleştirmeden elde edilen geometrik bilgileritümleştiren melez bir yüzey geriçatım yöntemi geliştirilmiştir. Çıkış noktamız, optiküçgenleştirmeden gelen erim bilgisinin yakalayamadığı yerlerdeki yüzey bilgisini silüetbilgisiyle tamamlamak olmuştur. Önerilen yöntemde, kalibre edilmiş bir kamera aracılığıylaelde edilen nesne silüet ve lazer imgeleri, tümleştirmenin gerçekleşeceği sabit bir 3B genelkoordinat sistemine geri izdüşürülür. lk aşamada, silüetlerden yola çıkarak nesnenin sekizlibir ağaç yapısı ile ifade edilen hacimsel gösterimi elde edilir. Bu hacimsel ağaç yapısıgösterimi, ikinci aşamada, silüet tabanlı sistemlerde eksik olan içbükeylik bilgisini gerikazanmak üzere, erim bilgisini kullanarak yontulur. Yontulmuş hacimsel gösterimin nesneyüzeyine karşılık gelen her bir kübünün her köşe noktasında, ortalama bir eşyüzey değeri erimve silüet bilgilerinden ayrı ayrı elde edilmiş kısmi yüzey modelleri kullanılarak biriktirilir.Biriktirilmiş eşyüzey değerlerinden ?marching cubes? algoritması kullanılarak elde edilenüçgenleştirilmiş yüzeyin pürüzlerinden arındırılmasıyla, geriçatılan model son halini almışolur. Önerilen tekniğin başarımı birçok gerçek nesne üzerinde sınanmış ve elde edilensonuçlar tezde sunulmuştur.Danışman: Yücel Yemez Tarih:Enstitü Müdürü: Yaman Arkun Tarih:","A Volumetric Fusion Technique for SurfaceReconstruction from Silhouettes andOptical TriangulationAbstractOptical triangulation, an active reconstruction technique, is known to be an accuratemethod but has several shortcomings due to occlusion and laser reflectance properties ofthe object surface that often lead to holes and inaccuracies on the recovered surface.Shape from silhouette, on the other hand, is a passive reconstruction technique that yieldsrobust, hole-free reconstruction of the visual hull of the object. In this thesis, a hybridsurface reconstruction method that fuses geometrical information obtained fromsilhouette images and optical triangulation is proposed. Our motivation is to recover thegeometry from silhouettes on those parts of the surface which the range data fail tocapture. Silhouettes and laser range images of the object are acquired with a calibratedcamera and re-projected onto a fixed 3D world coordinate system where the fusionprocess takes place. A volumetric octree representation is first obtained from thesilhouette images and then carved by range points to amend the missing cavityinformation inherent in silhouette-based techniques. An average isolevel value on eachcorner of each surface cube in the carved octree structure is accumulated using partialsurface triangulations obtained separately from range data and silhouettes. The marchingcubes algorithm is applied for triangulation of the resulting isolevel surface and the finalshape is constructed by fairing the 3D model. The performance of the proposed techniqueis demonstrated on several real objects.Advisor: Yücel Yemez Date:Director: Yaman Arkun Date:"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Müzik türlerinin sınıï¬andırılması, müzik bilgi erişimi sistemlerinde ve farklı medya or-u u u sütamlarında ünemli bir araş olarak kullanılmaktadır. Oznitelik şıkarma ve sınıï¬andırıcıo c ctasarımı müzik türlerinin sınıï¬andırılmasında iki onemli problem olarak karşımıza şıkmaktadır.u u s cBu tezde, istatistiksel olarak daha iyi sınıï¬andırıcı oluşturabilmek icin Gauss karışımı modellis s(GMM) sınıï¬andırıcıların farklılıklar güzetilerek yükseltilmesi (Boosting) ve müzik türleri-o u u uarası benzerliklerini(IGS) kullanarak yükseltme algoritmasına alternatif bir yüntem sunul-u omaktadir. Sınıï¬andırıcı yükseltilmesi, her bir eğitilmiş sınıï¬andırıcıda sınıï¬andırılmasi zoru g solan ürneklerin yeniden modellenmesi ve yeni modellenen bu sınıï¬andırıcıların ardısık olarakobirleştirilmesi ile sağlanır. Bu tezde, gauss karışım modellerinin en büyük olabilirlik kestirimines g s uudayanan eğitim yünteminin sınıï¬andırıcı yükseltme yapısına uyumunu sağlayan yeni birg o u gteknik sunulmaktadir. Sınıï¬andırıcı yükseltilmesi düsüncesinden yola cıkarak geliştirdiğimizu uu s gIGS düzeneğinde ise, benzer kesişim üznitelik vektürleri olarak adlandırdığımız yanlış sınıï¬andırılanu g so o g svektürler biraraya geldikten sonra gauss karışımlarıyla yeniden modellenir ve müzik türlerio s u uarasındaki bozulma oranının düsük olması işin test aşamasında yanlış tanınan ürnekleruu c s s osistemden atılır. Tezin son kısmında ise müzik türlerinin birbirine benzerliklerini Euclidian-u uuzaklığı esas alarak otomatik bir şekilde bulan bir yüntem geliştirilmiştir.g s o s sii","Music genre classiï¬cation is an important tool for music information retrieval systemsand has been ï¬nding important applications in various media platforms. Two importantproblems of the automatic music genre classiï¬cation are feature extraction and classiï¬er de-sign. There are recent works on these problems with promising future research directions.This thesis investigates discriminative boosting of classiï¬ers to improve the automatic musicgenre classiï¬cation performance. Two-class of classiï¬ers, boosting of the Gaussian mixturemodel based classiï¬ers and classiï¬ers that are using the inter-genre similarity information,are proposed. Boosting is a technique that combines sequentially trained classiï¬ers, wherein each new classiï¬er a better modeling of hard-to-classify samples is done, and the over-all performance is boosted in the combined classiï¬er. In this thesis a novel extension isproposed to the maximum-likelihood based training of the Gaussian mixtures to integrateGMM classiï¬er into boosting architecture. Later, the boosting idea is modiï¬ed to bettermodel the inter-genre similarity information over the mis-classiï¬ed feature population. Oncethe inter-genre similarities are modeled, elimination of the inter-genre similarities reducesthe inter-genre confusion and improves the identiï¬cation rates. Finally, an auto-clusteringscheme is build to determine similar music genre types for hierarchical classiï¬er structure.Experimental results with promising identiï¬cation improvements are provided.ii"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde ses, dudak dokusu, dudak geometrisi ve dudak devinimlerini birle tiren yeni bir çok-kipli konu macı/konu ma tanıma sistemi sunulmaktadır. Konu macı ve konu ma tanımauygulamalarında ses, dudak ye inli i ve/veya dudak geometri bilgisini beraber kullananbirkaç çalı ma mevcuttur. Bu çalı mada konu macı tanıma ve konu ma okuma için, ses,dudak ye inlik ve/veya geometri bilgisi ile birlikte ya da bu bilgilerin yerine, açık dudakdevinim bilgisinin kullanımı önerilmekte; konu öznitelik seçimi ile ayırım analiziçerçevesinde incelenmektedir. çalı ma iki önemli soruya cevap aramaktadır: i) Açık dudakdevinim bilgisi yararlı mıdır? ve ii) Devinim bilgisi yararlı ise, sözü edilen uygulamalardaeniyi dudak devinim öznitelikleri nelerdir? Konu macılar arasında en yüksek ayrımı sa layanöznitelikler, konu macı tanıma probleminde eniyi dudak devinim öznitelikleri olmaklaberaber konu ma okumada eniyi öznitelikler, en yüksek fonem/kelime/deyi tanıma oranınaeri enlerdir. Ses doru u, mel frekans kepstral katsayıları ile katsayıların birinci ve ikincitürevleriyle gösterilirken, dudak doku kipi, dudak bölgesinin ye inlik de erlerinin 2B-AKD(Ayrık Kosinüs Dönü ümü) katsayıları ile ifade edilmektedir. Birden çok dudak devinimöznitelik adayı ele alınmaktadır: dudak bölgesi içinde ızgara-tabanlı yo un devinimöznitelikleri, dudak çevriti üzerinde devinim öznitelikleri ve son olarak dudak ekilparametreleri ile bunların bile imleri. Buna ek olarak, konu macı tanıma ve konu maokumada eniyi dudak devinim özniteliklerini belirlemek üzere iki basamaklı yeni birayrımsama analizi tanıtılmaktadır. Ses, dudak dokusu ve dudak devinim kiplerinintümle tirilmesi Güvenilirlik A ırlıklı Toplama karar kuralıyla gerçekle tirilmi tir. Deneyselsonuçlarda, önerilen ayırımsal analizin dudak deviniminin tek-kipli ba arımını oldukçageli tirdi i görülmektedir. Bunun yanında, ses ve dudak doku bilgisi ile birlikte açık dudakdevinim bilgisinin kullanımı, iki-kipli konu macı/konu ma tanıma sistemlerininba arımlarında ilave kazanım sa lamaktadır.","In this thesis a new multimodal speaker/speech recognition system that integrates audio, liptexture, lip geometry, and lip motion modalities is presented. There have been several studiesthat jointly use audio, lip intensity and/or lip geometry information for speaker identificationand speech recognition applications. This work proposes using explicit lip motioninformation, instead of or in addition to audio, lip intensity and/or geometry information, forspeaker identification and speech-reading within a unified feature selection and discriminationanalysis framework, and addresses two important issues: i) Is using explicit lip motioninformation useful? and ii) if so, what are the best lip motion features for these twoapplications? The best lip motion features for speaker identification are considered to be thosethat result in the highest discrimination of individual speakers in a population, whereas forspeech-reading, the best features are those providing the highest phoneme/word/phraserecognition rate. The audio modality is represented by the well-known mel-frequency cepstralcoefficients (MFCC) along with the first and second derivatives, whereas lip texture modalityis represented by the 2D-DCT coefficients of the luminance component within a boundingbox about the lip region. Several lip motion feature candidates are considered including densemotion features within a bounding box around the lip, lip contour motion features, lip shapefeatures, and combinations of them. Furthermore, a novel two-stage discriminant analysis isintroduced to select the best lip motion features for speaker identification and speech-readingapplications. The fusion of audio, lip texture and lip motion modalities is performed by the so-called Reliability Weighted Summation (RWS) decision rule. Experimental results show thatthe proposed discriminative analysis significantly improves the unimodal performance of thelip motion modality. Moreover, using explicit lip motion information in addition to audio andlip texture yields further performance gains in bimodal speaker/speech recognition systems."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖzetSayısal Şebeke (Grid) sistemleri atıl halde duran kullanıcı bilgisayarları yada yüksekbaşarım bilgisayarları gibi coğrafi olarak dağıtılmış kaynakları bir araya getirerek dahayüksek bir hesaplama gücü ortaya çıkarmayı hedefler. Bu yüksek hesaplama gücü büyükölçekte hesaplama ve veri gerektiren bilimsel problemlerin çözümünde kullanılabilir. Busistemlerde piyasa ekonomisi modellerinin kullanılması sayesinde, kişilerin kar maksadıyladaha çok kaynağı paylaşıma açacağı düşünülmektedir. Bu tezde, böyle bir sistemdekullanılmak üzere yeni bir iş zamanlama sezgisel (heuristic) yöntemi geliştirdik. Yöntemkısaca kullanıcının işlerini belirtilen zaman ve para kısıtlarında bitirmeye çalışıyor. Buamaçla, mevcut zaman ve para miktarını dikkate alarak zaman, para yada her ikisini birden eniyi şekilde kullanmaya çalışıyor. Bu yöntemi benzer çalışmalardaki yöntemlerle, simülasyonadayalı deneylerle karşılaştırıp başarılı sonuçlar elde ettik. Bunun yanı sıra, kaynak sahiplerininve kullanıcıların ekonomik yönden beklentilerinin karşılanması ve etkin iş zamanlamasonuçları üretebilmek için mal piyasası ve açık arttırma modellerine dayanan piyasa modellerigeliştirdik. Yaptığımız deneysel çalışmalar bu modellerin birbirlerine karşı olan avantaj vedezavantajlarını ortaya çıkardı.","AbstractComputational grids have emerged to exploit geographically distributed resourcessuch as clusters or idle personal computers to solve large-scale computational and datademanding scientific problems. It has been considered that developing computationalgrid economy systems in which users pay for using resources or services, would motivatepeople to share their resources making the computing power economically available thatthe communities require. In this thesis, we present a novel economic-based jobscheduling heuristic to be used in such a grid system. The heuristic basically tries tocomplete a sequential workflow or a parameter sweep application using one or moreoptimization strategies (cost, time or time-cost) according to the deadline and budgetconstraints of the user. The experimental results reveal that our heuristic outperforms therelated heuristics in the literature. Besides, we present two market models, a commoditymarket and a combinatorial double auction model, that are expected to meet therequirements of the resource owners and users in the economic respect and ensureefficient scheduling in a computational grid economy system. We performed simulationexperiments to compare the market models, and the experimental results demonstrate thatthe models have both advantages and drawbacks in terms of achieving social welfare inthe market."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖzetProteinler hayatın yapıtaşlarıdır. Bu yapıtaşlarının yapıları ise işlevlerinde, dolayısıyla da canlıorganizmaların işlevlerinde hayati bir rol oynar. Protein yapısının tespiti için her seferindeöncekilerden daha etkili yöntemler geliştirilse de, hala bir proteinin amino asit dizisini bulmakkatlanmış yapısını bulmaktan daha kolaydır ve bilinen protein yapıları ile bilinen dizilerin sayılarıarasındaki fark ivmelenerek artmaktadır. Yapı tahmin yöntemleri bu farkın kapanmasında yardımcıolabilir.Bu çalışmada, yapı tahmininin (hem ikincil hem üçüncül yapı) çeşitli yönlerini inceledik. Örüntütabanlı protein yapı tahmini kurallarından oluşan bir liste öğrenen bir işlemsel süreç (Açgözlü KararListesi öğrenici, veya İngilizce kısaltmasıyla GDL) geliştirdik. Sonuçta oluşan kural listeleri kısa,okunaklı ve yoruma açıktır. Yöntemimizin ikincil yapı tahminlerindeki başarımı, 513 protein zinciriiçeren artıksız bir veri kümesi üzerinde (CB513) 7-kat çapraz doğrulama kullanarak tasdiklendi.Yöntemin ikincil yapı tahminindeki genel üç-durumlu doğruluğu, sadece dizi bilgisini kullanarak%62.5 ve çoklu dizi hizalaması kullanarak %69.2. GDL'i bir proteinin üçüncül yapısını omurgasınıniki-düzlemli açıları phi ve psi üzerinden tahmin etmek için kullandık. Açıların gösteriminde kullanılanufalanmanın üçüncül yapı tahminlerinin başarımına etkisi incelendi.Mevcut yapı tahmini yaklaşımları, doğruluğu yorumlanabilirliğin önünde tutarak gitgidekarmaşıklaşan modeller inşa ediyorlar. İnanıyoruz ki, GDL modellerinin sadeliği, proteinlerin yereldizisi ve yapıları arasındaki ilişkiye bilimsel bir sezgi sağlamaktadır.Anahtar kelimeler: protein yapı tahmini, ikincil, üçüncül, açgözlü karar listesi öğrenicivolkan [at] su.sabanciuniv.edu","AbstractProteins are building blocks of life. Structure of these building blocks plays a vital role in theirfunction, and consequently in the function of living organisms. Although, increasingly effectivemethods are developed to determine protein structure, it is still easier to determine amino acidsequence of a protein than its folded structure and the gap between number of known structures andknown sequences is increasing in an accelerating manner. Structure prediction algorithms may helpclosing this gap.In this study, we have investigated various aspects of structure prediction (both secondary andtertiary structure). We have developed an algorithm (Greedy Decision List learner, or GDL) that learnsa list of pattern based rules for protein structure prediction. The resulting rule lists are short, humanreadable and open to interpretation. The performance of our method in secondary structure predictionsis verified using seven-fold cross validation on a non-redundant database of 513 protein chains(CB513). The overall three-state accuracy in secondary structure predictions is 62.5% for singlesequence prediction and 69.2% using multiple sequence alignment. We used GDL to predict tertiarystructure of a protein based on its backbone dihedral angles phi and psi. The effect of anglerepresentation granularity to the performance of tertiary structure predictions has been investigated.Existing structure prediction approaches build increasingly sophisticated models emphasizingaccuracy at the cost of interpretability. We believe that the simplicity of the GDL models providesscientific insight into the relationship between local sequence and structure in proteins.Keywords: protein structure prediction, secondary, tertiary, greedy decision list learnervolkan [at] su.sabanciuniv.edu"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,"I present a distributed implementation of a novel approach to multipointvideoconferencing rooted in a peer-to-peer model of media transmission. In this thesis,principals of the peer-to-peer system that I developed are described together with itsperformance evaluation. Besides, a formal verification for the proposed approach hasbeen done. The system focuses on the needs of participants with symmetric, lowbandwidth connections to the Internet. It does not require additional hardware, as inMultipoint Control Units, or network infrastructure support such as multicast. Withalmost no additional demands on the networking and computing resources needed for apoint-to-point videoconference, the new approach would be able to extend a point-to-point conference into a multipoint videoconference. In comparison to offered approach, Ialso include a brief survey on existing peer-to-peer solutions for interactive mediaapplications.Mostly, it is not fair to share equal responsibility among all users, since it is a wellknown fact that Internet provides an environment where various users (categorizedaccording to the computing power or bandwidth capacity) interact together. So, for thesake of performance and solution applicability, it will be good to share responsibilityamong users according to their available resources. Thus, this thesis also offers anoptimization over the proposed peer-to-peer video dissemination solution where routingis done considering the hosts? resources."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, dosya sistemi ya da veritabanı depolama yünetim birimi gibiokoşut zamanlı ortamda şalıştırılan bir yazılım bileşeninin şalıştırılabilir birs cs s csbelirtime uygunluËunu denetlemek işin bir şalışma zamanı denetleme tekniËig c cs gsunulmaktadır. Belirtim her bir veri yapısı operasyonu işin atomik bir metotcsaËlamaktadır. Belirtim gerşekleştirimden ayrı olarak saËlanabileceËi gibig c s g ggerşekleştirimin atomik olarak şalışacak hale getirilmiş bir versiyonu da belir-c s cs sËtim olarak kullanılabilir. TekniËimiz iki aşamadan oluşmaktadır. Ilk aşamada,g s s sgerşekleştirim şalışma sırasında yürütme bilgisini bir günlüËe kaydedecekc s cs uu u ugËşekilde donatılır. Ikinci aşamada, ayrı bir sınama birimi günlüËe kaydedilmişs s u ug syürütme bilgisini kullanarak belirtimin bir ürneËini şalıştırır ve kayıtlı yürüt-uu o g cs uuËmenin belirtime ait yürütmeye uygunluËunu denetler. Ilgili tekniËin geneluu g guygulanabilirliËine ve ülşeklenebilirliËine, ayrıca denetlenen bileşenin koşutg oc g s szaman üzelliËine ve başarımına etkisinin en aza indirilmesine ünem verilmiştir.o g s o sSonuş olarak, koşut zamanlı programlar işin standart test yüntemine ünemlic s c o oyenilikler getiren bir doËrulama metodu geliştirilmiştir.g s sGerşekleştirimin belirtime uygunluËu arıtma adında bir doËruluk kri-c s g gteri olarak bişimsel-leştirilmiştir. Bu kritere güre gerşekleştirimin her birc s s o c syürütmesi işin ona denk belirtime ait bir yürütme bulunmalıdır. Yünteminuu c uu oyeni ozellikleri arasında en ünemlisi girdi/şıktı arıtma ve gürüş arıtma adındaü o c o usiki arıtma kriterinin tanımlanmasıdır. Boxwood adında endüstriyel ülşekteu ocveri yapısı gerşekleştirimleri işeren bir sistemin doËrulanması şalışmalarıc s c g csbu tanımlara motivasyon kaynaËı olmuştur. Girdi/şıktı arıtma ve gürüşg s c o usarıtmanın denetlenmesi VYRD (VerifYing concurrent programs by RuntimeReï¬nement-violation Detection) adlı doËrulama aracı bünyesinde gerşekleşti-g u c srilmiştir. VYRD, Boxwood sisteminin, Java sınıf kütüphanesinden birkaşs uu csınıfın doËrulanması amacıyla uygulanmıştır. Sonuşta, Boxwood sistemindeg s cdaha ünce tespit edilmeyen bir hataya ek olarak, Java sınıf kütüphanesindeo uuve test amaşlı yazılan ürneklerde daha ünceden bilinen hatalar yakalanmıştır.c o o sDeneyimler, ünerilen tekniËin pratikte uygun bir şalışma maliyeti olduËunuo g cs gortaya koymuştur.s","In this thesis, we present a runtime technique for checking that a con-currently accessed data structure implementation, such as a ï¬le system orthe storage management module of a database, conforms to an executablespeciï¬cation that contains an atomic method per data structure operation.The speciï¬cation can be provided separately or a non-concurrent, ?atom-ized? interpretation of the implementation can serve as the speciï¬cation.The technique consists of two phases. In the ï¬rst phase, the implementationis instrumented in order to record information into a log during execution.In the second, a separate veriï¬cation thread uses the logged information todrive an instance of the speciï¬cation and to check whether the logged exe-cution conforms to it. We paid special attention to the general applicabilityand scalability of the techniques and to minimizing their concurrency andperformance impact. The result is a lightweight veriï¬cation method thatprovides a signiï¬cant improvement over testing for concurrent programs.We formalize conformance to a speciï¬cation using the notion of reï¬ne-ment: Each trace of the implementation must be equivalent to some trace ofthe speciï¬cation. Among the novel features of our work are two variationson the deï¬nition of reï¬nement appropriate for runtime checking: I/O andview reï¬nement. These deï¬nitions were motivated by our experience withindustrial-scale concurrent data structure implementations in the Boxwoodproject, a novel storage infrastructure. I/O and view reï¬nement checkingwere implemented as a veriï¬cation tool named VYRD (VerifYing concurrentprograms by Runtime Reï¬nement-violation Detection). VYRD was appliedto the veriï¬cation of Boxwood, Java class libraries. It was able to detectpreviously unnoticed subtle concurrency bugs in Boxwood, the known bugsin the Java class libraries and manually constructed examples. Experimentalresults indicate that our techniques have modest computational cost."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez `Uyarlanır Ölçeklenebilir Video Kodlama' genel konusu altında üç ana başlıkta hazırlanmıştır.Birinci çalışmada etkili zamansal ölçeklenebilirlik sağlanması amacıyla H.264 standardı içerisindedevinim dengeli zamansal filtreleme(DDZF) öneriyoruz. DDZF geleneksel olarak dalgacıkdönüşümüyle yapılan tam ölçeklenebilir video kodlamasında kullanılır. Ancak devinim dengeli 5-3dalgacıkları kaldırma işlemi yapılarak filtreleme görüntü değişimi olan yerlerde ve videoçerçevelerinde yeni çıkan bölgelerin kodlanmasında başarısız olmaktadır. H.264 standardı iki yönlüdevinim dengeleme için uyarlanabilir blok büyüklüğü, ileri-geri ve iki yönlü modlar arasındauyarlanabilir mod seçimi,bloksuzlaştırma filtresi ve örtüşmeli devinim dengeleme gibi gelişmiştekniklere sahiptir. Bu nedenle devinim dengeli 5-3 dalgacık filtresine benzer şekilde H.264 standardıiçerisinde blok tabanlı uyarlanabilir DDZF uygulamak için bir görüntü grubu yapısı öneriyoruz. DiğerDDZF tabanlı dalgacık dönüşümü video kodlayıcıların sonuçlarıyla birlikte karşılaştırmak üzeresonuçlarımızı sunuyoruz. Önerdiğimiz DDZF yapısı `Sıralamalı B-Resimleri' ya da ` ZamansalPiramit' ismiyle H.264/AVC referans yazılımına da dahil edilmiştir.İkinci çalışmada ölçeklenebilir video kodlamada içeriğe bağlı en iyi ölçekleme operatörü seçimiüzerinde çalışılmıştır. Ölçeklenebilir video kodlayıcıları, her biri içeriğe ve bit-hızına bağlı olarakdeğişik tipte ve miktarda bozuluma neden olan zamansal, uzaysal ve kalitesel olmak üzere üç çeşitölçeklenebilirlik olanağı sağlamaktadır. Genelde bir tek ölçekleme operatörü videonun bütünkısımları için uygun olmamaktadır; bu nedenle videonun değişik içerikteki her bir parçası içinölçekleme operatörü o parçanın içeriğine bağlı olacak şekilde değiştirilmelidir. Bu çalışmada, videobir içerik inceleme metoduyla içeriğine bağlı olarak değişik kısımlara ayrılmış kabul edilmiş ve her birzamansal video parçası en düşük bozunuma sonuç veren en-iyi ölçekleme operatörüyle ölçeklenmiştir.Bit-hızı azalımı, uzaysal genişlik değişimi ve zamansal ölçeklemenin yarattığı bozulum, düzlük,blokluluk, zamansal atlama ve bozunukluluk metrikleriyle ölçülmüştür. En-iyi ölçekleme operatörüayrı bozulum metriklerinin lineer kombinasyonuyla oluşturulan genel bozulum metriğine göre endüşük bozulumu veren operatör olarak bulunmuştur. Bu lineer kombinasyonun katsayıları içeriğe göreayarlanarak bulunmuştur. Önerilen bozulum metriği ve en-iyi operatör bulma prosedürü futbolvideolarıyla iki öznel test yapılarak gerçeklenmiştir.Üçüncü kısımda ise içeriğe ve kanal koşullarına uyarlanır çok-tanımlamalı video kodlama yöntemi ileuyarlanır video iletimi üzerinde çalışılmıştır. İletişim kanallardaki sıkışmanın neden olduğu paketkayıpları ve gecikme değişimleri, gecikmeye duyarlı multimedya akışı işlemlerini zorlaştırmaktadır.Çok tanımlı video kodlama yöntemleri ile paket kayıplarının yarattığı bu etki azaltılabilmektedir.Ancak bu zamana kadar geliştirilen çok tanımlı video kodlama teknikleri, kanal koşullarına uyumsağlayamamakta, zaman içerisinde tanım sayısının, tanımların içerisine eklenen gereksiz bit miktarininve her tanım için harcanan bit miktarının değiştirilmesine izin vermemektedir. Önerilen çok tanımlıvideo kodlama tekniği bütün bahsedilen değişikliklere olanak sağlamakta ve birçok çok tanımlı videokodlama tekniğinden daha iyi sıkıştırma performansı sergilemektedir. Bu çalışmada önerilensıkıştırma tekniği diğer benzer tekniklerle birçok değişken koşulda karşılaştırılmış, önerilen tekniğindiğer tekniklere hem sağladığı çok yönlü kanala uyarlanabilme özelliği açısından hem de sıkıştırmaperformansı /video görüntü kalitesi (PSNR) açısından üstün olduğu gösterilmiştir.Danışman: Tarih:Enstitü Müdürü: Tarih:","This thesis is composed of three main parts which include three contributions in slightly differentfields, all lying on the same framework: Adaptive Scalable Video Coding. First part is aboutintegration of motion compensated temporal filtering (MCTF), the basis for temporal scalabilityin scalable video coding methods, to the latest non-scalable video compression standard, i.e.,H.264/AVC. We propose a GOP structure to implement block-based adaptive MCTF within theH.264/AVC syntax using stored B-pictures, similar to the motion-compensated 5/3 waveletfiltering. We provide experimental results to compare the results of our proposed codec withthose of other scalable wavelet video coders which use MCTF. The proposed scheme is alsointegrated into H.264/AVC reference software as `Hierarchical B pictures? or `TemporalPyramid? and it is currently under investigation of MPEG Core Experiments for the upcomingScalable Video Coding standard (SVC).Secondly, we worked on content adaptive scalability type selection problem. State of the artscalable video coders provide different options, such as temporal, spatial and SNR scalability,where bitrate reduction using each scalability type results in different kinds and/or levels of visualdistortion depending on the content and the bitrate. In most cases, a single scalability type doesnot fit the whole video well, and scaling option selection can be optimized for each temporalsegment depending on the content of that segment and the target bitrate. This dependencybetween selection of scalability type, video content, and bitrate is not well investigated in theliterature. In this work, assuming that the video is temporally segmented by some content analysisscheme, we propose a method to choose the best scaling type for each temporal segment thatresults in minimum visual distortion among temporal, spatial and SNR scalability for fullyembedded scalable video coders. We employ an objective distortion measure that consists of alinear combination of four component measures, which are a flatness measure, a blockinessmeasure, a blurriness measure, and a temporal jerkiness measure, to quantify artifacts caused bybitrate reduction by spatial size reduction, frame rate reduction, and quantization parameterscaling. Two subjective tests have been performed to validate the proposed procedure for shot-based selection of optimal scalability type on soccer videos. Soccer videos whose bitrate arereduced from 600 kbps to 100-300 kbps by the proposed content-adaptive selection of scalabilitytype have been deemed visually superior to those whose bitrates are reduced by a singlescalability option for the entire test sequence.Finally, we worked on adaptive peer-to-peer (P2P) streaming using scalable multiple descriptioncoding. Efficient P2P video streaming is a challenging task due to time-varying nature of both thenumber of available peers and network/channel conditions. To this effect, we propose i) a newflexible scalable multiple description coding (MDC) method, where the number of descriptions,and the rate and redundancy level of each description can be adapted on the fly (by post-processing of a fully-embedded scalable coded bitstream), and ii) a new adaptive TCP FriendlyRate Controlled (TFRC) P2P streaming system based on this new MDC scheme. Theoptimization of the design parameters of the proposed MDC scheme according to networkconditions is discussed within the context of the proposed adaptive P2P streaming framework,where the number and quality of available streaming peers/paths are a priori unknown and vary intime. Experimental results, by means of NS-2 network simulation of a P2P video streamingsystem, show that adaptation of the number and rate of descriptions/layers and the redundancylevel of each description according to network conditions yields significantly superiorperformance when compared to other scalable MDC schemes using a fixed number ofdescriptions/layers with fixed rate and redundancy level."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek Lisans Tez Özet FormuÖğrencinin Adı : Başak MutlumAna Bilim Dalı : Bilgisayar MühendisliğiTez Başlığı : Sözdizim ve Anlam Benzerliğine Dayalı Sözcük AnlamıBelirlemeÖzetSözcük Anlamı Belirleme, anlamı belirsiz bir sözcüğe bulunduğu bağlama göre anlambelirlenmesi işlemidir. Sözcük Anlamı Belirleme henüz çözümü bulunamamış bir problemdir.Diğer doğal dil işleme yöntemlerinin de gereksinimlerini karşılayabilmek için bu problemeetkili bir çözüm bulunmalıdır. Bugüne kadar olan sözcük anlamı belirleme çalışmalarındahem öğreticiyle öğrenme hem de öğreticisiz öğrenme algoritmaları denenmiş; öğreticiyleöğrenme yöntemlerinden daha başarılı sonuçlar elde edilmiştir. Fakat ilk anlam buluşsalı veöğreticiyle öğrenme yöntemleri doğal sınırlarına ulaştığından, öğreticisiz öğrenme yöntemleridaha ayrıntılı incelenmelidir.Bu tezde, anlam benzerliğine ve sözdizimine dayalı bir öğreticisiz öğrenmealgoritması kullanılmıştır. Bu algoritma, iki farklı sözcük benzer yerel bağlamlardakullanılırsa benzer anlamlara sahip olurlar mantığını kullanmaktadır. Eğitim evresinde, 100milyon sözcükten oluşan bir eğitim verisi ayrıştırılmış ve yerel bağlama dayalı özniteliklerbelli kurallar doğrultusunda özütlenmiştir. Anlamı belirsiz sözcükler ve bu sözcüklerle benzerbağlamda bulunan sözcükler arasındaki benzerlik değerleri hesaplanmıştır, bir enbüyütmealgoritması yardımıyla sözcüklerin anlamları bulunmuştur. Sistemin performansıSENSEVAL-2 ve SENSEVAL-3 verileri üzerinde denenmiş ve %59 başarı elde edilmiştir.Danışman: Deniz Yüret Tarih: 22.09.2005Enstitü Müdürü: Tarih:","M.S. Thesis Abstract FormName of the Student : Basak MutlumProgram of Study : Computer EngineeringThesis Title : Word Sense Disambiguation Based on SenseSimilarity and Syntactic ContextAbstractWord Sense Disambiguation (WSD) is the task of determining the meaning ofan ambiguous word within a given context. It is an open problem that has to besolved effectively in order to meet the needs of other natural languageprocessing tasks. Supervised and unsupervised algorithms have been triedthroughout the WSD research history. Up to now, supervised systems achievedthe best accuracies. However, these systems with the first sense heuristic havecome to a natural limit. In order to make improvement in WSD, benefits ofunsupervised systems should be examined.In this thesis, an unsupervised algorithm based on sense similarity andsyntactic context is presented. The algorithm relies on the intuition that twodifferent words are likely to have similar meanings if they occur in similar localcontexts. With the help of a principle-based broad coverage parser, a 100-million-word training corpus is parsed and local context features are extractedbased on some rules. Similarity values between the ambiguous word and thewords that occurred in a similar local context as the ambiguous word areevaluated. Based on a similarity maximization algorithm, polysemous words aredisambiguated. The performance of the algorithm is tested on SENSEVAL-2 andSENSEVAL-3 English all-words task data and an accuracy of 59% is obtained.Advisor: Deniz Yuret Date: 22.09.2005Director: Date:"
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETÇE Kablosuz teknolojilerdeki hızlı gelişmeler, yarının ağlarının çok yönlü ve farklı nitelikteki sistem yüklerini uyarlamak yaklaşımlarla desteklemesini gerektirir. Geleceğin kablosuz sis temleri, küresel veri ağı internetle muntazam bağlantıyı sağlamak amacıyla tamamıyla İP mimarisini destekleyecektir. Kablosuz ağlar üzerinden taşınacak veri tipi de zengin çoklu- ortam ve etkileşimli içeriğe dönük olarak paket anahtarlamalı yapıya dönüşmektedir. Bu yüzden kablosuz ağların tasarımı fiziksel katmandan uygulama katmanına kadar içerik ak tarımının pek çok yönünü ele almalıdır. Bu noktada çapraşık-katman tasarımı geleceğin kablosuz sistemleri için çok önemlidir. Arka-plan verisi, etkileşimli web servisleri veya du raksız ses/görüntü gibi veriler artık daha fazla paket verisine yöneldiği için, hem kullanıcı isteklerini karşılamak hem de genel sistem başarmamı artırmak için verimli kaynak yönetim mekanizmaları geliştirilmelidir. Hizmet niteliği bu yönden hem kablolu hem de kablosuz sistemler için önemli bir unsurdur. Tezde yer alan bu çalışma, yeni standart haline gelen IS-856 sistemi için hizmet kalite sine vakıf kaynak yönetimi algoritmaları üzerinde yoğunlaşmıştır. Hizmet kalitesine vakıf algoritmalar önerilmiş olup literatürde bulunan algoritmalara göre çeşitli sistem başarım değerlendirmeleri yapılmıştır. Ayrıca geniş bantlı bir sistem analizi de 3x çoklu taşıyıcı kul lanan IS-856 sistemi için yapılmıştır. 3. nesil sistem değerlendirmelerinin yanı sıra, 3. nesil ve kablosuz yerel ağların bütünleştirme mekanizmaları da incelenmiştir. Bütünleşik kaynak yönetimi/rota tespiti algoritma başarımı da ayrıca çalışılmıştır. vı","ABSTRACT Rapid developments in the wireless technologies require the networks of tomorrow be more versatile and able to handle different types of load with adaptive approaches. The future wireless systems will employ all-IP architectures to provide seamless interconnection with the global data network, the Internet. The type of content that will be carried over the wireless networks is also changing towards rich multimedia and interactivity via the use of packet switching. Hence the design of these wireless systems requires the consider ation of many aspects of content transport from physical layer issues to application layer requirements. At this point a cross-layer design is essential for future wireless systems. Since all types of content, whether they be background data, interactive web services or streaming audio/video, are becoming more packet oriented, efficient resource management methodologies should be developed to satisfy user requirements as well as increase overall system performance. QoS is thus an important issue in the design of data networks, whether wireless or wireline. The study involved in this thesis focuses on the QoS aware resource management al gorithms for the recently standardized IS-856 data only cellular system. QoS aware algo rithms are proposed and numerous system evaluations are performed with respect to the algorithms already present in the literature. A wideband study is also conducted by utilizing 3x MC option for the IS-856 system. Besides 3G system evaluations, 3G/WLAN integra tion methodologies are also investigated. System performance of an integrated schedul ing/routing algorithm is studied."
Koç Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET Verimli telsiz paket veri erişimini sağlamanın en iyi yolu çoklu-kullanıcı çeşitlemesinin kullanılmasıdır. IS-856 sistemi uyarlanmaz kodlama ve modülasyon türlerini kullanarak sistem kaynaklarım belli bir zaman diliminde sadece bir kullanıcıya ayırır. Bu sonuca dayanarak son zamanlarda IS-856 sistemi geliştirilmiştir. IS-856 sisteminde kaynak ayırımım çizelgeleme algoritması sağlar ve onun uygun tasarımı, iyi sistem performansının sağlanması için belkide en önemli yönlerinden birisidir. Tez, beş tanesi yeni olan çok sayıda algoritmayı açıklamaktadır. Önerilen algoritmalar optimal gecikme performanslarından uzaklaşmadan yüksek sistem iş çıkarma performansına ulaşarak en iyi toplam performansa sahip gözükmektedirler. Tez, IS-856 sisteminde her biri farklı hizmet niteliklerine gereksinim duyan çok sayıda servisi verilebileceğinin kolayca ayarlanabileceğim göstermiştir. Kapsamlı performans değerlendirmeleri iyi sistem performansının çoklu-servis senaryosunda da desteklendiğini göstermiştir. Ayrıca, tez çoğula iletim servisinin IS-856 sisteminde sağlanmasını açıklamıştır. Seçilen çizelgeleme algoritması birbiri ardına sisteme giren kullanıcıların iş çıkarma yeteneğinden başka maruz kalman ortalama gecikmelerimde etkiler. Tezin önerdiği yeni giriş metodu, Ayni Zamanli İki Kullanicili Sistem, ve onun çizelgeleme algoritmalari gecikme değerlerini yaklaşık olarak %50 azaltmaktadır. Günümüzdeki 3. nesil taşıyıcı teknolojileri kapsama, bantgenişliği, hizmet nitelikliği ve maliyet açısından son kullanıcının bütün ihtiyaçlarını karşılayamamaktadır. 4. nesil ağlar önceki teknolojilerin dezavantajlarım ortadan kaldıran ve çok büyük sayıda giriş metodu içeren heterojen ağlardır. Bundan dolayı, tezde günümüzde olan baza ağ topolojilerinin birleştirilmesinden oluşan yeni giriş metotları gösterilmiştir. Bu giriş metotları gezgin kullanıcıların ofislerinden ve evlerinden uzakta olduklarında telsiz veri bağlantısını sağlarlar. Öngörülen sistem, IS-856 ağ iş çıkarma gücünü %50'den fazla arttırır.","ABSTRACT Efficient wireless packet data access is possible by exploiting the multi-user diversity using an opportunistic multiple access system that allocates system resources to one user at a time while employing adaptive coding and modulation. Based on this outcome, recently the IS-856 system was developed. A scheduling algorithm provides resource allocation in the IS-856 system, and its proper design is perhaps one of the most crucial aspects for ensuring good system performance. The thesis presents a number of such algorithms, five of which are novel. The proposed algorithms appear to have the best overall performance of achieving high system throughput without diverging much from the optimal latency performance. The thesis shows that the IS-856 system can easily be adjusted to provide a multitude of services, each with different QoS requirements. Extensive performance evaluations show that good system performance can be maintained in the multi-service scenario. The thesis also presents the means of providing multicast service provisioning in the IS-856 system. The selection of the scheduling algorithm influences not only the system throughput but also the average delay exposed by users in between successive accesses to the system. The thesis proposes new access method, Two Users At A Time System, and its scheduling algorithms which decrease latency figures approximately 50%. Present 3G carrier technologies cannot separately cover all the demands of the end- user in terms of coverage, bandwidth, quality of service (QoS) and cost. The 4G networks are heterogeneous networks that eliminate previous technologies drawbacks and contain large number of different access methods. Therefore, the thesis demonstrates new access methods are needed to combine some of the existing network topologies. These access methods provide wireless data connectivity for nomadic users when away from their offices and homes. The envisioned corporation increases the IS-856 network throughput more than 50%. IV"
