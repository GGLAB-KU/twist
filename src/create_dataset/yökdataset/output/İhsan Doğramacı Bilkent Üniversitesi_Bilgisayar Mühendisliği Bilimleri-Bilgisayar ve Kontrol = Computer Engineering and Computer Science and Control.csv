university,konu,tr,en
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan müdahelesi gerektirmeyen ve çok fazla miktarda mikroskop görüntüleri elde edebilen yüksek çıktılı mikroskopi sistemleri son zamanlarda popülerlik kazanmaktadır. Öte yandan, bu kadar fazla miktarda görüntünün geleneksel yöntemlerle analiz edilmesi aylarca sürebileceği için, bu geleneksel yöntemlerin uygulanması pratikte neredeyse imkansızdır. Ayrıca, analiz tamamen, analiz eden ki ̧sinin yorumuna bağlı olduğu için, sonuçlar önemli miktarda değişkenlik gösterebilir. Buna çözüm olarak objektif ve hızlı olan otomatik karar destek sistemlerinin geliştirilmesine olan ilgi günden güne artmaktadır. Bu sistemler, analizlerini hücre seviyesinde gerçekleştirdiği için hücre bölütlemesi modellerine gereksinim duymaktadır. Hücre bölütlemesinin başarısı da bütün sistemin başarısını etkilemektedir. Başarılı bir hücre bölütlemesi modeli elde edebilmek için üstesinden gelinmesi gereken zorluklar bulunmaktadır. Bu zorluklardan bir tanesi hücrelerin tabakta çok katmanlı çoğalması sonucu görüntü üzerinde hücrelerin kümelenmiş görülmesidir. Bu hücreler birbirinden ayrılması gerektiği için bunların bölütlenmesi ek bir efor gerektirmektedir. Diğer bir zorluk, hücre içinde bulunan piksellerin yoğunluk değerlerinin görüntü üzerinde homojen dağılmaması ve birbirine dokunan hücrelerde, hücre sınırlarındaki piksel yoğunluk değerlerinin yeterli farklılıkta olmaması gibi kusurlardır. Bir diğer zorluk ise hücrelerin morfolojik karakterlerinin farklılık göstermesidir. Hücre hattı tipine bağlı olarak hücreler farklı görünümlere sahip olabilir. Farklı hücre görünümlerinde çalışabilecek ve hücre içindeki kusurları çözebilecek kapsamlı bir hücre bölütlemesi modeli geliştirmek çözüme açık ve zor bir problemdir. Bu zorlukların üstesinden gelebilmek için hücre bölütlemesini iki kısımda ele almaktayız. (1) Mikroskopi görüntüleri için, hücre bölütleştirme problemini basitleştirmede bize yardımcı olabilecek yeni gösterimler bulmaya odaklanmaktayız. Bu sayede, hücre içindeki kusurların ve hücre görünümündeki farklılıkların üstesinden gelebileceğimizi ve hücre konumlarını daha iyi vurgulayabileceğimizi düşünmekteyiz. (2) Hücreler için kusursuz bir gösterim bulmanın pratikte neredeyse imkansız olması motivasyonuyla, daha gelişmiş bir gösterim metodu geliştirme üzerine odaklanmaktayız. Dolayısıyla, gösterim kusurlarının üstesinden gelebilecek, daha gelişmiş hücre bölütleme tekniklerinin geliştirilmesi üzerinde çalışmaktayız. Bu bağlamda, bu tez, üç yeni hücre bölütlemesi modeli sunmaktadır. Bunlardan ikisi, aynı zamanda, yeni gösterim modeli de ortaya koymaktadır. Deneylerimizde, sunduğumuz algoritmaları, floresan ve faz kontrast mikroskoplarında elde edilmiş görüntülerde test ettik ve önceki hücre bölütlemesi metotlarıyla karşılaştırdık. Deneylerimiz, önerdiğimiz algoritmaların hücre bölütlemesinde daha etkin ve bahsi geçen zorluklara karşı daha gürbüz olduğunu göstermektedir.","High-throughput microscopy systems have become popular recently, which facilitate to acquire boundless microscopy images without requiring human intervention. However, the analysis of such amount of images using conventional methods is nearly impractical since the analysis can take up to months. Additionally, a considerable amount of observer variability may occur since the analysis completely relies on interpretation of the analysts. As a remedy for that, automated decision support systems, which are objective and rapid, have gained more attention. Since these systems conduct analyses at cellular level, they require a cell segmentation model, results of which directly affect the performance of the entire system. There are several challenges in cell segmentation, each of which should be addressed carefully in order to have an accurate cell segmentation model. One challenge is that cells can be grown in multilayer on the plate, which makes them appear as clusters on the image. Segmentation of these cells requires extra effort since they should be splitted from each other. Another challenge is the imperfections on the image such as inhomogeneities of pixel intensities in a cell and insufficient pixel intensity differences at the border of overlapping cells. Yet-another challenge is the heterogeneity in the morphological characteristics of cells. Depending on cell line types, cells may appear in various outlooks. Developing a generic cell segmentation model, which can handle different cells outlooks and imperfections, is an open and challenging problem. In order to tackle with these challenges, we deal with the cell segmentation problem in two parts: (1) We focus on finding a new representation for microscopy images, helping us simplify the cell segmentation problem, so that imperfections in cells and inhomogeneities in their visual properties can be alleviated, and cell locations can be emphasized better. (2) We focus on developing a more advanced cell segmentation method, with the motivation that it is almost impossible to obtain a perfect representation in practice. Thus, we work on developing more sophisticated cell segmentation techniques that overcome deficiencies on the representation. To this end, this thesis introduces three new cell segmentation models, two of which introduce a new cell representation technique as well. In our experiments, we tested our algorithms on various microscopy images obtained under the fluorescence and phase contrast microscopes and compared them with the previous cell segmentation methods. Our experiments show that the proposed algorithms are more effective in segmenting cells and more robust to the aforementioned challenges."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"'Otobüs faktörü', 'kamyon faktörü' olarak da bilinir, bir yazılım projesinin ne kadar savunmasız olduğunu ölçen bir metriktir ve projenin durması için kaç kişinin projeden ayrılması gerektiğini belirler. Yazılım projeleri için otobüs faktörünü hesaplama üzerine mevcut araştırmalar bulunsa da, otobüs faktörünü görselleştirmek üzerine sınırlı çalışma vardır. Görsel olarak sunulan sayısal otobüs faktörü, karar vericilerin projedeki iş yükünü ve bilgi dağılımını yönetmelerine, ayrıca planlama ve işe alım kararlarında yardımcı olabileceğine inanıyoruz. Bu tezde, Git geçmişinden yararlanarak, yazılım projeleri için otobüs faktörü ve projeye katkılarını görselleştirmek için BFViz adlı bir araç geliştirilmiş ve değerlendirilmiştir. Bu araç, etkileşimli gezilebilir bir ağaç haritası, filtreleme, bireysel katkı verileri ve katkıda bulunanın ayrılışı simülasyonu ile bir dosya tarayıcısı benzeri bir arayüz sunan bir web uygulamasıdır. Bu uygulama, dört kullanıcı değerlendirme turu ile doğrulanmıştır. Proje sahiplerinden ve mühendislik yöneticilerinden geliştiricilere kadar çeşitli kullanıcılar, üzerinde çalıştıkları açık kaynaklı bir projede aracı kullanarak görevleri tamamlamıştır. Kullanıcılardan, yarı yapılandırılmış bir mülakat ve özellik sıralama etkinliği kullanılarak geri bildirim alınmıştır. Görevlerin genel tamamlama oranı %79.55 olarak hesaplanmıştır. Tüm vaka çalışması katılımcıları, metin raporları yerine BFViz'i kullanılarak otobüs faktörü verilerini anlamayı tercih etmiştir. Ortalama sıralama ile en üst üç özellik, katkıda bulunanların listesi, dosya ve klasörlerin görselleştirmesi ve simülasyon modu olmuştur.","'Bus factor', also known as 'truck factor', is a measure of how vulnerable a software project is based on the minimum number of people who would have to leave the project (be 'hit by a bus') for it to stall. There is existing research on how to calculate bus factor for software projects but limited work on visualizing the bus factor. We believe providing visualization along with conventionally provided numerical bus factor results will help decision-makers manage the workload and knowledge distribution across the project and also help in planning and hiring decisions. This thesis proposes, implements, and evaluates a tool named BFViz to visualize bus factor and contributions for software projects from pre-processed Git history. It is a web application that provides a file-browser-like interface with an interactively navigable treemap. Additionally, it has filename-based filtering, individual contribution data for files and folders, and simulation of contributor departure. The tool is validated with a round of four user evaluations where users, ranging from project owners and engineering managers to developers, complete tasks using the tool on an open-source project that they are involved in and provide feedback with a semi-structured interview and a feature ranking activity. The overall task completion rate for the tasks was 79.55%. All case study participants preferred BFViz over text reports to understand bus factor data. The top three features, by mean ranking, were the contributors' list, the files and folders' visualization, and the simulation mode."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Görüntü tamamlama, görüntülerden istenmeyen pikselleri silmeyi ve bunları anlamsal olarak tutarlı ve gerçekçi bir şekilde doldurmayı içeren bir görevdir. Bu göreve yönelik iki yeni yaklaşım sunuyoruz. İlk yaklaşım, bir görüntüden hangi nesnenin çıkarılacağını belirlemek için doğal dil girdisi kullanıyor. Bu görev için GQA-Inpaint adlı bir veri seti oluşturuyor ve bu veri seti üzerinde metin komutuna dayalı olarak görüntülerden nesneleri çıkarabilen bir difüzyon tabanlı görüntü tamamlama modeli eğitiyoruz. İkinci yaklaşım, gerçekçi görüntü tamamlama ve düzenleme amacıyla silinmiş görüntüleri StyleGAN'ın gizli uzayına geri döndürmek gibi zorlu bir görevi başarıyor. Bu görev için, kodlanmış olan silinmiş görüntü özelliklerini rastgele örneklerden elde edilen StyleGAN'ın stil vektörleri ile birleştirmek amacıyla bir kodlayıcı ve bir karıştırma ağı öğrenmeyi öneriyoruz. Aynı silinmiş resim için çeşitli görüntü tamamlama sonuçları elde etmek adına, kodlanmış özellikleri ve rastgele örneklenen stil vektörlerini karıştırma ağı aracılığıyla birleştiriyoruz. Yöntemlerimizi modellerin kalitesini ölçen farklı değerlendirme metrikleriyle karşılaştırıyoruz ve önemli niceliksel ve niteliksel iyileştirmeler gösteriyoruz.","We present two novel approaches to image inpainting, a task that involves erasing unwanted pixels from images and filling them in a semantically consistent and realistic way. The first approach uses natural language input to determine which object to remove from an image. We construct a dataset named GQA-Inpaint for this task and train a diffusion-based inpainting model on it, which can remove objects from images based on text prompts. The second approach tackles the challenging task of inverting erased images into StyleGAN's latent space for realistic inpainting and editing. For this task, we propose learning an encoder and a mixing network to combine encoded features of erased images with StyleGAN's mapped features from random samples. To achieve diverse inpainting results for the same erased image, we combine the encoded features and randomly sampled style vectors via the mixing network. We compare our methods with different evaluation metrics that measure the quality of the models and show significant quantitative and qualitative improvements."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzün teknolojik devrim çağında, üretilen verilerin hacmi, özellikle görsel analiz açısından, bu tür ölçekteki ilişkisel verileri analiz etmek için önemli bir zorluk teşkil etmektedir. Çizgeler, varlıkları temsil eden düğümlerle ilişkisel verileri düzenlemenin ve temsil etmenin etkili bir yolunu sağlar. Tersine, ilişkileri temsil eden kenarlar, karmaşık büyük ölçekli verilerin kapsamlı ve sezgisel bir görünümünü oluşturur. Karmaşık çizgelerin iyi temsil edilen bir görselleştirmesi, kullanıcıların ilişkileri anlamalarına, yeni içgörüler ortaya çıkarmalarına ve gizli kalıpları keşfetmelerine olanak tanır. Bu amaçla, çizgelerle temsil edilen büyük ölçekli ilişkisel verileri etkili bir şekilde analiz etmek için bir karmaşıklık yönetimi çerçevesi sunuyoruz. Çizge karmaşıklığını yönetmek için mevcut yöntemler bağımsız olarak çalışır ve art arda uygulanan tutarsızlıklara ve karışıklığa yol açabilir. Karmaşıklık Yönetimi Çizge Görselleştirme çerçevesi (CMGV), özel bir yerleştirme algoritması aracılığıyla kullanıcının zihinsel haritasının korunmasını sağlarken yaygın olarak kullanılan karmaşıklık yönetimi tekniklerini entegre eden yeni bir yaklaşım sunar. Çerçeve, hem çizge temsili hem de karmaşıklık yönetimi için sezgisel bir Çizge Karmaşıklık Yönetimi Modeli (CMGM) sunar. CMGV, çizge öğelerini filtreleme, gizleme, gösterme, daraltma ve genişletme dahil olmak üzere yaygın olarak kullanılan karmaşıklık yönetimi görevlerini destekler. Daha da önemlisi, CMGV, işleme yönteminden bağımsız olacak şekilde tasarlanmıştır ve farklı çizge işleme kütüphaneleriyle sorunsuz bir şekilde entegre edilebilir. Bu, işleme kütüphanesi ve CMGM arasında çizge modellerini senkronize eden bir uzantı aracılığıyla mümkündür. Rastgele oluşturulmuş çizgeler üzerinde gerçekleştirilen deneylerimiz, CMGV'nin kullanıcı çizgesini olduğu gibi bırakarak ardışık çizge karmaşıklığı yönetimi işlemlerini kusursuz bir şekilde gerçekleştirdiğini ve hem çalışma zamanı hem de genel kabul görmüş çizge düzeni kriterleri açısından mevcut karmaşıklık yönetimi çözümlerinden daha iyi performans gösterdiğini doğrulamaktadır. Küçük ve orta boyutlu çizgeler içeren etkileşimli uygulamalarda kullanılabilecek kadar hızlıdır.","In today's era of technological revolution, the sheer volume of data being produced poses a significant challenge for analyzing relational data of such scale, particularly in terms of visual analysis. Graphs provide an effective way of organizing and representing relational data, with nodes representing entities. In contrast, edges representing relationships, a comprehensive and intuitive view of complex large-scale data is created. A well-represented visualization of complex graphs allows users to understand relationships, uncover new insights, and discover hidden patterns. To this end, we introduce a complexity management framework for effectively analyzing large-scale relational data represented as graphs. Existing methods for managing graph complexity work independently and may lead to inconsistencies and confusion consecutively applied. The Complexity Management Graph Visualization framework (CMGV) presents a novel approach integrating commonly used complexity management techniques while ensuring the preservation of the user's mental map through a specialized layout algorithm. The framework introduces an intuitive Graph Complexity Management Model (CMGM) for both graph representation and complexity management. CMGV supports commonly utilized complexity management tasks, including filtering, hiding, showing, collapsing, and expanding graph elements. Importantly, CMGV is designed to be independent of the rendering method and can be seamlessly integrated with different graph rendering libraries. This is possible through an extension that synchronizes the graph models between the rendering library and CMGM. Our experiments performed on randomly generated graphs verify that CMGV flawlessly performs consecutive graph complexity management operations, leaving the user graph intact, and outperforms existing complexity management solutions in terms of both runtime and generally accepted graph layout criteria. It is fast enough to be used in interactive applications with small to medium-sized graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet ve sosyal medyada yer alan yanlış bilgiler toplum üzerinde oluşturabileceği güvensizlik ve sağlık, enerji, politika, terörizm ve afetler gibi önemli alanlarda insan karar mekanizmalarında oluşturabileceği etki yüzünden ciddi bir endişe haline gelmiştir. Yanlış bilgilerin tespiti ve yayılımının önüne geçmek için hesaplamalı yöntemler devreye alınmaya başlamıştır. Bu yöntemler, sahte haberler ve trol hesapların algoritmik açıdan tespiti ve yanlış bilgilerin sosyal medyada yayılması üzerine geliştirilmektedir. Fakat, internette ve sosyal medyada yer alan yanlış bilgi problemi sürekli ilgi ve araştırma gerektiren karmaşık ve devam eden bir problemdir. Biz bu problemin çözümünde üç farklı açıdan katkı sağlamaktayız. İlk olarak, çeşitli sosyal ağ senaryolarında mo-delleme, simulasyon, görselleştirme ve analiz için kullanılabilecek Crowd isimli genişletilebilir bir sosyal ağ çatısı tasarlayıp geliştirdik. İkinci katkımız ise, yanlış bilgi yayılımının oyunlaştırılarak ağda yer alan düğümler arasında bir işbirlikçi oyun haline getirilmesi ve yayılımın farklı şartlarda nasıl gerçekleştiğinin anlaşılması üzerinedir. Sonrasında, düğümlerin ağ seviyesinde kontrol edildiği ağ seviyesinde bir oyun tasarladık. Bu oyunda, Multi Agent Deep Deterministic Policy Gradients yöntemini baz alarak geliştirdiğimiz derin pekiştirmeli öğrenme metodunun page-rank, centrality ve CELF gibi düğüm seçme algoritmalarından daha iyi performans verdiğini gösterdik. Son olarak, yanlış bilgi probleminde şeffaflık, değişmezlik ve doğruluk gibi kriterleri sağlayabilecek bir blokzincir ve derin öğrenme hibrit yöntemini kitle kaynak kullanımı ile önerdik. İtibar sistemleri üzerine iyi bilinen saldırılar altında detaylı simulasyonlar ile yöntemin performansı ölçülmüş ve Twitter üzerinde yer alan bir çalışmayla kıyas yapılmıştır.","Misinformation on the internet and social media has become a pressing concern due to its potential impacts on society, undermining trust and impacting human decisions on global issues such as health, energy, politics, terrorism, and disasters. As a solution to the problem, computational methods have been employed to detect and mitigate the spread of false or misleading information. These efforts have included the development of algorithms to identify fake news and troll accounts, as well as research on the dissemination of misinformation on social media platforms. However, the problem of misinformation on the web and social networks remains a complex and ongoing challenge, requiring continued attention and research. We contribute to three different solution aspects of the problem. First, we design and implement an extensible social network simulation framework called Crowd that helps model, simulate, visualize and analyze social network scenarios. Second, we gamify misinformation propagation as a cooperative game between nodes and identify how misinformation spreads under various criteria. Then, we design a network-level game where the nodes are controlled from a higher perspective. In this game, we train and test a deep reinforcement learning method based on Multi-Agent Deep Deterministic Policy Gradients and show that our method outperforms well-known node-selection algorithms, such as page-rank, centrality, and CELF, over various social networks in defending against misinformation or participating in it. Finally, we promote and propose a blockchain and deep learning hybrid approach that utilizes crowdsourcing to target the misinformation problem while providing transparency, immutability, and validity of votes. We provide the results of extensive simulations under various combinations of well-known attacks on reputation systems and a case study that compares our results with a current study on Twitter."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Depresyon tespiti, erken teşhis ve müdahale potansiyeli dolayısıyla bilimsel açıdan önemli ölçüde ilgi çekmektedir. Bu sebeple, bu tezde depresyon şiddetinin tahmini için yalnızca metin özniteliklerine bağlı kalan yeni bir yaklaşım önerilmektedir. Önerilen bu yaklaşım, dönüştürücü tabanlı bir yapı içerisinde duygulanım (duygu ve his) ve kişilik özniteliklerini farklı ancak birbirine bağlı kipler hâlinde entegre etmektedir. Bu tezin ana katkısı, farklı metin kiplerinden elde edilen bilgileri birleştirmeyi sağlayan maskeli ve çok kipli ortak çapraz dikkat füzyon yaklaşımıdır. Bu füzyon yaklaşımı, modelin sadece metin verileri içindeki gizli bağlamsal ipuçlarını ayırt etmesine değil, aynı zamanda modaliteler arasındaki karmaşık bağımlılıkları da anlamasına olanak tanımaktadır. Önerilen mimaride var olan bileşenler ile var olmayan bileşenler ayrıntılı olarak incelenmek üzere kapsamlı deneysel değerlendirmelere tabi tutulmaktadır. Bu değerlendirmeler, her kipin ayrı ayrı incelendiği tek kipli bir ortamda gerçekleştirilen deneyleri de içerir. Değerlendirmelerden elde edilen bulgular, önerilen mimarinin kendi kendine yeterli etkinliğini doğrulamaktadır. Bunlara ek olarak, bu tezde konuşma içeriği içindeki cümlelerin önemini inceleyerek belirli metin ipuçlarının katkısına dair değerli bilgiler sunulmaktadır. Aynı zamanda, önerilen yöntemin farklı depresyon şiddeti aralıkları için değerlendirmeleri yer almaktadır. Son olarak, önerilen yöntem farklı işitsel, görsel ve metinsel özellik kombinasyonları kullananan mevcut en ileri düzey çalışmalar ile karşılaştırılmaktadır. Sonuçlar, önerilen yöntemin depresyon şiddeti tahmininde umut verici sonuçlar elde ettiğini ve diğer yöntemleri geride bıraktığını göstermektedir.","The detection of depression has gained a significant amount of scientific attention for its potential in early diagnosis and intervention. In light of this, we propose a novel approach that places exclusive emphasis on textual features for depression severity estimation. The proposed method seamlessly integrates affect (emotion and sentiment), and personality features as distinct yet interconnected modalities within a transformer-based architecture. Our key contribution lies in a masked multimodal joint cross-attention fusion, which adeptly combines the information gleaned from these different text modalities. This fusion approach empowers the model not only to discern subtle contextual cues within textual data but also to comprehend intricate interdependencies between the modalities. A comprehensive experimental evaluation is undertaken to meticulously assess the individual components comprising the proposed architecture, as well as extraneous ones that are not inherent to it. The evaluation additionally includes the assessments conducted in a unimodal setting where the impact of each modality is examined individually. The findings derived from these experiments substantiate the self-contained efficacy of our architecture. Furthermore, we explore the significance of individual sentences within speech content, offering valuable insights into the contribution of specific textual cues and we perform a segmented evaluation of the proposed method for different ranges of depression severity. Finally, we compare our method with existing state-of-the-art studies utilizing different combinations of auditory, visual, and textual features. The final results demonstrate that our method achieves promising results in depression severity estimation, outperforming the other methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son zamanlarda verilerin büyümesi katlanarak arttı ve ilişkisel verilerin görsel analizini giderek karmaşık hale getirdi. Bu tür verileri görsel olarak çekici bir şekilde sunmak, analiz sürecini basitleştirmeye yardımcı olabilir. Merkezi yönlendirilmiş veya hiyerarşik bir parça ve birbirine bağlı yönlendirilmemiş bileşenlerden oluşan hibrit çizgeler, karmaşıklığını yönetirken değişen soyutlama düzeylerine sahip ilişkisel verileri temsil etmek için pratik bir yapı sunar. Verilerdeki ilişkileri anlamak, içgörüleri keşfetmek ve önemli kalıpları elde etmek için bu tür çizgelere yönelik iyi optimize edilmiş bir çizge düzenine ihtiyaç vardır. Buna yanıt olarak, hibrit çizgeler için özel olarak tasarlanmış yeni bir çizge düzeni algoritması olan HySE'yi (Hybrid Spring Embedder) sunuyoruz. HySE, hem yönlendirilmiş hem de yönlendirilmemiş arasındaki uyumu korurken, yalnızca çizgenin yönlendirilmemiş kısmında değil, aynı zamanda hiyerarşide de optimize edilmiş kuvvet yönlendirmeli düzenin estetiğini ve kalitesini elde etmek için popüler yay yerleştirmeyi temel alan bütünsel bir yaklaşımdan yararlanır. Yerleştirme algoritması, yönlendirilmiş çizge elemanlarının sıralama bilgilerinin, popüler yaklaşımlardan biriyle önceden hesaplandığını varsayar. Daha sonra uygun başlangıç konumlarını bulur ve yönlendirilmemiş parçaları yerleşime entegre etmek için kuvvet yönlendirmeli yerleşim tekniğini kullanır, kenarları modellemek için yay kuvvetlerini ve düğümler için itici elektrik kuvvetlerini uygular. Yinelemeli bir yöntem olan HySE, enerjinin en aza indirildiği bir denge durumuna yakınsar, bu da karmaşık hibrit çizgeler için görsel olarak hoş ve yorumlanabilir düzenler sağlar. İyi tasarlanmış bir süreçle rastgele oluşturulan bileşik hibrit çizgeler üzerinde gerçekleştirilen deneyler, HySE'nin kalite açısından en son teknoloji algoritmalar kadar iyi performans gösterdiğini doğrulamaktadır. Aynı zamanda küçük ve orta boyutlu çizgelerde de bilinen algoritmaların hızını yakalar.","In recent times, the growth of data has been exponential, making the visual analysis of relational data progressively complex. Presenting such data in a visually appealing manner can help simplify the analysis process. Hybrid graphs, comprising a central directed or hierarchical part and interconnected undirected components, offer a practical structure for representing relational data with varying levels of abstraction while managing its complexity. To comprehend the relationships in data, discover insights, and get important patterns, a well-optimized graph layout for such graphs is needed. In response, we present HySE (Hybrid Spring Embedder), a novel graph layout algorithm tailored for hybrid graphs. HySE makes use of a holistic approach based on the popular spring embedder to achieve the aesthetics and quality of an optimized force-directed layout, not only on the undirected part of the graph but also on the hierarchy while maintaining the cohesion between both directed and undirected elements of the graph. The layout algorithm assumes the rank information of directed graph elements is already calculated with one of the popular approaches. Then, it finds appropriate initial positions and uses a force-directed layout technique to integrate the undirected parts into the layout, applying spring forces to model the edges, and repulsive electric forces for the nodes. Iteratively, HySE converges to an equilibrium state with minimized energy, resulting in visually pleasing and interpretable layouts for intricate hybrid graphs. Experiments performed on graphs, generated randomly through a well-designed process, validate that HySE performs as well as the state-of-the-art algorithms in terms of quality. It also matches the speed of well-established algorithms as well in small-to-medium-sized graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin öğrenmedeki ilerlemeler, son derece gerçekçi sahte insan yüzlerinin yaratılmasını kolaylaştırdı ve deepfake çağını başlattı. Bu kadar inandırıcı derecede orijinal sahte içerik üretme potansiyeli, hem bireylere hem de toplumlara verebileceği potansiyel zarar nedeniyle endişelere yol açıyor. Mevcut çalışmalar ağırlıklı olarak gerçek ve sahte görseller veya videolar arasında ayrım yapan ikili yaklaşımlara odaklanmaktadır. Ancak bu yaklaşım zaman alıcı olabilir ve eğitim için çok sayıda farklı sahte örnek gerektirebilir. Ayrıca, farklı modeller kullanılarak oluşturulan benzersiz deepfake içeriği tespitten kaçabilir ve bu da tüm deepfake'lerin yakalanmasını zorlaştırabilir. İki potansiyel çözüm öneriyoruz. İlk olarak, yalnızca gerçek veriler üzerinde eğitim veren ve hem gerçek hem de sahte veriler üzerinde testler yapan saf bir yaklaşım olan tek sınıflı bir sınıflandırma yöntemi öneriyoruz. İkincisi, saf olmayan bir yaklaşım olarak çapraz manipülasyon tekniğinin kullanılması; bu, makine öğrenimi modelinin eğitimi sırasında görünmeyen/bilinmeyen manipüle edilmiş örneklerin kullanımına görüntü manipülasyonlarının uygulanması anlamına gelir. Bu süreçteki etkinlik, derin sahtekarlıkların tespitini artıran farklı modellerin bir kombinasyonu kullanılarak elde edilebilir. Bu, $\ell_p$-norm kısıtlamasını içeren öğrenme tabanlı sistemleri ayarlanabilir p-norm kurallarıyla birleştirerek yapılır, böylece topluluk öğreniminde tabanlı öğrenenler arasındaki ayrımcı bilgileri geliştirmek için hem seyrek hem de seyrek olmayan çözümler sağlanır. Derin sahte tespitte kullanılan geleneksel konudan bağımsız öğrenme yöntemlerinin aksine, konuya bağlı bir öğrenme yaklaşımı öneriyoruz. İlk bulgularımız, bu çok yönlü yaklaşımın deepfake'leri etkili bir şekilde tespit edebildiğini, FaceForensics++ veri setinin yanı sıra UCI ve Keel veri setlerini de içeren genel tek sınıf sınıflandırma veri setlerinde hem saf hem de saf olmayan yaklaşımlarda etkileyici sonuçlar ortaya koyduğunu gösteriyor.","Advancements in deep learning have facilitated the creation of highly realistic counterfeit human faces, ushering in the era of deepfakes. The potential to generate such convincingly authentic fake content prompts concerns due to the potential harm it could inflict on individuals and societies alike. Current studies predominantly focus on binary approaches that differentiate between real and fake images or videos. However, this approach can be time-consuming, requiring a multitude of diverse fake examples for training. Furthermore, unique deepfake content generated using different models may elude detection, making it challenging to apprehend all deepfakes. We propose two potential solutions. First, we suggest a one-class classification method, a purist approach that trains solely on real data and tests on both real and fake data. Second, using a cross-manipulation technique as a non-purist approach, which refers to the application of image manipulations to a use unseen/unknown manipulated samples during the training of the machine learning model. Efficacy in this process can be achieved by using a combination of different models, which enhances the detection of deep fakes. This is done by merging learning-based systems involving an $\ell_p$-norm constraint with adjustable p-norm rules, thereby providing both sparse and non-sparse solutions to enhance discriminatory information between based learners in ensemble learning. Contrary to conventional subject-independent learning methods employed in deep fake detection, we propose a subject-dependent learning approach. Our preliminary findings suggest that this multifaceted approach can effectively detect deepfakes, demonstrating impressive results on the FaceForensics++ dataset as well as on generic one-class classification datasets including the UCI, and Keel datasets in both pure and non-pure approaches."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Histopatolojik görüntü sınıflandırmasının en yaygın yöntemi, büyük tüm slayt görüntülerinden toplanan küçük pencerelerin gösterimlerini birleştirerek bir tahmin yapmaktır. Dönüştürücü modelleri, özgün öz-dikkat stratejileri sayesinde pencereler arasındaki uzak mesafe ilişkilerini anlayabildikleri ve önemli bölgeleri saptayabildikleri için bu alanda dikkat çeken bir alternatiftir. Ancak dizi tabanlı mimarileri yüzünden görüntülerin iki boyutlu doğasını doğrudan anlayamazlar. Bir tüm slaytın uzamsal bağlamı iki aşamada modellenir. İlk aşamada pencereler tek boyutlu bir dizi halinde sıralanır, daha sonra bu sıra bilgisi modele enjekte edilir. Ancak, yaygın kullanılan uzamsal bağlamı modelleme yöntemleri sabit boyutlardaki görüntüler için tasarladıkları için bu pencerelerin görüntü içindeki dağılımını tam anlamıyla anlayamazlar. Bu pencereler bir araya gelerek teşhis bakımından anlamlı yapılar oluşturabilecekleri için, pencerelerin uzamsal dağılımını anlayabilecek bir uzamsal bağlamı modelleme yöntemi öneriyoruz. Bunu yerelliği koruyan diziler oluşturarak başarıyoruz. Oluşturulan dizileri farklı bilgi enjekte etme yöntemleriyle test ettik. Önerdiğimiz dönüştürücü tabanlı tüm slayt sınıflandırma yönteminin başarımını The Cancer Genome Atlas'ın akciğer kanseri veri kümesinde değerlendirdik. Deneysel sonuçlarımıza göre, boşluk dolduran eğrileri kullanan yöntemimiz %87.6 doğruluğa erişerek, temel ve modern yöntemlerden daha yüksek başarım göstermiştir.","The common method for histopathology image classification is to sample small patches from the large whole slide images and make predictions based on aggregations of patch representations. Transformer models provide a promising alternative with their ability to capture long-range dependencies of patches and their potential to detect representative regions, thanks to their novel self-attention strategy. However, as sequence-based architectures, transformers are unable to directly capture the two-dimensional nature of images. Modeling the spatial context of an image for a transformer requires two steps. In the first step the patches of the image are ordered as a 1-dimensional sequence, then the order information is injected to the model. However, commonly used spatial context modeling methods cannot accurately capture the distribution of the patches as they are designed to work on images with a fixed size. We propose novel spatial context modeling methods in an effort to make the model be aware of the spatial context of the patches as neighboring patches usually form diagnostically relevant structures. We achieve that by generating sequences that preserve the locality of the patches. We test the generated sequences by utilizing various information injection strategies. We evaluate the performance of the proposed transformer-based whole slide image classification framework on a lung dataset obtained from The Cancer Genome Atlas. Our experimental evaluations show that the proposed sequence generation method that utilizes space-filling curves to model the spatial context performs better than both baseline and state-of-the-art methods by achieving 87.6% accuracy."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu kip tıbbi görüntüleme, doku morfolojisi ve işlevi hakkında tamamlayıcı bilgiler sağlayarak çeşitli hastalıkların tanı ve tedavisinde güçlü bir araçtır. Ancak, farklı kip veya kontrastlardan çoklu görüntüler elde etmek çoğu zaman tarama süresi, maliyet ve hasta konforu gibi çeşitli faktörler nedeniyle tatbiki olmayabilir, veya tümüyle imkansız olabilmektedir. Tıbbi görüntü çevirimi, kaynak kip görüntülerine dayanarak hedef kip görüntülerini sentezlemek için umut verici bir çözüm olarak ortaya çıkmıştır. Mevcut olmayan görüntüleri sentezleme yeteneği, çok kipli protokollerinin yaygınlığını ve kullanışlılığını artırırken muayene maliyetlerini ve iyonize radyasyon ve kontrast ajanları gibi toksisite maruziyetini azaltır. Mevcut tıbbi görüntü çevirimi yöntemleri öne çıkan şekilde evrişimli sinir ağları (CNN) omurgalı üretici çekişmeli ağlar (GAN) üzerine kurulmuştur. CNN'ler kompakt filtrelerle yerel işlem yapmak için tasarlanmıştır ve bu model varsayımı sınırlı bağlamsal duyarlılığa eğilimlidir. Bu arada, GAN'ler tek atışlı örnekleme ve görüntü dağılımının dolaylı karakterizasyonu nedeniyle sınırlı örnek sadakati ve çeşitliliğinden muzdariptir. CNN tabanlı GAN modellerinin zorluklarını aşmak için, bu tezde ilk olarak evrişimli ve dönüştürücü modüllerden sinerjik olarak temsilleri birleştiren yenilikçi birikimli artık dönüştürücü (ART) bloklarından yararlanan ResViT'i tanıtıyoruz. Ardından SynDiff'i tanıtıyoruz; büyük difüzyon adımları ve çekişmeli izdüşümler yoluyla gürültüyü ve kaynak görüntülerini hedef görüntünün üzerine ilerleyici bir şekilde eşleyen, görüntü dağılımının doğrudan bir bağıntısını yakalayan ve örnek kalitesini ve hızını iyileştiren koşullu bir difüzyon modeli. ResViT, değişen kaynak-hedef kip yapılandırmaları için ayrı sentez modelleri oluşturma ihtiyacını ortadan kaldırmak için birleştirilmiş bir uygulama sağlarken, SynDiff eşleştirilmemiş veri kümelerinde denetimsiz eğitimi döngüsel tutarlı bir mimari aracılığıyla mümkün kılar. ResViT ve SynDiff'i çok kontrastlı MRI'da eksik dizilerin sentezlenmesinde ve MRG'den BT görüntülerinin sentezlenmesinde gösteriyoruz ve tıbbi görüntü çevirimindeki son teknoloji performanslarını gösteriyoruz.","Multi-modal medical imaging is a powerful tool for diagnosis and treatment of various diseases, as it provides complementary information about tissue morphology and function. However, acquiring multiple images from different modalities or contrasts is often impractical or impossible due to various factors such as scan time, cost, and patient comfort. Medical image translation has emerged as a promising solution to synthesize target-modality images given source-modality images. Ability to synthesize unavailable images enhance the ubiquity and utility of multi-modal protocols while decreasing examination costs and toxicity exposure such as ionizing radiation and contrast agents. Existing medical image translation methods prominently rely on generative adversarial networks (GANs) with convolutional neural networks (CNNs) backbones. CNNs are designed to perform local processing with compact filters, and this inductive bias is prone to limited contextual sensitivity. Meanwhile, GANs suffer from limited sample fidelity and diversity due to one-shot sampling and implicit characterization of the image distribution. To overcome the challenges with CNN based GAN models, in this thesis, first ResViT was introduced that leverages novel aggregated residual transformer (ART) blocks that synergistically fuse representations from convolutional and transformer modules. Then SynDiff is introduced, a conditional diffusion model that progressively maps noise and source images onto the target image via large diffusion steps and adversarial projections, capturing a direct correlate of the image distribution and improving sample quality and speed. ResViT provides a unified implementation to avoid the need to rebuild separate synthesis models for varying source-target modality configurations, whereas SynDiff enables unsupervised training on unpaired datasets via a cycle-consistent architecture. ResViT and SynDiff was demonstrated on synthesizing missing sequences in multi-contrast MRI, and CT images from MRI, and their state-of-the-art performance in medical image translation was shown."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kendiliğinden örgütlenen kentsel büyüme modları, yüksek kaliteli kentsel alanlar üretme, uygun fiyatlı konut sağlama ve şehirler içinde ekonomik fırsatlara daha geniş erişim sağlama gibi potansiyelleri barındırır. Bu tip yerleşimler, ve tabandan yukarı örgütlenen tasarım süreçleri kısıtlı alanda ve özellikle eğimli arazilerde, çevresel etmenler açısından da verimli çözümler sunar. Bu tip doğrusal olmayan, karmaşık ve dinamik sıralı kentsel toplama süreçlerinin modellenmesi, uyarlanabilir, sıralı kararlar almayı gerektirir. Bu çalışmada, belirli performans hedeflerini en üst düzeye çıkaran kendiliğinden örgütlenmiş yerleşimler oluşturmak için derin pekiştirmeli öğrenme (Deep Reinforcement Learning: DRL) yaklaşımı önerilir. Bu yaklaşım kapsamında, kendiliğinden örgütlenen yerleşimlerin morfogenez problemini simüle edecek tekil ajanlara dayalı (single-agent) bir pekiştirmeli öğrenme (Reinforcement Learning: RL) çerçevesi önerilmiştir. Önerilen çerçeve, iki hücresel otomata temelli kentsel büyüme modeline dayanan üç ortam geliştirilerek, derin Q-öğrenme (DQN) ve Proksimal Politika Optimizasyonu (Proximal Policy Optimization: PPO) algoritmaları kullanılarak, RL ajanlarının eğitilmesiyle ve bu ortamlardaki performans metriklerine dayalı sıralı kentsel oluşum politikaları üretilmesi yoluyla test edilir. Ajanlar, performansı en üst düzeye çıkarmak, geçiş hakkını korumak ve topografik kısıtlamalara uyum sağlamak için kentsel biçimi uyarlarken, yerleşimleri sırayla büyütmeyi sürekli olarak öğrenirler. Bu çalışmada önerilen yöntem sadece önceden belirlenmiş performans hedeflerine dayalı olarak kendi kendine organize edilen yerleşim büyümesini modellemek için değil, aynı zamanda benzer nitelikli tasarım problemlerinin formülizasyonu için de kullanılabilir bir çerçeve sunar.","Self-organized modes of urban growth could result in high-quality urban space and have notable benefits such as providing affordable housing and wider access to economic opportunities within cities. Modeling this non-linear, complex, and dynamic sequential urban aggregation process requires adaptive sequential decision-making. In this study, a deep reinforcement learning (DRL) approach is proposed to automatically learn these adaptive decision policies to generate self-organized settlements that maximize a certain performance objective. A framework to formulate the self-organized settlement morphogenesis problem as single-agent reinforcement learning (RL) environment is presented. This framework is then verified by developing three environments based on two cellular automata urban growth models and training RL agents using the Deep Q-learning (DQN) and Proximal Policy Optimization (PPO) algorithms to learn sequential urban aggregation policies that maximize performance metrics within those environments. The agents consistently learn to sequentially grow the settlements while adapting their morphology to maximize performance, maintain right-of-way, and adapt to topographic constraints. The method proposed in this study can be used not only to model self-organized settlement growth based on preset performance objectives but also could be generalized to solve various single-agent sequential decision-making generative design problems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"daha sonra doldurulacaktırBu tez havadan görüntü işleme ile ilgili iki ana çalışmayı içermektedir. İlk çalışmada (bu tezin ilk ana bölümünde), hava görüntülerindeki nesnelerin tespitine yönelik yeni yaklaşımlar sunuyoruz. Bu tez yazıldığında, birçok hava kıyaslama veri kümesinde birçok metrikte en son teknolojiye sahip sonuçları veren CenterNet'i temel alan yeni bir nesne algılama algoritması tanıttık. Bu bölümde İHA'lar tarafından çekilen hava görüntülerinde farklı kayıp fonksiyonlarının ve mimarilerin nesnelerin tespit performansının arttırılmasına etkisi incelenmektedir. Önerilen yaklaşımlarımızın, havadan görüntülerdeki nesnelerin tespitine yönelik öğrenme sürecinin belirli yönlerinin iyileştirilmesine yardımcı olduğunu gösterdik. Güncel derin öğrenme tabanlı denetlenen nesne algılama algoritmalarını eğitmek için ek açıklamaların kullanılabilirliği önemlidir. Günümüzde birçok algoritma girdi olarak hem kızılötesi (IR) hem de görünür (RGB) görüntü çiftlerini kullanıyor. Ancak büyük veri kümeleri (VisDrone veya ImageNet gibi) genellikle görünür spektrumda yakalanır. Bu nedenle, bu tezin ikinci bölümünde, mevcut veri kümeleri için görünür görüntülerin kızılötesi eşdeğerlerinin yapay olarak üretilmesine yönelik alan aktarımı tabanlı bir yaklaşım sunulmaktadır. Bu tür görüntü çiftleri, gelecekteki çalışmalarda her iki mod için de nesne algılama algoritmalarını eğitmek amacıyla kullanılabilir.","This thesis contains two main works related to aerial image processing. In the first work (in the first main part of this thesis), we present novel approaches to detect objects in aerial images. We introduce a novel object detection algorithm based on CenterNet which yields the state-of-the-art results in many metrics on many aerial benchmark datasets, when this thesis was written. In this part, we study the effect of different loss functions, and architectures for improving the detection performance of objects in aerial images taken by UAVs. We show that our proposed approaches help improving certain aspects of the learning process for detecting objects in aerial images. To train recent deep learning-based supervised object detection algorithms, the availability of annotations is essential. Many algorithms, today, use both infrared (IR) and visible (RGB) image pairs as input. However, large datasets (such as VisDrone or ImageNet) typically are captured in the visible spectrum. Therefore, a domain transfer-based approach to artificially generate infrared equivalents of the visible images for existing datasets is presented in the second part of this thesis. Such image pairs, then, can be used to train object detection algorithms for either mode in future work."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kod gözden geçirme süreci, yazılım geliştirme takımları tarafından birçok amaçla yürütülmektedir. Bu amaçlardan biri de yazılım kalitesini korumaktır. Yazılım kalitesi, sürdürülebilirliği de ele alan geniş bir kavramdır. Bu çalışmada, kod gözden geçirme süreci kalitesinin, bu süreçteki kötü uygulamalar açısından ele alarak yazılım sürdürülebilirliğine olan etkisini kod kusurları açısından inceliyoruz. Diğer bir deyişle, kod gözden geçirme sürecindeki kötü uygulamalar ile kod kusurları arasında ilişki olup olmadığını korelasyon analizi ile araştırıyoruz. Çalışmada nicel analiz, odak küme çalışması ile desteklenerek yazılım geliştiricilerin konu hakkındaki görüşleri alınmıştır. Beklentilere ters olarak, kod gözden geçirme sürecindeki kötü uygulamaların kod kusurlarının yoğunluğu ile ilişkisinin zayıf olduğu veya ilişkisinin bulunmadığını saptadık. Sonrasında yaptığımız incelemelerde, gözden geçirme sürecinin kalitesinden bağımsız olarak, her 10 gözden geçirmenin 8'inde kod kusurlarının ne arttığı ne de azaldığı görülmüştür. Elde edilen beklenmedik sonuçların ardında, odak küme çalışmamızdaki verileri de göz önünde bulundurarak birden çok neden belirledik. Ek olarak, yazılım geliştiricilerinin hala kod gözden geçirme sürecinin yazılım kalitesini artırmada yardımcı olduğunu düşündüğü saptanmıştır. Elde ettiğimiz sonuçlar, geliştiricilerin kod gözden geçirme süreci yürütme amaçlarını güncellemesi gerektiğine ve süreç içerisindeki uygulamalarını yeniden değerlendirerek modern araçlarla ve güncel gerçekliklerle uyumlu hale getirmeleri gerektiğine işaret ediyor.","The code review process is conducted by software teams with various motivations. Among other goals, code reviews act as a gatekeeper for software quality. Software quality comprises several aspects, maintainability (i.e., code quality) being one of them. In this study, we explore whether code review process quality (as evidenced by the presence of code review process smells) influences software maintainability (as evidenced by the presence of code smells). In other words, we investigate whether smells in the code review process are related to smells in the code that was reviewed by using correlation analysis. We augment our quantitative analysis with a focus group study to learn practitioners' opinions. Contrary to our own intuition and that of the practitioners in our focus groups, we found that code review process smells have little to no correlation with the level of code smells. Further investigations revealed that the level of code smells neither increases nor decreases in 8 out of 10 code reviews, regardless of the quality of the code review. We identified multiple potential reasons behind the counter-intuitive results based on our focus group data. Furthermore, practitioners still believe that code reviews are helpful in improving software quality. Our results imply that the community should update our goals for code review practices and reevaluate those practices to align them with more relevant and modern realities."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Pek çok açık kaynaklı yazılım projesi, olay kaydı takibi için GitHub Issues kullanmaktadır. Diğer olay kaydı takip araçlarının aksine, GitHub Issues kullanıcılara esneklik sağlamakta ve kullanımı için standart bir yol sunmamaktadır. Bu esneklik ve standart eksikliği, yazılım geliştirme ve bakım süreçlerini olumsuz etkileyebilecek eksik olay kayıtlarına yol açabilmektedir. GitHub, bu problemi engellemek için 2016'da olay kaydı şablonlarını kullanıma sundu ve birçok proje bu özelliği kullanmaya başladı. Bu tez, açık kaynaklı projeleri inceleyerek olay kaydı şablonu kullanımının mevcut durumunu ve tarihsel gelişimini ortaya koymayı amaçlamaktadır. Bu amaçla, 100 popüler açık kaynaklı yazılım projesinden 350 şablonu ve bu şablonların önceki sürümlerini analiz ettik. Açılan olay kayıtlarının şablonlara uygunluklarını ve şablonların etkisini anlamak içinse 1,916,057 olay kaydını inceledik. Son olarak, şablonlar hakkındaki görüşlerini anlamak için açık kaynak yazılım geliştiricilerle bir anket gerçekleştirdik. Analiz sonucunda, şablonların neredeyse her zaman kullanıldığını gözlemledik (100 projeden 99'u). Tarihsel analizimiz, daha yapılandırılmış ve kısıtlı şablonların normal şablonlara göre popülerlik kazanmaya başladığını gösteriyor. Ayrıca yaptığımız istatiksel analiz, şablon kullanımı durumunda olay kayıtlarının istatistiksel olarak daha hızlı çözüldüğünü (p değeri 0.00, etki büyüklüğü 0.59) ve daha az etkileşimle tamamlandığını göstermektedir. Ankete göre ise, geliştiricilerin %85'i şablonların faydaları konusunda hemfikir ve geliştiriciler şablon kullanımının orijinal GitHub Issues olay kaydı sisteminin esnekliği ile Jira gibi diğer olay kaydı takip sistemlerinin katılığı arasında dengeli bir orta nokta oluşturduğuna inanıyorlar.","Many open-source software projects use GitHub Issues for issue tracking. Unlike other issue trackers, the initial versions of GitHub Issues were highly flexible and had no standard way of using it. Its unstructured nature may have made it prone to incomplete issue reports that may negatively affect software development and maintenance productivity. To potentially address these problems, GitHub introduced issue templates in 2016. This thesis aims to reflect the current status of issue template usage by mining open-source projects. Also, we analyze how the templates have evolved since their introduction in 2016 and further investigate the impact of issue templates on several issue tracking metrics, such as time to resolution, the number of reopens, and the number of comments. We evaluated 350 templates and their previous versions from 100 large-scale and popular open-source projects. We also analyzed 1,916,057 issues to understand their conformance to templates and the impact of issue templates. Lastly, we conducted a survey with open-source software maintainers to understand their opinions about issue templates. We found that issue templates are almost always used (99 out of 100 projects). The historical analysis suggests that issue forms, which are more structured issue templates, started to gain popularity over vanilla issue templates. We also observed that issues created when the project has an issue template are statistically resolved faster (p-value 0.00, effect size 0.59) and have less number of comments. Similarly, when issue forms are used, time to resolution, the number of reopenings and the length of discussion significantly decrease. According to the survey, 85% of project maintainers agree with the benefits and they believe issue templates construct a balanced midpoint between the flexibility of vanilla issues and the strictness of other issue tracking systems such as Jira."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge uygulamalarının kullanımı bir çok alanda yaygındır ancak düzensiz ve veriye dayalı bellek erişim kalıpları, büyük boyutlu çizge verisi ile birleşince, genel amaçlı bilgi işlem sistemleri çizge işlemede düşük performans göstermektedir. Varolan çalışmalarda, FPGA temelli donanım hızlandırıcıları, performans prob- lemlerini azaltmak ve enerji verimliliğini artırmak için önerilirken program- lanabilirlik ve esneklik konuları arka planda kalmıştır. Bu tez, alana özgü bir işlemci tasarımının, bir Çip-Üstü-Sistem platformunda donanımsal olarak gerçeklenmesini ve buna eşlik eden bir bellek çatısı tasarımını sunmaktadır. Önerilen mimari, taban alınan tasarımın mikro-mimarisini iyileştirmekte, taban tasarımı Sistem-Üstü-Çip'e bir çip-üzeri veri iletişim yolu protokolü kullanarak entegre etmekte ve alternatif bellek çatılarının tasarımlarını karşılaştırmaktadır. Donanım gerçeklenmesi, son teknoloji bir değerlendirme kartında yapılmıştır. Popüler çizge denektaşları, tasarlanan sistemin performans ölçümlerinde kul- lanılmış ve en iyi performans çıktısı için çeşitli sistem parametreleri üzerinde has- sasiyet analizi yapılmıştır. Güç tüketimi ve kaynak kullanımı üzerine bir analiz de çalışmaya dahil edilmiştir. Genel itibariyle, denektaşına ve çizge verisine göre değişmekle beraber, averaj hızlanma değerleri %15 ile %25 arasında ölçülürken, çip-üzeri güç tüketim değeri 3.8 ile 4.2 Watt olarak ölçülmüştür.","The use of graph applications is common in many areas; however, irregular and data-driven memory access patterns combined with the large sizes of graph data results in performance loss in general-purpose computing systems. Existing stud- ies proposed hardware accelerators often implemented on FPGAs to alleviate per- formance problems and improve energy efficiency while having less emphasis on programmability and flexibility. This thesis presents a hardware implementation of a domain-specific processor design for graph applications on a system-on-chip (SoC) platform accompanied by a design of a memory framework. The proposed system architecture improves the micro-architecture of a baseline design, inte- grates the baseline with an efficient system-on-chip bus communication protocol, and compares alternative memory framework implementations. The hardware is implemented on a state-of-the-art evaluation board. Popular graph benchmarks are used for performance evaluations of the implemented system, and various sensitivity analysis is done on newly added system parameters to determine the optimal system configuration. An analysis of power consumption and resource utilization is also provided. Overall, average speed-ups vary between 15% and 25% depending on the benchmark and graph data, while on-chip power consump- tion varies between 3.8 to 4.2 Watts depending on the system clock frequency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tüm Genom Hizalama Problemi (WGA), ̈ozellikle pangenom oluşturma bağlamında genomik alanında önemli bir zorluktur. Burada, düğümlerinde hem mekansal hem de alfabetik bilgileri bir araya getiren yeni bir indeksleme yapısı olan Almaşık Lyndon Faktörizasyon Ağacı (ALFAğacı) öneriyoruz. ALFAğacı, büyük DNA dizileri hakkında bilgi depolamak ve geri almak için etkili bir araçtır. Belirli bir DNA dizisinden ALFAğacını oluşturmak için Idoneous adını verdiğimiz bir algoritma sunuyoruz. Algoritma, belirli boyutlardaki aralıkları oluşturarak, bu aralıklar içinde eşleşmeleri belirleyerek ve hizalama işlemleri aracılığıyla bir doğrulama kontrolü gerçekleştirerek çalışır. Algoritma verimli ve ölçeklenebilir olduğundan, WGA için değerli bir araçtır. ALFAğacının önemli özellikleri şunlardır: 1) büyük DNA dizilerini depolamak için kompakt ve verimli bir veri yapısı; 2) belirli bir DNA dizisinin belli bölgeleri hakkında bilgiyi etkili bir şekilde geri alabilme; 3) hem mekansal hem de alfabetik bilgilere uyum sağlama yeteneği; ve 4) büyük DNA dizilerine ölçeklenebilme. Farklı genomlardaki deneysel sonuçlarımız, parametre seçimlerinin kapsama ve benzerlik üzerindeki etkilerini vurgulamaktadır. Idoneous, kapsama açısından rekabetçi bir performans sergilemekte ve farklı hizalama senaryoları için hassasiyet ve özgüllük ayarlamasında esneklik sağlamaktadır. ALFAğacı, WGA algoritmalarının performansını önemli ölçüde artırma potansiyeline sahiptir. ALFAğacının genomik alanına değerli bir katkı olduğuna inanıyor ve araştırmacıların keşif hızını hızlandırmak için kullanmasını umuyoruz.","The Whole Genome Alignment Problem (WGA) is an important challenge in the field of genomics, especially in the context of pangenome construction. Here we propose a novel indexing structure called the Alternating Lyndon Factorization Tree (ALFTree), which incorporates both spatial and lexicographical information within its nodes. The ALFTree is a powerful tool for WGA, as it can efficiently store and retrieve information about large DNA sequences. We present an algorithm, namely Idoneous, specifically designed to construct the ALFTree from a given DNA sequence. The algorithm works by generating intervals of specific sizes, identifying matches within these intervals, and performing a sanity check through alignment procedures. The algorithm is efficient and scalable, making it a valuable tool for WGA. Some of the key features of the ALFTree are 1) compact and efficient data structure for storing large DNA sequences; 2) efficient retrieval of information about specific regions of a DNA sequence; 3) ability to handle both spatial and lexicographical information; and 4) scalability to large DNA sequences. Our experimental results on different genomes highlight the effects of parameter selections on coverage and identity. Idoneous demonstrates competitive performance in terms of coverage and provides flexibility in adjusting sensitivity and specificity for different alignment scenarios. The ALFTree has the potential to significantly improve the performance of WGA algorithms. We believe that the ALFTree is a valuable contribution to the field of genomics, and we hope that it will be used by researchers to accelerate the pace of discovery."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışma Yargıtay'da emsal karar bulmak için derin öğrenme modelleri kullanmaktadır. Hukuk alanında çalışanların incelemesi gereken geniş çaplı hukuk veri tabanları ve bilgisayarların büyük miktarda metni kısa zamanda işleme kapasiteleri sebebiyle bulgetir algoritmaları hukuk alanında çalışanlar için faydalıdır. Bu tezde hukuksal yinelemeli sinir ağları (RNN) ve BERTurk-Legal modeli tanıtılmaktadır. Ayrıca Türkçe hukuk alanına özel yoğun kelime temsilleri tanıtılmaktadır. Buna ek olarak emsal karar yöntemi olarak RNN otokodlayıcılar, hukuksal RNN otokodlayıcılar, RNN otokodlayıcıların BM25 ile birleşimleri ve BERTurk-Legal modeli kullanılmıştır. En iyi sonuçlar BERTurk-Legal modeli ile elde edilmiştir.","This study utilizes deep learning models to retrieve prior legal cases in the Court of Cassation in Turkey. Given the vast legal databases that legal professionals need to navigate and the ability of computers to handle large amounts of text quickly, information retrieval algorithms prove beneficial for legal practitioners. In this thesis, we introduce our legal recurrent neural network (RNN) models and the BERTurk-Legal model. We also introduce dense word embeddings for the Turkish legal domain. Moreover, we employ RNN autoencoders, Legal RNN autoencoders, combinations of RNN autoencoders with BM25 algorithms, and BERTurk-Legal to retrieve prior legal cases. We obtain the best results with the BERTurk-Legal model."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada yüz özelliklerini düzenlemek için, ayrıştırılmış ve yorumlanabilir örtülü vektörler ile bir resimden resime çeviri sistemi önerilmektedir. Çalışmamızın odak noktası olan yüz özelliklerini düzenleme görevi, belirli bir özelliği kontrol edilebilir bir miktarda düzenleme ve bu süreçte diğer özellikleri koruyan ayrışık gösterimler öğrenme gibi zorluklara sahiptir. Bu zorlukları aşabilmek amacıyla önerilen sistemimizde, önceden eğitilmiş çekişmeli üretken ağlar (GAN) üzerinde uygulanan örtülü uzay ayrıştırması çalışmalarından ilham alarak, sistemimizde bulunan ve her biri farklı bir özelliği modelleyen diğer vektörlere dik bir doğrusal vektör elde ediyoruz. Sistemimiz, bu doğrusal vektörleri öğrenmek için diklik ve ayrıştırma üzerine optimizasyon fonksiyonları kullanmaktadır. Önerilen sistem, düzenlenmek istenen yüz resimlerini anlamsal olarak düzenlenmiş örtülü uzaya yansıtmak amacıyla kodlayıcı ve kod çözücüden oluşan ve dikkat mekanizması ile atlamalı bağlantılar içeren bir ağ mimarisi kullanmaktadır. Önerilen sistemin etkinliğini göstermek için, yüz özelliklerini düzenleme görevinde en iyi performansı gösteren modellerle detaylı karşılaştırmalar sunulmaktadır. Karşılaştırmalarımızda da görüldüğü gibi, çözümümüz bu görev için kullanılan güncel modellerden daha iyi bir performans göstermektedir.","We propose an image-to-image translation framework for facial attribute editing with disentangled interpretable latent directions. Facial attribute editing task faces the challenges of targeted attribute editing with controllable strength and disentanglement in the representations of attributes to preserve the other attributes during edits. For this goal, inspired by the latent space factorization works of fixed pretrained GANs, we design the attribute editing by latent space factorization, and for each attribute, we learn a linear direction that is orthogonal to the others. We train these directions with orthogonality constraints and disentanglement losses. To project images to semantically organized latent spaces, we set an encoder-decoder architecture with attention-based skip connections. We extensively compare with previous image translation algorithms and editing with pretrained GAN works. Our extensive experiments show that our method significantly improves over the state-of-the-arts."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"5G'nin ticari kullanımları ile, 5G ötesi ve 6G ağlara ilişkin araştırmalar başlatılmıştır. Gelecek nesil ağların ihtiyaç ve taleplerini karşılama kapsamında, öngörülen kullanım üç kullanım durumu olarak beklenilmektedir: yoğun Makine-Tipi Haberleşme (MTC), yüksek-güvenilir düşük-gecikmeli haberleşme, ve geliştirilmiş taşınabilir geniş bant. Tez, yoğun Makine-Tipi Haberleşme (mMTC) kullanım durumuna yoğunlaşmaktadır. Yeşil haberleşme ve ağ oluşumu başlığı altında araştırılan tüketimi azaltmaya yönelik enerji verimliliği; Yoğun Makine Tipi Haberleşme araştırmalarının tamamlayıcı alt alanlardan biridir. Yoğun Makine Tipi Haberleşme, endüstriyel ve tıbbi Nesnelerin İnterneti (IoT) tipi teknolojiler, yoğun giriş için ağ oluşturma yetenekleri ve geliştirilmiş haberleşme yanında sürdürülebilirlik ve güç verimliliğini de gerektirecektir. Veri Hızı Ölçütlemeli Çoklu Erişim (RSMA) üzerindeki araştırma ilerlemesi; geniş ağ yükleri aralığında, 5G'de kullanılan geleneksel giriş şemalarına göre; spektral verimlilik, enerji verimliliği, güvenilirlik, Nitelikli Servis (QoS) ve serbestlik derecesi geliştirmeleri için bir aday yoğun giriş şeması sunmaktadır. Bu tez kapsamında Ağın beklenen Bilgi Yaşı (AoI) ağırlıklı kullanımını en aza indirmek için, bir yaşa duyarlı güç tahsis politikası önermekteyiz. Bu, AoI konsepti ile RSMA çerçevesini birleştiren, bilgimiz dahilinde literatürdeki ilk çalışmadır. Aşağı bağlantı aktarımı için, beklenen AoI kullanımı ağırlıklı toplamının (WSAoI) en aza indirilmesini Markov Karar Süreci (MDP) olarak formüle etmekteyiz ve WSAoI'yı en aza indirmek için, RSMA, OMA, ve NOMA arasında seçim yapacak en uygun baz istasyonu politikasını ve alt uygun politikasını araştırmaktayız. En uygun bir politikanın varlığını kanıtlıyoruz. Hesap karmaşıklığı eylem elemesi, ve en uygun performansa yakın bir başarıyı gösteren bir standart altı politika kullanılarak azaltılmaktadır. WSAoI ile ağ enerji verimliliği arasındaki dengeyi de araştırmaktayız. Uyarlamalı RSMA ¸seması, WSAoI açısından, uyarlamalı RSMA/NOMA/OMA ve OMA/NOMA şemalarına göre daha üstün performans göstermektedir. Örneğin, ¨ RSMA seçildiğinde, performans ölçüsü olan WSAoI, seçili 14, 15 ve 16 dB SNR değerleri için NOMA/OMA durumuna göre ortalama %35.8, %15.7, ve %12.7 daha iyidir. Ortak ve ¨özel mesajlar için hedeflenen veri hızlarının, WSAoI'yı en aza indiren güç tahsisinde ¨önemli bir rol oynadığı gözlenmektedir. Genel olarak, sinyal gürültü oranı arttıkça politikanın RSMA modunda ¸çalışma olasılığının arttığı görülmektedir. NOMA/OMA yerine RSMA kullanılırsa, güç tüketiminde; seçili 4, 3 ve 2 WSAoI değerleri için ortalama %65.8, %62.3, ve %59.6 tasarruf sağlanabilir.","With the commercial deployments of 5G, research in Beyond 5G (B5G) and 6G networks has started. Within the context of meeting all needs and demands of future generation networks, the predicted usage is envisaged in three cases: massive Machine-Type Communications (MTC), ultra-reliable low-latency communications, and enhanced mobile broadband. This thesis focuses on massive Machine-Type Communications (mMTC). Energy efficiency, under the banner of green communications and networking is one of the branches complementary to the research conducted on MTC. mMTC, industrial and medical Internet of Things (IoT) type technologies will demand not only networking capabilities for massive access, enhanced communications, but also sustainability and power efficiency. Rate Splitting Multiple Access (RSMA) presents a candidate massive access scheme with spectral efficiency, energy efficiency, reliability, Degree-of-Freedom (DoF) and Quality of Service (QoS) enhancements in most of user deployments and network loads over traditional access schemes used in 5G. Within the scope of the thesis, we propose an age-aware power allocation policy for minimizing the network's Weighted-Sum Average AoI (WSAoI). To our knowledge, this is the first work in the literature which combines the Age of Information (AoI) concept and RSMA framework. For downlink communication, we formulate the network's WSAoI minimization as a Markov Decision Process (MDP) and investigate an optimal as well as suboptimal policies for the Base Station (BS) to select a scheme among RSMA, Orthogonal Multiple Access (OMA), and Nonorthogonal Multiple Access (NOMA). We prove existence of an optimal policy. Complexity of computation is reduced by using an action elimination technique, and by using a sub-optimal policy with performance close to the optimal. We also investigate the tradeoff between energy and the WSAoI of the network. The adaptive RSMA only scheme outperforms adaptive RSMA/NOMA/OMA and OMA/NOMA on the basis of network's WSAoI. For example, when RSMA is selected, the performance metric, WSAoI, at 14, 15, and 16 dB SNR values, is on average, respectively 35.8%, 15.7%, and 12.7% less than the NOMA/OMA cases. Overall, it is seen that, the optimum policy becomes more likely to operate in the RSMA mode with an increase in Signal to Noise Ratio (SNR). By using RSMA scheme instead of NOMA/OMA scheme, power consumption can be saved in average 65.8%, 62.3%, and 59.6% for the selected WSAoI values of 4, 3, and 2, respectively."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüz fotoğraflarını sanatsal portre çizimlerine dönüştürmek, uzman sanatçılar için zaman alıcı bir iştir, dolayısıyla bu sürecin otomatikleştirilmesine ihtiyaç vardır. Bir fotoğraftan portre oluşturmak yalnızca basit bir görüntü dönüştürme işlemi değildir; karmaşıklıklarla doludur. Süreç, birçok karmaşık ayrıntıyı gözden kaçırırken temel yüz özelliklerini vurgulamalıdır. Bu nedenle, yetenekli bir görüntü dönüştürme aracının yüzün seçilmiş yönlerini algılaması ve koruması gerekir. Araştırmamızda, yalnızca eşleştirilmemiş verilerden öğrenen ve hiçbir ekstra etiketleme içermeyen bir portre çizimi yöntemi öneriyoruz. Denetimsiz özellik öğreniminden yararlanan yöntemimiz, başarılı genelleştirme davranışı gösterir. İlk katkımız, görüntülerin üst düzey detaylarını denetlenmeyen parçalarla ve sığ ağların kimlik koruma davranışını birleştiren bir görüntü çeviri mimarisidir. İkinci katkımız yeni bir asimetrik poz temelli döngü tutarlılığı kayıp fonksiyonudur. Bu kayıp, bir giriş görüntüsünün bir portreye dönüştürüldükten ve tekrar giriş görüntüsüne dönüştürüldükten sonra yeniden oluşturulmasını gerektiren döngü tutarlılığı kaybı üzerindeki kısıtlamayı hafifletir. Son olarak hem alan içi hem de alan dışı görüntüler üzerinde kapsamlı deneyler yaptık ve yöntemimizi en son teknoloji yaklaşımlarla karşılaştırdık. Üç veri kümesinde hem niceliksel hem de niteliksel olarak önemli gelişmeler gösteren modelimizi sunuyoruz.","Translating face photos to artistic drawings by hand is a complex task that typically needs the expertise of professional artists. The demand for automating this artistic task is clearly on the rise. Turning a photo into a hand-drawn portrait goes beyond simple transformation. This task contemplates a sophisticated process that focuses on highlighting key facial features and often omits small details. Thus, designing an effective tool for image conversion involves selectively preserving certain elements of the subject's face. In our study, we introduce a new technique for creating portrait drawings that learn exclusively from unpaired data without the use of extra labels. By utilizing unsupervised learning to extract features, our technique shows a promising ability to generalize across different domains. Our proposed approach integrates an in-depth understanding of images using unsupervised components and the ability to maintain individual identity, which is typically seen in simpler networks. We also present an innovative concept: an asymmetric pose-based cycle consistency loss. This concept introduces flexibility to the traditional cycle consistency loss, which typically expects an original image to be perfectly reconstructed after being converted to a portrait and then reverted. In our comprehensive testing, we evaluate our method with both in-domain and out-domain images and benchmark it against the leading methods. Our findings reveal that our approach yields superior results, both numerically and in terms of visual quality, across three different datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar grafiği ve bilgisayarlı görü alanlarında, tek bir görüntüden üç boyutlu insan dokusunun doğru bir şekilde tahmin edilmesi kritik bir görevdir. Bu süreç, insanların çeşitli pozlardaki giriş görüntülerini parametrik (UV) uzaya dönüştüren bir haritalama fonksiyonu geliştirmeyi ve görünmeyen kısımların görünümünü etkin bir şekilde çıkarmayı içerir. Üç boyutlu insan doku tahmininin kalitesini artırmak için, çalışmamız, deformasyonlu evrilim kullanan bir yöntem sunmaktadır. Bu evrilim, gelişmiş bir derin sinir ağı aracılığıyla öğrenilen ofsetlerle özgün bir şekilde karakterize edilmiştir. Ek olarak çalışmamızda, görünümü genelleştirmeyi belirgin şekilde iyileştiren yenilikçi bir döngü tutarlılığı kaybı tanıtıyoruz. Metodumuz, renk doğruluğunu artırmayı amaçlayan belirsizliğe dayalı, piksel seviyesinde görüntü yeniden yapılandırma kaybı ekleyerek daha da iyileştirilmiştir. Alanın öncü yöntemleriyle kapsamlı karşılaştırmalar yaparak, yaklaşımımızın alanda dikkate değer niteliksel ve niceliksel ilerlemeler gösterdiğini ortaya koymaktadır.","In the fields of graphics and vision, accurately estimating 3D human texture from a single image is a critical task. This process involves developing a mapping function that transforms input images of humans in various poses into parametric (UV) space, while also effectively inferring the appearance of unseen parts. To enhance the quality of 3D human texture estimation, our study introduces a framework that utilizes deformable convolution for adaptive input sampling. This convolution is uniquely characterized by offsets learned through a sophisticated deep neural network. Additionally, we introduce an innovative cycle consistency loss, which markedly enhances view generalization. Our framework is further refined by incorporating an uncertainty-based, pixel-level image reconstruction loss, aimed at augmenting color accuracy. Through comprehensive comparisons with leading-edge methods, our approach demonstrates notable qualitative and quantitative advancements in the field."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayarlı görünün önemli alanlarından biri olan nesne tanıma alanındaki son gelişmeler, kamera donanımlı İnsansız Hava Araçları (İHA) için ortaya çıkan yeni çözümler sağlar. İnsansız Hava Araçları'ndan alınan görüntüler üzerinde çalışan, nesne tanıma çözümlerinin başarımı, yerden alınan görüntüler için nesne tanıma çözümlerinin performansı ile karşılaştırıldığında daha sınırlıdır. Mevcut nesne tanıma çözümleri, İHA'ların doğasından kaynaklanan nedenlerle İHA veri kümelerinde daha düşük performans göstermektedir. Bu nedenler: (i) farklı nesne boyutlarına sahip büyük drone veri setlerinin olmaması, (ii) drone görüntülerinin hem ölçek hem de yönelim olarak daha büyük varyanslara sahip olması (iii) yer ve hava arasındaki şekil ve doku özelliklerindeki farklılık. Bu sebeplerden dolayı, tek kademeli nesne tanıma yöntemlerinin bir parçası olan YOLO tabanlı modeller, İHA tabanlı uygulamalarda daha düşük performans göstermektedir. Bu tezde, YOLO algoritması üzerine YOLODrone ve YOLODrone+, İHA görüntülerindeki nesneleri tanımak için geliştirilmiştir. Modellerin performansı VisDrone2019 ve SkyDataV1 veri kümelerinde test edilip; YOLOv3 ve YOLOv5 ile karşılaştırıldığında sonuçlarda iyileştirme gözlemlenmiştir.","Recent advances in computer vision yield emerging novel applications for camera-equipped unmanned aerial vehicles such as object detection. The accuracy of the existing object detection solutions running on images acquired by Unmanned Aerial Vehicles (UAVs) is limited when compared to the performance of the object detection solutions designed for ground-taken images. Existing object detection solutions demonstrate lower performance on aerial datasets because of the reasons originating from the nature of the UAVs. These reasons can be summarized as: (i) the lack of large drone datasets with different types of objects, (ii) the larger variance in both scale and orientation of objects in drone images, and (iii) the difference in shape and texture of the features between the ground and the aerial images. Due to these reasons, YOLO-based models, a popular family of one-stage object detectors, perform insufficiently in UAV-based applications. In this thesis, two improved YOLO models: YOLODrone and YOLODrone+ are introduced for detecting objects in drone images. The performance of the models are tested on VisDrone2019 and SkyDataV1 datasets and improved results are reported when compared to the original YOLOv3 and YOLOv5 models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derleyiciler yüzlerce eniyileme seçeneği sunar ve iyi bir eniyileme dizisi seçmek yoğun çaba ve uzman girdisi gerektirdiği için karmaşık ve zaman alan bir iştir. Bu nedenle, literatürde eniyilemeri otomatik olarak bulmaya odaklanan birçok araştırma mevcuttur. Bu araştırmaların çoğu statik, uzamsal veya dinamik özellikleri kullanırken son zamanlarda, derin sinir ağlarını kaynak koduna doğrudan uygulamaya yönelik araştırmalara ağırlık verilmektedir. Biz de bu tez çalışmasında, kaynak kodunu çizge biçiminde temsil ederek statik özellikleri, uzamsal özellikleri ve derin sinir ağlarını birleştirdik. Otomatik olarak en uygun eniyileme seçeneklerini bulmak için ise bir Çizge Yapay Sinir Ağı (GNN) modeli eğittik. Modeli test etmek için sekiz adet ikili eniyileme bayrağı ile Polybench ve cBench adında iki adet veri kümesi seçtik. 47 program ve 256 adet eniyileme seti kombinasyonu kullanarak 12000 çizgeden oluşan bir veri seti oluşturduk. Modelimizi bu çizgeleri kullanarak eğittik ve test ettik. Sonuçlar, tüm eniyileme seçeneklerinin etkinleştirildiği duruma kıyasla maksimum %48,6 hız artışı elde edebileceğimizi gösterdi.","Compilers provide hundreds of optimization options, and choosing a good optimization sequence is a complex and time-consuming task. It requires extensive effort and expert input to select a good set of optimization flags. Therefore, there is a lot of research focused on finding optimizations automatically. While most of this research considers using static, spatial, or dynamic features, some of the latest research directly applied deep neural networks on source code. We combined the static features, spatial features, and deep neural networks by rep- resenting source code as graphs and trained Graph Neural Network (GNN) for automatically finding suitable optimization flags. We chose eight binary optimization flags and two benchmark suites, Polybench and cBench. We created a dataset of 12000 graphs using 256 optimization flag combinations on 47 benchmarks. We trained and tested our model using these benchmarks, and our results show that we can achieve a maximum of 48.6% speed-up compared to the case where all optimization flags are enabled."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"C¸ ok-g¨orevli ¨o˘grenmenin, problemin farklı y¨onlerini yansıtmak ¨uzere yardımcı g¨orevleri tanımlamasının ve bunları ana g¨orev olan segmentasyon ile e¸s zamanlı olarak ¨o˘grenmesinin, yo˘gun bir tahmin a˘gının segmentasyon g¨orevindeki performansını artırmak i¸cin etkili bir strateji oldu˘gu kanıtlanmı¸stır. Onceki ¸calı¸smalar, ¨ yardımcı g¨orevlerini Oklid uzayında tanımlamı¸slardır. Ancak, bazı b¨ol¨utleme ¨ g¨orevlerinde, ilgi b¨olgesinin dokusundaki karma¸sıklık ve y¨uksek de˘gi¸skenlik, Oklid ¨ geometrisindeki d¨uzg¨unl¨uk kısıtlamasına uymayabilir. Bu tez, bir g¨or¨unt¨udeki doku karma¸sıklı˘gını birbirine benzer desenler aracılı˘gıyla nicelle¸stirmek i¸cin fraktal geometriyi kullanan yeni bir ¸cok-g¨orevli a˘g olan MTFD-Net modelini tanımlayarak bu sorunu ele almaktadır. Bu ama¸cla, bir g¨or¨unt¨uy¨u fraktal boyut bir haritasına d¨on¨u¸st¨urmeyi ve bunun ¨o˘grenilmesini yardımcı bir g¨orev olarak tanımlamayı ¨oneriyoruz. Bu, bilgisayarlı tomografi g¨or¨unt¨ulerinde sol kulak¸cık segmentasyonunun iyile¸stirilmesine y¨onelik ana segmentasyon g¨orevine yardımcı denetim sa˘glamak amacıyladır. Bildi˘gimiz kadarıyla, bu, bir yardımcı g¨orevi tanımlamak i¸cin fraktal geometriyi kullanan ve bunu ¸cok-g¨orevli bir ¨o˘grenme modelinde, b¨ol¨utleme g¨orevine paralel olarak ¨o˘grenen yo˘gun tahmin a˘gının a˘gı ¨oneren ilk ¸calı¸smadır. Deneylerimiz, ¨onerilen MTFD-Net modelinin, benzerlerine kıyasla daha do˘gru sol kulak¸cık segmentasyonuna olanak sa˘gladı˘gını ortaya koymu¸stur.","Multi-task learning proved to be an effective strategy to increase the performance of a dense prediction network on a segmentation task, by defining auxiliary tasks to reflect different aspects of the problem and concurrently learning them with the main task of segmentation. Up to now, previous studies defined their auxiliary tasks in the Euclidean space. However, for some segmentation tasks, the complexity and high variation in the texture of a region of interest may not follow the smoothness constraint in the Euclidean geometry. This thesis addresses this issue by introducing a new multi-task network, MTFD-Net, which utilizes the fractal geometry to quantify texture complexity through self-similar patterns in an image. To this end, we propose to transform an image into a map of fractal dimensions and define its learning as an auxiliary task, which will provide auxiliary supervision to the main segmentation task, towards betterment of left atrium segmentation in computed tomography images. To the best of our knowledge, this is the first proposal of a dense prediction network that employs the fractal geometry to define an auxiliary task and learns it in parallel to the segmentation task in a multi-task learning framework. Our experiments revealed that the proposed MTFD-Net model led to more accurate left atrium segmentation, compared to its counterparts."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde yüksek kaliteli resim düzenleme elde etmek için yeni bir görüntü yeniden yapılandırma metodu ve yeni bir yapay sinir eğitim hattı sunuyoruz. Gerçek görüntüleri StyleGAN'ın gizli alanına cevirmek kapsamlı bir şekilde incelenen bir sorundur. Ancak görüntü yeniden yapılandırma ve resim düzenlenebilirliği arasında bir takas vardır. Düşük oranlı gizli alanların, yüksek doğrulukta yeniden yapılanma için ifade gücü sınırlıdır. Öte yandan, yüksek oranlı gizli alanlar düzenleme kalitesinin bozulmasına neden olur. Bu çalışmada, yüksek doğruluklu ters çevirme elde etmek için, daha düşük gizli kodların kodlayamadığı, yüksek gizli kodlardaki artık özellikleri öğreniyoruz. Bu, yeniden yapılandırma sırasında görüntü ayrıntılarının korunmasını sağlar. Yüksek kaliteli düzenleme elde etmek için, gizli kodlardaki manipülasyonlara uyum sağlamak üzere artık özelliklerin nasıl dönüştürüleceğini öğreniyoruz. Artık özellikleri çıkarmak için ağımızı eğitiyor ve bunları yeni bir eğitim hattı ve döngü tutarlılığı kullarak karşımıza çıkan düzenlemelere göre değiştiriyoruz. Yöntemimizin kapasitesini göstermek adında kapsamlı deneyler yapıp onu en son teknolojiye sahip ters çevirme yöntemleriyle karşılaştırdık. Kalitatif ölçümler ve görsel karşılaştırmalar önemli gelişmeler göstermektedir.","We present a novel image inversion framework and a training pipeline to achieve high-fidelity image inversion with high-quality attribute editing. Inverting real images into StyleGAN's latent space is an extensively studied problem, yet the trade-off between image reconstruction fidelity and image editing quality remains an open challenge. The low-rate latent spaces are limited in their expressiveness power for high-fidelity reconstruction. On the other hand, high-rate latent spaces result in degradation in editing quality. In this work, to achieve high-fidelity inversion, we learn residual features in higher latent codes that lower latent codes were not able to encode. This enables preserving image details in reconstruction. To achieve high-quality editing, we learn how to transform the residual features for adapting to manipulations in latent codes. We train the framework to extract residual features and transform them via a novel architecture pipeline and cycle consistency losses. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir hastalığın nedenlerini anlayabilmek için hastalıkla ilişkili proteinlerin belirlenmesi ve önceliklendirilmesi önemli bir bilimsel problemdir. Ağ (network) bilimi bu proteinlerin önceliklendirilmesi için önemli bir disiplin haline gelmiştir. Halen tamamen tedavi edilemeyen bir otoimmün hastalık olan multipl skleroz (MS) demiyelinizasyon olarak adlandırılan tahrip edici bir biyolojik olay ile karakterize edilir. Demiyelinizasyon, yüksek önem taşıyan sinir kılıfı miyelinin ve miyelini üreten hücreler olan oligodendrositlerin savunma hücreleri tarafından tahrip edilmesidir. Oligodendrosit ve savunma hücrelerinin proteinlerinden oluşan protein ağında özel nitelikleri bulunan proteinlerin tespit edilmesi, hastalık ile ilgili faydalı bilgiler açığa çıkarabilir. Bu amaçla, demiyelinizasyonda rol alan iki hücrenin etkileşimini sağlayan proteinler arasından köprüler olarak tanımladığımız hücre içi ve hücreler arası protein ağları için en önemli protein çiftlerini araştırdık. Oligodendrosit ile makrofaj ve T hücresi olmak üzere iki tip savunma hücresinin oluşturduğu iki protein ağını analiz ettik. Hücreler arası etkileşim sağlayan proteinleri ağ analiz teknikleri ve tam sayılı programlama ile önceliklendiren BriFin olarak adlandırdığımız bir metot geliştirdik. BriFin'in öncelikli olarak belirlediği proteinlerin bir kısmının ilişkili literatürde hastalık ile hâlihazırda ilişkilendirildiğini gösterdik. Oligodendrosit-makrofaj ağı için, BriFin'in tespit ettiği proteinlerin parametrizasyona bağlı olarak %77 ila %100'ünün MS ile ilişkili olduğunu gösterdik. Ayrıca, BriFin ile önceliklendirdiğimiz 4 proteini deneysel olarak araştırarak 2 tanesinin mRNA ekspresyon seviyelerinin bir grup MS hastasında anlamlı ölçüde azaldığını gözlemledik. Dolayısıyla burada, iki hücre arasındaki etkileşimlerin önemli rol oynadığı süreçleri analiz etmek için kullanılabilecek bir model olan BriFin'i sunuyoruz.","Identifying and prioritizing disease-related proteins is an important scientific problem to understand disease etiology. Network science has become an important discipline to prioritize such proteins. Multiple sclerosis (MS), an autoimmune disease which still cannot be cured, is characterized by a damaging process called demyelination. Demyelination is the destruction of the crucial nerve sheath, myelin, and oligodendrocytes, the cells producing myelin, by immune cells. Identifying the proteins having special features on the network formed by the proteins of oligodendrocyte and immune cells can reveal useful information about the disease. To this end, we investigated the most significant protein pairs for the intra- and intercellular protein networks that we define as bridges among the proteins providing the interaction between the two cells in demyelination. We analyzed two protein networks including the oligodendrocyte and each type of two immune cells, macrophage and T-cell. We developed a model called BriFin that prioritizes contact protein pairs using network analysis techniques and integer programming. We showed several proteins it prioritized have already been associated with MS in the relevant literature. For the oligodendrocyte-macrophage network, we showed that 77\% to 100\% of the proteins BriFin detected, depending on the parametrization, are MS-associated. We further experimentally investigated 4 proteins prioritized by BriFin, and observed that the mRNA expression levels of 2 out of these 4 proteins significantly decreased in a group of MS patients. We therefore here present a model, BriFin, which can be used to analyze processes where interactions of two cell types play an important role."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Organ segmentasyonu, hastalık teşhisinde ve radyasyon tedavisi planlamasında çok önemli bir rol oynar. Manuel segmentasyon zaman alıcı ve maliyetli olmasının yanı sıra gözlemciler arası değişkenliğe de yatkın olduğundan, risk altındaki organların (RAO'ların) verimli ve otomatik segmentasyonu daha önemli bir hale gelir. Risk altındaki organların derin öğrenme kullanılarak otomatik segmentasyonu, özellikle merkez kesitlerin şeklinin apikal ve bazal kesitlerden farklı olduğu organlarda, hata yaparak dış bölgeleri tahmin etmeye eğilimlidir. Bu tez, şekil ve anatomik içerik hakkındaki ön bilgileri derin öğrenmeye dayalı organ segmentasyonuna dahil etmek için yeni bir yöntem sunar. Bu ön bilgiyi nicelemek için, organın şekli, konumu ve organ pozisyonunun çevredeki organlara göre ilişki karakteristiklerini yakalayan mesafe dönüşümlerini kullanır. Bu tezde, mesafe dönüşümü regresyonunun tek başına veya sınıflandırma ile birlikte kullanılmasının organ segmentasyonu ağının genel performansını iyileştirdiğini göstermek için çeşitli mesafe dönüşümlerinin rolü araştırılmıştır. Mesafe dönüşümü olarak, her piksel ile organın merkezi arasındaki mesafe veya örneğin yemek borusu ve omurga gibi iki organ arasındaki en yakın mesafe kullanılmıştır. Bu tez, tek görev regresyon modelinde kullanıldığında, bu mesafe dönüşümlerinin segmentasyon sonuçlarını iyileştirdiğini göstermiştir. Ayrıca, sınıflandırmanın diğer görev olduğu çok görevli bir ağda kullanıldığında, sınıflandırma görevi için düzenleyici olarak çalışıp segmentasyonu iyileştirdiği gözlemlenmiştir. Deneyler, 265 hastanın bilgisayarlı tomografi (BT) toraks veri seti üzerinde gerçekleştirilmiştir. Kalp, yemek borusu, akciğerler ve omurga ilgi bölgeleri olarak seçilmiştir. Sonuçlar, önerilen model kullanılarak ROA'lar için segmentasyon yapıldığında, temel ağ mimarilerine kıyasla, f-skorlarında önemli bir artış ve Hausdorff mesafelerinde azalma olduğunu göstermiştir.","Organ segmentation plays a crucial role in disease diagnosis and radiation therapy planning. Efficient and automated segmentation of the organs-at-risk (OARs) requires immediate attention since manual segmentation is a time consuming and costly task that is also prone to inter-observer variability. Automatic segmentation of organs-at-risk using deep learning is prone to predicting extraneous regions, especially in apical and basal slices of the organs where the shape is different from the center slices. This thesis presents a novel method to incorporate prior knowledge on shape and anatomical context into deep-learning based organ segmentation. This prior knowledge is quantified using distance transforms that capture characteristics of the shape, location, and relation of the organ position with respect to the surrounding organs. In this thesis, the role of various distance transform maps has been explored to show that using distance transform regression, alone or in conjunction with classification, improves the overall performance of the organ segmentation network. These maps can be the distance between each pixel and the center of the organ, or the closest distance between two organs; such as the esophagus and the spine. This thesis showed that when used in a single-task regression model, these distance maps improved the segmentation results. Moreover, when used in a multi-task network with classification being the other task, they acted as regularizers for the classification task and yielded improved segmentations. The experiments were conducted on a computed tomography (CT) thorax dataset of 265 patients and the organs of interest are the heart, the esophagus, the lungs, and the spine. The results revealed a significant increase in f-scores and decrease in the Hausdorff distances for the OARs when segmented using the proposed model compared to the baseline network architectures."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Matris tamamlama problemi için paralel olasılıksal gradyan alçalma(SGD) algoritması incelenmektedir. Literatürdeki uygulamalar bayat veri kullanımı ve iletişim ücretlerinin paralel SGD'nin performansını etkileyen önemli etmenler olduğunu göstermektedir. Ilk olarak SGD algoritmasını ve paralel SGD için matris bölümlemesini incelenmektedir. Daha sonra paralel SGD'nin performansını iyileştirmek için yeni bir algoritma önermekteyiz. Bu algoritma iterasyon içi senkronizasyonlarla (alt-iterasyon olarak adlandırılmaktadır) iletişim ücretlerini ve bayat veri kullanımını azaltmayı amaçlamaktadır. Deney sonuçları alt-iterasyon kullanımında bayat veri kullanımının %95 oranında ve iletişim hacminin %47 oranında azaldığını göstermektedir. Bununla beraber, alt-iterasyon kullanımının test verisi hatasını alt-iterasyonsuz algoritmayla karşılaştırıldığında %60 oranında iyileştirebildiği görülmüştür.","We investigate parallelization of the stochastic gradient descent (SGD) algorithm for solving the matrix completion problem. Applications in the literature show that stale data usage and communication costs are important concerns that affect the performance of parallel SGD applications. We first briefly visit the stochastic gradient descent algorithm and matrix partitioning for parallel SGD. Then we define the stale data problem and communication costs. In order to improve the performance of parallel SGD, we propose a new algorithm with intra-iteration synchronization (referred as sub-iterations) to decrease communication costs and stale data usage. Experimental results show that using sub-iterations can decrease staleness up to 95% and communication volume up to 47%. Furthermore, using sub-iterations can improve test error up to 60% when compared to the conventional parallel SGD implementation that does not use sub-iterations."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmanın amacı, Olasılıklsal Gradyan Alçalma (SGD) algoritmasının dağınık bellekli yüksek başarımlı hesaplama platformlarında verimli bir şekilde paralelleştirmelerini araştırmaktır. Paralel SGD'nin yüzlerce işlemciye kadar ölçeklenebilirliğini göstermek için işlemciler arasında asenkron iletişim kurabilen, hibrit mimarye sahip ve merkezsizleştirilmiş bir SGD algoritması öneriyoruz. İş birimleri arası iletişim için Message Passing Interface'i (MPI) ve iş birimleri içersindeki paralellik içinse POSIX iş parçacıklarını kullanıyoruz. Dört farklı kıyaslama veri seti kullanarak yöntemimizi test ettik. Deneysel sonuçlar, önerilen algoritmanın görece seyrek veri setleri üzerinde 6 kata kadar daha fazla verim elde ettiğini ve hem esnek bir bölümleme şeması hemde yüksek düzeyde ölçeklenebilir hibrit bir mimari sağlarken görece yoğun veri setleri üzerinde de mevcut en gelişmiş algoritmalarla karşılaştırılabilir sonuçlar verdiğini göstermektedir.","The purpose of this study is to investigate the efficient parallelization of the Stochastic Gradient Descent (SGD) algorithm for solving the matrix completion problem on a high-performance computing (HPC) platform in distributed memory setting. We propose a hybrid parallel decentralized SGD framework with asynchronous communication between processors to show the scalability of parallel SGD up to hundreds of processors. We utilize Message Passing Interface (MPI) for inter-node communication and POSIX threads for intra-node parallelism. We tested our method by using four different real-world benchmark datasets. Experimental results show that the proposed algorithm yields up to 6 times better throughput on relatively sparse datasets, and displays comparable performance to available state-of-the-art algorithms on relatively dense datasets while providing a flexible partitioning scheme and a highly scalable hybrid parallel architecture."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hata takibi, bir yazılımda bulunan problemleri izleme ve raporlama sürecidir. Literatürde kabul görmüş bir hata takip süreci olmasa da, en işlevsel hata takip süreci için bazı kurallar ve kabul gören uygulamalar, birçok şirket ve açık kaynaklı yazılım projesi tarafından kullanılmaktadır. Farklı platformlar arasında küçük farklılıklar olsa da tüm bu kural ve uygulamaların temel amacı daha verimli bir hata takip süreci gerçekleştirmektir. Yazılım mühendislerinin, bu kabul gören uygulamaları izlememesi, yalnızca hata takip sürecinin faydalarını azaltmakla kalmaz, aynı zamanda yazılım geliştirme yaşam döngüsünün diğer aşamalarını da olumsuz etkiler. Bu çalışmanın amacı, hata takip sürecinde meydana gelen kötü uygulamaları yani hata takip süreci kokularını incelemektir. Bunun için, çok sesli bir literatür taramasının sonuçlarına dayanarak, akademik ve gri literatürdeki 60 kaynağı analiz ettik ve hata takip sürecindeki 12 kötü uygulamayı içeren bir taksonomi önerdik. Kötü uygulamaları nicel olarak analiz etmek için 4'ü Jira tabanlı, 2'si Bugzilla tabanlı olmak üzere 6 projeden toplanan hata raporlarını inceledik. Ardından, yazılım mühendislerinin, bu taksonomi hakkındaki düşüncelerini öğrenmek için farklı ülkelerden 30 yazılım profesyoneli ile bir hedefli anket gerçekleştirdik. Ayrıca, kötü uygulamaların, bu uygulamalardan etkilenen hatalardaki kapanma süresi ve yeniden açılma sayısını istatistiksel olarak nasıl etkilediğini analiz ettik. Ampirik sonuçlara dayanarak, tüm projelerde önemli sayıda kötü uygulamanının bulunduğunu ve bazı kötü uygulamaların kalite ve hız üzerinde istatistiksel olarak önemli etkileri olduğunu gözlemledik. Anket sonuçları, yazılım profesyonellerinin çoğunun kötü uygulama taksonomimizle aynı fikirde olduğunu gösterdi. Ampirik analiz, hata takibindeki kötü uygulamaların açık kaynak kodlu projeler üzerinde önemli bir etkiye sahip olduğunu ortaya koymaktadır. Bu çalışmada önerilen taksonomi, hata takip sürecinde kötü uygulamaları tespit etmek ve önlemek için geliştirilecek olan araçlara bir temel oluşturabilir.","Bug tracking is the process of monitoring and reporting malfunctions or issues found in software. While there is no consensus on a formally specified bug tracking process, some certain rules and best practices for an optimal bug tracking process are accepted by many companies and open-source software (OSS) projects. Despite slight variations between different platforms, the primary aim of all these rules and practices is to perform a more efficient bug tracking process. Practitioners' non-compliance with the best practices not only impedes the benefits of the bug tracking process but also negatively affects the other phases of the life cycle of software development. The goal of this study is to gain a better knowledge of the bad practices that occur during the bug tracking process, that is \textit{bug tracking process smells}. In this study, based on the results of a multivocal literature review, we analyzed 60 sources in academic and gray literature and propose a taxonomy of 12 bad practices in the bug tracking process, that is \textit{bug tracking process smells}. To quantitatively analyze these process smells, we inspected bug reports collected from six projects (four of them are Jira-based and the other two are Bugzilla-based). To get an idea about the perception of practitioners about the taxonomy of bug tracking process smells, we conducted a targeted survey with 30 software practitioners from different countries. Moreover, we statistically analyzed the impact of bug tracking process smells on the resolution time and reopening count of bugs. We observed from our empirical results that a considerable amount of bug tracking process smells exist in all projects and some of the process smell categories have a statistically significant impact on quality and speed. Survey results showed that the majority of software practitioners agree with our taxonomy of bug tracking process smells. The empirical analysis reveals that bug tracking process smells have a significant impact on OSS projects. In practice, the proposed taxonomy may serve as a foundation for best practices and tool assistance for detecting and avoiding bug tracking process smells."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kavram kayması, girdi verileri ve hedef etiketleri arasındaki ilişkinin zaman içinde değişmesi olarak tanımlanabilir. Bu çalışmada, kavram kayması içeren metin akışlarının anlık sınıflandırılması incelenmektedir. Kavram kayması, zamanla değişen veri varyasyonlarına bağlı olarak modellerin yanlış tahminlerinin artmasına ve performanslarının düşmesine neden olur. Bunu engellemek için kavram kaymasına adapte olabilen modeller geliştirmek gereklidir. Sunduğumuz Uyarlanabilir Sinir Topluluğu Ağı'nı (AdaNEN) modeli, metin akışlarında kavram kaymasına uyum sağlayabilen, topluluk tabanlı yeni bir sinirsel yaklaşım mimarisidir. Özgün AdaNEN mimarisi, çevrimiçi uyarlanabilir öğrenme ortamlarının yarattığı çoğu soruna çözüm üretir. Gelişen metin akışı sınıflandırması problemi, görece az araştırılmıştır. Mevcut çalışmaların çoğu, sayısal akışlarda kavram kayması algılama ve işlemeye yöneliktir. Bunun nedeninin açık ve büyük ölçekli deneysel verilerin eksikliği olduğunu düşünüyoruz. Bu doğrultuda, mevcut literatürdeki bir yaklaşıma dayalı, gerçek dünya metin veri kümelerine çeşitli kavram kaymaları ekleyerek gelişen metin akışları oluşturmaya yönelik, mevcut yeni bir yöntem öneriyoruz. AdaNEN'i teknolojiyi temsil eden 12 referans model ile sekiz veri seti üzerinden karşılaştırılarak yaklaşımımızın kapsamlı bir değerlendirmesini sunuyoruz. Deneysel sonuçlarımız, önerilen yöntemimiz AdaNEN'in, koruyucu verimlilikle tahmine dayalı mevcut yaklaşımlardan tutarlı bir şekilde daha iyi performans gösterdiğini göstermektedir.","We study on-the-fly classification of evolving text streams in which the relation between the input data target labels changes over time---i.e. ``concept drift''. These variations decrease the model's performance, as predictions become less accurate over-time and they necessitate a more adaptable system. We introduce Adaptive Neural Ensemble Network (AdaNEN), a novel ensemble-based neural approach, capable of handling concept drift in text streams. With our novel architecture, we address some of the problems neural models face when exploited for online adaptive learning environments. The problem of evolving text stream classification is relatively unexplored and most existing studies address concept drift detection and handling in numerical streams. We hypothesize that the lack of public and large-scale experimental data could be one reason. To this end, we propose a method based on an existing approach for generating evolving text streams by inducing various types of concept drifts to real-world text datasets. We provide an extensive evaluation of our proposed approach using 12 state-of-the-art baselines and eight datasets. Our experimental results show that our proposed method, AdaNEN, consistently outperforms the existing approaches in terms of predictive performance with conservative efficiency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge uygulamaları birçok alanda kullanılmaktadır ancak ağır, düzensiz ve veriye dayalı bellek erişim kalıpları nedeniyle genel amaçlı bilgi işlem sistemlerinde düşük performans göstermektedir. Gerçek hayat çizgelerinin farklı topolojik yapıları da performansı etkiler. Performans sorunlarını azaltmak ve enerji verimliliği sağlamak için birçok donanım hızlandırıcı önerilmiş olsa da, programlanabilirlik ve esneklik yeterince ele alınmamıştır. Bu tez, RISC-V Komut Kümesi Mimarisi'nin (KKM) genişletilmesine dayalı alana özgü bir işlemci tasarımı sunar. Önerilen yaklaşımda, derleyici ve bir yazılım kütüphanesi tarafından desteklenen yeni komutlar kullanılır. Yeni komutların yürütülmesi için mikro mimari tasarım, bir hızlı işlem belleği (HİB), önyükleyici ve tıkanmasız bir önbellek sistemine dayanmaktadır. Özel işlemci, System Verilog HDL kullanılarak gerçekleştirildi ve Xilinx'in Vivado Design Suite kullanılarak simüle edildi. LLVM Derleyici Çatısı, derleyici desteği ve optimizasyonu için kullanıldı. Öte yandan, özel komutları kullanmak için yazılım kütüphanesi, Topla-Uygula-Dağıt (TUD) paradigmasını kullanır. Sistem, iyi bilinen grafik denektaşlarında değerlendirilirken, minimum maliyetle en iyi performansı elde etmek için çeşitli parametreler üzerinde duyarlılık analizi yapıldı. Performans, hem yerli denektaşlarında hem de yazılım kütüphanesinde ölçüldü. Bunun yanında, derleyici desteği performans üzerindeki etkisi açısından değerlendirildi. Düşük maliyetli performans ölçümleri farklı denektaşları için ortalama %10 ile \%49 arasında hızlanma gösterirken, tek çekirdekli mimari %73'e kadar ulaşabilir.","Graph applications are employed in many fields but show poor performance on general-purpose computing systems due to heavy, irregular, and data-driven memory access patterns. The diverse topology of real-life graphs also affects the performance. Even though many hardware accelerators have been proposed to mitigate performance issues and provide energy efficiency, programmability and flexibility have not been sufficiently addressed. This thesis presents a domain-specific processor design based on extending the RISC-V Instruction Set Architecture (ISA). The proposed approach uses new instructions supported by the compiler and software library. Micro-architectural design for executing the new instructions is based on a scratchpad-memory (SPM), prefetcher, and a non-blocking cache system. The custom processor is implemented using System Verilog HDL and simulated with Xilinx's Vivado Design Suite. LLVM Compiler Framework is used for compiler support and optimization. The software library for utilizing the custom instructions uses Gather-Apply-Scatter (GAS) paradigm. The system is evaluated on well-known graph benchmarks, while sensitivity analysis is done on various parameters for achieving the best performance with minimal cost. Performance is measured on both native benchmarks and the software library. In addition, compiler support is evaluated for its effect on performance. Cost-efficient performance evaluations show average speedups between 10% and 49% for different benchmarks, while the single-core architecture can achieve up to 73%."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğal Dil İşleme'nin (DDİ) hukuk alanındaki uygulamaları araştırmacıların son yıllarda artarak ilgisini çekerken Türk hukuku için olası uygulamaları inceleyen bir çalışma bulunmamaktadır. Biz, ilk olarak DDİ'nin halihazırda literatürde olan hukuki uygulamalarını gözden geçiriyor, sonra da Türk mahkemeleri özelinde dava metinlerinden mahkeme kararını tahmin etme problemini ele alıyoruz. Bu tezin alana başlıca iki katkısından ilki, Türk yüksek mahkemelerinden Anayasa Mahkemesi ve İstinaf Mahkemelerinin karar metinlerinden kapsamlı bir derlem oluşturulmasıdır. İkinci katkısı ise, bu dava metinleri üzerinden mahkemenin kararını tahmin etmede çeşitli DDİ yöntemlerinin nicel olarak incelenmesi ve kıyaslanmasıdır. Karar tahmin etmede kullanılan algoritmalar Karar Ağaçları, Rastgele Ormanlar, Destek Vektörü Makineleri ve muhtelif derin öğrenme yöntemlerine dayanmaktadır. Sözü geçen derin öğrenme yöntemleri Geçitli Mükerrer Hücreler, tek yönlü ve çift yönlü Uzun-Kısa Vade Hafıza ağları ve bunların dikkat mekanizması eklenmiş çeşitleridir. Bütün bu algoritmaların karar tahminindeki performanslarını her bir mahkeme için mukayeseli bir biçimde ortaya koyuyoruz. Türk yüksek mahkemelerinin kararlarının özellikle derin öğrenme yöntemleri kullanılarak yüksek isabet oranlarıyla tahmin edilebileceğini gösteriyoruz. Diğer ülkelerin mahkemeleri için farklı dillerdeki dava metinlerinde yapılmış benzer çalışmalara yakın sonuçlar elde ediyoruz.","The use of Natural Language Processing (NLP) in the field of law has become a topic of interest in the recent years. Applications to Turkish law, however, have remained unexplored to this day. In this thesis, first, a review of existing NLP applications in law is provided, and then, the problem of predicting Turkish court decisions is studied using NLP techniques. An extensive corpus that consists of case texts from Turkish higher courts, namely, the Constitutional Court and District Courts, is compiled. In addition, a numerical analysis and comparison of NLP methods at predicting the outcomes of these higher court cases is provided. The methods used for prediction are based on Decision Trees, Random Forests, Support Vector Machines and various deep learning models; specifically Gated Recurrent Units, unidirectional and bidirectional Long Short-Term Memory networks, and their attention-integrated counterparts. Prediction results for all algorithms are presented comparatively across all courts. The results show that decisions of Turkish higher courts can be predicted with high accuracy, especially with deep learning-based methods. Similar performance to existing work in the literature on case outcome prediction, which focus on different languages and different legal systems, is achieved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Teknolojideki ilerleme, yüksek teknoloji uydulardan ve uçak algılayıcılarından yüksek miktarda bilgi birikimine sebep oldu. Söz konusu bu platformlardan elde edilen bu yüksek çözünürlüklü görüntüler insanların yeryüzünü daha iyi anlamasını sağladı. Uzaktan algılama görüntü analizi alanındaki ilk çalışmalar, görüntü sınıflandırma alanında manzara, arazi ve uçsuz bucaksız alanları anlamlandırmaya çalışan çalışmalardı. Son zamanlarda çok yüksek çözünürlüklü elektro-optik görüntülerde nesne tanıma ve görüntü bölütleme popüler hale geldi. Modern nesne tanıyıcıların iki tür mimarisi vardır. İki aşamalı nesne tanıyıcıları, bir bölge önerme mekanizması ile olası nesne konumlarını tarar ve nesneleri sınıflandırmak için bu önceden seçilmiş bölgeler üzerinde bağlanım yapar. Öte yandan, tek aşamalı nesne tanıyıcıları, herhangi bir ön seçim olmaksızın, görüntü üzerinde genel bağlanım ve sınıflandırma gerçekleştirir. Bölge önerme mekanizması doğruluğu artırır, ancak sezim hızını yavaşlatır. Bu sebeple, tek aşamalı nesne tanıyıcılar, gerçek zamanlı sezim için daha iyi bir seçenek olmalarına rağmen, daha düşük doğruluğa sahiplerdir. Halka açık veri kümeleri genellikle yatay sınırlayıcı kutu etiketi sağlarlar. Bununla birlikte, uzaktan algılama görüntüleri, yönlü sınırlayıcı kutular için çok daha uygun olan küçük, sıkışık veya büyük en-boy oranlı nesneler ile doludur. On beş nesne sınıfını içeren en kapsamlı uzaktan algılama nesne tanıma veri setlerinden (DOTA) birinde tek aşamalı nesne tanımaya odaklanmaktayız. Tek aşamalı nesne tanıyıcılarımız, en öne çıkan tek aşamalı nesne tanıma mimarilerinden birine (YOLO) dayanmaktadır. Yatay ve yönlü sınırlayıcı kutu etiketleri, yönlendirilmiş nesne tanıma ayarının genel olarak eksen hizalı nesne algılama ayarına göre üstün bir performansa sahip olduğunu göstermek ve ayrıca tek aşamalı nesne tanımanın iki aşamalı nesne tanıma ile karşılaştırıldığında rekabetçi sonuçlar elde edebildiğini ispatlamak için kullanılmıştır. Yönlü nesne tanıma yaklaşımımız, açı bağlanım problemini, bir sınıflandırma problemine dönüştürür ve tek aşamalı nesne tanıyıcılara kolayca uygulanabilir. Deneylerimiz, açısal sınıflandırmaya dayalı yönlü nesne tanıyıcısının, yatay nesne tanıyıcısından daha iyi bir performans gösterdiğini ispatlamaktadır. Ayrıca çalışmamız, doğrulama veri setinde yayınlanmış çalışmalardaki çift aşamalı nesne tanıma yöntemlerine göre %3.07, çapasız nesne tanıma yöntemlerine göre %12.84 gelişme göstermektedir.","Advances in technology resulted in an enormous amount of information collected from high technology satellites and aircraft sensors. These high-resolution images obtained by the said platforms enabled humans to understand the Earth better. The first studies in the field of remote sensing image analysis have focused on image classification, which studies the understanding of scenes, terrain, and vast fields. Recently object detection and image segmentation on very high-resolution electro-optical images have become popular. Modern object detectors have two types of architecture. Two-stage object detectors screen probable object locations with a region proposal mechanism and regress over these preselected regions to classify objects. On the other hand, one-stage object detectors perform global regression and classification over the image without any preselection. The region proposal mechanism improves the accuracy but slows the detection speed, making one-stage object detectors better for real-time detection, which have lower accuracy. Publicly available data sets generally provide horizontal bounding box labels. However, remote sensing images are dominated by small, congested, or large aspect ratio objects, which is suitable for oriented bounding boxes. We study one-stage object detection in one of the most extensive remote sensing object detection data sets (DOTA), which includes fifteen object classes. Our one-stage object detectors are based on one of the most prominent one-stage object detector architectures (YOLO). Horizontal and oriented bounding box labels are used to obtain results to show that the oriented object detection setting has a surpassing performance over axis-aligned object detection setting in general and also to show that one-stage object detection can have competitive results compared to two-stage object detection methods. Our oriented object detection approach converts angle regression into a form of classification and is easily applicable to the one-stage detectors. Our experiments show that rotated object detection based on angular classification performs better than horizontal object detector. It also shows 3.07% improvement over published two-stage methods and 12.84% improvement over published anchor-free methods on the validation data set."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kod yorumları, kaynak kodun anlaşılmasında ve yazılımın sürdürülebilirliğinde önemli bir rol oynar. Yazılım mühendisliğinde koda yorum eklemek genellikle iyi bir uygulama olarak kabul edilir. Fakat, düşük kaliteli yorumlar, yazılım kalitesi üzerinde negatif bir etkiye sahip olabilir veya kodun anlaşılması için etkisiz olabilir. Bu çalışmada, akademik ve gri literatür taraması yapılarak 11 türden oluşan satır içi kod yorum uygunsuzlukları sınıflandırıldı. Daha sonra, yarısının Java ve diğer yarısının Python projeleri olduğu sekiz açık kaynaklı projeden 2447 satır içi yorumu manuel olarak etiketlendi. Uygunsuzlukların hem Java hem de Python projelerinde değişen derecelerde var olduğu gözlenmiştir. Ayrıca, bu uygunsuzlukların kod anlama ve yazılım sürdürülebilirliği üzerindeki etkilerini öğrenmek için 41 yazılım geliştirici ile bir anket gerçekleştirildi. Anket katılımcıları genel olarak sınıflandırma konusunda hemfikir olduklarını belirtmiştir, ancak bazı senaryolarda bazı yorum uygunsuzluklarının kod anlama üzerinde olumlu etkisi olabileceğini bildirmişlerdir. Bunlara ek olarak, etiketli veri setini kullanarak yorum uygunsuzluklarını otomatik kategorize etmek için çeşitli makine öğrenmesi tabanlı modeller geliştirilmiş ve en iyi model 0,53'lik bir F1 puanı elde etmiştir. Etiketlenmiş veri kümemizi herkese açık şekilde paylaşıyor ve bu çalışmanın yazılım mühendisliği uygulayıcılarına, araştırmacılarına ve eğitimcilerine olan çıkarımlarına değiniyoruz.","Code comments play a vital role in source code comprehension and software maintainability. It is common for developers to write comments to explain a code snippet, and commenting code is generally considered as a good practice in software engineering. However, low-quality comments can have a detrimental effect on software quality or be ineffective for code understanding. In this study, we conducted a multivocal literature review and created a taxonomy of inline code comments smells consisting of 11 types. Afterward, we manually labeled 2447 inline comments from eight open-source projects where half of them were Java, and another half were Python projects. We found out that the smells exist in both Java and Python projects with varying degrees. Moreover, we conducted an online survey with 41 software practitioners to learn their opinions on these smells and their effect on code comprehension and software maintainability. The survey respondents generally agreed with the taxonomy; however, they reported that some smell types might have a positive effect on code comprehension in certain scenarios. Additionally, using our labeled dataset, we developed various machine learning-based models to categorize the smell types automatically. Our best model achieved an F1 score of 0.53. We share our manually labeled dataset online and provide implications of this study for software engineering practitioners, researchers, and educators."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hesaplamalı akışkan dinamiği simülasyonları genellikle büyük, karmaşık ve dış bükey yapıda olmayan sonlu eleman kümeleri üretir. Bu verilerin dağıtık sistemlerde interaktif doğrudan hacim görüntülenmesi esnasında hesaplama düğümleri arasında meydana gelen adil olmayan iş bölümü, elde edilen kısmi çözümlerin birleştirilmesi aşamasında sorun teşkil edebilir. Bu tarz durumlarda görüntülenmek isteyen hacim modelleri için doğru ve yerinde çalışan bir yöntem önermek ışın takibi yapılırken ışınların küme sınırları arasında farklı hesaplama düğümlerine erişmelerinden ötürü zor bir süreçtir. Bu tezde Grafik İşlemci Ünitesi tabanlı, ölçeklenebilir, hafıza verimi yüksek bir doğrudan hacim görselleştirme yöntemi sunuyoruz. Bu yöntem hem simülasyon yeri ve anında, hem de sonrasında kullanım için tasarlanmıştır. Yaklaşımımız XOR-tabanlı sıkıştırma tekniklerini kullanarak düzensiz hacim elemanlarının bellek kullanımını azaltmakta ve büyük hiyerarşik yapılarak gerek kalmadan hızlandırılmış ışın yürüyüş ü sağlamaktadır. Ayrıca, bu karmaşık dış bükey yapıda olmayan ağların ışın yürüyüşü sırasında elde edilen kısmi çözümleri (renk değerleri) birleştiren ""derin birleştirici"" algoritması sunulmuştur. Buna ek olarak, önerilen yöntemin tek ekran kartı ile çalışan bir senaryoda ikincil efektlerin kullanımına uygun olduğu gösterilmektedir. Yöntemimiz büyük veri-paralel sistemlerde iyi bir şekilde ölçeklenebilmekte ve interaktif hızlarda görüntü alabilmektedir. Fun3D'nin Küçük Mars İniş Aracı (14GB / 798.4 milyon eleman) ve Devasa Mars İniş Aracı (111.57GB / 6.4 milyar eleman) modellerini sırasıyla saniyede 14 ve 10 kare olarak görüntüleyebilmektedir. Bu performansa Texas Advanced Computing Center (TACC)'da yer alan Frontera süper bilgisayarında küçük model için 72, devasa olan için 80 GPU kullanılarak erişilmiştir.","Computational fluid dynamic simulations often produce large clusters of finite elements with non-trivial, non-convex boundaries and uneven distributions among compute nodes, posing challenges to compositing during interactive volume rendering. Correct, in-place visualization of such clusters becomes difficult because viewing rays straddle domain boundaries across multiple compute nodes. We propose a GPU-based, scalable, memory-efficient direct volume visualization framework suitable for in situ and post hoc usage. Our approach reduces memory usage of the unstructured volume elements by leveraging an exclusive or-based index reduction scheme and provides fast ray-marching-based traversal without requiring large external data structures built over the elements. Moreover, we present a GPU-optimized deep compositing scheme that allows correct order compositing of intermediate color values accumulated across different ranks that works even for non-convex clusters. Furthermore, we illustrate that we can achieve secondary effects such as shadows and gradient shading using our method for single GPU setups. Our approach scales well on large data-parallel systems and achieves interactive frame rates during visualization. We can interactively render Fun3D Small Mars Lander (14 GB / 798.4 million finite elements) and Huge Mars Lander (111.57 GB / 6.4 billion finite elements) data sets at 14 and 10 frames per second using 72 and 80 GPUs, respectively, on the Frontera supercomputer at The Texas Advanced Computing Center (TACC)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kısıtlı Delaunay Üçgenleme (KDÜ) problemini çözebilen bir böl-ve-yönet algoritması öneriyoruz. Algoritmamız üç aşamadan oluşmaktadır: Girdi Bölme, Yüzey Kapatma, ve Birleştirme. Problemin boyutunu küçültmek için önce girdiyi birkaç parçaya bölüyoruz. Yeni parçaları su geçirmez hale getirmek adına açık yüzeyleri kapatmak için 2D Üçgenleme uyguluyoruz. Her parça daha sonra işlenmek üzere TetGen [Hang Si, TetGen, a Delaunay-Based Quality Tetrahedral Mesh Generator, ACM Transactions on Mathematical Software, Cilt 41, Sayı 2, Makale No. 11, 36 sayfa, Ocak 2015] programına gönderiyoruz. Sonunda, nihai çözümü hesaplamak için her bir dörtyüzlü ağı birleştiriyoruz. Ek olarak, girdi üçgenlerini korumak için girdi bölme aşamasında eklediğimiz köşeleri kaldırma işlemi uyguluyoruz. Yeni köşe eklemeyen ve de köşeleri geri silme işlemini ortadan kaldıran alternatif bir yaklaşım da mümkündür; ancak, bu yaklaşım her zaman doğru bir şekilde çalışmamaktadır. Yöntemimizin yararı, bellek kullanımını azaltabilmesi ya da işlemin hızının artırabilmesidir. Yöntemimiz TetGen'in bellek yetersizliğinden dolayı yapamadığı girdileri başarı ile işleyebilmektedir. Ayrıca, bu yöntemin dörtyüzlü ağ kalitesini artırabildiğini de gözlemledik.","We propose a divide-and-conquer algorithm that can solve the Constrained Delaunay Tetrahedralization (CDT) problem. It consists of three stages: Input Partitioning, Surface Closure, and Merge. We first partition the input into several pieces to reduce the problem size. We apply 2D Triangulation to close the open boundaries to make new pieces watertight. Each piece is then sent to TetGen [Hang Si, TetGen, a Delaunay-Based Quality Tetrahedral Mesh Generator, ACM Transactions on Mathematical Software, Vol. 41, No. 2, Article No. 11, 36 pages, January 2015] for processing. We finally merge each tetrahedral mesh to calculate the final solution. In addition, we apply post-processing to remove vertices we introduced during the input partitioning stage to preserve the input triangles. An alternative approach that does not insert new vertices and eliminates the need for post-processing is also possible but not robust. The benefit of our method is that it can reduce memory usage or increase the speed of the process. It can even tetrahedralize meshes that TetGen cannot do due to the memory's insufficiency. We also observe that this method can increase the overall tetrahedral mesh quality."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Işın izleme performansını artırmak amacıyla etkin ve kompak dörtyüzlü örgü yöntemleri öneriyoruz. Dörtyüzlü örgü bilgisini, önbellek verimini artırmak amacıyla uzay eğrisi kullanarak sıralıyoruz. En önemlisi, çok hızlı çalışan ışın takip yöntemleri sunuyoruz. Dörtyüzlü örgüleri ışın izleme işlemlerinde kullanabilmek icin gerekli yöntemlerin detaylarını verip bu yöntemlerin grafik işlemci birimlerinde nasıl kullanılacağını tarif ediyoruz. Bulguları ve sonuçları kapsamlı deneylerle gösteriyoruz. Geliştirdiğimiz yöntemler, mevcut dörtyüzlü gezim yöntemlerinden daha hızlı çalışmaktadır. Bu yöntemler, aynı zamanda yaygın olarak kullanılan k-d ağacı ve sınırlayıcı hacim hiyerarşisi gibi ışın izleme hızlandırıcı yapılarına yakın bir performans sergilemektedir. Geliştirdiğimiz yöntemler, diğer dörtyüzlü tabanlı yöntemlere göre daha az bellek kullanmaktadır; böylelikle grafik işlemci ünitesi gibi belleğin kısıtlı oldugu yerlerde daha büyük sahnelerin görselleştirilmesine olanak sağlamaktadır. Bunlarla birlikte, yöntemlerimizin hacim görselleştirilmesi, animasyon uygulamaları için iki seviyeli hibrit hızlandırıcı yapılarına uyarlanması ve iki ve üç boyutlu üçgenlemelerde noktaların sorgularının yapılması gibi uygulamalarda nasıl kullanılabileceğini anlatıyoruz. Son olarak, çok büyük sahnelerin dörtyüzlemesinin yapılmasını sağlayan pratik bir yöntem sunuyoruz.","We propose compact and efficient tetrahedral mesh representations to improve the ray-tracing performance. We reorder tetrahedral mesh data using a space-filling curve to improve cache locality. Most importantly, we propose efficient ray traversal algorithms. We provide details of the regular ray tracing operations on tetrahedral meshes and the Graphics Processing Unit (GPU) implementation of our traversal method. We demonstrate our findings through a set of comprehensive experiments. Our method outperforms existing tetrahedral mesh-based traversal methods and yields comparable results to the traversal methods based on the state-of-the-art acceleration structures such as k-dimensional (k-d) tree and Bounding Volume Hierarchy (BVH) in terms of speed. Storage-wise, our method uses less memory than its tetrahedral mesh-based counterparts, thus allowing larger scenes to be rendered on the GPU. We also describe additional applications of our technique specifically for volume rendering, two-level hybrid acceleration structures for animation purposes, and point queries in two-dimensional (2-D) and three-dimensional (3-D) triangulations. Finally, we present a practical method to tetrahedralize very large scenes."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son birkaç on yılda nesne tanıma ve sınıflandırmadaki önemli ilerlemelere rağmen, gerçek dünya senaryolarında tüm sınıflardan temsili eğitim örnekleri toplamanın oldukça pahalı olduğu veya sistemin açıkta kalabileceği çeşitli durumlar vardır. test zamanında öngörülemeyen yeni örneklere. desen sınıflandırma problemi genellikle açık küme olarak adlandırılır. Tanıma görevi, eğitim süresi boyunca modele tüm veri dağıtımına ilişkin sınırlı ve eksik bilgi verildiği durumlarda. Test aşamasında, sınıflandırıcının daha önce görülen sınıfları doğru bir şekilde sınıflandırmasını ve görünmeyenleri etkili bir şekilde reddetmesini gerektiren bilinmeyen sınıflarla karşılaşılabilir. Diğerlerinin yanı sıra, tek sınıflı sınıflandırma, açık küme tanıma problemine makul bir çözüm olarak hizmet eder. Bununla birlikte, mevcut tek sınıflı sınıflandırıcıların sınırlamaları vardır. Klasik çekirdek tabanlı yaklaşımlar, makul performans elde etmek için dikkatlice tasarlanmış özellikler gerektirir, ancak eğitim kümesi safsızlıklarına karşı iyi bir sağlamlık sağlayarak istatistiksel öğrenme teorisinde sağlam bir temele dayanır. Diğer yandan, daha yeni derin öğrenme tabanlı yöntemler, ilgili özellikleri doğrudan verilerden öğrenmeye odaklanır, ancak genellikle genellikle iyi genellemeyen ve her yerde mevcut olan gürültüye karşı sağlam olmayan geçici tek sınıf kayıp işlevlerine dayanır. eğitim setinde kontaminasyon. Bu tezde, iki öğrenme formalizmini ortak bir çatı altında toplayarak hem çekirdek tabanlı hem de derin öğrenme yaklaşımlarının avantajlarından yararlanan yeni bir yaklaşım sunuyoruz. Özellikle, önerilen yöntem, Hilbert uzayındaki diskriminant üzerinde bir Tikhonov düzenlemesine tabi bir çekirdek Fisher boş-uzay kaybını optimize etmek için derin evrişimsel öznitelikleri öğrenir. Bu nedenle, eğitim seti kontaminasyonuna karşı sağlam iken derin bir uçtan uca şekilde eğitilebilir. Çeşitli değerlendirme ayarlarında farklı görüntü veri setleri üzerinde yapılan kapsamlı deneyler sayesinde, önerilen yaklaşımın eğitim setinin bozulduğu ve içerdiği senaryoda anomali tespiti için mevcut en son teknoloji yöntemlerden oldukça sağlam ve daha etkili olduğu gösterilmiştir. gürültülü örnekler Aynı zamanda, önerilen yaklaşımlar, denetimsiz bir senaryoda, veri noktalarını örneklerin çoğuna uygunluklarına göre sıralamak için etkin bir şekilde kullanılabilir.","Despite significant advances in object recognition and classification over the past couple of decades, there are various situations where collecting representative training samples from all classes in real-world scenarios is quite expensive, or the system may be exposed to unpredictable novel samples at the test time. The pattern classification problem is commonly referred to as an open-set recognition task in such cases where limited and incomplete knowledge of the entire data distribution is provided to the model during the training time. During test phase, unknown classes can be faced which requires the classifier to accurately classify the previously seen classes while effectively rejecting unseen ones. Among others, one-class classification serves as a plausible solution to the open-set recognition problem. Nevertheless, current one-class classifiers have their limitations. Classical kernel-based approaches require carefully designed features to obtain reasonable performance but rest on a solid basis in statistical learning theory, providing good robustness against training set impurities. More recent deep learning-based methods, on the other hand, focus on learning relevant features directly from the data but typically rely on ad hoc one-class loss functions, which very often do not generalize well and are not robust against the omnipresent noise and contamination in the training set. In this thesis, we introduce a novel approach which leverages the advantages of both kernel-based and deep-learning approaches by bringing the two learning formalisms under a common umbrella. In particular, the proposed method learns deep convolutional features to optimize a kernel Fisher null-space loss subject to a Tikhonov regularisation on the discriminant in the Hilbert space. As such, it can be trained in a deep end-to-end fashion while being robust against training set contamination. Through extensive experiments conducted on different image datasets in various evaluation settings, the proposed approach is shown to be quite robust and more effective than the current state-of-the-art methods for anomaly detection in the scenario where the training set is corrupted and contains noisy samples. At the same time, the proposed approaches can be effectively utilized in an unsupervised scenario to rank the data points based on their conformity with the majority of samples."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bulut veri merkezleri; müşterilere sunulan düşük maliyet, yüksek kullanılabilirlik, güvenilirlik, dayanıklılık ve ölçeklenebilirlik gibi faydalar sebebi ile revaçtadır. Ancak, bulut hizmet sağlayıcıları, hizmet seviyesi anlaşmalarını yerine getirirken, müşterilerine yüksek kalite hizmet vermek ile yükümlüdür. Bu durum, veri merkezi yönetiminde kaynakların etkin ve verimli kullanım ihtiyacını ortaya çıkarmaktadır. Proaktif kaynak paylaştırma ve ölçekleme politikaları elde etme yolunda, bulut ortamlardaki iş yükünün doğru tahmin edilmesi kritik öneme sahiptir. İş yükü tahmini; iş yükü verisindeki yüksek boyutluluk, varyans ve karmaşıklık sebebi ile zorlu bir problemdir. Buna ek olarak, iş yükü tahmin modellerinin, yeterli sayıda geçmiş iş yükü gözlemleri ile çalışması, aynı zamanda geleceğe yönelik uzun tahmin ufku ile başa çıkabilmesi beklenmektedir. Bu problemi çözmek ve zorluklarını irdelemek amacı ile bulut veri merkezlerindeki sanal makinelerin CPU kullanımını tahmin eden, beş farklı derin öğrenme tabanlı çok değişkenli zaman serisi tahmin modeli kurulmuştur. Model performansları iki gerçek dünya veri seti kullanılarak karşılaştırılmış ve analiz edilmiştir: Alibaba küme izleri ve BitBrains izleri. Karşılaştırmalı çalışma ve analiz sonucunda, Dikkat Mekanizmalı Kodlayıcı-Kodçözücü Uzun-Kısa Süreli Bellek Ağının bulut veri merkezlerinde iş yükünün tahmini için etkin ve verimli bir çözüm olduğu görülmüştür.","Cloud computing and use of cloud data centers are in high demand due to their benefits to customers including but not limited to low cost, high availability, reliability, robustness and scalability. Cloud service providers are obliged to fulfill service level agreements that promise high quality of service to their customers. This brings out the need for effective and efficient utilization of data center resources, especially the resources of the compute servers. To achieve proactive and effective resource allocation and scaling policies, accurate prediction of workloads in cloud computing environments plays a critical role. Cloud workload prediction is a challenging task due to high dimensionality, variance and complexity of the workload data. In addition, workload prediction models are expected to work with sufficient amount of past observations to correctly learn workload patterns, at the same time, handle longer forecast horizons accurately. In order to tackle this problem and address the challenges, we investigated and compared five deep learning-based schemes for multivariate time series forecasting to predict the CPU utilization of virtual machines in cloud data centers. The performance of the deep learning schemes is analyzed and compared by using two real-world data sets: Alibaba cluster trace and Bitbrains trace. Our study reveals the relative strengths and weaknesses of the compared schemes for cloud workload prediction. We also observed that, among the compared schemes, Encoder-Decoder LSTM Network with Attention is a more effective solution for workload prediction in cloud computing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kod degerlendirme, modern yazılım gelistirme sırasında yapılan degisiklerde cıkabilecek potansiyel problemleri saptama ve kaynak kodunu gelistirmede oldukca onemli bir asamadır. Gelistiriciler ideal kod inceleme surecinde birtakım kurallara baglı kalmaktadırlar. Bu kurallardan sapmalar, kod degerlendirme uygunsuzlukları (smells) olarak da bilinir ve kod degerlendirmenin cıktılarını dogrudan etkiler. Bu arastırma, deneysel olarak kod degerlendirme surecindeki uygunsuzlukları ile kod degerlendirme etkisi arasında iliskiyi (paylasılan bilgi, tekrar acılan konular, degerlendirme surecini tamamlamak icin gecen sure, terk edilmis cekme istekleri (PR), degerlendirme yorumlarının sayısı, buyuk olcekli degisikliklerin degerlendirmeye katkısı). kod degerlendirme uygunsuzlukları ve uygunsuzlukların etkileri arasındaki iliski korelasyon tabanlıdır, fakat neden tabanlı bir iliski garanti degildir. Uygunsuzlukların kod degerlendirmeye etkisini sayısal bir sekilde analiz edebilmek icin, Gerrit'ten dort acık kaynak kodlu projesinden (Qt, Eclipse, Wireshark ve LibreOffice) 244,903 PR inceledik. Sonuc olarak, cekme isteklerinde onerme-taraflı (commit) uygunsuzlukların varlıgını kesfettik ve bu uygunsuzlukların degerlendirme-taraflı uygunsuzluklara sebep oldugunu gorduk. Ayrıca degerlendirme asamalarında tekrarlanan bu tur uygunsuzlukların degerlendirme surecinde daha agır is yuklerine sebep oldugunu acıga cıkardık. Bunlara ek olarak, dort proje icin de, pinpon degerlendirmelerin ve buyuk olcekli degisikliklerin sonrasında hatatabanlı konuların tekrar acılmasının daha olası oldugunu gorduk. Uygunsuz degerlendirmelerin tamamlanma surelerinin uygunlara gore daha cok oldugunu fark ettik. Ayrıca, bir PR'deki surec uygunsuzluklarının varlıgı, o cekme talebindeki inceleme yorumlarının sayısını etkiler.","Code review is a valuable modern software development practice for identifying potential problems in changesets and improving source code quality in software projects. Developers adhere to specific rules and best practices in an ideal code review process. Deviations from these best practices can potentially lead to unintended consequences. These deviations, also known as code review process smells, affect different parts of the code review process and its outcomes. This research empirically analyzes the correlations between process smells, and the impact of code review process smells (shared knowledge, reopened issues, time to complete review (TTCR), abandoned pull requests (PRs), number or review comments, and contribution of large changeset reviewers). The relationship between smells and their impact is correlation-based, but a causal relationship is not guaranteed because proving such relationships is not straightforward. To quantitatively analyze the impact of smells in the code review process, we investigated 244,903 PRs from four open source software (OSS) projects from Gerrit (Qt, Eclipse, Wireshark, and LibreOffice). As a result, we discovered that the existence of a committers' side smell in a pull request (PR) could lead to the occurrence of reviewers' side smells in the same PR. We also observed that reviewers assigned repeatedly by the same author to review the PR have a heavier review workload. Furthermore, the ping pong reviews and large changesets are most likely to increase the bug reopen ratio. In addition, we noticed that the time to complete review of a smelly PR is likely to be more than time to complete review of nonsmelly PR. Also, the existence of process smells in a PR effect the number of review comments in that pull request."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yaygın olarak kullanılan iki önemli paralel ayrışım algoritmaları, seyrek tensör ayışımı için CPD-ALS ve düşük kerteli seyrek matris ayrışımı için dağıtık tabakalı olasılıksal gradyan alçalma (SGD), ölçeklenebilirlik anlamında zayıf kalmaktadır. CPD-ALS algoritmasında bir işlemciye atanan bir tensör/alt-tensör ile ilişkili hesaplamasal yük, CSF veri yapısı kullanıldığında tensörün sıfırdışı girdilerinin sayısı ve ayrıca tensörün fiber sayılarının bir fonksiyonudur. Tensör fiberleri, sıfırdışı girdilerin bölümlenmesine bağlı olarak parçalanır, bu da işlemcilerin hesaplamasal yüklerini dengelemeyi zor bir problem haline getirir. Bu problemin çözümü için mevcut bir ince taneli hiperçizge modeli üzerine iki yeni strateji önerilmiştir: fiber yüklerini de hesaplayarak gerçek yükü modelleyen özgün bir ağırlıklandırma şeması ve hesaplamasal yükteki artışı azaltmayı hedefleyen yeni fiber hiperkenarlarının hiperçizgeye eklenmesi.CPD-ALS ayrıca işlemci sayısı arttıkça artan gereken çok sayıda işlemciler arası doğrudan mesaj nedeniyle yüksek gecikim maliyeti ortaya çıkarmaktadır. Bu mesajların sayısını, K işlemcili bir bilgisayar için O(lgK) ile limitleyen ve lgK aşamada gerçekleyen bir yaklaşım önermekteyiz. Ayrıca, bu yeni yaklaşımın gerektirdiği iletişimi modelleyen bir hiperçizge tabanlı bölümleme yöntemi önermekteyiz. Mevcut tabakalı SGD (SSGD) uygulamalarında, iletişim hacmi girdi matrisinin boyutlarından biri ile orantılıdır ve ölçeklenebilirliği engeller. İletişim hacmini azaltmak için SSGD algoritmasının doğruluğu için gerekli olan temel verilerin işlemciler arası doğrudan mesajlar ile değiş tokuş edilmesi önerilmiştir. Bu yöntem, iletişim hacmini azaltmak için paha biçilmez olsa da, mesaj sayısının üst sınırını O(K)'dan O(K^2)'ye artırarak algoritmayı gecikim maliyetlerine bağlı hale getirmektedir. Sadece temel verinin iletişimini O(K logK) mesaj ile değiş tokuş eden yeni bir Tut-ve-Birleştir algoritması önerilmiştir. Yüksek başarımlı hesaplama sistemleri üzerinde gerçekleştirilen kapsamlı deneyler, CPD-ALS ve tabakalı SGD algoritmalarını ölçeklendirmede önerilen yöntem ve modellerin önemini göstermektedir.","Two important and widely-used factorization algorithms, namely CPD-ALS for sparse tensor decomposition and distributed stratified SGD for low-rank matrix factorization, suffer from limited scalability. In CPD-ALS, the computational load associated with a tensor/subtensor assigned to a processor is a function of the nonzero counts as well as the fiber counts of the tensor when the CSF storage is utilized. The tensor fibers fragment as a result of nonzero distributions, which makes balancing the computational loads a hard problem. Two strategies are proposed to tackle the balancing problem on an existing fine-grain hypergraph model: a novel weighting scheme to cover the cost of fibers in the true load as well as an augmentation to the hypergraph with fiber nets to encode reducing the increase in computational load. CPD-ALS also suffers from high latency overhead due to the high number of point-to-point messages incurred as the processor count increases. A framework is proposed to limit the number of messages to O(lgK), for a K-processor system, exchanged in lgK stages. A hypergraph-based method is proposed to encapsulate the communication of the new lgK-stage algorithm. In the existing stratified SGD implementations, the volume of communication is proportional to one of the dimensions of the input matrix and prohibits the scalability. Exchanging the essential data necessary for the correctness of the SSGD algorithm as point-to-point messages is proposed to reduce the volume. This, although invaluable for reducing the bandwidth overhead, would increase the upper bound on the number of exchanged messages from O(K) to O(K^2) rendering the algorithm latency-bound. A novel Hold-and-Combine algorithm is proposed to exchange the essential communication volume with up to O(K logK) messages. Extensive experiments on HPC systems demonstrate the importance of the proposed algorithms and models in scaling CPD-ALS and stratified SGD."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüz dünyasında veri miktarı katlanarak arttığı için ilişkisel verilerin görsel analizi daha zor hale gelmektedir. Bu nedenle, bu tür verilerin etkili görsel gösterimi, analiz sürecini basitleştirmek için önemli bir gerekliliktir. Bileşik çizgeler, hem farklı düzeylerde gruplamalar veya soyutlamalar içeren ilişkisel verileri temsil etmek hem de onların karmaşıklığını yönetmek için pratik bir yapı sunar. Ek olarak, bu çizgelerin otomatik ve iyi yerleşimi, kullanıcıların ilişkileri anlamalarına, yeni içgörüler ortaya çıkarmasına ve verilerde gizlenmiş önemli kalıpları bulmasına olanak tanır. Bu amaçla, bileşik çizgeler için, kullanıcı tarafından belirlenen yerleştirme kısıtlamalarını da destekleyen fCoSE adlı yeni bir yerleştirme algoritması sunuyoruz. fCoSE, belirtilen kısıtlamaları karşılarken ve bileşik yapıları düzgün bir şekilde görüntülerken, izgesel yerleştirmenin hızını, kuvvet-yönlendirilmiş yerleştirmenin estetiği ve kalitesi ile birleştirir. Önce izgesel bir yöntem yardımıyla taslak bir yerleşim oluşturur, daha sonra ilk defa sunulan buluşsal yöntemleri kullanarak yerleştirme kısıtlamalarını sağlar ve son olarak, sağlanmış olan kısıtlamaları sürdürmek için değiştirilmiş kuvvet-yönlendirilmiş bir bileşik çizge yerleştirme yöntemi aracılığıyla yerleşimi güzelleştirir. Hem gerçek dünya hem de rastgele oluşturulmuş çizgeler üzerinde gerçekleştirilen deneylerimiz, fCoSE'nin hem hız hem de genel kabul görmüş çizge yerleşim kriterleri açısından rakiplerini geride bıraktığını ve küçük ila orta ölçekli çizgeleri destekleyen etkileşimli uygulamalarda kullanılabilecek kadar hızlı olduğunu göstermektedir.","Visual analysis of relational data becomes more challenging in today's world as the amount of data increases exponentially. Effective visual display of such data is therefore a key requirement to simplify the analysis process. Compound graphs present a practical structure for both representing the relational data with varying levels of groupings or abstractions and managing its complexity. In addition, a good automatic layout of these graphs lets users understand relationships, uncover new insights and find important patterns hidden in the data. To this end, we introduce a new layout algorithm named fCoSE (fast Compound Spring Embedder) for compound graphs with support for user-specified placement constraints. fCoSE combines the speed of spectral layout with the aesthetics and quality of force-directed layout while satisfying specified constraints and properly displaying compound structures. The algorithm first generates a draft layout with the help of a spectral approach, then enforces placement constraints by using newly introduced heuristics and finally polishes the layout via a force-directed layout algorithm modified to maintain enforced constraints. Our experiments performed on both real-life and randomly generated graphs verify that fCoSE outperforms its competitors in terms of both speed and generally accepted graph layout criteria and is fast enough to be used in interactive applications with small to medium-sized graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Meme kanseri kadınlarda en sık görülen kanserdir ve ölüm oranı yüksektir. Uzmanların meme kanseri biyopsi örneklerini daha iyi analiz etmesine yardımcı olmak için bilgisayarla görü teknikleri kullanılabilir. Çizge sinir ağları (ÇSA), meme kanseri görüntülerinin sınıflandırılmasını çözmek için yaygın olarak kullanılmaktadır. Bu alandaki görüntülerin boyutları değişkendir ve ÇSA'lar değişen boyuttaki girdilere uygulanabilir. Çizgeler, çizgenin düğümleri arasındaki ilişkileri saklayabilir ve bu, ÇSA'ların bir çözüm olarak tercih edilmesinin bir başka nedenidir. Bu tezde meme histopatolojisi görüntülerinde ilgi bölgelerin (İB) sınıflandırılmasında alt çizge madenciliğinin kullanımını inceliyoruz. Çizgenin düğümleri olarak çekirdek açısından zengin bölgelerde örneklenen pencereleri kullanarak İB örneklerini çizge ile temsil ediyoruz. Histopatoloji görüntülerini sınıflandırırken hem mikro hem de makro düzeyde bilgi gereklidir. Pencereler, mikro düzeydeki bilgileri modellemek için kullanılır. Sıklıkla ortaya çıkan alt çizgeleri belirlemek için elde edilen çizgelere alt çizge madenciliği uyguluyoruz. Her alt çizge, daha üst düzey bilgileri temsil etmek için kullanılabilecek az sayıda pencereden ve bunların ilişkilerinden oluşur. Ayrıca, daha büyük boyutlu pencereler içeren bir kayar pencere mekanizması uygulayarak İB düzeyindeki özellikleri çıkarıyoruz. İB düzeyindeki özellikler, alt çizge özellikleri ve çizge evrişimli ağlardan elde edilen üçüncü bir gösterim, İB'ler hakkında makro düzeydeki bilgileri modellemek için birleştirilir. Ayrıca alt çizgeleri ek düğüm olarak çizge temsiline yerleştirmeyi de çalışıyoruz. Önerilen modeller, tam spektrumdan dört tanı kategorisini içeren zorlu bir meme patolojisi veri setinde değerlendirilmiştir. Deneyler, alt çizgelerin görüntü gösterimine dahil edilmesinin sınıflandırma doğruluğunu geliştirdiğini ve birleştirilmiş özellik gösteriminin bir ablasyon çalışmasında bireysel gösterimlerden daha iyi performans gösterdiğini göstermektedir.","Breast cancer is the most common cancer in women and has a high mortality rate. Computer vision techniques can be used to help experts to analyze the breast cancer biopsy samples better. Graph neural networks (GNN) have been widely used to solve the classification of breast cancer images. Images in this field have varying sizes and GNNs can be applied to varying sized inputs. Graphs can store relations between the vertices of the graph and this is another reason why GNNs are preferred as a solution. We study the use of subgraph mining in classification of regions of interest (ROI) on breast histopathology images. We represent ROI samples with graphs by using patches sampled on nuclei-rich regions as the vertices of the graph. Both micro and macro level information are essential when classifying histopathology images. The patches are used to model micro-level information. We apply subgraph mining to the resulting graphs to identify frequently occurring subgraphs. Each subgraph is composed of a small number of patches and their relations, which can be used to represent higher level information. We also extract ROI-level features by applying a sliding window mechanism with larger sized patches. The ROI-level features, subgraph features and a third representation obtained from graph convolutional networks are fused to model macro-level information about the ROIs. We also study embedding the subgraphs in the graph representation as additional vertices. The proposed models are evaluated on a challenging breast pathology dataset that includes four diagnostic categories from the full spectrum. The experiments show that embedding the subgraphs in the graph representation improves the classification accuracy and the fused feature representation performs better than the individual representations in an ablation study."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dinamik Voltaj/Frekans Ölçekleme (DVFS), Merkezi İşlemci Ünitelerinin (CPU) güç tüketimini verimli hale getirmek için kullanılan en temel yaklaşımdır. DVFS tekniğini Grafik İşlemci Ünitelerinde Genel Amaçlı Hesaplama (GPGPU) işlemlerinde kullanmak için birtakım araştırmalar yapılmıştır. Ancak, iş parçacıklarının Grafik İşlemci Ünitelerinde (GPU) büyük ölçüde paralel yürütülmesi ve Paylaşımlı Çoklu İşlemciler (SM) arasındaki yük dengesizliği nedeniyle, GPU çekirdekleri için en uygun küresel frekansı bulmak kolay bir iş değildir. Ayrıca, literatürde yer alan öneriler çoğunlukla, bir uygulamadaki optimal voltaj ve frekansın bir sonraki işletimde kullanılması için kurulan çevrimdışı bir modele dayanmaktadır. Bu çalışmada, bir analitik model ve genetik algoritma bileşimi kullanılarak, SM frekansının dinamik olarak ayarlanması, böylece çevrimdışı yürütmeye gerek kalmadan GPU'nun güç tüketiminin en az performans kaybıyla azaltılması hedeflenmektedir. Yaklaşımımızı çeşitli özelliklere sahip farklı alanlardan 16 GPU çekirdeği kullanarak test ettik. Elde ettiğimiz sonuçlar %0.95'ten daha az performans kaybıyla GPU'nun toplam enerjisinden ortalama %9,6 tasarruf sağlayabileceğimizi gösteriyor. Ayrıca önerilen yaklaşımın nasıl daha da geliştirilebileceği ve olası yeni yaklaşımlar tartışılmıştır.","Dynamic Voltage/Frequency Scaling (DVFS) is the primary approach to optimizing Central Processing Units (CPUs) power consumption. A handful of approaches are conducted using this technique in General Purpose Graphics Processing Units (GPGPUs). However, due to the massively parallel execution of threads on GPUs and load imbalance on Streaming Multiprocessors (SMs), finding the best global frequency for GPU cores is not a simple task. Moreover, the proposed approaches in the literature mostly rely on an offline model, where the optimal voltage and frequency for an application is found to be used in the next execution. In this work, we use a combination of an analytical model and a genetic algorithm to adjust per SM frequency dynamically, aiming at decreasing GPU's power consumption with the least amount of performance loss without a need for offline execution. We tested our approach using 16 GPU kernels from different domains with ranging features. Our results show that we can save 9.6% of GPU's total energy on average with less than 0.95% performance loss. We also discuss further improvements and possible extensions to the proposed approach."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Herkesin zaman zaman kötü günleri olduğu doğrudur. Ne yazık ki, bu günler depresyondan muzdarip insanlar ̧için çok daha fazladır, ve neredeyse her gün en basit şeyleri bile yapabilmek için bir motivasyon savaşı verirler. Bu sırada sürekli olarak umutsuzluk, fiziksel ve duygusal yorgunluk ve üzüntü ile uğraşırlar. Bu hastalıktan muzdarip insan sayısının giderek arttığı göz önüne alındığında, otomatik depresyon ̧siddeti değerlendirme sistemlerine duyulan ihtiyacın ̈önemi anlaşılmaktadır. Bu hastalığın ̧cözümüne yardımcı olmak adına, depresyon ̧siddeti tahmini için girdi olarak konuşma dökümlerini kullanan modüler bir derin ̈oğrenme ardışık düzeneği ̈oneriyoruz. Ardışık düzeneğimiz aracılığıyla, popüler derin öğrenme mimarilerinin depresyon değerlendirmesi için temsiller oluşturmadaki rolünü araştırıyoruz. Metin kipine ilişkin depresyon değerlendirme literatürünü genişletmek amacıyla cümle istatistiklerinin kapsamlı bir analizini ve bunların model eğitimi ̈uzerindeki etkilerini sunuyoruz. Ayrıca, depresyon değerlendirmesi için duygu bilgilerinin kullanımına ilişkin bir araştırmaya da yer veriyoruz. Önerilen mimarilerin değerlendirilmesi, halka açık Genişletilmiş Tehlike Analizi Mülakat Derlemi veri setinde (E-DAIC) gerçekleştirilmiştir. Sonuçlar ve tartışmalar aracılığıyla, cümleler arasındaki zamansal dinamiklerden yararlanmadan depresyon değerlendirmesi için bilgilendirici temsillerin elde edilebileceğini gösteriyoruz. Önerdiğimiz bu zamana bağlı olmayan model, Uyum Korelasyon Katsayısı (CCC) açısından güncel en iyi teknolojiden %8.8 daha iyi performans göstermektedir. Eğitilmiş modeller ve veri istatistiklerine ilişkin bulgularımız ışığında, tekrarlayan yapıların eğitim sırasında belirli dizi uzunluklarına karşı nasıl bir ̈onyargıya sahip olabileceğini ve ̧cıkarım aşamasında daha kısa cümlelerin daha bilgilendirici olabileceğini tartışıyoruz. Deneysel sonuçlarımız, ̈onceki literatürün aksine, duygu bilgisinden ziyade anlamsal bilgiye güvenmenin depresyon değerlendirmesi için daha güvenilir olabileceğini düşündürmektedir.","It is true that everyone has bad days from time to time. Unfortunately, for people suffering from depression, every day is a constant battle for motivation to do even the simplest of things, all the while dealing with hopelessness, physical and emotional fatigue, and sadness. Considering the ever increasing number of people suffering from this disease, the necessity for automated depression severity assessment systems is profound. These systems can be used in treatment procedures, and the findings provided from learned models can help us better understand the dynamics of depression. To help in the solution to this illness, we propose a modular deep learning pipeline that uses speech transcripts as input for depression severity prediction. Through our pipeline, we investigate the role of popular deep learning architectures in creating representations for depression assessment. To extend the depression assessment literature on text modality, we provide a thorough analysis of sentence statistics and their effects on model training. We also present an investigation regarding the use of sentiment information for depression assessment. Evaluation of the proposed architectures is performed on the publicly available Extended Distress Analysis Interview Corpus dataset (E-DAIC). Through the results and discussions, we show that informative representations for depression assessment can be obtained without exploiting the temporal dynamics between sentences. Our proposed non-temporal model outperforms the state of the art by %8.8 in terms of Concordance Correlation Coefficient (CCC). In light of our findings on trained models and data statistics, we discuss how recurrent structures can have a bias toward certain sequence lengths during training and that shorter."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Farklı uygulamalara ait organizasyonların sahip olduğu yapılandırılmış verilerin büyük bir kısmı tipik olarak İlişkisel Veritabanı Yönetim Sistemlerinde depolanır ve ilişkisel veritabanlarından istenen bilgileri çıkarmak için iyi bir Yapılandırılmış Dil Sorgusu (SQL) bilgisi gerekir. Veritabanlarında yer alan bilgilere erişmesi gereken sıradan kullanıcılar erişim için gerekli beceri ve bilgiye sahip değildirler. Ayrıca, bazı uzman kullanıcılar bile veritabanını oluşturan şematik yapıları yete-rince bilmedikleri durumda karmaşık SQL sorguları oluşturmayı zor bulabilirler. Bu durum sonucunda, son zamanlarda kullanıcılar tarafından doğal dilde formüle edilen sorguların, veritabanı sistemleri tarafından işlenecek SQL sorgularına çevrilmesi için önemli miktarda araştırma yapılmaktadır. Bu tezde, doğal dilden SQL'e çeviride kullanılacak bazı derin akıllı stratejiler sunuyoruz. Veritabanları için Doğal Dil Arayüzü (NLIDB) sistemlerini daha etkin duruma getirebilmek amacıyla yeni bir yöntem olan TranSQLate'i öneriyoruz. Stratejilerimizi Vanilla ve T5 dönüştürücü modellerine üç farklı şekilde uyguluyoruz. Zenginleştirilmiş girdilerle, zenginleştirilmemiş versiyonlara kıyasla çeviri doğruluğunda %16,7, SacreBLEU skorunda 6,5 puan ve n-gram hassasiyetinde 18 puana varan iyileşme elde ediyoruz. Önerdiğimiz yöntem, IMDB, scholar ve Yelp veri kümeleri üzerinde çeviri doğruluğu açısından, son teknoloji sistemler NALIR, TEMPLAR ve DBTagger'da kullanılan stratejilere üstünlük sağlamaktadır","A large amount of the structured data owned by different enterprises is typically stored in Relational Database Management Systems, and a decent knowledge of Structured Language Query (SQL) is required to extract desired information from the relational databases. Many naive users need to access the information from databases, and they do not have the necessary skills or knowledge. Additionally, even some expert users might find it challenging to provide complex SQL queries when they do not know the schema underlying the database. To this end, a considerable amount of research has been conducted recently for the translation of queries formulated by users in a natural language to SQL queries to be processed by database systems. In this thesis, we provide some deep intelligent strategies to be used in natural language to SQL translation. We propose TranSQLate, a novel method to enrich the input sequences and provide more effective Natural Language Interface to Database (NLIDB) systems. We apply our strategies to the Vanilla transformer and T5 transformer models in three different ways. With enriched inputs, we achieve up to 16.7% improvement in translation accuracy, 6.5 points in SacreBLEU score, and 18 points in the n-gram precision, compared to not enriched versions. Our method surpasses the strategies used in the state-of-the-art systems NALIR, TEMPLAR, and DBTagger, in terms of translation accuracy over IMDB, scholar, and Yelp datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dış nükleer tabaka (DNT) ve dış pleksiform tabaka (DPT) arasındaki dış retinada bulunan bir tabaka olan Henle lifi tabakası (HLT), düzgün doğrusal fotoreseptör aksonlarından ve Müller hücre çıkıntılarından oluşur. Öte yandan, standart optik koherans tomografi (OKT) görüntülerinde bu tabaka genellikle DNT tabakasına dahil edilir. Bunun nedeni, OKT görüntülerinde HLT konturlarının algılanmasındaki zorluktur. Görüntüleme ışınları altında gösterdiği yansıma farklılığından ötürü HLT konturlarını belirlemek için yöneltmeli OKT'ye ihtiyaç duyulur. Bu ise ek görüntüleme gerektiren bir süreçtir. Bu tez, FourierNet adını verdiğimiz bir şekil koruyan ağ modelini sunarak bu soruna çözüm önerir. Bu model, HLT bölütlemesinde yöneltmeli OKT görüntüleri kullanılarak ulaşılacak hedef performansı, standart OKT görüntüleri ile başarır. FourierNet, ağ eğitme sürecinde HLT şekil bilgisini kullanan yeni bir kademeli ağ tasarımıdır. Bu tasarım, HLT konturları üzerinde Fourier tanımlayıcıları hesaplayarak ve bu tanımlayıcıları öğrenmek için ek bir regresyon işi tanımlayarak, şekil bilgisinin ifade edilmesini önerir. FourierNet, HLT bölütlemesini regresyon ve sınıflandırma işlerinin eşzamanlı öğrenimi olarak formüle eder. Bu işlerde, şekil bilgisini kodlamak için girdi görüntüsünden Fourier tanımlayıcılar tahmin edilir ve bu tanımlayıcılar girdi görüntüsü ile birlikte HLT bölütleme haritasını oluşturmak için kullanılır. Otuz OKT taramasındaki toplam 1470 OKT görüntüsü üzerinde yapılan deneyler, HLT şeklinin Fourier tanımlayıcılar ile nicelleştirilmesinin ve bunların bölütleme ana işi ile birlikte eş zamanlı olarak öğrenilmesinin önemli ölçüde daha iyi sonuçlar verdiğini göstermiştir. Bu bulgular, yöneltmeli OKT görüntülerine olan ihtiyacı azaltarak HLT bölütlemesini kolaylaştıran şekil koruyan bir ağ tasarlamanın verimliliğini göstermektedir.","Henle's fiber layer (HFL), a retinal layer located in the outer retina between the outer nuclear and outer plexiform layers (ONL and OPL, respectively), is composed of uniformly linear photoreceptor axons and Müller cell processes. However, in the standard optical coherence tomography (OCT) imaging, this layer is usually included in the ONL since it is difficult to perceive HFL contours on OCT images. Due to its variable reflectivity under an imaging beam, delineating the HFL contours necessitates directional OCT, which requires additional imaging. This thesis addresses this issue by introducing a shape-preserving network, FourierNet, which achieves HFL segmentation in standard OCT scans with the target performance obtained when directional OCT is available. FourierNet is a new cascaded network design that puts forward the idea of benefiting the shape prior of the HFL in the network training. This design proposes to represent the shape prior by extracting Fourier descriptors on the HFL contours and defining an additional regression task of learning these descriptors. FourierNet then formulates HFL segmentation as concurrent learning of regression and classification tasks, in which Fourier descriptors are estimated from an input image to encode the shape prior and used together with the input image to construct the HFL segmentation map. Our experiments on 1470 images of 30 OCT scans reveal that quantifying the HFL shape with Fourier descriptors and concurrently learning them with the main task of segmentation leads to significantly better results. These findings indicate the effectiveness of designing a shape-preserving network to facilitate HFL segmentation by reducing the need to perform directional OCT imaging."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Telefon görüşmeleri üzerinden gerçekleştirilen bir sosyal mühendislik dolandırıcılığı yöntemi olan sesli kimlik avı, telefon kullanımının yaygınlaşmasından bu yana dünya çapında büyük bir sorun haline geldi. Bu sahtekarlıkları tespit etmek için geleneksel ve modern yöntemler şu şekilde listelenebilir; müşteri davranışlarının görsel analizi, kural tabanlı karar sistemleri, kümeleme algoritmaları, karar ağaçları, sığ sınıflandırıcılar ve derin öğrenme modelleri gibi makine öğrenme modelleri. Görsel analiz sadece insan uzmanlığına bağlıdır ve etkili olabilmesi için çok yüksek iş gücü gerektirir. Kural tabanlı sistemler uç durumlar için kullanışlıdır ancak konsept kaymalarına karşı savunmasızdır. En gelişmiş yöntemler genellikle makine öğrenimi yaklaşımlarını kullanır. Ancak, makine öğrenmesi yöntemlerinin, uzmanlar tarafından yapılan özellik mühendisliği çalışması ve yüksek hesaplama gücü gibi gereklilikleri ve gizlilik ihlalleri gibi kısıtları vardır. Bu nedenle Turkcell Teknoloji ile ortaklaşa çalışarak, bu sorunu çözmek için geleneksel yöntemlerin avantajlarından ve son teknoloji yöntemlerin etkinlik ve verimliliğinden yararlanan bir sistem geliştirmeyi hedefledik. Dolandırıcı kullanıcıları tespit etmek için bir birleşik öğrenme modelini mevcut bir görselleştirme aracına entegre ettik. Bu araç, ilişkisel verileri bilgi çizgeleri olarak görselleştirirken, bilgileri ve istatistiksel verileri çizelgeler ve metinler olarak gösteren bir araçtır. Birleşik öğrenme modelimiz iki derin sinir ağı ve bir karar ağacı sınıflandırıcısından oluşturmaktadır. Varyansı azaltmak ve daha kararlı bir model oluşturmak için iki farklı yapay sinir ağından faydalandık. İlk yapay sinir ağı; bir giriş katmanından, Rektifiye Edilmiş Doğrusal Birim (ReLU) aktivasyon fonksiyonunu kullanan 200 yapay nöron içeren iki gizli katmandan, gizli katmanları takip eden nöron seyreltme katmanlarından ve sigmoid aktivasyon fonksiyonlu bir yapay nöron içeren çıkış katmanından oluşmaktadır. Aşırı uyumlamayı önlemek için bu ağda nöron seyreltme katmanları kullanılmıştır. Oluşturduğumuz ikinci sinir ağı, aktivasyon fonksiyonu ReLU olan, sırasıyla 64, 64 ve 32 yapay sinir hücresi sahip 3 gizli katmana sahiptir. Bu modellerde kullanmak için Turkcell dolandırıcılık uzmanlarıyla birlikte 20'si ham olmak üzere toplam 34 özellik tasarlanmıştır. Son sonuçların hesabı, modellerin çıktılarının ortalamaları alınarak yapılmıştır. Modelimizin başarısı, sınıf dengesizliği yüksek olduğu için F1 skoru ile ölçülmüştür. Modelimiz 0.82 kesinlik değeri, 0.83 duyarlılık değeri ve 0.82 F1 skoruna sahiptir. Ayrıca modelimizin görselleştirme aracına entegrasyonu ile mobil şebeke operatörlerinin dolandırıcılık olaylarını daha etkin bir şekilde inceleyerek buna göre hareket etmelerini sağlayan bir çerçeve oluşturulmuştur.","Voice phishing, a method of social engineering fraud performed over phone calls, has been a major problem globally since the use of phones became widespread. Traditional and modern methods to detect these fraud schemes include visual analysis of the customers' behaviour, rule-based systems and machine learning models such as clustering, decision trees, shallow classifiers and deep learning models. Visual analysis depends only on human expertise and requires very high labor force to be effective. Rule-based systems are useful for extreme cases but are vulnerable to concept drifts. The-state-of-the-art methods generally utilize machine learning approaches. However, they require one or more of feature engineering done by experts, high computational power and privacy infringements. Therefore, in collaboration with Turkcell Technology, we aimed to develop a system that benefits from the advantages of the traditional methods while exploiting the effectiveness and efficiency of the state-of-the-art ones to tackle this issue. In doing so, we integrated an ensemble learning model to an existing visualization tool for detecting fraud users. This tool visualizes relational data as knowledge graphs, shows the informational data as texts and statistical data with charts and texts. Our ensemble learning model has two deep neural networks and one decision tree classifier. Multiple neural networks are used to reduce the variance and make a more stable model. One of them is composed of an input layer, two hidden layers with 200 nodes using Rectified Linear Unit (ReLU) activation function, each followed by a dropout layer and an output layer of one node with sigmoid activation function. We used dropout layers in this network to prevent over-fitting. The second neural network we built has 3 hidden layers instead with node numbers 64, 64 and 32, respectively, with ReLU as their activation function. To feed these models, a total of 34 features, 20 of which are raw, have been engineered with Turkcell fraud experts. The aggregation of the outputs is done by taking their average. We measured the success of our model by calculating the F1 Score as the class imbalance is high. Our model's F1 score is 0.82 with a precision of 0.82 and a recall of 0.83. Also, with the integration of our model into this visualization tool, a framework was formed allowing mobile network operators to examine and detect fraud cases more efficiently and act accordingly."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Filmleri fragmanları aracılı ̆gıyla tanıtmak, izleyicilerin ve yatırımcıların filmin gelecekteki ba ̧sarısı hakkında beklentiler olu ̧sturmasına yardımcı olabilecek de ̆gerli bilgiler sa ̆glar. Yakın zamanda yapılan ara ̧stırmalar, izleyicilerin sinema salonları yerine evde dijital yayın platformları aracılı ̆gıyla film izlemeyi tercih etti ̆gini do ̆gruladı ve bu da film fragmanlarının ki ̧siye ̈ozel olarak g ̈osterilmesine neden oldu. Ayrıca, farklı ilgi grupları i ̧cin olu ̧sturulan reklamlar, hem kullanıcılar hem de reklam verenler i ̧cin ̈onemli derecede iyile ̧stirilmi ̧s bir deneyim sa ̆glayabilir. Gecmi ̧ste fragman olu ̧sturma s ̈urecini otomatikle ̧stirmek i ̧cin birka ̧c giri ̧simde bulunuldu, ancak yapay zeka tarafından olu ̧sturulan fragmanlar, edit ̈orlerin yarattıklarından daha az ̧cekici olarak kabul edildi. Derin ̈o ̆grenmedeki en son geli ̧smelerin kullanımı ve veri k ̈umelerinin daha fazla kullanılabilirli ̆gi, otomatik fragman olu ̧sturma s ̈urecini hızlandırabilir. ̈Uretilen her film, temsil etti ̆gi bir dizi t ̈urle etiketlenmi ̧stir. B ̈oylece her filmin temsil etti ̆gi her t ̈ur i ̧cin ayrı fragmanını olu ̧sturularak izleyiciye ki ̧siselle ̧stirilmi ̧s reklam sunulabilir. Bildi ̆gimiz kadarıyla, t ̈ure ̈ozel fragmanlar aracılı ̆gıyla filmlerin ki ̧siselle ̧stirilmi ̧s reklamları, otomatik fragman olu ̧sturma ̧calı ̧smalarında ilk deneme olacaktır. Bu g ̈orev icin belirli bir filmden belirli bir t ̈ur ̈un temsili sahnelerini ̧cıkaran bir araca gereksinimiz vardır. Bu sahneler her t ̈ur i ̧cin bir fragman tasla ̆gı olu ̧sturmak uzere birlestirilir. Taslak, yaratıcı post prod ̈uksiyon s ̈urecinden ge ̧cirilir. Bu tezde, sahneleri bir dizi t ̈ure ayıran bir derin ̈o ̆grenme a ̆gı geli ̧stirdik. Bu a ̆gı e ̆giten bir ̈o ̆grenim veri seti olu ̧sturmak i ̧cin, temsili t ̈urleriyle etiketlenmi ̧s bir dizi sahne derledik. Ağımız, deneysel ikili modellerden ̈o ̆grenilen hiper parametrelerle ̧cok etiketli bir sınıflandırma g ̈orevini yerine getirdi. ̈O ̆grenme s ̈ureci, g ̈orsel ̈ozelliklerin, işitsel ̈ozelliklerin ve bunların kombinasyonunun kullanımını i ̧cererek ger ̧cekle ̧sti. Modelin nihai sonucundaki sınıflandırma performansı, insan algısı ile karsılastırılarak degerlendirildi.","Promoting movies through their trailers provides valuable information that can help viewers and investors form expectations about the movie's future success. Recent research confirmed that the audience prefers to watch movies through at-home-streaming services rather than at the theaters which resulted in movie trailers being shown privately. Moreover, advertisements created for different interest groups can provide a drastically improved experience for users and advertisers alike. There have been few attempts to automatize the trailer generation process however, AI-generated trailers were considered less attractive than editors' creations. Fortunately, the use of the most recent advancements in deep learning and the greater availability of datasets can accelerate the automated trailer generation process. Every movie produced is labeled with a set of genres that it represents. Thus, it is possible to generate multiple trailers of the same movie for different genres to offer personalized advertisements to the audience. To the best of our knowledge, personalized advertisements of movies via genre-specific trailers will be the first attempt in the automated trailer generation studies. For this task, we needed a tool that extracts representative scenes of a particular genre from a given movie. Then, these scenes can be concatenated to form a draft of a trailer for each genre. The draft can be finalized through the creative post-production process. In this thesis, we developed a deep learning network that classifies scenes into a set of genres. In order to construct a training dataset to train this network, we compiled a set of scenes that are labeled with their representative genres. Our network accomplishes a multi-label classification task with hyperparameters learned from experimental binary models. The learning process comprises the use of visual features, audio features, and their combination. The final result of the model is evaluated by comparing its classification performance with human perception"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sentetik olarak oluşturulmuş medya içeriği, çevrimiçi alanda bilgi güvenliği için önemli bir tehdit oluşturmaktadır. Ünlülerin, politikacıların ve sıradan vatandaşların manipüle edilmiş videoları ve görüntüleri, yanlış beyan ve iftira amaçlıysa, kişinin itibarına ciddi zarar verebilir. Bu tür içeriğin erken tespiti, şüpheli bilgilerin hızlı yayılmasının zamanında önlenmesi için çok önemlidir. Geçtiğimiz yıllarda, önemli sayıda derin sahte algılama metodu, uzay-zaman tutarsızlıklarını ortaya çıkarma amacıyla, bir ön işleme adımı olarak, hareket büyütme teknolojisin kullanılabileceğini ileri sürdü. Fakat, sıklıkla kullanılan hareket büyütme metodları, sınırlı bir hareket kümesi için optimize edildiğinden ve etki alanlarının dışında kullanıldığında önemli görsel yapaylıklar gösterdiğinden, bu tür bir ön işleme yaklaşımı optimal değildir. Bu amaçla, hareket büyütmeyi ayrı bir işlem adımı olarak uygulamak yerine, bir CNN-LSTM sınıflandırma ağına, ek olarak eklenen, eğitilebilir, hareket büyütmeden esinlenen, öznitelik manipulasyonu unitesini test etmeyi öneriyoruz. Yaklaşımımızda, tam entegrasyondan ziyade, video sınıflandırma görevinde, hareket büyütme benzeri mimarilerin kullanımını anlamak için ilk adımları atmayı hedefliyoruz. Sonuçları DeepFakes sahte oluşturma yöntemini kullanarak oluşturulan, ve beş binden fazla sentetik video içeren Celeb-DF veri kümesinde test ediyoruz. Sonuçlarımızın tutarlılığını göstermek için aynı deneysel kurulumla birden fazla deney gerçekleştirerek ortalama doğruluğu sunuyoruz. Deneylerimizde, öznitelik manipulasyonu unitesini ağa dahil edildiğinde doğruluğun ortalama 3% artışlnı gözlemliyoruz.","Synthetically generated media content poses a significant threat to information security in the online domain. Manipulated videos and images of celebrities, politicians, and ordinary citizens, if aimed at misrepresentation, and defamation can cause significant damage to one's reputation. Early detection of such content is crucial to timely alleviation of further spread of questionable information. In the past years, a significant number of deepfake detection frameworks have proposed to utilize motion magnification as a preprocessing step aimed at revealing transitional inconsistencies relevant to the prediction outcome. However, such an approach is sub-optimal since often utilized motion manipulation approaches are optimized for a limited set of controlled motions and display significant visual artifacts when used outside of their domain. To this end, rather than apply motion magnification as a separate processing step, we propose to test trainable motion magnification-inspired feature manipulation units as an addition to a convolutional-LSTM classification network. In our approach, we aim to take the first step at understanding the use of magnification-like architectures in the task of video classification rather than aim at full integration. We test out results on the Celeb-DF dataset which is composed of more than five thousand synthetically generated videos generated using DeepFakes fake generation method. We treat manipulation unit as another network layer and test the performance of the network both with and without it. To ensure the consistency of our results we perform multiple experiments with the same configurations and report the average accuracy. In our experiments we observe an average 3% jump accuracy when the feature manipulation unit is incorporated into the network."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada, evrişimsel özdikkat birimlerinin uzamsal gösterim öğrenmede kullanılmasını ve transformatör ağlarının konuşma modellerinin omurgasını oluşturmasını öneriyoruz. Tasarladığımız derin ağ görsel, ses, ve konuşma kipi bazlı derin ağ eğitimi ve gösterim öğrenimi yaptıktan sonra elde ettiği kip gösterimlerini füzyona uğratarak bu kanallardan gelen bilgiyi birleştirmektedir. Deneysel çalışmaların sonucunda, tasarlanan derin ağ ile literatürde popüler Real-Life Trial (RLT) veri tabanı üzerinde yapılan tüm çalışmaları geçen bir performans elde edilmiştir. Dahası, önerdiğimiz model, yeni topladığımız çok-kipli, kontrolsüz şartlarda düşük riskli senaryolar içeren ve doğruluk araştırmalarıyla etikenlenmiş PoliDB isimli veri tabanı üzerinde de yüksek sınıflandırma performansı göstermiştir. Yaptığımız analiz sonunda (1) evrişimsel özdikkat birimlerinin aldatma tespiti için anlamlı gösterimleri, diğer alternatiflere göre üç kata kadar daha az parametre kullanarak elde ettiğini gösteriyor; (2) izlenimsel aldatma niyetinin sürekli bir fonksiyon olduğunu ve videolar süresince her zaman biriminde aynı değerde yada miktarda aldatıcı davranış görülmediğini gösteriyor; (3) elde ettiğimiz bulgular, gözlenen davranışların birbirinden bağımsız olarak incelenmesinin aldatıcı niyeti tespit etmek için güvenilir bir bilgi kaynağı olmadığını işaret ediyor.","In this study we propose the use of self-attention for spatial representation learning, while explore transformers as the backbone of our speech model for inferring apparent deceptive intent based on multimodal analysis of speech videos. The proposed model applies separate modality-specific representation learning from visual, vocal, and speech modality representations and applies fusion afterwards to merge information channels. We test our method on the popular, high-stake Real-Life Trial (RLT) dataset. We also introduce a novel, low-stake, in-the-wild dataset named PoliDB for deceit detection; and report the first results on this dataset as well. Experiments suggest the proposed design surpasses previous studies performed on RLT dataset, while it achieves significant classification performance on the proposed PoliDB dataset. Following our analysis, we report (1) convolutional self-attention successfully achieves joint representation learning and attention computation with up to three times less number of parameters than its competitors, (2) apparent deceptive intent is a continuous function of time that can fluctuate throughout the videos, and (3) studying particular abnormal behaviors out of context can be an unreliable way to predict deceptive intent."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz sensör ağları (WSN) birçok uygulamaya sahiptir ve IoT sistemlerinin önemli bir parçasıdır. Bir WSN'nin birincil işlevi, sensör düğümleri tarafından kapsanan belirli noktalardan veri toplamak ve toplanan verileri daha sonraki işlemler için uzak merkezi birimlere iletmektir. IoT kullanım durumlarında, bir WSN altyapısının birçok uygulama tarafından paylaşılması gerekebilir. Ayrıca belirli bir noktadan veya alt bölgeden toplanan veriler birden fazla uygulama ihtiyacını karşılayabilmektedir. Dolayısıyla, bu gibi durumlarda verilerin bir kez algılanması, uygulamaların kabul oranını artırmak, uygulamaların bekleme sürelerini, toplam çalışma sürelerini, enerji tüketimini ve ağdaki trafiği azaltmak için avantajlıdır. Bu yaklaşımı izleme noktası tabanlı paylaşılan veri yaklaşımı olarak adlandırıyoruz. Bu tezde, her biri bir WSN'nin kapsadığı alanda bazı noktaların izlenmesini gerektiren uygulamaların yerleştirilmesine ve çizelgelenmesine odaklanıyoruz. Bu iki problemi çözmek için genetik algoritma tabanlı yaklaşımlar öneriyoruz. Ek olarak, hızlı karar vermenin gerekli olduğu durumlarda faydalı olacak açgözlü algoritmalar öneriyoruz. Kapsamlı simülasyon deneyleri gerçekleştirdik ve algoritmalarımızı literatürdeki yöntemlerle karşılaştırdık. Elde edilen sonuçlar, çeşitli metrikler açısından algoritmalarımızın etkinliğini göstermektedir.","Wireless sensor networks (WSNs) have many applications and are an essential part of IoT systems. The primary functionality of a WSN is to gather data from certain points that are covered with sensor nodes and transmit the collected data to remote central units for further processing. In IoT use cases, a WSN infrastructure may need to be shared by many applications. Moreover, the data gathered from a certain point or sub-region can satisfy the need of multiple applications. Hence, sensing the data once in such cases is advantageous to increase the acceptance ratio of the applications and reduce waiting times of applications, makespan, energy consumption, and traffic in the network. We call this approach monitoring point-based shared data approach. In this thesis, we focus on both placement and scheduling of the applications, each of which requires some points in the area a WSN covers to be monitored. We propose genetic algorithm-based approaches to deal with these two problems. Additionally, we propose greedy algorithms that will be useful where fast decision-making is required. We realized extensive simulation experiments and compared our algorithms with the methods from the literature. The results show the effectiveness of our algorithms in terms of various metrics."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir çok gerçek dünya uygulaması, hızla oluşturulan verilerle başa çıkabilmek için çok etiketli veri akışlarını benimsemektedir. Bu tür akışlar için, kavram kayması olarak da bilinen veri dağılımındaki değişiklikler, mevcut sınıflandırma modellerinin etkinliğini hızla kaybetmesine neden olur. Bu tezde sınıflandırıcılara yardımcı olmak için, çok etiketli veri akışları için verilerdeki etiket bağımlılıklarını kullanan denetimsiz bir kavram sapma detektörü olan ""Label Dependency Drift Detector"" (LD3) adlı yeni bir algoritma öneriyoruz. Çalışmamız, bir veri birleştirme algoritmasından faydalanır ve üretilen sıralamayı kavram kaymasını tespit etmek için kullanan bir etiket etkisi sıralama yöntemini kullanarak etiketler arasındaki dinamik zamansal bağımlılıklardan yararlanır. LD3, çok etiketli sınıflandırma problem alanındaki ilk denetimsiz kavram kayması algılama algoritmasıdır. Bu çalışmada, LD3'ü, 12 veri seti ve bir temel sınıflandırıcı kullanarak problem alanına uyarladığımız 14 popüler denetimli kavram kayması algılama algoritması ile karşılaştırarak kapsamlı bir değerlendirme yapıyoruz. Sonuçlar, LD3'ün hem gerçek dünya hem de sentetik veri akışlarında karşılaştırılabilir dedektörlerden %19.8 ile %68.6 arasında daha iyi tahmin performansı sağladığını göstermektedir.","Many real-world applications adopt multi-label data streams as the need for algorithms to deal with rapidly generated data increases. For such streams, changes in data distribution, also known as concept drift, cause the existing classification models to rapidly lose their effectiveness. To assist the classifiers, we propose a novel algorithm called Label Dependency Drift Detector (LD3), an implicit (unsupervised) concept drift detector using label dependencies within the data for multi-label data streams. Our study exploits the dynamic temporal dependencies between labels using a label influence ranking method, which leverages a data fusion algorithm and uses the produced ranking to detect concept drift. LD3 is the first unsupervised concept drift detection algorithm in the multi-label classification problem area. In this study, we perform an extensive evaluation of LD3 by comparing it with 14 prevalent supervised concept drift detection algorithms that we adapt to the problem area using 12 datasets and a baseline classifier. The results show that LD3 provides between 19.8% and 68.6% better predictive performance than comparable detectors on both real-world and synthetic data streams."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Paralelleştirilmiş Yüksek Performanslı Hesaplama (HPC) uygulamalarının başarımı, arka plandaki işlemci-işlemci iletişiminin verimliliğine bağlıdır. Gecikim-darboğazlı uygulamalarda performans, en fazla mesaj gönderen işlemci tarafından limitlenir. Gecikim ek yükünü düşürmek için en fazla mesaj gönderen işlemcinin başka bir işlemci ile eşlendiği iki-fazlı mesaj paylaşma-temelli bir algoritma önermekteyiz. Birinci fazda, en fazla mesaj gönderen işlemci, sistemdeki diğer işlemciler içerisinden en fazla ortak giden mesaja sahip olduğu işlemci ile eşlenir. İkinci fazda ise, en fazla mesaj gönderen işlemci, en az mesaj gönderen işlemci ile eşlenir. Her iki fazda da eşlenen işlemciler, ortak giden mesajları aralarında paylaşarak mesaj sayılarını düşürmektedir. Bu, özellikle de en fazla mesaj gönderen işlemcinin gönderdiği mesaj sayısını düşürmektedir. Çekirdek işlem olarak seyrek matris-vektör çarpımı kullanılmış ve testler 512 işlemcili bir sistemde yapılmıştır. Önerilen mesaj-paylaşma-temelli algoritma en fazla mesaj gönderen işlemcinin gönderdiği mesaj sayısında %84, sistemdeki toplam mesaj sayısında %60 düşüşe imkan tanımıştır.","The performance of paralellized High Performance Computing (HPC) applications is tied to the efficiency of the underlying processor-to-processor communication. In latency-bound applications, the performance runs into bottleneck by the processor that is sending the maximum number of messages to the other processors. To reduce the latency overhead, we propose a two-phase message-sharing-based algorithm, where the bottleneck processor (the processor sending the maximum number of messages) is paired with another processor. In the first phase, the bottleneck processor is paired with the processor that has the maximum number of common outgoing messages. In the second phase, the bottleneck processor is paired with the processor that has the minimum number of outgoing messages. In both phases, the processor pair share the common outgoing messages between them, reducing their total number of outgoing messages, but especially the number of outgoing messages of the bottleneck processor. We use Sparse Matrix-Vector Multiplication as the kernel application and a 512-processor setting for the experiments. The proposed message-sharing algorithm achieves a reduction of 84% in the number of messages sent by the bottleneck processor and a reduction of 60% in the total number of messages in the system."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom verilerinin gizliliğini koruyarak paylaşılması, büyük verinin genomik alanına getirdiği bilimsel ilerlemenin önünde büyük bir engel olarak duruyor. Genomik verilerin paylaşımı için genomik veri paylaşım beacon protokolü adlı topluluk odaklı bir protokol yaygın olarak benimsenmiştir. Sistem, veri kümesindeki belirli alellerin varlığına ilişkin yalnızca evet/hayır sorgularına izin vererek, veri paylaşımı için güvenli, uygulaması kolay ve standartlaştırılmış bir arayüz sağlamayı amaçlamaktadır. Ancak yakın zamanda beacon protokolünün kimlik tespiti saldırılarına karşı savunmasız olduğu gösterildi. Bu tezde, genomik veri paylaşan beacon sistemlerine yönelik tehditlerin yalnızca kimlik tespiti saldırıları ile sınırlı olmadığını gösteriyoruz. Genomik veri paylaşan beacon sistemlerinin yeni bir güvenlik açığını tanımlıyor ve analiz ediyoruz: genom yeniden inşası. Saldırgan, kurbanın son güncelleme beacon sistemine eklendiğini bildiğinde, kurbanın genomunun önemli bir bölümünü başarılı bir şekilde yeniden inşa etmenin mümkün olduğunu gösteriyoruz. Özellikle bir saldırganın böyle bir saldırıyı verimli ve doğru bir şekilde yürütmek için genomdaki doğal korelasyonları ve kümeleme tekniklerini nasıl kullanabileceğini gösteriyoruz. Ayrıca aynı güncelleme sırasında beacon sistemine birden fazla kişi eklense bile, kurbanın saldırgan tarafından kolayca erişilebilinecek özelliklerini (ör. göz rengi veya saç tipi) kullanarak kurbanın genomunu yüksek doğrulukla tanımanın mümkün olduğunu gösteriyoruz. Ayrıca hassas bir fenotiple ilişkili olmayan bir beacon sistemi kullanılarak inşa edilmiş bir genomun, hassas fenotiplere sahip beacon sistemlerine (ör. HIV+) kimlik tespiti saldırıları için nasıl kullanılabileceğini gösteriyoruz. Bu çalışmanın sonucu, beacon sistemi operatörlerine (ve beacon katılımcılarına) beacon içeriğinin ne zaman ve nasıl güncelleneceği konusunda rehberlik edecek ve bilinçli kararlar vermelerine yardımcı olacaktır.","Sharing genome data in a privacy-preserving way stands as a major bottleneck in front of the scientific progress promised by the big data era in genomics. A community-driven protocol named genomic data-sharing beacon protocol has been widely adopted for sharing genomic data. The system aims to provide a secure, easy to implement, and standardized interface for data sharing by only allowing yes/no queries on the presence of specific alleles in the dataset. However, beacon protocol was recently shown to be vulnerable against membership inference at- tacks. In this thesis, we show that privacy threats against genomic data sharing beacons are not limited to membership inference. We identify and analyze a novel vulnerability of genomic data-sharing beacons: genome reconstruction. We show that it is possible to successfully reconstruct a substantial part of the genome of a victim when the attacker knows the victim has been added to the beacon in a recent update. In particular, we show how an attacker can use the inherent correlations in the genome and clustering techniques to run such an attack in an efficient and accurate way. We also show that even if multiple individuals are added to the beacon during the same update, it is possible to identify the victim's genome with high confidence using traits that are easily accessible by the attacker (e.g., eye color or hair type). Moreover, we show how a reconstructed genome using a beacon that is not associated with a sensitive phenotype can be used for membership inference attacks to beacons with sensitive phenotypes (e.g., HIV+). The outcome of this work will guide beacon operators on when and how to update the content of the beacon and help them (along with the beacon participants) make informed decisions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Glioma hastalarında tümörün tam rezeksiyonu sağkalım için önemlidir. Gross total rezeksiyon sağlanmış olsa bile, eksizyon kavitesinde kalan mikro ölçekli tümörlü doku nüks riski taşır. Yüksek Çözünürlüklü Sihirli Açı Döndürmeli Nükleer Manyetik Rezonans (HRMAS NMR) tekniği, biyobelirteç metabolit pik uzunluklarını kullanarak sağlıklı ve tümörlü numuneleri başarılı bir şekilde ayırt edebilir. Bu yöntemin küçük ve işlenmemiş numuneler üzerinde hızlı ve hassas bir şekilde çalışması ameliyat sırasında gerçek zamanlı analiz için uygun olmasını sağlar. Ancak, bu metodun ameliyat sırasında kullanımı için kimya bilgisine sahip bir teknisyenin ve tümör metabolizması hakkında bilgi sahibi bir patoloğun ameliyata katılması gereklidir. Ayrıca, bu yöntem ile yapılan analiz bilinen tümör biyobelirteçleri ile kısıtlı ve onların varlığına yöneliktir. Bu çalışmada, bahsedilen analizi HRMAS NMR spektrumu üzerinde makine öğrenimi kullanarak, bilinen tümor biyobelirteç metabolitlerinden bağımsız, gerçek zamanlı ve doğru bir biçimde gerçekleştirebileceğimizi gösterdik. Bu amaç doğrultusunda, glioma ve sağlıklı numuneler içeren, kantitatif bir patoloji analizi ile etiketlenmiş, yeni ve büyük bir HRMAS NMR veri kümesi (n = 565) kullandık. Rastgele orman temelli bir yaklaşımın tümorlü ve sağlıklı numuneleri ayırt etmekte başarılı olduğunu (%85.6 medyan AUC ve %93.4 medyan AUPR) gözlemledik. Ayrıca, iyi ve kötü huylu tümörlü numunelerin benzer bir yaklaşım ile ayrılabildiğini (%87.1 medyan AUC ve %96.1 medyan AUPR) gösterdik. Modelimizin sonuçlarını yorumlamak için özellik (pik) önem derecesini analiz ettik. Kreatin ve 2-hidroksiglutarat gibi bilinen malignite biyobelirteçlerinin tümör ve sağlıklı hücreleri ayırt etmede önemli bir rol oynadığını doğruladık ve yeni biyobelirteç bölgeleri önerdik.","Complete resection of the tumor is important for survival in glioma patients. Even if the gross total resection was achieved, left-over micro-scale tissue in the excision cavity risks recurrence. High Resolution Magic Angle Spinning Nuclear Magnetic Resonance (HRMAS NMR) technique can distinguish healthy and malign tissue efficiently using peak intensities of biomarker metabolites. The method is fast, sensitive and can work with small and unprocessed samples, which makes it a good fit for real-time analysis during surgery. However, only a targeted analysis for the existence of known tumor biomarkers can be made and this requires a technician with chemistry background, and a pathologist with knowledge on tumor metabolism to be present during surgery. Here, we show that we can accurately perform this analysis in real-time and can analyze the full spectrum in an untargeted fashion using machine learning. We work on a new and large HRMAS NMR dataset of glioma and control samples (n = 565), which are also labeled with a quantitative pathology analysis. Our results show that a random forest based approach can distinguish samples with tumor cells and controls accurately and effectively with a median AUC of 85.6% and AUPR of 93.4%. We also show that we can further distinguish benign and malignant samples with a median AUC of 87.1% and AUPR of 96.1%. We analyze the feature (peak) importance for classification to interpret the results of the classifier and validate that known malignancy biomarkers such as creatine and 2-hydroxyglutarate play an important role in distinguishing tumor and normal cells and suggest new biomarker regions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Büyük ve yapılandırılmamış veri kümelerinde eşleşen görüntülerin bulunması, birçok bilgisayarla görme uygulamasında önemli bir rol oynar. Derin öğrenme tabanlı çözümlerin ortaya çıkmasıyla birlikte, görüntü erişimi gibi çeşitli görsel görevler başarıyla ele alınmaktadır. Görsel benzerliği öğrenmek, görüntü eşleştirme ve erişimi görevleri için çok önemlidir. Kapsül ağlar olarak adlandırılan alternatif bir derin öğrenme mimarisi, nesne ve parçaları arasındaki temel uzamsal ilişkiyi kaybetmeden nesneyi tanımlayan daha zengin bilgilerin öğrenilmesini sağlar. Ayrıca, genel tanımlayıcılar, görüntüleri temsil etmek için yaygın olarak kullanılmaktadır. Önerilen mimari, görüntü erişimi performansını artırmak için küresel tanımlayıcıların gücünü ve revize edilmiş kapsül ağlarını birleştirir. Nesne görüntülerinin çoklu görünümlerinden yararlanır ve nesneler ile parçaları arasındaki uzamsal ilişkiyi vurgular. Görüntü temsillerini güçlendirmek için alt özellikleri paralel olarak geliştiren Mekansal Gruplama Geliştirme stratejisi ve görüntülerin dahili temsilleri içindeki küresel bağımlılıkları araştıran öz-dikkat katmanları kullanılır. Yaklaşım, bireysel görüntüler için sınıflandırmayı öğrenmek yerine hem üçlü kayıp hem de maliyete duyarlı düzenli çapraz entropi kaybını kullanarak benzer görüntüler arasındaki benzerlikleri ve benzer olmayan görüntüler arasındaki farklılıkları yakalar. Deneylere dayalı olarak, sonuçlar Stanford Online Products için literatürde yer alan mevcut yaklaşımlardan daha iyi sonuçlar vermektedir.","Finding matching images across large and unstructured datasets plays an important role in many computer vision applications. With the emergence of deep learning-based solutions, various visual tasks such as image retrieval have been successfully addressed. Learning visual similarity is crucial for image matching and retrieval tasks. An alternative deep learning architecture, named capsule networks, enables learning richer information that describes the object without losing the essential spatial relationship between the object and its parts. Besides, global descriptors are widely used for representing images. The proposed architecture combines the power of global descriptors and revised capsule networks to enhance image retrieval performance. It benefits from multiple views of object images and highlights the spatial relationship between objects and their parts. Spatial Grouping Enhance strategy, which enhances sub-features parallelly, and self-attention layers, which explore global dependencies within internal representations of images, are utilized to empower the image representations. The approach captures resemblances between similar images and differences between the non-similar images using both triplet loss and cost-sensitive regularized cross-entropy loss instead of learning classification for individual images. Based on the experiments, the results are superior to the state-of-the-art approaches for Stanford Online Products."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde mobil uygulamaların yaygınlaşması ve buna bağlı olarak veri trafiğinin artmasıyla birlikte, bu verileri hızlı ve yüksek kaliteli bir şekilde sunmak kablosuz hücresel ağlar için giderek zorlaşmaya başlamıştır. Bu talebi karşılamak için, çoklu-erişimli uç hesaplama (MEC) olarak adlandırılan, bulut sunucuların bilgi işlem yeteneklerini uç noktalarda da sağlayarak mesafeye dayalı yüksek gecikmeyi ve veri trafiğini azaltma imkanı sunan bir model oluşturulmuştur. Bu modelin önemli bir uygulaması da, verilerin hızlı dağıtım amacıyla uç noktalar olarak tanımlanan baz istasyonlarının önbelleklerinde depolanması işlemidir. Bu noktada, artan veri trafiğinin özelikle sosyal medya uygulama kullanıcıları tarafından yaratıldığının görülmesi, bu uygulamaların performansını iyileştirmek için baz istasyonlarında etkili depolama tekniklerinin araştırılmasına yol açmıştır. Bu tezde, mobil kullanıcılar arasındaki sosyal etkileşimin ağdaki veri dağıtım modelleri üzerinde güçlü bir etkiye sahip olduğu gerçeğinden hareketle, sosyal gruplara bağlı bir uç önbellekleme sistem modeli önerilmektedir. Kullanıcıların ilgi alanlarına göre manuel olarak gruplandırıldığı literatürdeki diğer çalışmalardan farklı olarak bu çalışma, sosyal grupların kullanıcılar tarafından oluşturulduğu ve üyelerin aynı ilgi alanlarını paylaşmak zorunda olmadığı bir sisteme dayanmaktadır. Bu yöntemle sosyal grup üyelerinin konumları ve bu üyelerin ilgili uygulamaları kullanma istekliliği göz önünde bulundurularak, verileri etkili bir şekilde uçlarda depolayıp, kullanıcılar tarafından deneyimlenen kalitenin arttırılması ve mesafeye dayalı yüksek gecikmenin azaltılması hedeflenmektedir. Önerdiğimiz yöntemlerin performansları kapsamlı deneylerlerle değerlendirildiğinde, yöntemlerimizin gecikmeyi ve ağ yükünü önemli ölçüde azalttığı görülmektedir.","Increased demand for media content by mobile applications has imposed huge pressure on wireless cellular networks to deliver the content efficiently and effectively. To keep up with this demand, mobile edge computing (MEC), also called multi-access edge computing, is introduced to bring cloud computing and storage capabilities to the edges of the cellular networks, such as 5G, with the aim of increasing quality of service to applications and reducing network traffic load. One important application of multi-access edge computing is data caching. As significant portion of multimedia data traffic is generated from media sharing and social network services, various mobile edge caching schemes have emerged to improve the latency performance of these applications. In this thesis, driven from the fact that social interaction between mobile users has a strong influence on data delivery patterns in the network, we propose a socially-aware edge caching system model and methods that consider social groups of users in caching decisions together with storage and transmission capacities of edge servers. Unlike other studies, where users are manually grouped according to their interests, our approach is based on user-specified social groups, where users in a group are neither obligated to share the same interests nor be attentive to the shared content. Our methods cache content considering locations of members of social groups and the willingness of these members in using the related applications. We evaluate the performance of our proposed methods with extensive simulation experiments. The results show that our methods can significantly reduce user-experienced latency and network load."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ağrının doğası, onu, yoğunluk, süre ve yeri içeren çok boyutlu özelliklerine bağlı olan öznelliği nedeniyle değerlendirmeyi zorlaştırır. Bununla birlikte, ağrıyı objektif ve güvenilir bir şekilde değerlendirebilmek, uygun ağrı yönetimi müdahalesinin yanı sıra altta yatan tıbbi nedenin teşhisi için de çok önemlidir. Bu tezde, beyana dayanlı ağrı seviyesinin otomatik ölçümünü yapan video tabanlı bir yaklaşım önerilmektedir. Önerilen yöntem, bir kişinin yüz ifadesinin benzer bir ağrı seviyesi grubundaki başka bir kişininkine dönüştürülmesinden yararlanarak etkin bir yüz gösterimi öğrenmeyi amaçlamaktadır. Ayrıca, Gözlemci Ağrı Yoğunluğu ölçeği ile birlikte Görsel Analog Ölçek, Duygusal Ölçek ve Etkin Motivasyon Ölçeği gibi beyana dayalı ağrı ölçeklerinden yararlanmanın etkisi araştırılmaktadır. Bu amaçla, kişiler arasındaki yüz dönüşümünü öğrenmek için evrişimli bir otomatik kodlayıcı ağı önerilmiştir. Otomatik kodlayıcının optimize edilmiş ağırlıkları, daha sonra, bu ölçeklerin her biri açısından tahminlerin ortalama mutlak hatasını en aza indirirken, aralarındaki tutarlılığı en üst düzeye çıkararak daha da optimize edilen uzam-zamansal ağ mimarisinin parametrelerinin baslangıç değerleri olarak kullanılmaktadır. Önerilen yöntemin güvenilirliği, videolardan ağrı ölçümü için bir kıyaslama veri tabanı olan UNBC-McMaster Ağrı Arşivi üzerinde değerlendirilmiştir. Problemin zorlu doğasına karşın, elde edilen sonuçlar, en gelişmiş yöntemleri geride bırakmakta ve ağrı seviyesinin otomatik değerlendiriminin mümkün olduğunu ve klinik ortamlarda nicel bir değerlendirme sağlamak için destekleyici bir araç olarak kullanıma uygulanabilirliğini göstermektedir.","The nature of pain makes it difficult to assess due to its subjectivity and multi-dimensional characteristics that include intensity, duration, and location. However, the ability to assess pain in an objective and reliable manner is crucial for adequate pain management intervention as well as the diagnosis of the underlying medical cause. To this end, in this thesis, we propose a video-based approach for the automatic measurement of self-reported pain. The proposed method aims to learn an efficient facial representation by exploiting the transformation of one subject's facial expression to that of another subject's within a similar pain group. We also explore the effect of leveraging self-reported pain scales i.e., the Visual Analog Scale (VAS), the Sensory Scale (SEN), and the Affective Motivational Scale (AFF), as well as the Observer Pain Intensity (OPI) on the reliable assessment of pain intensity. To this end, a convolutional autoencoder network is proposed to learn the facial transformation between subjects. The autoencoder's optimized weights are then used to initialize the spatio-temporal network architecture, which is further optimized by minimizing the mean absolute error of estimations in terms of each of these scales while maximizing the consistency between them. The reliability of the proposed method is evaluated on the benchmark database for pain measurement from videos, namely, the UNBC-McMaster Pain Archive. Despite the challenging nature of this problem, the obtained results show that the proposed method improves the state of the art, and the automated assessment of pain severity is feasible and applicable to be used as a supportive tool to provide a quantitative assessment of pain in clinical settings."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İşlemci ve Alanda Programlanabilir Kapı Dizisi (CPU-FPGA) hibrit mimari-lerinin yaygınlaşmasıyla Yüksek Seviyeli Sentez (High-Level Synthesis-HLS) gibi soyut programlama yöntemlerine olan ilgi de artmıştır. Bu yöntemler FPGA programlamak için yapılması gereken bir çok işi programcının üzerinden almakta ve otomatikleştirmektedir. Böylece programcı alışık olduğu dil ve yöntemleri kullanarak FPGA için kod geliştirebilmektedir. Ancak bu iş akışı çoğu kez zahmetsiz olma kaygısıyla hızlı çalışmayı göz ardı edebilmekte ve bu nedenle çizge algoritmaları gibi yüksek performans gerektiren uygulamalarda başarısız kalmaktadır. Bu problemi çözmek maksadıyla yinelemeli grafik algoritmalar kapsamında, özel Yüksek Seviyeli Sentez (HLS) tabanlı optimizasyonlar geliştirdik. Özellikle, Sayfa Sıralama, Sığ Öncelikli Arama ve Bağlantılı Bileşenler algoritmalarını HLS ile hızlı bir donanımın sentezlenebilmesi için optimize ettik. FPGA üzerinde gerçekleştirilen verimli bir boru hattına sahip OpenCL çekirdeklerinin CPU üzerinde çalışan uygulamalara kıyasla üç kata kadar daha hızlı çalışabileceğini gösterdik. Çizgeleri boru hattına uygun biçimde kateden bir yöntem geliştirdik. Buna ek olarak, yöntemlerimizi programcıların kolayca kullanabilmesi için Yüksek Seviyeli Sentez (HLS) iş akışına uygun olarak yaptık.","The emergence of CPU-FPGA hybrid architectures creates a demand for high abstraction programming tools such as High-Level Synthesis (HLS). HLS handles most of the FPGA development tasks automatically, thus freeing up programmers to create applications effortlessly on FPGAs with familiar programming languages. However, HLS often trades speed for convenience, which makes it a poor choice when it comes to applications in which computational performance is a crucial requirement, such as graph algorithms. In the scope of iterative graph algorithms, we developed custom HLS-based optimizations. Specifically, we applied these on PageRank (PR), Breadth-First Search (BFS), and Connected Components (CC) algorithms so that they can be synthesized in a performant way by HLS tools. We observed that well-pipelined OpenCL kernels can provide up to three times speedups on the Intel Xeon-FPGA architecture compared to CPU implementations. We optimized the traversal of vertices for pipelining to execute applications faster. Furthermore, our approach relies on the HLS workflow to make it effortless for the programmer."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günlük hayatımızda neredeyse her gün aldatıcı davranışlarla karşılaşıyoruz. Aramızdaki en güvenilir olanlar bile bazen kasıtlı veya kasıtsız olarak aldatıcı olabilir. İnsanlar çoğu zaman yalanları tespit etmekte başarılı olamadığı için, özellikle mahkeme davaları gibi yüksek riskli senaryolarda otomatik bir aldatma tespit sisteminin kullanılması gerekmektedir. Videoları girdi olarak kullanan, tam otomatik, kişiliğe duyarlı bir aldatma algılama modeli öneriyoruz. Bildiğimiz kadarıyla, bir aldatma tespit görevinde deneklerin kişiliğini analiz etmeyi ilk düşünen biziz. Önerilen model, analizlerinde deneklerin kişilik özelliklerine ek olarak hem yüz ifadesini hem de sesle ilgili ipuçlarını kullanan çok modlu bir yaklaşımdır. Kişilik özellikleri çıkarıldıktan sonra ifade ipuçlarına dayalı aldatma öznitelikleri ile birleştirilir. Aldatma, ses ve kişilik modülleri, girdinin zamansal dinamiklerini daha iyi anlamak için 3DResNext ve CNN-GRU gibi uzaysal-zamansal mimarilerden oluşturulmuştur. Son olarak, GRU tabanlı bir füzyon modeli kullanarak ifade ve ses kiplerini birleştirir. Onerilen sistemin değerlendirmesi, gerçek hayattan alınmış mahkeme ¨ duruşmalarının kayıtlarını kullanan Real Life Trials veri seti üzerinde gerçekleştirilmiştir. Sonuçlar, kişilik özniteliklerinin kullanımının aldatma tespiti görevini kolaylaştırdığını göstermektedir. Aldatma özniteliklerine ek olarak kişilik öznitelikleri de kullanıldığında, aldatma modülünün performansında %20,4'e kadar (göreli) iyileşme gözlemlenmiştir. Bunun üzerine sesle ilgili ipuçları da dikkate alındığında ek olarak %15,4 (göreli) iyileşme elde ediyoruz.","We encounter with deceptive behavior in our daily lives, almost everyday. Even the most reliable ones among us can sometimes be deceptive either deliberately or unintentionally. Since people are not successful at detecting lies most of the time, it is necessary to use an automated deception detection system particularly in high-stake scenarios such as court trials. We propose a fully automated personality-aware deception detection model that uses videos as input. To our knowledge, we are the first to consider analyzing the personality of subjects in a deception detection task. The proposed model is a multimodal approach where it uses both facial expression and voice related cues in addition to personality traits of subjects in its analyses. After personality traits are extracted, they are combined with deception features which are based on expression cues. Deception, voice, and personality modules are constituted from the spatiotemporal architectures such as 3D-ResNext and CNN-GRU to better comprehend the temporal dynamics of the input. Finally, it combines expression and voice modalities using a GRU based fusion model. The evaluation of the proposed model is performed on Real Life Trials dataset that uses the records of court trials from real life. The results suggest that the use of personality traits facilitates the deception detection task. When the personality features are employed in addition to deception features, there is up to 20.4% (relative) improvement on the performance of the deception module. When the voice related cues are also considered upon that, we obtain 15.4% (relative) improvement additionally."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek çözünürlüklü uydu görüntülerinden ilgili nesnelerin otomatik olarak çıkarılması aktif bir araştırma alanı olmuştur. Pek çok güncel makale, gelişmiş bölümlendirime doğruluğu için derin öğrenmeye dayalı çeşitli anlamsal bölümlendirme teknikleri hakkında araştırmalar yapmaktadır. Mevcut literatür arazi örtüsü ve arazi kullanımı (örn. yapıların, yolların ve su alanının bölümlendirilmesi) ile ilgili zengin bilgiler sağlasa da, bunların çoğu elektro-optik tabanlı (EO) görüntüler üzerinde bölümlendirmeye odaklanmıştır. Güncel çalışmaların bir diğer odağı, görünür tayf kullanma sınırlamalarının üstesinden gelmek için bu tür ilgi nesnelerini Sentetik-Açıklık-Radarı tabanlı (SAR) görüntülerde bölümlere ayırmak olmuştur. Görünür tayfda alınan optik veriler hala birçok hava uygulamasında yaygın olarak tercih edilir ve kullanılırken, bu tür uygulamalar yüksek doğrulukla çalışmak için tipik olarak açık bir gökyüzüne ve minimum bulut örtüsüne ihtiyaç duyar. SAR görüntüleme, hava ve bulutun geleneksel optik sensörleri engellemesi (şiddetli hava koşulları ve bulut örtüsü sırasında olduğu gibi) gibi görünürlükle ilgili sorunları hafifletmek için alternatif bir görüntüleme tekniği olarak yararlı olmaktaktadır. Güncel segmentasyon teknikleri, U-Net'e dayalı pek çok derin öğrenme çözümleri kullanır. ˙ Ilgi ağı temelli derin öğrenmedeki son gelişmeler, SAR görüntü özellikleri ile birleştirildiğinde, özellikle düşük görüş koşullarında ilgi duyulan nesnelerin bölümlendirilmesini artırılabilir. Bu tezde, uydu SAR görüntülerinde anlamsal bölümlendirme için sıkıştırma ve ilgi tabanlı bir ağ önerilmiştir. Ozellikle, uzaktan ¨ algılama görüntülerindeki ilgi nesnelerini bölümlere ayırmak için U-Net tabanlı bir mimaride sıkıştırma ve ilgi kavramının nasıl kullanılabileceğini gösteriyoruz ve çok sayıda halka açık veri kümesi üzerindeki performansını inceliyoruz. Deneylerimiz, önerilen yöntemimizin kullanılan tüm veri kümelerinde çok sayıda temel ağla karşılaştırıldığında üstün sonuçlar verdiğini göstermektedir.","Automatic extraction of objects of interests from high-resolution satellite images has been an active research area. Numerous recent papers have investigated on various deep learning-based semantic segmentation techniques for improved segmentation accuracy. Despite the fact that existing literature provides a wealth of information on land cover and land use (e.g., segmentation of structures, roads, and water area), the majority of them have been focused on segmentation on electro-optical-based (EO) images. A recent focus has been segmenting such objects of interest in Synthetic-Aperture-Radar-based (SAR) images to overcome the limitations of using the visible spectrum. While the optical data taken at the visible spectrum is still widely preferred and used in many aerial applications, such applications typically need a clear sky and minimal cloud cover in order to function with high accuracy. SAR imaging is particularly useful as an alternative imaging technique to alleviate such visibility-related problems such as when weather and cloud may obscure conventional optical sensors (as in during severe weather conditions and cloud cover). Recent segmentation techniques use multiple deep solutions based on U-Net. Recent attention based developments in deep learning when combined with the SAR image features, segmentation of objects of interests can be increased especially under low visibility conditions. In this thesis, a squeeze and attention based network is proposed for semantic segmentation in satellite SAR images. In particular, we show how squeeze and attention concept can be used within a U-Net based architecture for segmenting objects of interests in remote sensing images and study its performance on multiple public datasets. Our experiments demonstrate our proposed method yields superior results when compared to multiple baseline networks on all the used datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgi görselleştirme, verinin görselleştirmesine ve incelenmesine yardımcı olan farklı yaklaşımların incelenmesidir. Bu alanda bulunan çok sayıda seçenek ve yöntem arasında ilişkisel verinin veya çizge görselleştirmenin algoritmik temeli olan ``çizge çizimi"" yer almaktadır. Çizge çizimi, veriyi geometrik şekiller ile sunmak ve 2 boyutlu veya 3 boyutlu bir alanda sergilemek için çizge teorisini ve görselleştirmeyi birleştirmektedir. Çok sayıda otomatik çizge düzeni bulunmaktadır. Bu düzenlerden biri kenarları dikey ve yatay bölümlerden oluşan ortogonal düzendir. Çizge nesnelerinin gruplandırılmasını veya kümelenmesini temsil etmek için bileşik çizge (compound graph) adı verilen özelleştirilmiş bir çizge türü kullanılmaktadır. Basit çizgeler için çok sayıda ortogonal düzen yaklaşımı sunulmuştur fakat bileşik çizgeler için ortogonal yerleştirme algoritması ile ilgili az sayıda araştırma mevcuttur. Bu tezde, halihazırda var olan Topoloji-Şekil-Metrik (Topology-Shape-Metrics (TSM)) yaklaşımını alıp tek tip düğüm büyüklüğüne sahip 4 dereceli küçük bileşik çizgelere hitap edecek şekilde genişleten C-TSM yaklaşımını sunuyoruz. İlk olarak, bileşik çizgeler basit çizgelere dönüştürülüyor ve ardından bu çizgeye değiştirilmiş TSM yaklaşımı uygulanıyor. Ortaya çıkan çıktı, bir sonradan işleme adımında bir araya toplanıyor ve ardından tekrar bir bileşik çizgeye çevriliyor. Algoritmamız üzerinde yaptığımız performans testlerinin sonuçları C-TSM yaklaşımının küçük boyutlu çizgelerde oldukça iyi çalıştığını ve çıktıyı birkaç saniye içinde verdiğini gösteriyor. Bu algoritma Javascript ve Python'da geliştirilmiştir ve Cytoscape.js uzantısı olarak mevcuttur. Algoritmanın kaynak kodu ve demo uygulaması GitHub'da bulunmaktadır.","Information visualization is the study of different approaches that aid in the visualization and examination of data. Among the broad variety of different options and techniques available in this field is ``Graph Drawing"", which is regarded as the algorithmic foundation of relational information or graph visualization. Graph drawing fuses graph theory and visualization for presenting data as geometric shapes and for laying them out in a 2-D or 3-D space. There exist many different types of automatic graph layouts. One such layout is the orthogonal graph layout in which edges are made up of horizontal and vertical segments. A specialized version of graphs called compound graphs are used to represent grouping or clustering of graph objects. Many orthogonal layout approaches have been presented for simple graphs but there is considerably less research available for orthogonal layout algorithms for compound graphs. In this thesis, we present C-TSM, which takes the already existing Topology-Shape-Metrics (TSM) approach and extends it to cater to 4-degree small compound graphs with uniform node sizes. First, compound graphs are converted to simple graphs and then the TSM approach is applied to it. The resulting output is compacted again in a post-processing step and then the graph is converted back to a compound graph. The results of performance tests on our algorithm show that C-TSM works considerably well on small-sized graphs and gives the output in up to a few seconds. This algorithm has been implemented in Javascript and Python and is available as a Cytoscape.js extension. The source code and a demo application are available on a GitHub repository."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kod gözden geçirme kaynak kodun başka bir geliştirici tarafından incelenmesini kapsayan kabul görmüş ve yaygın bir yazılım mühendisliği sürecidir. Yazılım endüstrisindeki yaygınlığına rağmen tipik yazılım mühendisliği programlarında kendine gerekli yeri bulamamaktadır. Bu tez yazılım mühendisliği programları ile endüstri arasındaki bu farkın \mbox{giderilebilmesi} için ciddi oyun tabanlı bir yaklaşım sunmaktadır. Ciddi oyun yaklaşımının öğretim hedefleri kod gözden geçirme prensiplerine giriş yapmayı kapsar. Bu tez bahsedilen hedeflerin gerçekleştirilebilmesi için, ciddi oyun platformunun dizaynını, realize edilmesini ve işlevsellik testlerinden geçirilmesini kapsar. Oyunun test süreci toplam 280 öğrenci içeren üç vaka analizinden oluşur. Sonuçların değerlendirilmesi katılımcıların vaka analizinden önceki ve sonraki kod gözden geçirmeye dair bilgi ve özgüvenlerinin analiziyle yapılmıştır. Bu analize olanak veren ciddi oyunun kullanımdan önce ve sonra uygulanan sınavlar ve anketlerdir. Analizler ögrencilerin ciddi oyuna karşı olumlu bir tutum sergilediğini göstermektedir. İstatistiki değerler de ögrenci bilgisinin ciddi oyunla etkileşimden sonra arttığına işaret etmektedir. Sonuç olarak ortaya konan kod gözden geçirme ciddi oyun platformu, kod gözden geçirme eğitiminde kullanılabilirliğini ispatlamıştır. Platform ve aidiyetindeki ölçüm materyalleri eğitimcilerin erişimi için internet ortamında mevcuttur.","Code Review is an accepted and widely utilized software engineering practice that focuses on improving code via manual inspections. However, this practice is not addressed adequately in a typical software engineering curriculum. We aim to help address the code review practice knowledge gap between the software engineering curricula and the industry with a serious game approach. We determine our learning objectives around introducing the code review process. In order to realize these objectives, we design, build and test a serious game. We then proceed with a three-step case study with 280 students. We evaluate the results by comparing the students' knowledge and confidence regarding code review before and after the case study, as well as by statistically evaluating how well they did both in the code review quizzes and the game levels themselves. Our analysis indicates that, students have a positive approach regarding playing the serious game while the statistical results show that students improve their knowledge by playing the game. We conclude that our code review serious game had a positive impact on the students and is helpful for introducing the code review process. The game and materials for the case studies are made available online for educators."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin öğrenme kullanarak yüz görüntülerinden akrabalık doğrulaması, çözülmemiş ve araştırma topluluğunun artan dikkatini çeken ilginç bir problemdir. Bununla birlikte, en yeni akrabalık doğrulama sistemleri, farklı yaş ve cinsiyete sahip denekler arasında akrabalık doğrulamasında sorunlara neden olan yaş ve cinsiyete bağlı doğuştan gelen yüz özelliklerinden muzdariptir. Bu çalışmada, daha sağlam bir doğrulama modeli elde etmek hedefiyle akrabalık doğrulamasında yaş ve cinsiyete bağlı yüz özelliklerinin olumsuz etkisini azaltmak için çeşitli yöntemler öneriyoruz. Önerilen yaklaşım, deneklerin yaşını ve cinsiyetini modellemek ve tamamen ortadan kaldırmasa da akrabalık doğrulamasındaki etkilerini azaltmak için güncel üretken çekişmeli ağ mimarilerinin kapsamlı modelleme yeteneklerini kullanır. Ayrıca, üretici modellerin hem imge uzayı hem de öğrenilmiş uzayında gerçekleştirilen yaş ve cinsiyet normalizasyonunun bireysel ve birleşik etkileri üzerinde kapsamlı bir analiz yapıyoruz. Son olarak, normalizasyon sürecinde yüz kimliği bilgilerini daha fazla vurgulamanın etkisini araştırıyoruz. En yeni akrabalık doğrulama modellerinden birini temel alarak, cinsiyet normalizasyonunun, benzer ve farklı cinsiyetteki akrabalar arasındaki doğrulama performansı farkını %6' ya kadar azalttığını gösteriyoruz. Ayrıca, yaş ve cinsiyetin ortak normalizasyonunun, iki farklı akrabalık veri setinde akrabalık doğrulamasını %5 ve %10' a kadar arttırdığını gösteriyoruz. Bu nedenle bu tez, kullanılan akrabalık doğrulama sisteminin çekirdek mimarisinde değişiklik yapmadan yaş ve cinsiyet özelliklerini normalize ederek akrabalık doğrulamasının güvenilirliğini ve sağlamlığını geliştirmek için genel yaklaşımlar önermektedir.","Kinship verification from facial images using deep learning is an interesting problem that is unsolved and gains growing attention of the research community. However, the most recent kinship verification systems suffer from age- and gender-related facial attributes that cause problems in kinship verification between subjects of different age and gender. In this study, we propose various methods to reduce the negative effect of the age- and gender-related facial attributes in kinship verification to achieve a more robust verification model. The proposed approach utilizes the comprehensive modeling capabilities of the recent generative adversarial network architectures to model the age and gender of subjects and reduce their effect in kinship verification, if not remove entirely. Furthermore, we conduct a thorough analysis over individual and combined effects of age and gender normalization, performed in both image and latent space of the generative models. Lastly, we investigate the impact of additional emphasis on the facial identity information during the normalization process. Taking one of the most recent kinship verification models as our baseline, we show that gender normalization has reduced the verification performance gap between subject pairs with the same and different gender, up to 6%. Furthermore, joint normalization of age and gender improves the kinship verification accuracy up to 5% and 10% on two different in-the-wild kinship datasets. Therefore, this thesis proposes generic approaches to improve the reliability and robustness of kinship verification by normalizing the age and gender attributes without making changes in the core architecture of the employed kinship verification system."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gen kopya sayısı varyantlarının (CNV'ler) doğru ve verimli tespiti, karmaşık genetik hastalıklarla önemli ilişkileri nedeniyle kritik öneme sahiptir. Tüm genom dizilimi (WGS) verilerini kullanan algoritmalar, çoğunlukla geçerli istatistiksel varsayımlarla kararlı sonuçlar verse de, tüm ekzom dizileme (WES) verilerinde kopya sayısı tespiti, nispeten daha düşük doğruluk gösterir. WES verileri uygun maliyetli, kompakt ve nispeten her yerde mevcut olduğundan bu talihsiz bir durumdur. Darboğaz öncelikle hedeflenen yakalamanın bitişik olmayan doğasından kaynaklanmaktadır: hedefenen genomik hibridizasyondaki önyargılar, GC içeriği, hedefleme probları ve sıralama sırasında numune gruplaması. Burada, eşleşen WES ve WGS verilerini kullanan ve kullanıma hazır herhangi bir WES tabanlı germline CNV arayan tarafından bildirilen kopya numarası varyasyonlarını düzeltmeyi öğrenen yeni bir derin öğrenme modeli DECoNT sunuyoruz. DECoNT'u 1000 Genom Projesi verileri üzerinde eğitiyoruz ve son teknoloji algoritmaların çoğaltma çağrısı hassasiyetini verimli bir şekilde üç katına ve silme çağrısı hassasiyetini iki katına çıkarabileceğimizi gösteriyoruz. Ayrıca modelimizin (i) sıralama teknolojisinden, (ii) ekzom yakalama kitinden ve (iii) CNV arayandan bağımsız olarak performansı sürekli olarak geliştirdiğini gösteriyoruz. DECoNT'u evrensel bir exome CNV çağrı parlatıcısı olarak kullanmak, WES veri setlerinde germline CNV tespitinin güvenilirliğini artırma potansiyeline sahiptir.","Accurate and efficient detection of copy number variants (CNVs) is of critical importance due to their significant association with complex genetic diseases. Although algorithms that use whole genome sequencing (WGS) data provide stable results with mostly-valid statistical assumptions, copy number detection on whole exome sequencing (WES) data shows comparatively lower accuracy. This is unfortunate as WES data is cost efficient, compact and is relatively ubiquitous. The bottleneck is primarily due to non-contiguous nature of the targeted capture: biases in targeted genomic hybridization, GC content, targeting probes, and sample batching during sequencing. Here, we present a novel deep learning model, DECoNT, which uses the matched WES and WGS data and learns to correct the copy number variations reported by any off-the-shelf WES-based germline CNV caller. We train DECoNT on the 1000 Genomes Project data, and we show that we can efficiently triple the duplication call precision and double the deletion call precision of the state-of-the-art algorithms. We also show that our model consistently improves the performance independent from (i) sequencing technology, (ii) exome capture kit and (iii) CNV caller. Using DECoNT as a universal exome CNV call polisher has the potential to improve the reliability of germline CNV detection on WES data sets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sözsüz davranışların yorumlanması, sosyal etkileşimlerin güvenilir şekilde incelenmesinde büyük önem taşımaktadır. Bu amaçla, literatürde ilk kez, romantik çiftlerin ikili etkileşimlerinde yüz ifadelerini otomatik olarak incelemekteyiz. Çalışmamızda, 167 çiftin bir anlaşmazlıklarını ve paylaştıkları olumlu bir deneyimi konuştukları videoları içeren, yakın zamanda toplanmış romantik ilişki veri kümesi kullanılmaktadır. Olumlu deneyim ve anlaşmazlıklar sırasındaki etkileşimleri ayırt etmek için, anomali tespiti literatüründen uyarlanmış, Derin Çoklu Örnekle Öğrenme (MIL) çatısı kullanılarak yüz ifadeleri modellenmektedir. Yüz davranışlarının zaman-uzamsal gösterimi, kısa video parçalarından üç boyutlu artık ağ aracılığıyla elde edilmekte ve MIL torbalarındaki örnek olarak kullanılmaktadır. Hedefimiz, kısa sürelerde gösterilen ayırt edici yüz özelliklerini ortaya çıkararak anlaşmazlıkları tespit edebilmektir. Bu amaçla, olumlu deneyim ve anlaşmazlık oturumlarındaki örneklerin gösterimleri, derin metrik öğrenme ile daha ayrılabilir olacak şekilde eniyilenmiştir. Ayrıca, daha güvenilir bir ikili etkileşim analizi için etkileşimdeki bireylerin yüz ifadeleri birlikte incelenmiştir. Deneylerimiz, yaklaşımımızın %71'lik bir başarıma ulaştığını göstermektedir. Birçok dayanak modelle karşılaştırmanın yanı sıra, aynı sınıflandırma problemi için altı katılımcıyla bir insan değerlendirme çalışması da gerçekleştirilmiştir. Yaklaşımımız, insanlardan %5 daha doğru tahminler sergilemekle birlikte tüm dayanak modellerden de iyi çalışmaktadır. Deneysel sonuçların önerdiği gibi, yüz davranışlarının güvenilir şekilde modellenmesi, ikili etkileşimlerin incelenmesine büyük bir katkıda bulunarak insanlardan daha iyi bir başarım sağlayabilmektedir.","Interpretation of nonverbal behavior is vital for a reliable analysis of social interactions. To this end, we automatically analyze facial expressions of romantic couples during their dyadic interactions, for the first time in the literature. We use a recently collected romantic relationship dataset, including videos of 167 couples while talking on a conflicting case and a positive experience they share. To distinguish between interactions during positive experience and conflicting discussions, we model facial expressions employing a deep multiple instance learning (MIL) framework, adapted from the anomaly detection literature. Spatio-temporal representation of facial behavior is obtained from short video segments through a 3D residual network and used as the instances in MIL bag formations. The goal is to detect conflicting sessions by revealing distinctive facial cues that are displayed in short periods. To this end, instance representations of positive experience and conflict sessions are further optimized, so as to be more separable using deep metric learning. In addition, for a more reliable analysis of dyadic interaction, facial expressions of both subjects in the interaction are analyzed in a joint manner. Our experiments show that the proposed approach reaches an accuracy of 71\%. In addition to providing comparisons to several baseline models, we have also conducted a human evaluation study for the same task, employing 6 participants. The proposed approach performs 5\% more accurately than humans as well as outperforming all baseline models. As suggested by the experimental results, reliable modeling of facial behavior can greatly contribute to the analysis of dyadic interactions, yielding a better performance than that of humans."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tahminleme işi akıllı ulaşım sistemleri ve bu sistemleri kullananan yolcular için önemli olmakla birlikte ulaşımla ilgili yönetim ve planlamada kritik role sahiptir, gecikme süreleri ve trafik hızı gibi değişkenler ulaşımla ilgili başlıca maliyetleri oluşturmaktadırlar. Her bir ulaşım değişkeni dinamik ulaşım ağlarında ilave yayılmalara sebep olmaktadır. Bunun sonucu olarak, ulaşım ağındaki bir düğümün ulaşım değişkeni örüntüsü ve ağdaki yeri diğer düğümlere yararlı bilgiler sağlayabilir. Biz bu çalışmada bir ulaşım ağındaki düğüme bağlı ulaşım değişkeninin tahminlemesinde ağ bilgisinin ve ulaşım değişkeni ile ilgili benzer düğümlerin davranışlarının ortaklaştırılması gerektiğini dikkate sunmaktayız. Bu çalışmada statik ve dinamik ulaşım ağlarında zaman serisi tahminlemesinde ağ tabanlı özellikleri kullanan Keşifçi Kümelemeli Tahminleme Modelini (KKTM) önermekteyiz. Keşifçi Kümelemeli Tahminleme Modeli, ulaşım ağındaki her bir grup düğüm için temsili bir zaman serisi oluşturur ve bu zaman serilerinin her biri için ağ tabanlı özellikleri bağlayıcı değişken olarak kullarak mevsimler otoregresif hareketli ortalamalar (SARIMA), uzun kısa vadeli hafıza ağları (LSTM), bağlayıcı değişkenli otoregresif hareketli ortalamalar (REG-ARIMA), bağlayıcı değişkenli uzun kısa vadeli hafıza ağları (REG-LSTM) gibi ortak modeller inşa eder. Bu ortak modeller daha sonra her bir düğümün verisine ayrı ayrı uyarlanarak düğümün ulaşım değişkeninin tahminlemesi yapılır. Bu tezde biz ulaşım ağındaki düğümleri gruplamak ve aynı zamanda tahminleme modellerine bağlayıcı değişken olarak girdi oluşturmak için ağ tabanlı teorik özellikleri ve düğüm vektörlerini çıkardık. Önerdiğimiz Keşifçi Kümelemeli Tahminleme Modelini (KKTM) iki ayrı veri seti (uçuş gecikmesi veri seti, trafik hızı veri seti) üzerinde test ettik. Deney sonuçlarına bakıldığında uçuş gecikmesi ve trafik hızı değişkenleri için önerilen keşifçi kümelemeli tahminleme modellerinin bireysel tahminleme modellerine göre daha hatasız tahminlemeler ortaya koyduğu görülmektedir. Kümeleme modellerinde düğümlerin merkezi olma özelliğini ölçen merkeziyet arasındalık skorunun etkili bir bağlayıcı değişken olduğu tespit edilmiştir. Kümelemeli modellerin dinamik olarak oluşturulan ağlarda sitatik ağlara göre daha başarılı olduğu görülmüştür. Bu tezde önerilen Keşifçi Kümelemeli Tahminleme Modeli uygulama alanı bağımsız kavramsal bir modeldir. Önerdiğimiz yöntem ağdaki benzer düğümlerde var olan tahminleme değişkeniyle ilgilili bilgileri ortaklaştırmayı sağladığı için zenginleştiriltimesi sağlanmış veri sayesinde daha güçlü tahminleme modelleri oluşturmaktadır.","Forecasting is a crucial tool for intelligent transportation systems and passengers of these systems and critical for transportation planning and management, as the transportation variable (e.g. delay, traffic speed) are among major costs in transportation. Each transportation variable may cause a further propagation in dynamic transport network. Hence, the transportation variable pattern of a node and the location of the node in the transport network can provide useful information for other nodes. We address the problem of forecasting transportation variable of a transport network node, utilizing the network information as well as the transportation variable patterns of similar nodes in the network. We propose ECFM, Exploratory Clustered Forecasting Modeling, on both static and dynamic transportation network which makes use of graph based features for time-series estimation. ECFM approach builds a representative time-series for each group of nodes in the transport network and fits a common model like Seasonal Autoregressive Integrated Moving Average (SARIMA), Long-Short Term Memory (LSTM), Regression with Autoregressive Integrated Moving Average errors (REG-ARIMA), Regression with Long-Short Term Memory errors (REG-LSTM) for each, using the network based features as regressors. The models are then applied individually to each node data for predicting the node's transportation variable. We perform a network based analysis of the transport network and identify graph-based features and we represent nodes as vectors that are used for both grouping nodes and as regressors in forecasting models. We evaluate proposed ECFM, Exploratory Clustered Forecasting Modeling, on two datasets (flight delay dataset, traffic speed dataset). The experiments show that ECFM provides accurate forecasts of delays/traffics compared to individual forecasting models. Centrality measure of nodes such as betweenness centrality score is found to be an effective regressor in the clustered modeling. Clustered models built on dynamic networks performs better compared to static networks. ECFM, Exploratory Clustered Forecasting Modeling, is an conceptual approach and it is domain independent. Our proposed approach tries to incorporate information, related to estimated variable, exist in similar nodes of the network. Thus, we can achieve to build robust estimation models on enriched data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir çok bilimsel ve gerçek hayatta karşılaşılan problem, seyrek matris veya daha genel haliyle çok boyutlu seyrek tensör hesaplamalarını gerektirmektedir. Seyrek matris hesaplamaları için, içerdiği işlemlerin doğal seri yapısı sebebiyle, seyrek üçgensel sistemlerin paralelleştirilmesi önemli zorluklar ortaya çıkarmaktadır. Seyrek üçgensel sistemleri paralelleştirmek için bir yaklaşım, seyrek üçgensel SPIKE (stSPIKE) algoritmasını kullanmaktır. İlk olarak paylaşımlı bellekler için önerilmiş olan stSPIKE, problemi daha küçük bağımsız sistemlere ayrıştırır ve çok daha küçük bir indirgenmiş seyrek üçgensel sistemin çözümünü gerektirir. Biz bu çalışmada, stSPIKE algoritmasını dağıtık bellekli sistemler için genişleterek yazılımını gerçekleştirdik. Daha sonra, stSPIKE algoritmasını kullanarak dağıtık bellekli paralel Gauss-Seidel (dmpGS) ve ILU (dmpILU) algoritmalarını önerdik. Ayrıca, dmpGS ve dmpILU çözümünde ortaya çıkan indirgenmiş sistemlerin boyutunu ve sıfırdışı eleman sayısını en aza indirmek amacıyla özgün hiperçizge bölümleme modelleri ve blok-içi yeniden sıralama yöntemleri önerdik. Diğer yandan seyrek tensör hesaplamaları konusunda, tensör ayrıştırma, çok boyutlu verilerin analizi için oldukça yaygın kullanılmaktadır. Kanonik çok öğeli ayrıştırma (CPD), en sık kullanılan tensör ayrıştırma yöntemlerinden biridir ve yaygın olarak CPD-ALS algoritması ile çözülür. CPD-ALS algoritmasının yüksek hesaplama ve hafıza talepleri sebebiyle, dağıtık bellekli paralel bir algoritma kullanmak verimlilik için kaçınılmazdır. Çok boyutlu kartezyen tensör bölümleme yöntemini benimseyen orta ölçekli CPD-ALS algoritması, seyrek tensör ayrıştırması için önerilmiş en başarılı dağıtık bellekli CPD-ALS algoritmalarından biridir. Biz, çok boyutlu kartezyen tensör bölümlemesinin iletişim hacmini en aza indirgemeyi, bölümleme hedefiyle doğru bir şekilde karşılayan özgün bir hiperçizge bölümleme modeli (CartHP) öneriyoruz. Gerçek hayat problemlerinden elde edilmiş seyrek matris ve tensörler üzerindeki geniş kapsamlı deneyler, önerilen algortimaların paralel ölçeklenebilirliğini ve önerilen hiperçizge bölümleme ve yeniden sıralama modellerinin etkinliğini doğrular niteliktedir.","Several scientific and real-world problems require computations with sparse matrices, or more generally, sparse tensors which are multi-dimensional arrays. For sparse matrix computations, parallelization of sparse triangular systems introduces significant challenges because of the sequential nature of the computations involved. One approach to parallelize sparse triangular systems is to use sparse triangular SPIKE (stSPIKE) algorithm, which was originally proposed for shared memory architectures. stSPIKE decouples the problem into independent smaller systems and requires the solution of a much smaller reduced sparse triangular system. We extend and implement stSPIKE for distributed-memory architectures. Then we propose distributed-memory parallel Gauss-Seidel (dmpGS) and ILU (dmpILU) algorithms by means of stSPIKE. Furthermore, we propose novel hypergraph partitioning models and in-block reordering methods for minimizing the size and nonzero count of the reduced systems that arise in dmpGS and dmpILU. For sparse tensor computations, tensor decomposition is widely used in the analysis of multi-dimensional data. The canonical polyadic decomposition (CPD) is one of the most popular tensor decomposition methods, which is commonly computed by the CPD-ALS algorithm. Due to high computational and memory demands of CPD-ALS, it is inevitable to use a distributed-memory-parallel algorithm for efficiency. The medium-grain CPD-ALS algorithm, which adopts multi-dimensional cartesian tensor partitioning, is one of the most successful distributed CPD-ALS algorithms for sparse tensors. We propose a novel hypergraph partitioning model, CartHP, whose partitioning objective correctly encapsulates the minimization of total communication volume of multi-dimensional cartesian tensor partitioning. Extensive experiments on real-world sparse matrices and tensors validate the parallel scalability of the proposed algorithms as well as the effectiveness of the proposed hypergraph partitioning and reordering models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek Matris-Vektör Çarpımı (SpMV), birçok bilimsel uygulamada kullanılan önemli bir çekirdek algoritmadır. SpMV, uzamsal yerellikten çok düşük oranda faydalanabilen, iletişime bağlı bir algoritmadır. Düzensiz bellek erişim kalıpları nedeniyle düşük hesaplama-iletişim oranı sergiler. Bu, DRAM trafiğinin önemli miktarda etkili kullanılamamsına ve zayıf bant genişliğine neden olur. Yakın zamanda yayınlanan Yayılma Bloklama (PB) metodolojisi, SpMV algoritmasını gruplama ve toplama şeklinde iki aşamada gerçekleştirerek bu iletişim darboğazının üstesinden gelir ve ek bellek erişimi ile daha iyi yerellik sağlar. PB yaklaşımına dayanarak, bu çalışmada, sıralı olarak birlikte çalışan üst düzey sentez kullanarak gruplama ve biriktirme aşamaları için iki FPGA çekirdeği tasarladık. Yaptığımız deneyler ve projeksiyonlar, tasarımımızın CPU temel uygulaması üzerinde 7,9 kata kadar hızlanma sağlayabileceğini gösteriyor.","Sparse Matrix-Vector Multiplication (SpMV) is an important core kernel used in many scientific applications. SpMV is a communication-bound algorithm that suffers poorly from spatial locality. It exhibits low computation-to-communication ratio due to its inherent irregular memory access patterns. This causes a significant waste of DRAM traffic and poor bandwidth utilization. Recently published Propagation Blocking (PB) methodology tackles this communication bottleneck by dividing the execution into binning and accumulation phases, allowing better locality in the cost of additional memory accesses. Building upon PB approach, in this study, we design two FPGA kernels for binning and accumulation phases using high-level synthesis, run together sequentially. Experimental results and projections on larger data show that our design can provide up to 7.9x speedup over the CPU baseline implementation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri akışı sınıflandırması, zamansal verilerdeki artış nedeniyle önemli bir araştırma konusu haline gelmiştir. Kavram kaymaları, veri akışı sınıflandırmasındaki en önemli sorunlardan biridir ve veri akışının istatistiki özelliklerinin zamanla değişmesi olarak tanımlanabilir. Verilerin zamanla değişmesi, statik tahmin modellerinin işlevsizleşmesine neden olur. Daha sağlam ve başarılı modeller geliştirmek için kavram kaymasına uyum sağlayabilen modeller gereklidir. Geniş Öğrenme Sistemi (BLS), artımlı öğrenme için yakın zamanda geliştirilen etkili bir geniş nöral mimaridir. BLS, büyük veri yığınları gerektirmesi nedeniyle kavram kaymalarını anlık tanımlamada ve uyum sağlamada başarısızdır. Bu çalışmada, kavram kaymalarına uyum sağlayabilen bir akış sınıflandırması için Geniş Topluluk Öğrenme Sistemi'ini (BELS) öneriyoruz. BELS, BLS'nin limitasyonlarını dinamik bir çıktı topluluğu katmanı kullanarak çözer ve ""sınıfının-en-iyisi"" model doğruluğunu artıran yeni bir güncelleme yöntemi kullanır. BELS'in matematiksel çıkarımının yanında, BLS ile karşılaştırılması da dahil 11 veri kümesi üzerinden gerçekleştirilen kapsamlı deney sonuçlarını sunuyoruz. Bu deneyler ile birlikte BELS'in teknolojiyi temsil eden yedi temel çizgiden önemli ölçüde üstün olduğunu gösteriyoruz. Önerilen yöntemimiz, BLS'ye kıyasla ortalama %44 ve diğer rekabetçi modellere kıyasla ortalama %29 oranında daha başarılıdır.","Data stream classification has become a major research topic due to the increase in temporal data. One of the biggest hurdles of data stream classification is the development of algorithms that deal with evolving data, also known as concept drifts. As data changes over time, static prediction models lose their validity. Adapting to concept drifts provides more robust and better performing models. The Broad Learning System (BLS) is an effective broad neural architecture recently developed for incremental learning. BLS cannot provide instant response since it requires huge data chunks and is unable to handle concept drifts. We propose a Broad Ensemble Learning System (BELS) for stream classification with concept drift. BELS uses a novel updating method that greatly improves best-in-class model accuracy. It employs a dynamic output ensemble layer to address the limitations of BLS. We present its mathematical derivation, provide comprehensive experiments with 11 datasets that demonstrate the adaptability of our model, including a comparison of our model with BLS, and provide parameter and robustness analysis on several drifting streams, showing that it statistically significantly outperforms seven state-of-the-art baselines. We show that our proposed method improves on average 44% compared to BLS, and 29% compared to other competitive baselines."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yeniden kullanım merkezli CNN hızlandırma yöntemi, CNN'nin giriş katmanında veya aktivasyon haritalarında benzer nöron vektörleri için hesaplamaları yeniden kullanarak CNN çıkarımını hızlandırır. Ancak bu yeni optimizasyon paradigması, yeniden kullanım merkezli CNN'de önemli bir adım olan nöron vektör benzerliği tespitindeki genel giderlerle büyük ölçüde sınırlıdır. Bu tez, yeniden kullanım merkezli CNN için mimari destek üzerine ilk derinlemesine araştırmayı sunmaktadır. Nöron vektör benzerlik tespitini geliştiren ve yeniden kullanım merkezli CNN çıkarımının enerji tüketimini azaltan bir donanım hızlandırıcı önerir. Hızlandırıcı, bankalı bellek alt sistemi ile birlikte çok çeşitli ağ ayarlarını destekleyecek şekilde gerçeklenir. Tasarım keşfi, RTL simülasyonu ve bir FPGA platformunda sentez yoluyla gerçekleştirilir. Eyeriss'e entegre edildiğinde, hızlandırıcı potansiyel olarak performansta 7.75 kata kadar iyileştirmeler sağlayabilir. Ayrıca işlemci üzerinde koşan yazılım tabanlı gerçekleme ile karşılaştırıldığında, benzerlik tespitini %95.46'ya kadar daha enerji verimli ve evrişim katmanını 3.63 kata kadar daha hızlı yapabilir.","Reuse-centric CNN acceleration speeds up CNN inference by reusing computations for similar neuron vectors in CNN's input layer or activation maps. This new paradigm of optimizations is however largely limited by the overheads in neuron vector similarity detection, an important step in reuse-centric CNN. This thesis presents the first in-depth exploration of architectural support for reuse-centric CNN. It proposes a hardware accelerator, which improves neuron vector similarity detection and reduces the energy consumption of reuse-centric CNN inference. The accelerator is implemented to support a wide variety of network settings with a banked memory subsystem. Design exploration is performed through RTL simulation and synthesis on an FPGA platform. When integrated into Eyeriss, the accelerator can potentially provide improvements up to 7.75X in performance. Furthermore, it can make the similarity detection up to 95.46% more energy-efficient, and it can accelerate the convolutional layer up to 3.63X compared to the software-based implementation running on the CPU."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge görselleştirme, ilişkisel verilerin düğümler (nesneler) ve kenarlar (bağlantılar) şeklinde gösterildiği bir bilgi görselleştirme alanıdır. Birçok kişi veya kuruluş, ilişkisel verilerin detaylı analiz ve yorumlanması için çizge görselleştirmeden faydalanır. Çizge görselleştirmede, birincil zorluklar arasında karmaşıklık yönetimi, etkili veritabanı sorgulama ve belirli alanlar için özelleştirme yer alır. Visu\textit{all}, web tabanlı görsel çizge analiz araçları oluşturmak için genel, son derece özelleştirilebilir ve kolayca yapılandırılabilir bir yazılım bileşeni sağlayarak bu sorunları çözmeyi amaçlar. Bu tür görsel analiz bileşenlerinin ihtiyaç duyduğu temel işlevler, çizge öğelerinin düzenini manuel veya otomatik olarak ayarlama, iç içe veya hiyerarşik çizimler için destek, veritabanı veya istemci tarafı verilerinin etkili sorgulanması, ilgilenilen çizge öğelerinin vurgulanması veya ayırt edilmesi, görsellerin ve stillerin özelleştirilmesi, kümeleme, çizge-teorik özelliklerin hesaplanması ve çizge öğelerinin zamana dayalı filtrelenmesini içerir. Visu\textit{all}, hızlı bir başlangıç için tüm bu işlevleri zaten sağlasa da, yazılımın alana özgü ihtiyaçlar için özelleştirilmesi hala kaçınılmazdır. Bu tür yazılım değişiklikleri, sistematik olmama ve kodun orijinal geliştirme ekibi tarafından varsayılan değişmezlerinin yok sayılması nedeniyle ihtilaflara neden olabilir. Bunları önlemek, anlaşılır ve sürdürülebilir özelleştirmeler yapmak için Visu\textit{all} modüler bir mimari sağlar. Ayrıca, sağlanan mimari korunduğu sürece, Visu\textit{all} geliştiricileri ve Visu\textit{all} tabanlı bileşenleri geliştirenler yazılımlarını kolayca doğrudan günceller. Veritabanı sorgularımızı yaklaşık yarım milyon çizge öğesi içeren bir veritabanında test ettik. İstemci tarafındaki operasyonlarımızı da bin civarı çizge öğeye kadar inceledik. Hem istemci tarafında hem de veritabanı işlemlerinde, işlemlerin en fazla birkaç saniye sürdüğünü gözlemliyoruz. Bu gözlemler Visu\textit{all}'un çizge görselleştirmelerin etkileşimli keşfi ve analizi için kullanışlı halde olduğunu gösteriyor.","Graph visualization is an area of information visualization, where relational data is depicted in the form of nodes (objects) and edges (links). Many people or organizations utilize graph visualization for insightful analysis and interpretation of relational data. In graph visualization, primary challenges include complexity management, efficient database querying, and customization for specific domains. Visu\textit{all} aims to solve these problems by providing a generic, highly customizable, and easily configurable software component for building web-based visual graph analysis tools. Essential functionalities needed by such visual analysis components include manually or automatically setting the layout of graph elements, support for nested or hierarchical drawings, efficient querying of the database or client-side data, emphasizing or highlighting graph elements of interest, customization of visuals and styles, clustering, calculating graph-theoretical properties, and time-based filtering of graph elements. Although Visu\textit{all} provides all these functionalities out of the box for jumpstarting, customization of software for domain-specific needs is still unavoidable. Such software changes might result in complications due to unstructured code and code ignoring the invariants assumed by the original development team. To prevent these and to facilitate easily maintainable customization, Visu\textit{all} provides a modular architecture. Furthermore, the developers straightforwardly upgrade the software so long as the Visu\textit{all} developers and the users developing visual analysis components based on Visu\textit{all} maintain the provided architecture. We tested our database queries on a database that contains about half a million graph elements. We also examined our client-side operations up to a thousand graph elements. In both client-side and database operations, we observe that operations take at most several seconds, making Visu\textit{all} convenient for interactive exploration and analysis of networks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan davranışını modellemek, otonom sistemlerin günlük hayata güvenli entegrasyonu için gerekli olan zor bir problemdir. Bu tez, insan davranışlarını statik ve dinamik modeller aracılığıyla modellemeye odaklanmaktadır. Bu tezin ilk katkısı, statik yinelenen akıl yürütme yaklaşımı ile pekiştirmeli öğrenmenin sinerjik bir birleşimi olan stokastik bir modelleme çerçevesidir. Önerilen yaklaşım, otoyol senaryolarında insan sürücü davranışlarını başarılı bir şekilde modellemektedir ve otonom araçların entegrasyon senaryolarının doğrulanması için kullanılabilir. Her ne kadar insan sürücü davranışları statik bir modelle başarılı bir şekilde modellense de, statik modeller, ajanların etkileşimler sırasında inançlarını/varsayımlarını güncelleyebileceklerini hesaba katmamaktadır. Bu tezin ikinci katkısı, zamana yayılan insan-insan etkileşimleri için yeni bir öğrenme modelidir. Hiyerarşik bir akıl yürütme çözüm kavramı aracılığıyla, denge kavramları, Gaus süreçleri ile birleştirilirmiştir. Sonuç olarak, özgün ve sınırlı rasyonel bir öğrenme yaklaşımı önerilmiştir.","Modeling human behavior is a challenging problem and it is necessary for the safe integration of autonomous systems into daily life. This thesis focuses on modeling human behavior through static and dynamic models. The first contribution of this thesis is a stochastic modeling framework, which is a synergistic combination of a static iterated reasoning approach and deep reinforcement learning. Using statistical goodness of fit tests, the proposed approach is shown to accurately predict human driver behavior in highway scenarios. Although human driver behavior are modeled successfully with the static model, the scope of interactions that can be modeled with this approach is limited to short duration interactions. For interactions that are long enough to induce adaptive behavior, we need models that incorporate learning. The second contribution of this thesis is a learning model for time extended human-human interactions. Through a hierarchical reasoning solution approach, equilibrium concepts are combined with Gaussian Processes to predict the learning behavior. As a result, a novel bounded rational learning model is proposed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge analiz uygulamalarında, çizgelerin önbellekte ve daha üst bellek sıradüzeninde zayıf uzamsal ve geçici yerellik kullanımı sebebiyle, ana bellek erişimlerinin bir darboğaz oluşturduğu bilinmektedir. Bu darboğaz önbelleklerde MSHR kullanımı ile ötelense de, problem devasa çizgelerle daha önemli hale gelmektedir. Hizmet dışı olan bir işlemcinin yeniden sıralama arabelleğine ihtiyaç duyan MSHR, sınırlı komut pencere uzunluğu sebebiyle bellek isteklerinin birikmesi sonucu kolayca doygunluğa ulaşmaktadır. Çizge uygulamalarında bellek darboğazı problemini gidermek için özel komutlar ile beraber Karalama Defteri Belleği (Scratchpad Memory (SPM)) önerilmiştir. Model, özel komutları yerleştirmek için x86 mimarili özel bir sıralı işlemci kullanılarak uygulanmış ve test edilmiştir. Özel komutlar ana bellekteki veriye erişim sağlarken birbirleriyle örtüşür ve merkez işlemci birimi (CPU) boru hattındaki diğer komutları engellemez. Bu tasarım endüstri seviyesinde bir simülatör olan GEM5 ile değerlendirilmiş ve önerilen sistemin testlerinde GAP uyguluma (benchmark) bulunan çizge çekirdekleri kullanılmıştır. Sistem sayfa sıralama (PageRank) algoritması için 7 kata kadar hız artışı gösterirken, tek kaynaklı en kısa yol (single-source shortest path), bağlı bileşenler (connected components) ve üçgen sayma (triangular counting) gibi diğer grafik çekirdeklerinde ortalama 2 kat hız artışı sağlamaktadır.","In graph analytic applications, main memory accesses prove to be a bottleneck as graphs have a poor spatial and temporal locality usage in the caches and higher memory hierarchy. Although this bottleneck is slightly mitigated with the use of miss status handling registers (MSHRs) in caches, the problem becomes more significant in the case of large graphs. The MSHR, which relies on an out-of-order processor's reorder buffer, becomes quickly saturated as the memory requests keep on piling up because of the limited instruction window size. To tackle the memory bottleneck for graph applications, the use of a Scratch-pad Memory (SPM) together with custom instructions is proposed. This model is implemented and tested on a custom in-order processor using the x86 architecture to accommodate the related custom instructions. The custom instructions provide non-blocking access to data from the main memory while overlapping with other non-blocking instructions in the CPU pipeline. This design is evaluated on an industry-level simulator, GEM5, and uses the graph kernels from the GAP Benchmark to test the proposed system. The system shows a speedup of up to 7x for PageRank while averaging a speedup of 1.5x for the other graph kernels such as Single-Source shortest path, Connected Components, and Triangle Counting."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İlişkisel veri tabanları, yapısal bir şekilde veri depolamayı sağlayan, en popüler ve yaygın olarak kullanılan alt yapılardan biridir. Veriye erişmek için, kullanıcılar ulaşmak istedikleri bilgileri Yapılandırılmış Sorgu Dili (SQL) kullanarak uygun sorgularla ifade etmek zorundadır. SQL oldukça anlatımcı ve esnek bir dildir; ancak kullanıcının, sorgunun yazıldığı veri tabanının temelinde yatan şemayı bilmesi ve SQL sözdizimine aşina olması gerekir, ki sıradan kullanıcılar için bu durum kolay değildir. Bu amaçla, derin öğrenme tekniklerinden yararlanarak ilişkisel veri tabanlarında kullanılmak üzere daha akıllı kullanıcı arayüzleri yapmak için iki farklı strateji önerilmektedir. İlk çalışmamızda, doğal dil sorgularını SQL'e tercüme etmeyi amaçlayan İlişkisel Veri Tabanına Yönelik Doğal Dil Arayüzleri'nde kullanılan anahtar sözcük eşleme problemi için çözüm sunulmaktadır. Anahtar sözcük eşleme, dizi etiketleme problemi olarak ele alınmaktadır. Doğal dil sorgularının sözcük türlerini de kullanan bu özgün yaklaşım, gözetimli derin öğrenme kullanmaktadır. DBTagger (Database Tagger) adı verilen bu yaklaşım, uçtan uca ve şemadan bağımsız bir çözümdür. Web arama motorlarında yaygın olarak kullanılan ve oldukça bilinen bir strateji olan sorgu tavsiyesi paradigması, uzman kullanıcıların sorgularını sıradan kullanıcılara tavsiye ederek bilgiye erişim sürecinde yardım eder. İkinci çalışmamızda, derin öğrenmeden yararlanarak ilişkisel veri tabanlarında kullanılmak üzere bağlamsal sorgu tavsiye algoritması, Conquer, sunulmaktadır. İlk olarak, veri tabanı satırlarının gizli uzaydaki temsillerini oluşturmak amacıyla Çizge Sinir Ağları kullanılmaktadır. SQL sorguları, sorgunun sonucunda dönen satırların temsillerinin ortalaması alınarak semantik vektör ile temsil edilmektedir. Tanık-Temelli bir yaklaşım olarak, tavsiyeler üretmek için sorguların nihai temsillerinin üzerine kosinüs benzerliği kullanılmaktadır.","Relational databases is one of the most popular and broadly utilized infrastructures to store data in a structured fashion. In order to retrieve data, users have to phrase their information need in Structured Query Language (SQL). SQL is a powerfully expressive and flexible language, yet one has to know the schema underlying the database on which the query is issued and to be familiar with SQL syntax, which is not trivial for casual users. To this end, we propose two different strategies to provide more intelligent user interfaces to relational databases by utilizing deep learning techniques. As the fi rst study, we propose a solution for keyword mapping in Natural Language Interfaces to Databases (NLIDB), which aims to translate Natural Language Queries (NLQs) to SQL. We defi ne the keyword mapping problem as a sequence tagging problem, and propose a novel deep learning based supervised approach that utilizes part-of-speech (POS) tags of NLQs. Our proposed approach, called DBTagger (DataBase Tagger), is an end-to-end and schema independent solution. Query recommendation paradigm, a well-known strategy broadly utilized in Web search engines, is helpful to suggest queries of expert users to the casual users to help them with their information need. As the second study, we propose Conquer, a CONtextual QUEry Recommendation algorithm on relational databases exploiting deep learning. First, we train local embeddings of a database using Graph Convolutional Networks to extract distributed representations of the tuples in latent space. We represent SQL queries with a semantic vector by averaging the embeddings of the tuples returned as a result of the query. We employ cosine similarity over the final representations of the queries to generate recommendations, as a Witness-Based approach. Our results show that in classi cation accuracy of database rows as an indicator for embedding quality, Conquer outperforms state-of-the-art techniques."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapılandırılmış Sorgu Dili (SQL) İlişkisel Veritabanı Yönetim Sistemlerinde yapısal şekilde depolanan veriye erişmek ve görüntülemek için sıkça kullanılan bir araçtır, ancak SQL'in temelinde var olan karmaşıklıklar, sorgularını doğal dil sorgusu olarak ifade edebilen sıradan kullanıcılar için engeller oluşturmaktadır. Biz bu engelleri aşmak için iki farklı çözüm öneriyoruz; açıklanabilir yapay zeka arayüzüne sahip bir Veritabanına Doğal Dil Arayüzü (NLIDB) boru hattı ve anlamsal arama stratejisi. İlk çözüm verilen doğal dil sorgusu SQL'e çevirmek için bir anahtar kelime eşleyici ile beraber SQL çevirme algoritmaları kullanan bir NLIDB boru hattı sunmaktadır. Önerilen boru hattı kullanıcıya açıklanabilir yapay zeka arayüzü ile sunulmakta, böylece kullanıcı, oluşturulan sorguyu anlamlandırabilmektedir. Yaklaşımımız iki son teknoloji sistemle karşılaştırılmıştır; NALIR+ ve Pipeline+. Yaklaşımımız imdb, scholar ve yelp veri kümelerinde sırasıyla tek tablo içeren SELECT-JOIN sorguları için \%88.9, \%100 ve \%60.0 ve çoklu tablo içeren SELECT-JOIN sorguları için \%68.6, \%87.0 and \%83.6 çeviri doğruluğuna erişerek NALIR+'yı geride bırakmaktadır. Yaklaşımımız imdb ve scholar veri kümesinde Pipeline+'dan daha iyi fakat Pipeline+ yelp veri kümesinde biraz daha iyi bir performans göstermektedir. İkinci çözüm bilgi çekme tabanlı yöntemler kullarak verilen sorgu için alakalı olan veritabanı satırlarını çeken bir anlamsal arama yaklaşımı önermektedir. Önerilen yaklaşım, her bir satır ve değerin düğümlerle, bu satır ve değerler arasındaki ilişkinin de kenarlarla temsil edildiği bir çizge kullanmaktadır. Sorgu ve veritabanı satırları Çizge Evrişimsel Ağlar kullanılarak vektör temsillerine dönüştürülmektedir. Bu vektör temsilleri üzerinde bir benzerlik hesaplaması gerçekleştirilmekte ve veritabanı satırları sorgu ile alakalarına göre sıralanmaktadır. Bu benzerlik hesaplaması için kosinüs ben-zerliği kullanılmıştır. Yaklaşımımız Spider veri kümesinden college şeması ile test edilmiştir ve \%42.8 ilk-5 doğruluğu elde edilmiştir.","Structured Query Language (SQL) is a commonly used tool to extract and present structured data stored in Relational Database Management Systems (RDBMSs), yet inherited complexities of SQL create barriers for naive users who are capable of expressing queries as natural language queries (NLQs). In order to tackle this barrier we propose two different solutions; a Natural Language Interface to Database (NLIDB) pipeline with an explainable AI interface and a semantic search strategy. The first solution introduces a NLIDB pipeline that uses SQL translation algorithms along with a keyword mapper to generate SQL queries for given NLQs. Proposed pipeline is presented to the user with an explainable AI interface so that the user can reason over the constructed query. We compared our approach with two state-of-art systems; NALIR+ and Pipeline+. Our approach surpass NALIR+ in imdb, scholar and yelp datasets achieving 88.9\%, 100\% and 60.0\% translation accuracy for single table SELECT-JOIN queries and 68.6\%, 87.0\% and 83.6\% translation accuracy for multiple table SELECT-JOIN queries, respectively. Our approach outperforms Pipeline+ in imdb and scholar datasets but Pipeline+ is slightly better in yelp dataset. The second solution proposes a semantic search approach that uses Information Retrieval based methods to retrieve related table rows for a given NLQ. The proposed approach uses the graph representation of the database where each row and value is represented with a node and edges represent the relation between them. Query and database rows are converted to vector representations using this graph representation and Graph Convolutional Networks (GCNs). A similarity calculation is performed using these vector representations and database rows are ranked according to their relevance to the query. Cosine distance metric is employed for similarity calculation. We tested our approach with college schema from Spider dataset collection and achieved a 42.8\% top-5 accuracy."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uzay-zamansal tahmin uygulamaları için serilerin değişken dinamiklerini modelleme problemini çalışmaktayız. Uzay-zamansal serilerin modellenmesi, doğal afet, sosyal medya ve suç olaylarını tahmin etme gibi birçok önemli uygulamada kullanılmaktadır. Bu problem bir çok araştırmaya konu olsa da birçok çalışma değişken dinamikler ve seyreklik gibi gerçek veriler üzerinde gözlemlenen durumları incelememektedir. Uzayda ve zamanda değişebilen dağılımları modelleyen yeni bir algoritma sunmaktayız. Algoritmamız verinin seyrekliğinden bağım-sız olarak çalışabilmekte ve doluluk oranı değişebilen serilerde çalışabilmektedir. Uzaysal düzlemi bir karar ağacı ile, her düğüm bir bölgeye karşı gelecek şekilde, bölmekteyiz. Uzaysal düzlemdeki farklı yerlerdeki olay dağılımlarını bireysel fakat etkileşimli noktasal süreç modelleri ile modellemekteyiz. Algoritmamız hem karar ağaçlarını hem de noktasal süreç parametrelerini birlikte gradyan tabanlı bir süreç ile optimize etmektedir. Yaklaşımımızı istatistiksel, olasılıksal ve derin öğrenme tabanlı modeller ile karşılaştırmaktayız, ve deprem ve suç olayları kayıtları gibi gerçek veriler üzerinde en iyi tahmin performansını elde ettiğini göstermekteyiz.","We investigate the challenging problem of modeling the non-stationary dynamics of spatio-temporal sequences for prediction applications. Spatio-temporal sequence modeling has critical real-life applications such as natural disaster, social, and criminal event prediction. Even though this problem has been thoroughly studied, many approaches do not address the non-stationarity and sparsity of the spatio-temporal sequences, which are frequently observed in real-life sequences. Here, we introduce a novel prediction algorithm that is capable of modeling non-stationarity in both time and space. Moreover, our algorithm can model both densely and sparsely populated sequences. We partition the spatial region with a decision tree, where each node of the tree corresponds to a subregion. We model the event occurrences in different subregions in space with individual but inter-acting point processes. Our algorithm can jointly optimize the partitioning tree and the interacting point processes through a gradient-based optimization. We compare our approach with statistical models, probabilistic approaches, and deep learning-based approaches, and show that our model achieves the best forecasting performance on real-life datasets such as earthquake and criminal event records."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tezin odak noktası Olasılıksal Gradyan Alçalma (SGD) algoritmasının çok çekirdekli sistemlerde parallelleştirilmesidir. Asenkron yöntemler ve 2 boyutlu ızgara bölümlemeden yararlanan blok tabanlı yöntemler, algoritmanın paylaşımlı bellekli sistemlerde paralelleştirilmesi için kullanılan yaygın yaklaşımlardır. Asenkron yöntemler bellek üzerindeki düzensiz erişimleri sebebiyle ön-bellek sorunlarıyla karşılaşmaktadır. Izgara tabanlı yöntemlerde ise işlek arasında yük dengesizliği problemi görülebilmektedir. Tezimizde mevcut paralel SGD algoritmalarının performans seviyeleri ve paralel darboğaz noktaları incelenmiştir ve bu darboğaz noktalarını hedef alan yeni algoritmalar önerilmiştir. 2 boyutlu bölümlemede işlekler arasında yük dengesizliği problemini çözmek için kutulama tabanlı algoritmalar önerilmiştir. Belleği etkili bir şekilde kullanmak için de ızgara tabanlı asenkron paralel SGD algoritması önerilmiştir. Bu algoritma belleği daha etkili kullanmak için sıfırdışı güncelleme sırasını, faktör güncelleme sırasını bozmadan değiştirmekte ve buna uygun olarak saklı faktör matrislerini bellek üzerinde değiştirmektedir. Elde ettiğimiz deney sonuçları, önerdiğimiz yöntemlerin alternatif yöntemlere göre önemli ölçüde daha iyi çalıştığını göstermektedir.","The focus of the thesis is efficient parallelization of the Stochastic Gradient Descent (SGD) algorithm for matrix completion problems on multicore architectures. Asynchronous methods and block-based methods utilizing 2D grid partitioning for task-to-thread assignment are commonly used approaches for shared-memory parallelization. However, asynchronous methods can have performance issues due to their memory access patterns, whereas grid-based methods can suffer from load imbalance especially when data sets are skewed and sparse. In this thesis, we first analyze parallel performance bottlenecks of the existing SGD algorithms in detail. Then, we propose new algorithms to alleviate these performance bottlenecks. Specifically, we propose bin-packing-based algorithms to balance thread loads under 2D partitioning. We also propose a grid-based asynchronous parallel SGD algorithm that improves cache utilization by changing the entry update order without affecting the factor update order and rearranging the memory layouts of the latent factor matrices. Our experiments show that the proposed methods perform significantly better than the existing approaches on shared-memory multi-core systems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Nöral sinir transfer modelleri, nöral ağlar kullanarak bir resmin içeriğini koruyarak, bu resme belirli bir sanatsal stili aktarmayı amaçlar. Stile özgün model ya da mimari gerektirmeden keyfi stiller aktarabilen modellere evrensel stil aktarımı (ESA) modelleri olarak bilnmektedir. ESA modelleri tipik olarak bir içerik ve stil resmini girdi olarak alıp, stillendirilmiş resmi çıktı olarak veririler. Bu nedenle, stil aktarımı için istenilen özelliklere sahip bir stil resminin bulunması gerekmektedir. Ancak bir stilin çeşitlemelerini ya da stillerin kombinasyonlarını aktarılması gereken uygulamalarda, buna uygun bir stil resmi bulunmayabilir ya da bulunması zor olabilir. Bu çalışmada, bir stil resmine gereksinim duymadan stil aktarımı yapabilen bir ağ sunuyoruz. Ağımız stil resmi yerine bir koşullandırma etiketi kabul etmekte ve stil transferini bu koşullandırmaya göre yapmaktadır. Koşullandırma etiketi birden çok stili barındırabilmektedir ve belirli bir etikete koşullandırılmış çeşitli stiller üretebilmektedir. Modelin tek bir koşul etiketiden birçok stile haritalamayı öğrenebilmesi gerekmektedir. Bu nedenle modelimiz, karmaşık ve çok modlu dağılımları gerçekçi bir şekilde üretebilen üretici çekişmeli ağlar (ÇÜA) üzerine kuruludur. Modelimiz girdi olarak istenilen stil sınıflarını belirten anlamsal bir koşullandırma vektörü alan ve stillendirmeyi yapmak için gerekli olan istatistkleri üreten bir koşullu çekişmeli üretici ağdır. Stil aktarımı yapabilmek amacıyla daha önceden geliştirilmiş, otokodlayıcı tabanlı bir stil aktarma modelini uyarlıyoruz. Bu model önce kodlayıcı ile içerik resminin evrişimsel öznitelik vektörlerini çıkartarak bunların üzerine ağartma dönüşümü uygulayarak çalışmaktadır. Daha sonra ağartılmış öznitelikler stil resminin öznitelikleriyle renklendirme dönüşünümüne sokularak stilr resminin kodları elde edilir ve son olarak kod çözücü, bu koddan stil aktarılmış resmi oluşturmak amacıyla kullanılır. Önerdiğimiz uyarlamada, kovaryans matrislerinin tam hallerini, aynı matrisin yalnızca köşegen elemanlarını kullanarak yakınsıyoruz. Aynı zamanda ÇÜA temelli modelimiz ile özniteliklerin istatistklerini direkt olarak üreterek modelin stil resmi girdisine olan ihtiyacını ortadan kaldırıyoruz. Eğitim ve doğrulama deneylerinde WikiArt verikümesinin bir alt kümesini kullanıyoruz. Hedeflenen stil istatistiklerinin yalnızca küçük bir bölümünü kullanan yakınsama metodumuzun, orijinal metotdan daha hızlı çalıştığını ve orijinal modelle benzer sonuçlar elde ettiğini gösteriyoruz. Aynı zamanda ÇÜA'nın geliştirdiğimiz gerçek stil resimlerine oldukça benzer, eğitim kümesinde bulunmayan stil kombinasyonları üretebildiğini göstermekteyiz.","Neural style transfer (NST) models aim to transfer a particular visual style to a image while preserving its content using neural networks. Style transfer models that can apply arbitrary styles without requiring style-specific models or architectures are called universal style transfer (UST) models. Typically a UST model takes a content image and a style image as inputs and outputs the corresponding stylized image. It is, therefore, required to have a style image with the required characteristics to facilitate the transfer. However, in practical applications, where the user wants to apply variations of a style class or a mixture of multiple style classes, such style images may be difficult to find or simply non-existent. In this work we propose a conditional style transfer network which can model multiple style classes. While our model requires training examples (style images) for each class at training time, it does not require any style images at test time. The model implicitly learns the manifold of each style and is able to generate diverse stylization outputs corresponding to a single style class or a mixture of the available style classes. This requires the model to be able to learn one-to-many mappings, from an input single class label to multiple styles. For this reason, we build our model based on generative adversarial networks (GAN), which have been shown to generate realistic data in highly complex and multi-modal distributions in numerous domains. More specifically, we design a conditional GAN model that takes a semantic conditioning vector specifying the desired style class(es) and a noise vector as input and outputs the statistics required for applying style transfer. In order to achieve style transfer, we adapt a preexisting encoder-decoder based universal style transfer model. The encoder component extracts convolutional feature maps from the content image. These features are first whitened and then colorized using the statistics of the input style image. The decoder component then reconstructs the stylized image from the colorized features. In our adaptation, instead of using full covariance matrices, we approximate the whitening and coloring transforms using diagonal elements of the covariance matrices. We then remove the dependence to the input style image by learning to generate the statistics via our GAN model. In our experiments, we use a subset of the WikiArt dataset to train and validate our approach. We demonstrate that our approximation method achieves stylization results similar to the preexisting model but with higher speeds and using a fraction of target style statistics. We also show that our conditional GAN model leads to successful style transfer results by learning the manifold of styles corresponding to each style class. We additionally show that the GAN model can be used to generate novel style class combinations, which are highly correlated with the corresponding actual stylization results that are not seen during training."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgi görselleştirme, soyut verileri estetik açıdan hoş ve görsel açıdan anlaşılması kolay olarak temsil etmeyi amaçlayan bir çalışma alanıdır. Görsel tasvirler ile insan kognisyonuyla sınırlı kavranılan içerikleri keşfedip pekiştirmek için çeşitli yaklaşımlar ve standartlar oluşturulmuştur. Karmaşık sistemler ve süreçler metin olarak açıklamak zor olduğu için genellikle çizge olarak modellenir. Bir görselleştirme türü olan çizge çizimi, çizgelerin geometrik temsillerini oluşturan kavramları ele alır. Çizgeleri görselleştirmek için otomatik yerleştirme algoritmaları tasarlamaya yönelik birçok araştırma vardır. Fakat sınırlı sayıda çalışma, kenar uçlarının köşelere bağlandığı spesiﬁk noktalar olan bağlantı noktalarını kullanır. Verilerdeki iç içe soyutlama seviyeleri için kullanılan bileşik çizgeler üzerindeki bağlantı noktası kısıtlarını destekleyen CoSEP adlı yeni bir otomatik yerleştirme algoritması öneriyoruz. CoSEP algoritması, güce dayalı bir algoritma olan Compound Spring Embedder'ı (CoSE) baz almaktadır. Mevcut ﬁziksel modelin üzerine ek olarak sezgisel yöntemler ve kuvvet türleri tanıtılmıştır. CoSE'nin yerleştirme yapısının temel olarak kullanılması, CoSEP'in tek tip olmayan köşe boyutlarını, isteğe bağlı iç içelik seviyesi, gelişigüzel iç içe yerleştirme düzeylerini ve birden fazla iç içe geçmiş seviyelerine yayılabilen çizgeler arası kenarların üstünden gelmesini sağlar. Deneylerimiz CoSEP'in, genel kabul görmüş çizge kriterlerine göre bağlantı noktası kısıtı olan bileşik çizgelerin yerleştirme kalitesini önemli ölçüde artırdığını, aynı zamanda küçük ve orta büyüklü çizgeler için etkileşimli uygulamalarda kullanıma uygun, en fazla birkaç saniyede çalıştığını göstermektedir. CoSEP algoritması JavaScript'te Cytoscape.js uzantısı olarak uygulanmıştır ve bir demo ile birlikte kaynaklar ilgili GitHub deposunda mevcuttur.","Information visualization is a field of study that aims to represent abstract data in an aesthetically pleasing and easy to comprehend visual manner. Various approaches and standards have been created to reinforce the discovery of unstructured insights that are limited to human cognition via visual depictions. Complex systems and processes are often modelled as graphs since it would be difficult to describe in text. A type of visualization, graph drawing, addresses the notion of creating geometric representations of graphs. There are plentiful research directed to designing automatic layout algorithms for visualizing graphs. Nevertheless, a limited number of studies utilize ports, which are dedicated connection points on the locations where edge ends link to their incident nodes. We propose a new automatic layout algorithm named CoSEP supporting port constraints on compound nodes used for nested levels of abstractions in data.The CoSEP algorithm is based on a force-directed algorithm, Compound Spring Embedder (CoSE). Additional heuristics and force types are introduced on top of existing physical model. Using CoSE's layout structure as a baseline enables CoSEP to handle non-uniform node sizes, arbitrary levels of nesting, and inter-graph edges that may span multiple levels of nesting. Our experiments show that CoSEP significantly improves the quality of the layouts for compound graphs with port constraints with respect to commonly accepted graph drawing criteria, while running in at most a few seconds, suitable for use in interactive applications for small to medium sized graphs. The CoSEP algorithm is implemented in JavaScript as a Cytoscape.js extension, and the sources along with a demo are available on the associated GitHub repository."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genetik veriler 1990 yılında başlayan İnsan Genetik Verisi Projesi'nden beri hem biyolojinin hem de bilgisayar bilimlerinin çalışma alanlarından biri olmuştur. O zamandan bu yana genetik veri dizilimi, hem sağlık sektörü için hem de sosyal kullanım için gittikçe daha ulaşılabilir ve karşılanabilir hale gelmiştir. Üzerlerinde araştırma yapılabilmesi adına bu genetik veriler hem halka açık internet sitelerinde hem de servis sağlayıcıları aracılığıyla paylaşılabilmektedir. Ancak, bu paylaşımlar kısmi yapıldığı durumlarda bile paylaşımcıların veri gizliliklerini (mahremiyetlerini) tehlikeye atmaktadır. Bu çalışmamızda, verilerin izinsiz paylaşım durumundaki sorumlu tutulabilme ilkesini odaklanılmıştır. Sorumlu tutulabilmeyi yüksek olasılıklarla garanti edebilmek için kullanılan yöntemlerden biri de filigran tekniğidir. Paylaşımcıların izni olmaksızın, verileri ifşa olmadan paylaşabilmek isteyen servis sağlayıcılarına karşı, inanç yayımı tekniği aracılığıyla genetik verinin üzerine uygulanabilecek yeni bir filigran metodu öneriyoruz. Yeni yöntemimizde, sağlanması hedeflenen üç kriter belirlenmiştir. Birincisi, kötü niyetli servis sağlayıcılarının silme ve değiştirme yapma ihtimaline karşın dayanıklı, onları yüksek olasılıklarla tespit edebilecek filigranlar üretmektir. İkincisi, herhangi bir servis sağlayıcısıyla paylaşım yapmak için oluşturulan bütün filigranların epsilon-lokal diferansiyel gizliliği sağlamasıdır. Üçüncü kriter ise filigranların oluşturulması sırasında filigran uzunluğunu -veriden sağlanan faydayı yüksek tutmak adına- olabildiğince kısa ve etkili tutmak, ve bu filigranların gerçek genetik veriden ayırt edilememesini sağlamaktır. Oluşturulan filigranların servis sağlayıcıları tarafından ""tek servis sağlayıcı saldırısı"" ve ""iş birliği saldırısı"" kullanılarak bozulmasını engellemek için genetik verilerle ilgili halka açık istatistiki değerler kullanılmaktadır. Bu değerler çekinik gen frekansı, bağlayış denksizliği, fenotip özellikleri ve aile bireylerinin genetik dizilimleri olabilir. Ayrıca servis sağlayıcılarının filigran oluşturma yöntemimizi ayrıntılarıyla bildiklerini varsayarak, olasılıksal bir inandırıcı yadsınabilirlik garantileyen ve okunan verinin kesinliğini azaltan lokal diferansiyel gizlilik yöntemi sistemimizde yer almaktadır. İstatistiki bilgilere göre verinin içine rastgele gürültü ekleyen, geleneksel diferansiyel gizlilik yöntemlerinden farklı olarak sistemimiz gürültüyü lokal olasılıklara bağlı kalarak eklemektedir.","Genome data is a subject of study for both biology and computer science since the start of Human Genome Project in 1990. Since then, genome sequencing for medical and social purposes becomes more and more available and affordable. For research, these genome data can be shared on public websites or with service providers. However, this sharing process compromises the privacy of donors even under partial sharing conditions. In this work, we mainly focus on the liability aspect ensued by unauthorized sharing of these genome data. One of the techniques to address the liability issues in data sharing is watermarking mechanism. In order to detect malicious correspondents and service providers (SPs) -whose aim is to share genome data without individuals' consent and undetected-, we propose a novel watermarking method on sequential genome data using belief propagation algorithm. In our method, we have three criteria to satisfy. (i) Embedding robust watermarks so that the malicious adversaries can not temper the watermark by modification and are identified with high probability (ii) Achieving -local differential privacy in all data sharings with SPs and (iii) Preserving the utility by keeping the watermark length short and the watermarks non-conflicting. For the preservation of system robustness against single SP and collusion attacks, we consider publicly available genomic information like Minor Allele Frequency, Linkage Disequilibrium, Phenotype Information and Familial Information. Also, considering the fact that the attackers may know our optimality strategy in watermarking, we incorporate local differential privacy as plausible deniability factor that induces malicious inference strength. As opposed to traditional differential privacy-based data sharing schemes in which the noise is added based on summary statistic of the population data, noise is added in local setting based on local probabilities."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genetik yapısal varyasyonlar (YV) kısaca DNA'nın içerik, kopya ve düzenindeki büyük çaplı değişikliklerdir. Her ne kadar yüksek çıktılı dizileme (YÇD) kullanılmaya başlandıktan sonra ciddi oranda aşama kaydedildiyse de, kırılma noktalarındaki dizi karmaşıklığı kompleks YV ve dengeli yeniden-düzenlenmelerin doğru tespiti muamması güçlüğünü sürdürmektedir. Yakın zamana kadar, okumalar kısa olduğunda bahsi geçen bölgelerdeki okuma hizalamasının zorluğu ve uzun okuma platformlarının yüksek hata oranları YV keşfi sürecindeki problemlerin temelini oluşturmaktaydı. Ancak, Pacific Biosciences şirketinin, $>99\%$ doğruluk payı ve $10-20$ kbps uzunlukta okuma yapabilen High Fidelity (Yüksek Doğrulukta, HiFi) dizileme metodunun ortaya çıkmasıyla, etkili YV keşfi ve kırılma noktası çözünürlüğünün iyileştirilmesi mümkün olmuştur. Biz bu çalışmayla uzun okuma teknolojileri kullanarak yüksek kırılma noktası çözünürlüğüne sahip büyük yapısal varyasyon keşfi yapan özgün bir algoritma olan DALEK'i sunuyoruz. DALEK, uzun okumalardaki ayrık dizi ve dizi derinliği sinyalleri kullanarak büyük (>10 kbps) silinme, inversiyon ve kesitsel duplikasyonları keşfetmektedir. Ayrıca algoritmanın parametrelerine göre hali hazırda yüksek hatalı olan Oxford Nanopore Technologies uzun okumalarından da YV tespiti yapabilmektedir.","Genomic structural variations (SVs) are briefly defined as large-scale alterations of DNA content, copy, and organization. Although significant progress has been made since the introduction of high throughput sequencing (HTS) in characterizing SVs, accurate detection of complex SVs and balanced rearrangements still remains elusive due to the sequence complexity at the breakpoints. Until very recently, the difficulty of read mapping in such regions when the reads were short and the high error rates of long read platforms kept the problem challenging. However, with the introduction of the Pacific Biosciences' High Fidelity (HiFi) sequencing methodology, powerful SV detection and breakpoint resolution became possible as a result of its capability to produce highly accurate (>99) long reads (10-20 kbps). Here, we introduce DALEK, a novel algorithm that aims to use long-read technologies to discover large structural variations with high break-point resolution. DALEK uses split read and read depth signatures from long read data to discover large (>10 kbps) deletions, inversions and segmental duplications. We also develop methods to detect large SVs in existing high-error Oxford Nanopore Technologies data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge analizi uygulamalarının önemi, giderek daha çok alanın sürekli büyüyen çizge yapılarını işlemeye başlamasıyla daha da belirgin hale gelmiştir. Bununla birlikte, genel amaçlı işlemciler çizge uygulamalarının geniş bellek ayak izi ve rastgele bellek erişimleri karşısında zorlanmaktadır. Dolayısıyla araştırmacılar alana özel donanım içerikli çözümlere yönelmiştir. Bu tezde, çizge uygulamalarının performansını bellek erişimlerini azaltarak artırmaya çalışan özel bir RISC-V çizge işlemcisi sunulmaktadır. Çizge işlemcisinin yeniliği içerdiği yazılım kontrollü Ayrıt Müsvedde (ESP), Düğüm Müsvedde (VSP) ve Global Müsvedde (GSP) isimli müsvedde belleklerine (scratch-pad memories - SPMler) dayanmaktadır. SPMler genel amaçlı sistemlerde bulunan geleneksel ön-belleklerin yerini almak için tasarlanmıştır. ESP çekirdeğin işlemlerine paralel olarak ayrıt verisiyle önceden yüklenirken, VSP düğüm ile ilgili bellek erişimlerinin neden olduğu çakışmaları azaltarak düğüm trafiğini hafifletir. GSP ise geriye kalan bellek erişimlerinin yükünü üstlenmektedir. Çizge işlemcisine gömülü bu yeni işlevselliğin yazılım tarafından kontrol edilebilir olması için RISC-V komut kümesi mimarisi SPM ile ilgili özel komutlarla genişletilmiştir. Komutların ihtiyaç duyduğu derleyici desteği de sağlanmıştır. Tasarımın gerektirdiği yazılım-donanım etkileşimini göstermek için yaygın olarak kullanılan PageRank, Tek Kaynaklı En Kısa Yol ve Sığ Öncelikli Arama algoritmaları çizge işlemcisi tarzında değiştirilmiştir. Bu uygulamalarla yapılan deneylerin sonuçları, çizge işlemcisinin aynı RISC-V çekirdeğini kullanan genel amaçlı bir işlemciye kıyasla veri yolunu durdurucu bellek erişimlerini yüzde 18 ile yüzde 72 daha az yaptığını göstermektedir.","As more and more domains have started to process ever-growing graphs, the importance of graph analytics applications became more apparent. However, general-purpose processors are challenged to deal with the large memory footprint and the associated random memory accesses in graph applications, directing researchers towards domain-specific solutions. In this dissertation, we present a custom RISC-V graph processor that tries to increase the performance of graph applications by reducing the memory accesses. The novelty of the graph processor lies in the design of our software-controlled scratch-pad memories: Edge Scratch-Pad (ESP), Vertex Scratch-Pad (VSP), and Global Scratch-Pad (GSP). While ESP is preloaded with the edge data in parallel with the execution, VSP relieves the vertex traffic by reducing the conflicts caused by the vertex-related memory accesses. GSP takes over the load of the rest of the memory accesses as these three SPMs replace the conventional caches found in general-purpose systems. For the software to control this new functionality embedded in the graph processor, we extended RISC-V instruction set architecture with custom SPM-related instructions. We provided compiler support for the instructions and we modified the widely used PageRank, Single-Source Shortest Path, and Breadth-First Search algorithms in graph processor fashion to demonstrate the software-hardware interaction needed for the design. The experimental results on these applications show that the graph processor makes 18% to 72% less datapath-blocking memory accesses compared to a general-purpose processor based on the same RISC-V core."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otizm Spectrum Bozukluğu (OSB) ve Zihinsel Engel (ZE) karmaşık genetik yapıları olup birbirlerine eşlik eden nörogelişimsel bozukluklardır. Büyük ölçekli gen sıralamaları araştırmalarına rağmen iki bozukluk için de sorumlu risk genleinin ancak küçük bir yüzdesi belirlenebilmiştir. Bu araştırmada OSB ve ZE için eşlik bilgisini de kullanarak çapraz hastalık analizi ile tahmin gücünü arttıran DeepND adında yeni bir çizge tabanlı gen risk önceliklendirme algoritması tasarladık. Model insan beyin gelişimini modelleyen gen eş ifade çizgelerini çizge evrişimsel sinir ağları ile işleyerek iki hastalık için de önemli mekansal-zamansal nörogelişimsel aralıkları öğrenebilir. Sonuçlarımız sunduğumuz mimarinin alandaki son teknolojinin sağladığı tahmin gücünü arttırdığını göstermektedir.İki bozukluk için de transkripsiyon regulatörlerinindeki zeginleştirilmeler gözlemlendi. OSB genleri arasındaki sıkı düzenleyici bağlantılara rağmen, bu bağlantıların OSB ve ZE genleri ile sadece ZE genleri arasında aynı derecede olmadığı belirlendi. Son olarak, OSB ve ZE bağlantılı kopya numara varyasyonlarını ve modelin emin olduğu yanlış bulguları inceleyerek birkaç yeni şüpheli risk geni belirlendi. DeepND mimarisi kolayca birbirlerine eşlik eden birçok hastalık için genellenebilir.","Autism Spectrum Disorder (ASD) and Intellectual Disability (ID) are comorbid neurodevelopmental disorders with complex genetic architectures. Despite large-scale sequencing studies only a fraction of the risk genes were identified for both. Here, we present a novel network-based gene risk prioritization algorithm named DeepND that performs cross-disorder analysis to improve prediction power by exploiting the comorbidity of ASD and ID via multitask learning. Our model leverages information from gene co-expression networks that model human brain development using graph convolutional neural networks and learns which spatio-temporal neurodevelopmental windows are important for disorder etiologies. We show that our approach substantially improves the state-of-the-art prediction power. We observe that both disorders are enriched in transcription regulators. Despite tight regulatory links in between ASD risk genes, such is lacking across ASD and ID risk genes or within ID risk genes. Finally, we investigate frequent ASD and ID associated copy number variation regions and confident false findings to suggest several novel susceptibility gene candidates. DeepND can be generalized to analyze any combinations of comorbid disorders."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda, Yapay Zeka (AI) ve Nesnelerin İnterneti (IoT) gibi büyük bir yenilik dalgasına tanık olduk. Bu akımda, yazılım araçları sürekli artan işlem gücü talep ediyor ve bu artık geleneksel işlemciler tarafından karşılanamıyor. Bu ihtiyaca yanıt olarak, Grafik İşleme Üniteler (GPU'lar), Alanda Programlanabilir Kapı Diziler (FPGA'lar) ve Yapay Zeka (AI) hızlandırıcılar dahil olmak üzere çok çeşitli donanımlar her gün piyasaya sürülüyor. Öte yandan, donanım platformları daha yüksek performans talebi nedeniyle daha fazla güce aç hale gelirken, eşzamanlı olarak transistör boyutunun küçülmesi ve voltajın azaltılması, devrelerde her zaman güvenilirlik endişelerini arttırmıştır. Bu, özellikle bir hatanın felaketle sonuçlanabileceği ulaşım ve havacılık endüstrileri gibi hataya duyarlı uygulamalar için geçerlidir. Güvenilirlik konularının, sert çevre koşulları gibi başka nedenleri de olabilir. Modern elektronik devrelerin bu iki sorunu, yani aynı anda daha yüksek performans ve güvenilirlik ihtiyacı, uygun çözümler gerektirir. FPGA'lar gibi yeniden yapılandırılabilir devrelere dayalı tasarım veya genel amaçlı işlemciler gibi Piyasadan Hazır Temin Edilebilen (COTS) bileşenlere dayalı tasarım uygun bir yaklaşım olabilir, çünkü bu platformlar çok çeşitli uygulamalarda kullanılabilir. Bu bağlamda, bu tezde üç çözüm önerilmiştir. Bu çözümler, 1) yedekli işlemciler kullanarak sistem düzeyinde güvenlik ve güvenilirliği, 2) birden çok hızlandırıcı kullanarak mimari düzeyinde performansı ve 3) yedekli transistörlerin kullanımıyla devre düzeyinde güvenilirliği hedefler. Özel olarak, ilk çalışmada, Piyasadan Hazır Temin Edilebilen (COTS) işlemcileri kullanarak güvenlik açısından kritik bilgisayarların tasarımında bazı yaygın paramet\-relerin katkısı tartışılmıştır. Yedekli mimariler Markov zinciri kullanılarak mo\-dellenmiştir ve sistem güvenliğinin parametrelere duyarlılığı analiz edilmiştir. En önemlisi, bu tür sistemlerde Yaygın Neden Arızalarının (CCF'ler) önemli varlığı araştırılmıştır. İkinci çalışmada, Yüksek Seviyeli Sentez (HLS) tabanlı, FPGA hız\-landırmalı, yüksek verimli / iş verimli, sentezlenebilir şablon tabanlı grafik işleme ünitesinin tasarımı ve uygulaması sunulmuştur. Bu ünite yazılım programcıları için bile FPGA ile kolay programlayabilme için basitleştirilmiştir. Sunulan yapı, yinelemeli grafik algoritmalarını uygulamak için özel olarak Intel'in son teknoloji ürünü Xeon + FPGA platformunda denenmiştir. Yüksek verimli boru hattının yanı sıra, iş verimli mod, yeni bir etkin liste tasarımıyla toplam grafik işleme süresini önemli ölçüde azaltır. Üçüncü çalışmada, güvenilirlik ve bellek boyutu arasında tercih yapmak için devre düzeyinde yeni bir teknik olan Ortak SRAM hücresi tanıtılmıştır. Bu fikir, ön-bellek, kayıt dosyası, FPGA BRAM veya FPGA arama tablosu (LUT) ve hatta mandallar ve Flip-Floplar gibi herhangi bir SRAM yapısı için de uygulanabilir. Hataya eğilimli koşullarda, yapı, dört hücrenin bir büyük ve sağlam bellek biti oluşturmak üzere devre seviyesinde birleştirileceği şekilde yapılandırılabilir. Üçlü Modüler Yedeklilik (TMR) gibi yaygın donanım yedekliliği tekniklerinin aksine, belirgin bir seçim ünitesi yoktur. Çözüm, temel olarak, güvenilir modun otomatik düzeltme ve tek hatalara karşı tam bağışıklık sağlayabildiği geçici hatalara odaklanır.","In recent years, we have witnessed a huge wave of innovations, such as in Artificial Intelligence (AI) and Internet-of-Things (IoT). In this trend, software tools are constantly and increasingly demanding more processing power, which can no longer be met by processors traditionally. In response to this need, a diverse range of hardware, including GPUs, FPGAs, and AI accelerators, are coming to the market every day. On the other hand, while hardware platforms are becoming more power-hungry due to higher performance demand, concurrent reduction in the size of transistors, and placing high emphasis on reducing the voltage, altogether have always been sources of reliability concerns in circuits. This particularly is applicable to error-sensitive applications, such as transportation and aviation industries where an error can be catastrophic. The reliability issues may have other reasons too, like harsh environmental conditions. These two problems of modern electronic circuits, meaning the need for higher performance and reliability at the same time, require appropriate solutions. In order to satisfy both the performance and the reliability constraints either designs based on reconfigurable circuits, such as FPGAs, or designs based on Commercial-Off-The-Shelf (COTS) components like general-purpose processors, can be an appropriate approach because the platforms can be used in a wide variety of applications. In this regard, three solutions have been proposed in this thesis. These solutions target 1) safety and reliability at the system-level using redundant processors, 2) performance at the architecture-level using multiple accelerators, and 3) reliability at the circuit-level through the use of redundant transistors. Specifically, in the first work, the contribution of some prevalent parameters in the design of safety-critical computers, using COTS processors, is discussed. Redundant architectures are modeled by the Markov chains, and sensitivity of system safety to parameters has been analyzed. Most importantly, the significant presence of Common Cause Failures (CCFs) has been investigated. In the second work, the design, and implementation of an HLS-based, FPGA-accelerated, high-throughput/work-efficient, synthesizable template-based graph processing framework has been presented. The template framework is simplified for easy mapping to FPGA, even for software programmers. The framework is particularly experimented on Intel state-of-the-art Xeon+FPGA platform to implement iterative graph algorithms. Beside high-throughput pipeline, work-efficient mode significantly reduces total graph processing run-time with a novel active-list design. In the third work, Joint SRAM (JSRAM) cell, a novel circuit-level technique to exploit the trade-off between reliability and memory size, is introduced. This idea is applicable to any SRAM structure like cache memory, register file, FPGA block RAM, or FPGA look-up table (LUT), and even latches and Flip-Flops. In fault-prone conditions, the structure can be configured in such a way that four cells are combined together at the circuit level to form one large and robust memory bit. Unlike prevalent hardware redundancy techniques, like Triple Modular Redundancy (TMR), there is no explicit majority voter at the output. The proposed solution mainly focuses on transient faults, where the reliable mode can provide auto-correction and full immunity against single faults."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"PageRank a ̆g analizinde kullanılan, her noktanın ̧cizge i ̧cerisindeki ̈oneminihesaplayan bir algoritmadır. ̈oz ̈unde Seyrek-Matris ve Vekt ̈or ̧carpımını e ̧sitolan bir problem olup aynı performans sıkıntılarından muzdariptir: d ̈uzensizhafıza eri ̧simi ve hesaplama-ileti ̧sim oranının d ̈u ̧s ̈uk olması. Dahası, PageRanki ̧cin hali hazırda mevcut bulunan Alanda Programlanabilir Kapı Dizisi (FPGA)hızlandırıcıları ya ̧cizgenin tamamının hızlandırıcının hafızasında bulunmasınıgerektiriyor ya da var olan kaynakları yeteri kadar etkili kullanamıyor. Yakınzamanda yayınlanmıs olan, Yayılma Tıkama (PB) metodu PageRank'i gruplamave toplama adı verilen iki adıma ayırarak performansını arttırmaktadır. Bizde bu makelede PB algoritmasının ilk a ̧saması FPGA ̈uzerinde ikini a ̧samasıise Merkezi ̇I ̧slem Birimi (CPU) ̈uzerinde ayrı ̧sık bi ̧cimde y ̈uksek hacimli olarak ̧calı ̧saca ̆gı bir tasarım sunduk. Daha ̈onceden var olan y ̈ontemlerin aksine bizimtasarımımız herhangi boyuttaki ̧cizgeleri, sadece hızlandırıcı ̈uzerindeki hafızayıkullanarak i ̧sleyebilmektedir. Deneylerimiz sonucunda elde etti ̆gimiz sonu ̧clar bizehızlandırıcımızın %40'a kadar hız artı ̧sı sa ̆gladı ̆gını g ̈ostermektedir.","PageRank is a network analysis algorithm that is used to measure the importance of each vertex in a graph. Fundamentally it is a Sparse Matrix-Vector multiplication problem and suffers from the same bottlenecks, such as irregular memory access and low computation-to-communication ratio. Moreover, the existing Field Programmable Gate Array (FPGA) accelerators for PageRank algorithm either require large portions of the graph to be in-memory, which is not suitable for big data applications or cannot fully utilize the memory bandwidth. Recently published Propagation Blocking(PB) methodology improves the performance of PageRank by dividing the execution into binning and accumulation phases. In this paper, we propose a heterogeneous high-throughput implementation of the PB algorithm where the binning phase executed on the FPGA while accumulation is done on a CPU. Unlike prior solutions, our design can handle graphs of any sizes with no need for an on-board FPGA memory. We also show that despite the low frequency of our device, compared to the CPU, by offloading random writes to an accelerator we can still improve the performance significantly. Experimental results show that with our proposed accelerator, PB algorithm can gain up to 40\% speedup."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İndirgeme işlemleri içeren uygulamalarda gönderme yükünü dengeleme incelenmektedir. Bu tür uygulamalarda, verilen bir hesaplama işi-işlemci ataması; işlemcilerin, muhtemelen diğer işlemciler tarafından indirgenmek üzere ürettiği kısmi sonuçlar oluşturmaktadır. Bu durum, işlemciler arası iletişime neden olmaktadır. İndirgeme iletişim işi atama problemi; indirgeme iletişim işlerinin, gönderme yükü en fazla olan işlemcinin yükünü en aza indirgeyecek şekilde işlemcilere atanması olarak tanımlanmaktadır. Bu indirgeme iletişim işi atama problemini çözmek için bir adet bağımsız iş atama problemi bazlı ve dört adet kutu istifleme problemi bazlı yeni algoritma sunulmaktadır. Sunulan algoritmaların başarımı, seyrek matris-seyrek matris çarpımı (SpGEMM) ve seyrek matris-matris çarpımı (SpMM) çekirdek işlemleri için doğrulanmıştır. Deneysel sonuçlar; azami iletişim yükü metriğinde, SpGEMM'de ortalama %23'e varan bir iyileştirme, SpMM'de ise ortalama %12'ye varan bir iyileştirme olduğunu göstermektedir.","We investigate balancing send volume in applications that involve reduce operations. In such applications, a given computational-task-to-processor mapping produces partial results generated by processors to be reduced possibly by other processors, thus incurring inter-processor communication. We define the reduce communication task assignment problem as assigning the reduce communication tasks to processors in a way that minimizes the send volume load of the maximally loaded processor. We propose one novel independent-task-assignment-based algorithm and four novel bin-packing-based algorithms to solve the reduce communication task assignment problem. We validate our proposed algorithms on two kernel operations: sparse matrix-sparse matrix multiplication (SpGEMM) and sparse matrix-matrix multiplication (SpMM). Experimental results show improvements of up to 23% on average for the maximum communication volume cost metric in SpGEMM and up to 12% improvement on average in SpMM."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Radarın ilettikleri darbelere göre tespiti ve sınıflandırılması elektronik harp sistemlerinde önemli bir uygulamadır. Mevcut çalışmaların çoğu, herhangi bir darbe tespiti yöntemi sunmadanö önceden sinyal darbesi tespitinin yapıldığını varsayarak modülasyonların sınıflandırılmasına odaklanır. Bu çalışmada, radar sinyallerinin otomatik darbe tespiti ve istemli darbe içi kipleme sınıflandırma için iki yeni derin öğrenme tabanlı teknik öneriyoruz. İlk yaklaşımda, ölçülen radar sinyalinin yeniden tayin edilmiş spektrogramı ve özel bir fonksiyon tarafından filtrelenen anlık fazlarının saptanan aykırı değerleri, çoklu evrişimli sinir ağlarını eğitmek için kullanılır. Ağlardan otomatik olarak çıkarılan özellikler, frekans ve faz modülasyonlu sinyalleri ayırt etmek için birleştirilir. İkincisinde, uçtan uca darbe tespiti ve modülasyon sınıflandırması için UKSB tabanlı çok görevli bir öğrenme modeli önerilmiştir. Bu alandaki başka önemli sorun, denetimli sinir ağı tabanlı modellerin eğitimi ve değerlendirilmesinde kullanılacak kamuya açık radar darbe verilerinin eksikliğidir. Bu sorunun üstesinden gelmek için gerçekçi darbeler ve gürültüler içeren 15'ten fazla ana faz ve frekans modülasyonu oluşturabilen bir IMOP ölçüm simülatörü geliştirdik. Simülasyon sonuçları, önerilen FFCNN ve MODNET tekniklerinin mevcut son teknoloji alternatiflerden daha iyi performans gösterdiğini ve çok çeşitli modülasyon türleri arasında kolayca ölçeklenebilir olduğunu göstermektedir.","Detection and classification of radar systems based on modulation analysis on pulses they transmit is an important application in electronic warfare systems. Many of the present works focus on classifying modulations assuming signal detection is done beforehand without providing any detection method. In this work, we propose two novel deep-learning based techniques for automatic pulse detection and intra-pulse modulation recognition of radar signals. As the first nechnique, an LSTM based multi-task learning model is proposed for end-to-end pulse detection and modulation classification. As the second technique, reassigned spectrogram of measured radar signal and detected outliers of its instantaneous phases filtered by a special function are used for training multiple convolutional neural networks. Automatically extracted features from the networks are fused to distinguish frequency and phase modulated signals. Another major issue on this area is the training and evaluation of supervised neural network based models. To overcome this issue we have developed an Intentional Modulation on Pulse (IMOP) measurement simulator which can generate over 15 main phase and frequency modulations with realistic pulses and noises. Simulation results show that the proposed FFCNN and MODNET techniques outperform the current state-of-the-art alternatives and is easily scalable among broad range of modulation types."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"1H Yüksek Çözünürlüklü Sihirli Açı Döndürmeli (HRMAS) Nükleer Manyetik Rezonans (NMR) katı numunelerde metabolitlerin bulunmasını sağlayan güvenilir bir teknolojidir. Hızlı geri dönüş süresi ile tümör ameliyatları sırasında, tümörün çıkarıldığı bölgede artakalmış olabilecek kanserli hücrelerin belirlenmesi ve cerrahın gerçek zamanlı olarak yönlendirilmesinde kullanılabilir. Fakat, tek boyutlu NMR verilerinde sinyallerin üst üste binme olasılığı olduğundan, bu durum metabolitlerin birbirinden ayrılmasını imkansızlaştırabilir. Bu durumda, Heteronükleer Tek Kuantum Uyumluluk Spektroskopisi (HSQC) NMR teknolojisi verilerin iki boyutlu halini oluşturmak için kullanılır (Hidrojen ve Karbon boyutları). Bu yüzden, bu iki boyutlu spektrumu hızlı bir şekilde elde etmenin tıbbi açıdan büyük bir önemi vardır. Fakat, ne yazık ki, bu spektrumu oluşturmak uzun süreler almaktadır ve bu durum gerçek zamanlı analizi imkansız kılmaktadır. Bu çalışmada, çoklu varyasyonlu bağlanım (NSPLR) ve istatistiksel tüm korelasyon spektroskopi (STOCSY) yöntemleri ile, HSQC'nin hidrojen ve karbon boyutları arasındaki bağlantıyı öğrenebileceğimizi gösterdik. Bu öğrenmenin küçük veri miktarına rağmen mümkün olduğunu ve HSQC metodunu kullanmak zorunda kalmadan sadece hidrojen boyutu ile karbon boyutunun tahmin edilebileceğini bulduk. Elimizde bulunan fare kohortumuz ile (80 adet, 5 farklı organ) NSPLR'yi kullanarak 0.971 ve STOCSY ile 0.957'lik ortalama R2 değerleri elde ettik. 15 insan numunesinde yaptığımız deneyler sonucunda 104 gruptan 39 farklı metabolitin %97 gibi yüksek bir doğruluk oranıyla bulunabileceğini gösterdik. Son olarak, Kreatin gibi hastanın ilaç ve tedaviye cevap vermesini ileriki safhalarda engelleyebilecek bir metaboliti, sinyali gizlenmiş olmasına rağmen, sadece hidrojen NMR teknolojisi ile bulabileceğimizi gösterdik. Pratikte bu bilgi cerrahın tümor bölgesinden daha fazla çıkarım yapmasını ve hastalığın ileride tekrarlanmasının önüne geçmesini sağlayabilir.","1H High-Resolution Magic Angle Spinning (HRMAS) Nuclear Magnetic Resonance (NMR) is a reliable technology used for detecting metabolites in solid tissues. Fast response time enables guiding surgeons in real time, for detecting tumor cells that are left over in the excision cavity. However, severe overlap of spectral resonances in 1D signal often render distinguishing metabolites im-possible. In that case, Heteronuclear Single Quantum Coherence Spectroscopy(HSQC) NMR is applied which can distinguish metabolites by generating 2D spectra (1H-13C). Unfortunately, this analysis requires much longer time and prohibits real time analysis. Thus, obtaining 2D spectrum fast has major implications in medicine. In this study, we show that using multiple multivariate regression and statistical total correlation spectroscopy, we can learn the relation between the1H and13C dimensions. Learning is possible with small sample sizes and without the need for performing the HSQC analysis, we can predict the 13C dimension by just performing1H HRMAS NMR experiment. We show on a rat model of central nervous system tissues (80 samples, 5 tissues) that our methods achieve 0.971 and 0.957 meanR2values, respectively. Our tests on 15 human brain tumor samples show that we can predict 104 groups of 39 metabolites with97% accuracy. Finally, we show that we can predict the presence of a drug resistant tumor biomarker (creatine) despite obstructed signal in1H dimension. In practice, this information can provide valuable feedback to the surgeon to further resect the cavity to avoid potential recurrence."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri merkezi yönetiminde en önemli konulardan biri kaynak ayrımıdır. Kaynak ayrımı konusu, sunucu kaynakları ve ağ bağlantıları olmak üzere iki ana başlıkta incelenmektedir. Sunucu kaynaklarının ayrımı problemi sanal makinelerin fiziksel makinelere atanması olarak ele alınmaktadır. Sunucu kaynaklarını (işlemci, hafıza, saklama, girdi/çıktı başarımı, vb.) çok boyutlu bir vektör uzayı olarak düşünerek, bir sanal makine ile bir fiziksel makinenin ne kadar uyumlu olduğunu belirten tasarım ölçütleri tanımlanmaktadır. Bu tasarım ölçütlerine uyan iki yeni ölçü önerilmektedir. Bu ölçüleri kullanarak farklı sanal makine yerleştirme seçeneklerini değerlendiren algoritmalar da sunulmaktadır. Bu algoritmalar bir sanal makine isteği kümesini bir fiziksel makine kümesine yerleştirmek için kullanılmaktadır. Önerdiğimiz ölçülerin başarımlarını yazındaki diğer ölçülerle karşılaştırmak için türdeş olmayan kutulara kutulama problemi için geliştirilmiş bir kıyaslama düzeni kullanılmaktadır. Kıyaslama sonuçları önerdiğimiz ölçülerin mevcut ölçülerden daha çok sayıda kutulama problemini çözdüğünü göstermektedir. Ağ kaynaklarının ayrımı problemi melez veri merkezleri kapsamında ele alınmaktadır. Önerdiğimiz dizge kalıbına göre veri merkezinde her raf üstü ağ anahtarı, 60 GHz bandında çalışan, 3 kanallı 802.11ad iletişim kuralını işleten iki adet radyoya sahip olmaktadır. Sunucular arası trafik akış bilgisi verildiğinde, raf üstü ağ anahtarları arasındaki kablosuz bağlar belirlenerek kablosuz ağ trafiği en yüksek değere ulaştırılmaktadır. Gerçek bir veri merkezi trafik akışı bilgisinden yola çıkarak sunucular arası trafik akışını rastgele oluşturan bir yöntem de önerilmektedir. Başarım, rastgele oluşturulan birçok trafik akışı için ölçülmekte ve sonuçlara göre, önerilen yöntemler kablolu ağdan kablosuz ağa büyük miktarda trafik aktarırken aynı zamanda düşük iletişim gecikmesi, yüksek iletim hacmi ve yüksek bant genişliği kullanım oranı elde etmektedir.","Resource allocation is one of the most important challenges in operating a data center. We investigate allocation of two main types of resources: servers and network links. Server resource allocation problem is the problem of how to allocate virtual machines (VMs) to physical machines (PMs). By modeling server resources (CPU, memory, storage, IO, etc.) as a multidimensional vector space, we present design criteria for metrics that measure the fitness of an allocation of VMs into PMs. We propose two novel metrics that conform to these design criteria. We also propose VM allocation methods that use these metrics to compare allocation alternatives when allocating a set of VMs into a set of PMs. We compare performances of our proposed metrics to the ones from the literature using vector bin packing with heterogeneous bins (VBPHB) benchmark. Results show that our methods find feasible solutions to a greater number of allocation problems than the others. Network resource allocation problem is examined in hybrid wireless data centers. We propose a system model in which each top-of-the-rack (ToR) switch is equipped with two radios operating in 60-GHz band using 3-channel 802.11ad. Given traffic flows between servers, we allocate wireless links between ToR switches so that the traffic carried over the wireless network is maximized. We also present a method to randomly generate traffic based on a real data center traffic pattern. We evaluate the performance of our proposed traffic allocation methods using randomly generated traffic. Results show that our methods can offload significant amount of traffic from wired to wireless network, while achieving low latency, high throughput, and high bandwidth utilization."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kısa okuma genom verilerinin işlenme süresini en aza indirmek için optimize edilmiş okuntu hizalama sistemleri gerekmektedir. Günümüzde birçok dizilim hizalama aracı mevcut, fakat bunlardan sadece birkaçı akıntı halindeki baz-çağrışımlarını doğrudan işleyebilme yeteneğine sahiptir. Anaakım hizalayıcıların okuntuları referansa hizalamaya başlayabilmesinden önce okuma işleminin bütünüyle tamamlanması gerekir. Okuma işleminin tamamlanması günler sürebilir. Çıktılar daha sonra, coğullama cözme işlemiyle, tekil okumalara dönüştürülür, bu işlem fazladan bir kaç saat daha sürebilir. Uçtan uca genom analiz süresi, yeni okumalar henüz üretilmekte iken hizalandırların aşamalı olarak hesaplanması halinde, önemli miktarda kısaltılabilir. Özellikle hayati durumlarda genom analizinin mümkün olduğunca çabuk yapılması önem taşımaktadır. Bu tez, kısa okumaların genom çizge yapılarına hizalandırılması için dağıtık akıntı işleme sistemi sunar. Genom okuma verilerinin yüksek miktarda paralel veri sunan doğasına karşılık yüksek miktarda paralel veri işleyebilen bilgisayım mimarisi gerekir. Bu nedenle {\algname{}} adlı sistemimizi bir çok okumayı aynı anda de Bruijn çizgesine hizalayabilecek şekilde tasarladık. Yöntemimiz Illumina gibi baz-çağrışım tabanlı okuma teknolojileri için özelleşmiştir. Sonuçlar okuma aygıtından son bazlar üretildikten kısa bir süre sonra çıkarılır.","Optimized the sequence alignment pipelines are needed to minimize the time required to complete processing the short-read genomic data. Today there are many sequence alignment tools exist, yet few of them are capable of directly ingesting the streaming base-call data. The sequencing has to be entirely completed before the mainstream aligners can begin mapping the reads to the reference. The sequencing process can take days to complete. The output is then needs to be demultiplexed into individual reads and aligned to the reference, which can take several more hours. Overall time of a genomic analysis can be shortened significantly by progressively computing the alignments at the time when the reads are still being generated. It is important to have genomic analysis done as quickly as possible, especially in life critical situations. Here we introduce a distributed stream processing framework for aligning short-reads into a graph representation of the genome. The massively parallel nature of the genomic sequencing data requires a massively parallel computation architecture. Thus we have designed our pipeline called {\algname{}} to align many reads to a de Bruijn graph in parallel. Our aligning method is specialized for the sequencing technologies that are based on base-call cycles, such as produced by Illumina. The results are made available soon after the final bases from the sequencing devices has been emitted."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Işın izleme yönteminin hesaplama maliyeti ışın-yüzey kesişim testlerinin sayısı ile doğru orantılıdır. Işın izleme için ışın-yüzey kesişim testlerinin hesaplama zamanı naif algoritma için $\mathcal{O}(N)$ olup $N$ sahnedeki nesne sayısıdır. Düzenli ızgara, sınırlayıcı hacim hiyerarşisi (BVH), kd ağacı, kısıtlı dört yüzlüleştirme gibi ışın izleme hızlandırma veri yapıları, gerçek zamanlı ışın izleme elde etmek için ışın-nesne kesişim testlerinin sayısını azaltmak için geliştirilmiştir. Işın izlemeyi hızlandırmak için kullanılabilecek bir hibrit hızlandırma yapısı, Sınırlayıcı Hacim Hiyerarşisi-Dörtyüzleme hibrit (BTH) hızlandırma yapısı öneriyoruz. BTH yapısı, BVH hiyerarşisinin bazı yapraklarının dörtyüzlü örgüler içerdiği bir BVH hiyerarşisinden oluşur. BTH yapısının oluşturulması için bir algoritma öneriyoruz. BTH yapısının oluşturulması için kullandığımız, dörtyüzlüleştirmenin ortalama en yakın isabet maliyetini tahmin etmek için yöntemler sunuyoruz. Ayrıca, önerilen BTH yapısını hiyerarşik hareketli dinamik sahneler için uyarlıyoruz. Dinamik sahneleri oluşturmak için iki seviyeli bir BVH-BTH hızlandırma yapısı sunuyoruz. Önerilen BTH yapısını çeşitli sahneler kullanarak test ediyoruz. Bazı durumlarda BTH yapısı, işleme süreleri açısından diğer hızlandırma yapılarına göre daha iyi performans göstermektedir. Hareketli sahneler için deneyler yapıyoruz. İki seviyeli BTH yapısının iki seviyeli BVH yapısından daha iyi performans göstermektedir.","The computational cost of the ray-tracing method is directly proportional to the number of ray-surface intersection tests. The naive ray-tracing algorithm requires $\mathcal{O}(N)$ computational cost for the ray-surface intersection calculations where $N$ is the number of primitives in the scene. Ray tracing acceleration data structures like the regular grid, bounding volume hierarchy (BVH), kd-tree, constrained tetrahedralization, has been developed to reduce the number of ray-object intersection tests to speed-up ray tracing. We propose a hybrid acceleration structure, the Bounding Volume Hierarchy-Tetrahedral mesh hybrid (BTH) acceleration structure, that can be used to speed-up ray tracing. BTH structure is composed of a BVH hierarchy where some of the leaves of the BVH hierarchy contain tetrahedralizations. We propose an algorithm for the construction of the BTH structure. We describe methods for approximating the average nearest-hit cost of a tetrahedralization, which we use for the construction of BTH. Besides, we can adapt the proposed BTH structure for dynamic scenes with hierarchical motion. We describe a two-level BVH-BTH acceleration structure for rendering animated scenes. We test the proposed BTH structure using various scenes. For some of the experiments, the BTH structure performs better against other acceleration structures in terms of rendering times. We perform experiments for animated scenes. We show that the two-level BTH structure outperforms the two-level BVH structure for the tested dynamic scenes."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Zayıf denetimli öğrenme (ZDÖ), çeşitli öğrenme problemlerini çözmek için gürültülü etiketler içeren verilerden faydalanmayı amaçlamaktadır. Biz ZDÖ yaklaşımlarını uzaktan algılama ve tıbbi görüntü analizi olmak üzere iki farklı alanda incelemekteyiz. Uzaktan algılama alanında, bir nesneyi birçok benzer alt kategoriden birine sınıflandırmayı amaçlayan çok kaynaklı ince taneli nesne tanıma problemine odaklanmaktayız. Bu problemde, verilen sınıf etiketine sahip bir nesne görüntüde bulunmakta, fakat nesnenin görüntü içindeki kesin konumu bilinmemektedir. Bu soruna ZDÖ perspektifinden yaklaşmakta ve iki paralel dal ile nesnelerin aynı anda lokalizasyonunu ve sınıflandırılmasını yapabilen tek kaynaklı bir derin öge dikkat modeli önermekteyiz. Daha sonra bu modeli, konum belirsizliği içermediği varsayılan bir referans kaynağın birden fazla kaynağın birleştirilmesine yardımcı olmak amacıyla kullanıldığı çok kaynaklı bir senaryoya uygun eklemelerle genişletmekteyiz. Önerilen tüm birleştirme stratejilerinin en gelişkin yöntemler ile karşılaştırıldığında daha yüksek doğruluk sağladığını ortaya koymaktayız. Ayrıca, her bir modeli çeşitli parametre karmaşıklık ayarlarında değerlendirerek derinlemesine bir karşılaştırma da yapmakta ve artan model kapasitesinin varsayılan kapasite ayarının daha üzerinde bir performans sergilediğini göstermekteyiz. Tıbbi görüntü analizi alanında, meme biyopsi tüm slaytlarından gelen değişken şekil ve boyutlardaki ilgi bölgeleri (İB) üzerinde meme kanseri sınıflandırması problemini ele almaktayız. Bu problemle ilgili yaklaşımlar tipik olarak İB'lerden örneklenen sabit boyutlu pencerelerin sınıflandırma sonuçlarının görüntü düzeyinde sınıflandırma puanları elde etmek için birleştirilmesi şeklinde olmaktadır. İlk olarak, İB'nin tamamının sınıflandırılması doğrultusunda, pencerelerden gelen bilginin komşu pencereler üzerinde kademeli bir şekilde iletilmesini amaçlayan bir çizge evrişimsel ağ (ÇEA) aracılığıyla yerel pencere arası bağlamdan faydalanmayı hedefleyen genel bir yöntem önermekteyiz. Zorlu bir veri kümesinde gerçekleştirdiğimiz 3 sınıflı bir İB sınıflandırma problemi üzerine yaptığımız deneylerde birkaç temel yaklaşımla yaptığımız kıyaslamalar sonucunda, önerilen modelin yaygın olarak kullanılan birleştirme kurallarından daha iyi performans sergilediğini göstermekteyiz. İkinci olarak, uzaktan algılama deneylerimizde kullandığımız ZDÖ sistemini yeniden gözden geçirip 4 sınıflı bir İB sınıflandırma problemine uygulamaktayız. Bu ZDÖ problemi için özel olarak tasarlanmış, İB çiftlerinden gelen pencere ve etiketleri birleştirerek öge dikkat modelinin birden çok etikete sahip örneklerden öğrenme yeteneğinden yararlanmayı amaçlayan yeni bir eğitim yöntemi önermekte ve bu önerilen yöntemin birkaç temel yaklaşıma kıyasla üstün performans sergilediğini gözlemlemekteyiz.","Weakly supervised learning (WSL) aims to utilize data with imprecise or noisy annotations to solve various learning problems. We study WSL approaches in two different domains: remote sensing and medical image analysis. For remote sensing, we focus on the multisource fine-grained object recognition problem that aims to classify an object into one of many similar subcategories. The task we work on involves images where an object with a given class label is present in the image without any knowledge of its exact location. We approach this problem from a WSL perspective and propose a method using a single-source deep instance attention model with parallel branches for joint localization and classification of objects. We then extend this model into a multisource setting where a reference source assumed to have no location uncertainty is used to aid the fusion of multiple sources. We show that all four proposed fusion strategies that operate at the probability level, logit level, feature level, and pixel level provide higher accuracies compared to the state-of-the-art. We also provide an in-depth comparison by evaluating each model at various parameter complexity settings, where the increased model capacity results in a further improvement over the default capacity setting. For medical image analysis, we study breast cancer classification on regions of interest (ROI) of arbitrary shapes and sizes from breast biopsy whole slides. The typical solution to this problem is to aggregate the classification results of fixed-sized patches cropped from ROIs to obtain image-level classification scores. We first propose a generic methodology to incorporate local inter-patch context through a graph convolution network (GCN) that aims to propagate information over neighboring patches in a progressive manner towards classifying the whole ROI. The experiments using a challenging data set for a 3-class ROI-level classification task and comparisons with several baseline approaches show that the proposed model that incorporates the spatial context performs better than commonly used fusion rules. Secondly, we revisit the WSL framework we use in our remote sensing experiments and apply it to a 4-class ROI classification problem. We propose a new training methodology tailored for this WSL task that combines the patches and labels from pairs of ROIs together to exploit the instance attention model's capability to learn from samples with multiple labels, which results in superior performance over several baselines."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri akışı madenciliği, bugün mevcut veri miktarının artması nedeniyle son yıllarda önemli bir araştırma alanı haline gelmiştir. Veri akışları genellikle verilerin karakteristik özellikleri zaman içinde değiştiği için durağan değildir. Bu olguya kavram sürüklenmesi denir. Sınıflandırıcıları geçersiz hale getirdiği ve tahmin başarısını düşürdüğü için literatürde büyük öneme sahip bir konudur. Kavram sürüklenmesinin olduğu durumlarda, daha sağlam ve etkili sınıflandırıcılara sahip olmak için verilerdeki değişimin uyarlanması gerekir. Kavram sürüklenmesini saptayan yöntemler, sınıflandırma modelleriyle birlikte çalışacak ve veri dağılımında önemli bir değişiklik gözlemlendiğinde bunları güncelleyecek şekilde tasarlanmıştır. Bu çalışmada, D3 ve OCDD adlarında iki güdümsüz kavram sürüklenmesi tespit yöntemi sunulmaktadır. D3'te, veri dağıtımındaki değişikliği izlemek için kayan bir pencere üzerinde ayrıştırıcı sınıflandırıcı kullanılmaktadır. Eski ve yeni veriler kullanılan sınıflandırıcı ile başarılı olarak ayrıştırılabilirse kavram sürüklenmesi tespit edilmektedir. OCDD'de, kayan bir pencere üzerinde tek-sınıflı sınıflandırıcı kullanılmaktadır. Kayan pencerede belirlenen aykırı değerlerin sayısını izlenmektedir. Aykırı değerlerin sayısındaki değişimin yeni bir kavramın işaretleri olduğunu iddia edilmekte ve kavram sürüklenmesi tespitini anomali tespitinin sürekli formu olarak tanımlanmaktadır. Aykırı değerlerin yüzdesi önceden belirlenmiş bir eşiğin üzerindeyse kavram sürüklenmesi tespit edilmektedir. Çalışmada literatürde yaygın olarak kullanılan 13 veri setini kullanarak kavram sürüklenmesi tespiti yöntemleri üzerinde kapsamlı bir değerlendirme yapılmıştır. Sonuçlar, OCDD'nin hem gerçek hem de sentetik veri kümelerinde önemli ölçüde daha iyi tahmin performansına sahip modeller üreterek diğer yöntemlerden daha iyi tahmin sağladığını göstermektedir. D3 ise diğer yöntemlerle benzer sonuçlar vermektedir.","Data stream mining has become an important research area over the past decade due to the increasing amount of data available today. Sources from various domains generate limitless volume of data in temporal order. Such data are referred to as data streams, and generally, they are nonstationary as the characteristics of the data evolve over time. This phenomenon is called concept drift, and it is an issue of great importance in the literature since it makes models outdated and decreases their predictive performance. In the presence of concept drift, adapting the change in data is necessary to have more robust and effective classifiers. Drift detectors are designed to run jointly with the classification models, updating them when a significant change in the data distribution is observed. In this study, we propose two unsupervised concept drift detection methods: D3 and OCDD. In D3, we use a discriminative classifier over a sliding window to monitor the change in the distribution of data. When the old and the new data are separable with the discriminative classifier, a drift is signaled. In OCDD, we use a one-class classifier over a sliding window. We monitor the number of outliers identified in the sliding window. We claim that the number of outliers are the signs of a new concept, and define concept drift detection as the continuous form of anomaly detection. A drift is signaled if the percentage of the outliers are over a pre-determined threshold. We perform a comprehensive evaluation on the latest and the most prevalent concept drift detectors using 13 datasets. The results show that OCDD outperforms the other methods by producing models with significantly better predictive performances on both real-world and synthetic datasets. D3 is on par with the other methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom dizilemesindeki hızlı ilerleme ve dizilim maliyetlerindeki azalma, genomik verilerin y¨uksek kullanılabilirli˘gine yol a¸cmı¸stır. Bu verilerin incelenmesi, hastalık ¸ca˘grı¸sımları ve evrimimiz hakkındaki kilit soruların cevaplanmasına b¨uy¨uk ¨ol¸c¨ude yardımcı olabilir. Bununla birlikte, katılımcıların hassas bilgileri hakkındaki artan gizlilik endi¸seleri nedeniyle, ¨onemli sonu¸clara ve genomik ¸calı¸smaların verilerine (genom ¸capında ili¸ski ¸calı¸smaları - GWAS gibi) eri¸smek yalnızca g¨uvenilir ki¸silerle sınırlıdır. Ote yandan, biyomedikal atılımlara ve ke¸siflere ¨ giden yolu a¸cmak, genomik veri k¨umelerine a¸cık eri¸sim verilmesini gerektirir. Gizlilik koruma mekanizmaları, ki¸silerin genetik verilerinin gizlili˘gini korurken bu t¨ur verilere daha geni¸s eri¸sim sa˘glamak i¸cin bir ¸c¨oz¨um olabilir. Ozellikle, genomik veriler hakkındaki ¨ozet istatistikleri payla¸sırken, diferansiyel gizlilik (DP) kavramının uygulanmasına y¨onelik ilgi artmaktadır. DP, veri seti hakkında istatistiksel bilgiler payla¸sırken veri bankasına ¨uyelik ¸cıkarım riskini ¨onlemek i¸cin matematiksel bir yakla¸sım sa˘glar. Ote yandan, DP, genomik veri k¨umeleri ¨ i¸cin ortak bir durum olan veri seti elemanları arasındaki korelasyonu (aile ¨uyelerinin genomları arasındaki do˘gal korelasyonlar) dikkate almadı˘gı i¸cin bilinen bir dezavantaja sahiptir. Bu DP'nin sundu˘gu gizlilik garantilerini bozabilir. Statik ve dinamik genomik veri k¨umelerine odaklanan bu tezde DP'nin bu dezavantajını g¨osteriyor ve onu hafifletmek i¸cin teknikler ¨oneriyoruz. ˙Ilk olarak, ger¸cek hayattaki bir genomik veri k¨umesi kullanarak, veri k¨umesindeki girdiler arasındaki korelasyonları kullanarak, farklı ¨ozel sorgu sonu¸clarına bir ¨ozellik ¸cıkarsama saldırısının fizibilitesini g¨osteriyoruz. Sayıda gizlilik kaybını, k¨u¸c¨uk alel frekansını (MAF) ve ki-kare sorgularını g¨osteriyoruz. Sonu¸clar, veri k¨umesinde birbirine ba˘gımlı elemanlar oldu˘gunda g¨uvenlik a¸cı˘gı ¨ol¸ce˘gini g¨osteriyor. Elde etti˘gimiz sonu¸clar, saldırganın, kullanıcı ¨uyelerinin genomları arasındaki korelasyonları kullanarak bir toplam sorgusunun farklı sonu¸clarından bir kullanıcı hakkındaki hassas genomik verileri ¸cıkarabildi˘gini g¨ostermektedir. Sonu¸clarımız ayrıca statik ve dinamik genomik veri k¨umelerinde farklı MAF sorgularının sonu¸clarını kullanarak ve elemanlar arasındaki ba˘gımlılı˘gı kullanarak, bir saldırganın bir hedefin genomu hakkında (orijinal gizlilik garantilerine kıyasla) %50'ye kadar daha hassas bilgi ortaya ¸cıkarabildi˘gini g¨ostermektedir. Ayrıca, saldırganın, bir hedefin ba¸ska bir genomik veri k¨umesine (¨orn., hassas bir ¨ozellik ile ili¸skili) ¨uyeli˘gini ¸cıkarmak i¸cin nitelik ¸cıkarım saldırısından elde edilen ¸cıkarımsal genomik verileri kullanabilece˘gini g¨osteriyoruz. Bir log olabilirlik oranı (LLR) testi kullanarak, sonu¸clarımız, saldırganın ¸cıkarım g¨uc¨un¨un, b¨oyle bir saldırıda, ¸cıkarımsal (ve dolayısıyla kısmen yanlı¸s) genomlar kullanarak bile ¨onemli ¨ol¸c¨ude y¨uksek olabilece˘gini g¨ostermektedir. Son olarak, elemanlar arasındaki ba˘gımlılı˘gı g¨oz ¨on¨unde bulundurarak gizlilik garantilerini elde etmek i¸cin genomik veri k¨umelerinden istatistiklerin gizlili˘gin korunması i¸cin bir mekanizma ¨oneriyoruz. Mekanizmamızı farklı genomik veri k¨umeleri ¨uzerinde de˘gerlendirerek, ¨onerilen mekanizmamızın geleneksel DP tabanlı ¸c¨oz¨umlerden %50'ye kadar daha iyi gizlilik sa˘glayabildi˘gini ampirik olarak g¨osteriyoruz. Anahtar s¨ozc¨ukler : Genomik veri k¨umesi, diferansiyel gizlilik, ¸cıkarsama saldırısı","The rapid progress in genome sequencing and the decrease in the sequencing costs have led to the high availability of genomic data. Studying these data can greatly help answer the key questions about disease associations and our evolution. However, due to growing privacy concerns about the sensitive information of participants, accessing key results and data of genomic studies (such as genomewide association studies - GWAS) is restricted to only trusted individuals. On the other hand, paving the way to biomedical breakthroughs and discoveries requires granting open access to genomic datasets. Privacy-preserving mechanisms can be a solution for granting wider access to such data while protecting their owners. In particular, there has been growing interest in applying the concept of differential privacy (DP) while sharing summary statistics about genomic data. DP provides a mathematically rigorous approach to prevent the risk of membership inference while sharing statistical information about a dataset. However, DP has a known drawback as it does not take into account the correlation between dataset tuples, which is a common situation for genomic datasets due to the inherent correlations between the genomes of family members. This may degrade the privacy guarantees offered by the DP. In this Thesis, focusing on static and dynamic genomic datasets, we show this drawback of the DP and we propose techniques to mitigate it. First, using a real-world genomic dataset, we demonstrate the feasibility of an attribute inference attack on differentially private query results by utilizing the correlations between the entries in the dataset. We show the privacy loss in count, minor allele frequency (MAF), and chi-square queries. The results explain the scale of vulnerability when we have dependent tuples in the dataset. Our results demonstrate that the adversary can infer sensitive genomic data about a user from the differentially private results of a sum query by exploiting the orrelations between the genomes of family members. Our results also show that using the results of differentially-private MAF queries on static and dynamic genomic datasets and utilizing the dependency between tuples, an adversary can reveal up to 50% more sensitive information about the genome of a target (compared to original privacy guarantees of standard DP-based mechanisms), while differentially-privacy chi-square queries can reveal up to 40% more sensitive information. Furthermore, we show that the adversary can use the inferred genomic data obtained from the attribute inference attack to infer the membership of a target in another genomic dataset (e.g., associated with a sensitive trait). Using a log-likelihood-ratio (LLR) test, our results also show that the inference power of the adversary can be significantly high in such an attack even by using inferred (and hence partially incorrect) genomes. Finally, we propose a mechanism for privacy-preserving sharing of statistics from genomic datasets to attain privacy guarantees while taking into consideration the dependence between tuples. By evaluating our mechanism on different genomic datasets, we empirically demonstrate that our proposed mechanism can achieve up to 50% better privacy than traditional DP-based solutions. Keywords: Genomic datasets, differential privacy, inference attacks"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Transpozaz-Erişimli Kromatin Dizinlemesi ile Tahlili (ATAC-seq) kromotinin açıklığını ölçmeye yarayan bir genetik yöntemidir. Genomdaki açık bölgeler, DNA ikileşmesi ve transkripsiyonda rol oynamaktadır. Bu tahlilin nükleozom haritalaması, transkripsiyon düzenleyici unsurların tespiti, kanser ve bağışıklık sistemi yaşlanması alanlarında kullanılmaktadır. Teknolojik gelişmeler sayesinde bu teknik, tek hücre seviyesinde kullanılabilir ve buna tek çekirdek ATAC-seq (snATAC-seq) adı verilmektedir. Tek hücre seviyesine inebilen ATAC-seq sayesinde nadir rastlanan hücre tipleri keşfedilebilir ve onların düzenleme ağlarındaki rolleri belirlenebilir. Diğer tek hücre seviyesindeki teknikler gibi snATAC-seq de ikili hücrelerden zarar görmektedir. İkililer, birden fazla hücrenin aynı anda yakalanması ve dizinlenmesi ile meydana gelir ve daha sonra yapılacak çözümlemeleri kötü etkiler. snATAC-seq verisinin bir özelliği, herhangi bir genom bölgesinde sadece annenin ve babanın kromozomlarından gelen iki keşisen okuma olabilir. Eğer bir bölgede ikiden fazla kesişen okuma görülürse, bu; ikili hücreler veya bir dizinleme hatası nedeniyle meydana gelebilir. Biz de bu biyolojik özelliği kullanarak ikili hücreleri tespit etmeye yarayacak DoubletDetector adındaki aracı geliştirdik. Bu araç her hücre için kaç tane genom bölgesinde ikiden fazla kesişen okuma olduğunu saymakta ve bu durumun sık karşılaşıldığı hücereleri işaretlemektedir. Daha sonra bu ikiliyi meydana getiren hücreleri hangi hücre tiplerinden olduğunu da tespit etmektedir. Bu aracı tek çekirdekli çevresel kan hücreleri (PBMC) ve adacık hücreleri ile denedik ve simüle edilen ikili hücreleri \%90 oranında doğru tespit edebildik. Bu yapay hücrelerin hangi hücre tiplerinden geldiklerini ise \%78 oranında doğruladık. DoubletDetector, başarılı şekilde hem çok-kaynaklı hem de tek-kaynaklı ikili hücreleri tespit edebilen ilk araçtır.","Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) is a simple and effective technique in genomic studies that shows the chromatin accessibility of the genome. The open regions of the genome play an important role in DNA replication and transcription. It has many practical applications such as nucleosome mapping, identifying regulatory elements, cancer research and immune system aging. With the development of the technology used, this technique is now applied at single cell level in the form of single nucleus ATAC-seq (snATAC-seq). Single cell level resolution helps further the possible implications of ATAC-seq by helping in detection of rare cell types that play roles in the regulatory networks. Like other single cell technologies, snATAC-seq suffers from the existence of doublet cells that occur when multiple cells are simultaneously captured and sequenced which confounds downstream analyses. A unique property of snATAC-seq data is that at a given loci in the genome there can be at most two overlapping reads, one from the maternal and other from the paternal chromosome. When a loci has more than 2 reads this can be due to doublets or alignment/sequencing errors. We propose a count-based method, DoubletDetector, that makes use of this property to detect doublets. It identifies doublets by counting the number of loci within the cell that has more than 2 ATAC-seq reads. It also finds the types of the cells that formed the doublets, to further help understand their nature. DoubletDetector achieved high recall near 90\% for detecting simulated doublets in human PBMC and islet snATAC-seq samples. Artificial doublets were then traced back to their cells of origin with near 78\% recall using a marker peak-based algorithm. DoubletDetector is the first method to effectively identify both homotypic and heterotypic doublets from snATAC-seq."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek ve düzensiz uygulamalar paralelleştirilirken iletişim yükünün azaltılması için birden fazla iletişim maliyeti ölçütünün (hacim ve gecikim) kapsanması önemlidir. İletişim hiperçizge modeli birden fazla iletişim maliyeti ölçütünü kapsayacak şekilde iki fazlı bir yöntem olarak önerilmiştir. Toplama-iletişim-hiperçizge modeli işlemciler arasındaki gönderim-hacmi dengesini doğru bir şekilde modelleyememektedir. Önerdiğimiz yeni köşe ağırlığı şeması ile bölüm ağırlıklarının doğru gönderim-hacmine karşılık gelmesi sayesinde işlemciler arasındaki gönderim hacmini dengelemekteyiz. Ayrıca bu hiperçizge modeli bölümleme sırasında toplam iletişim hacminde artışa neden olmaktadır. Bu artışı azaltma amacı ile özyinelemeli ikiye-bölümleme (ÖB) yaklaşımı kullanan ve her ikiye bölümleme sırasında köşeleri karşılıklı yer değiştiren bir yöntem önermekteyiz. Performans değerlendirmesi için toplama-görevi ataması ortaya çıkartan bilinen en yaygın uygulamalardan olan sütun-paralel SyMV işlemini kullanmaktayız. 313 matris üzerinde yapılan kapsamlı deneylerle gösterildiği üzere önerdiğimiz yöntemler, var olan modele göre tüm iletişim maliyeti ölçütlerinde kayda değer iyileşme elde etmiştir. Bu iyileşmeler, yüksek düzensizlikteki 70 SyMV örneği için 512 işlemcide ortalama %30 daha az paralel koşma elde edilmesini sağlamıştır. İletişim hiperçizge modelini daha da geliştirerek bir işlemci tarafından gönderilen en fazla mesaj sayısını da kapsamasını sağlamaktayız. Bu amaç için hiperçizgenin bölümlenmesinde kullanılmak üzere, ÖB yaklaşımı ile gerçeklediğimiz yeni bir toplam paha ölçütü önermekteyiz. Ayrıca toplam iletişim hacmindeki artışı azaltmak için, bu artışı doğrudan bölümleme hedefi ile modelleyen yeni bir tür hiperkenar önermekteyiz. 300 matris üzerinde 1024 işlemci için gerçekleştirdiğimiz deneylerde önerdiğimiz yöntemler iletişim maliyeti ölçütlerinde kayda değer iyileşme elde etmiş ve bu iyileşmeler daha iyi sütun-paralel SyMM koşma zamanı sağlamıştır. Seyrek tensör bölümlemesi amacıyla bölümleme üzerine herhangi bir topolojik kısıt gerektirmeyen, genel orta-taneli hiperçizge modeli önermekteyiz. Önerdiğimiz model verilen tensörün sıfırdışı-ayrık bileşen tensörlerine ayrılması tabanlıdır. Sonrasında her bir bileşen tensörü için boyut-bağımlı iri-taneli hiperçizge oluşturmaktayız. Önerdiğimiz hiperkenar bütünleştirme yöntemi ile bu boyut-bağımlı iri-taneli hiperçizgelerden bileşik orta-taneli hiperçizge oluşturmaktayız. Önerdiğimiz bileşik orta-taneli hiperçizge modeli toplam iletişim hacminin en aza indirilmesini doğru bir şekilde kapsamaktadır. Önerdiğimiz ayırma sezgiseli ile tensörün yoğun dilimlerindeki sıfırdışı elemanları ayırarak bileşen tensörlerinde seyrek dilimler elde etmekteyiz. Ayrıca ÖB yaklaşımı kullanarak ayırma sezgiselinin daha kaliteli sonuçlar elde etmesini sağlamaktayız. Önerdiğimiz orta-taneli üç-kümeli çizge modeli ile toplam iletişim hacmini arttırma pahasına daha hızlı bölümleme zamanı elde etmekteyiz. Gerçek problemlerden elde edilmiş 10 tensör üzerinde 1024 işlemciye kadar gerçekleştirdiğimiz paralel deneyler önerdiğimiz yöntemlerin geçerliliğini doğrulamaktadır.","Encapsulating multiple communication cost metrics, i.e., bandwidth and latency, is proven to be important in reducing communication overhead in the parallelization of sparse and irregular applications. Communication hypergraph model was proposed in a two-phase setting for encapsulating multiple communication cost metrics. The reduce-communication hypergraph model suffers from failing to correctly encapsulate send-volume balancing. We propose a novel vertex weighting scheme that enables part weights to correctly encode send-volume loads of processors for send-volume balancing. The model also suffers from increasing the total communication volume during partitioning. To decrease this increase, we propose a method that utilizes the recursive bipartitioning (RB) paradigm and refines each bipartition by vertex swaps. For performance evaluation, we consider column-parallel SpMV, which is one of the most widely known applications in which the reduce-task assignment problem arises. Extensive experiments on 313 matrices show that, compared to the existing model, the proposed models achieve considerable improvements in all communication cost metrics. These improvements lead to an average decrease of 30% in parallel SpMV time on 512 processors for 70 matrices with high irregularity. We further enhance the reduce-communication hypergraph model so that it also encapsulates the minimization of the maximum number of messages sent by a processor. For this purpose, we propose a novel cutsize metric which we realize using RB paradigm while partitioning the reduce-communication hypergraph. We also introduce a new type of net for the communication hypergraph which models decreasing the increase in the total communication volume directly with the partitioning objective. Experiments on 300 matrices show that the proposed models achieve considerable improvements in communication cost metrics which lead to better column-parallel SpMM time on 1024 processors. We propose a hypergraph model for general medium-grain sparse tensor partitioning which does not enforce any topological constraint on the partitioning. The proposed model is based on splitting the given tensor into nonzero-disjoint component tensors. Then a mode-dependent coarse-grain hypergraph is constructed for each component tensor. A net amalgamation operation is proposed to form a composite medium-grain hypergraph from these mode-dependent coarse-grain hypergraphs to correctly encapsulate the minimization of the communication volume. We propose a heuristic which splits the nonzeros of dense slices to obtain sparse slices in component tensors. We also utilize the well-known RB paradigm to improve the quality of the splitting heuristic. We propose a medium-grain tripartite graph model with the aim of a faster partitioning at the expense of increasing the total communication volume. Parallel experiments conducted on 10 real-world tensors on up to 1024 processors confirm the validity of the proposed hypergraph and graph models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kod gözden geçirme, yazılım geliştirme yaşam döngüsünün önemli aşamalarından birisidir. Bu sürecin temel amacı, kod değişikliğindeki olası hataları kod tabanına göndermeden önce saptamaktır. Kod gözden geçirme süreci konusunda resmi bir fikir birliği olmasa da, birçok şirket ve açık kaynak toplulukları bu süreçte izlenmesi gereken aşamalar ve örnek uygulamalar konusunda birleşmişlerdir. Bu uygulamaların temel amacı, daha hızlı ve etkili kod gözden geçirme süreçlerine sahip olmaktır. Yazılım geliştiricilerinin bu örnek uygulamalara uymayışı, kod gözden geçirme sürecinin avantajlarını ortadan kaldırdığı gibi, yazılım geliştirme yaşam döngüsünün diğer aşamalarını da olumsuz etkileyebilir. Bu çalışmanın amacı, yazılım geliştiricilerin kod gözden geçirme sürecindeki kötü alışkanlıklarını ampirik olarak incelemektir. Bu amaçla, akademik ve gri literatür taraması yaparak kod gözden geçirme sürecindeki kötü alışkanlıklar bir araya getirildi. Daha sonra, toplanılan veriler 32 tecrübeli yazılım geliştiricisine sorularak onaylatıldı ve bunun sonucunda, kod gözden geçirme sürecindeki kötü alışkanlıkların sınıflandırılması tamamlandı (kod gözden geçirme yoksunluğu, aynı kişilerin kod gözden geçirmesi, kod gözden geçirme döngüsünün uzaması, özensiz kod gözden geçirme, uzun süren kod gözden geçirme, içeriği belli olmayan kod gözden geçirme). Bu alışkanlıkları nicel olarak değerlendirmek için, sekiz açık kaynak projeden topladığımız 283,354 kod gözden geçirme süreci analiz edildi. Sonuç olarak, bu kötü alışkanlıkların ciddi miktarlarda açık kaynak projelerde var olduğu gözlenmiştir.","Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices is to lead a faster and more effective code review process. Non-conformance of developers to this process does not only hinder the advantages of the code review but can also negatively affect the other steps of the software development life cycle. The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells. To this end, we first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on the multivocal literature review and expert opinion of experienced developers, a taxonomy of code review smells (lack of code review, review buddies, reviewer-author ping pong, looks good to me (LGTM) reviews, sleeping reviews, missing context in reviews and large changesets) is introduced. To quantitatively demonstrate the existence of these smells, we analyze 283,354 code reviews collected from eight OSS projects. We observe that a considerable number of code review smells exist in all projects with varying degree of ratios."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek verimli dizilemede (HTS) son zamanlarda elde edilen gelişmeler çok sayıda DNA parçasının (okumanın) hızlı üretimini kolaylaştırmaktadır. Her geçen gün, bu bağlamda veri üretimi daha az masraflı hale gelmesine rağmen mevcut veriyi bir bütün olarak hizalama programında işlemek hala bilgisayımsal açıdan pahalıdır. Hizalamanın son evresinde, kısa okumaların referans genomu üzerindeki aday pozisyonları, ilgili referans kısmıyla aralarındaki farka bağlı olarak hata payını en aza indirecek şekilde doğrulanmaktadır. Bu bakımdan, okumalar ve referans genomu kısımları arasında karşılaştırma yapmak, geleneksel olarak devirgen programlama algoritmaları içeren yaklaşık karakter dizgisi eşleştirme tekniklerini gerektirmektedir. Herbir okuma ve referans kısmı için devirgen programlama uygulamak, hizalamayı, haritalama işleminin bilgisayımsal olarak pahalı aşaması haline getirmektedir. Bu yüzden, bu aşamayı hızlandırmanın bütün olarak hizalama performansını işletim zamanı açısından iyileştirmesi beklenmektedir. Bu tezde, doğrulama öncesi, devirgen programlama üzerindeki bilgisayımsal yükü azaltmak için belirli bir hata eşiğinin üzerinde olan dizi çiftlerini eleyen bir ön-hizalama filtresi olan GateKeeper-GPU'yu sunuyoruz. Filtreleme için GateKeeper algoritmasını seçiyoruz. GateKeeper'ı geliştiriyoruz ve onu, bilgisayımsal açıdan ağır işi, yüksek oranda paralel milyonlarca izlekle yapmaktan kaynaklanan fayda ile performansı artırmak için CUDA çatısı ile GPGPU (genel kullanım grafik işleme ünitesi) platformuna adapte ediyoruz. GateKeeper-GPU, mrFAST ile entegre edildiğinde doğrulama aşamasına 2.9 kata kadar ve hizalama işleminin bütününe 1.4 kata kadar hızlandırma sağlarken asıl GateKeeper'a göre 52 kata kadar daha az yanlış kabul edilen dizi çifti üretmektedir.","Recent advances in high throughput sequencing (HTS) facilitate fast production of short DNA fragments (reads) in numerous amounts. Although the production is becoming inexpensive everyday, processing the present data for sequence alignment as a whole procedure is still computationally expensive. As the last step of alignment, the candidate locations of short reads on the reference genome are verified in accordance with their difference from the corresponding reference segment with the least possible error. In this sense, comparison of reads and reference segments requires approximate string matching techniques which traditionally inherit dynamic programming algorithms. Performing dynamic programming for each of the read and reference segment pair makes alignment, a computationally-costly stage for mapping process. So, accelerating this stage is expected to improve alignment performance in terms execution time. Here, we propose, GateKeeper-GPU, a fast pre-alignment filter to be performed before verification to get rid of the sequence pairs, which exceed a predefined error threshold, for reducing the computational load on the dynamic programming. We choose GateKeeper as the filtration algorithm, we improve and implement it on a GPGPU platform with CUDA framework to obtain benefit from performing compute-intensive work with highly parallel and independent millions of threads for boosting performance. GateKeeper-GPU can accelerate verification stage by up to 2.9x and provide up to 1.4x speedup for overall read alignment procedure when integrated with mrFAST, while producing up to 52x less number of false accept pairs than original GateKeeper work."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Histopatolojik değerlendirme, kanser teşhisi ve derecelendirmesi için günümüzde kullanılan araçtır. Öte yandan, bu değerlendirme, cam slayt üzerindeki histopatolojik numunenin uzman bir patolog tarafından mikroskop altında ayrıntılı olarak incelenmesini ve yorumlanmasını gerektirdiğinden, zaman alıcı ve hatalara açık bir işlemdir. Son yıllarda üretilen düşük maliyetli ve yüksek teknolojili tam slayt dijital tarayıcılar, histopatolojik örnekleri dijital ortama aktararak, fiziksel cam slayt örneklerin dezavantajlarını ortadan kaldırmaktadır. Dijital patoloji, dijitalleştirilmiş histopatolojik görüntüleri nicel olarak analiz eden yardımcı bilgisayarlı araçlar sağlayarak geleneksel inceleme yaklaşımlarının sorunlarını azaltmayı amaçlamaktadır. Geleneksel makine öğrenmesi yöntemleri, histopatolojik görüntülerden manuel tanımlanmış öznitelikler çıkarmayı ve bu öznitelikleri bir sınıflandırma veya bölütleme algoritması tasarımında kullanmayı önermektedir. Bu yöntemlerin performansı esas olarak kullandıkları özniteliklere dayanmaktadır ve bu nedenle, bu yöntemlerin başarıları, kullandıkları özniteliklerin histopatoloji alanını başarılı bir şekilde temsil etme yeteneklerine bağlıdır. Son yıllarda önerilen çalışmalar, geleneksel yaklaşımların karmaşık öznitelik çıkarma prosedürlerinden kaçınarak, açıklayıcı ve gürbüz öznitelikleri doğrudan görüntülerden öğrenmek için derin mimariler kullanmaktadır. Derin öğrenme yöntemleri birçok sınıflandırma ve bölütleme probleminde iyi performans gösterse de, sıklıkla kullandıkları evrişimsel sinir ağları eğitim için etiketlenmiş verilere ihtiyaç duymaktadır ve bu da, histopatoloji alanındaki mevcut verilerin çoğunu kapsayan etiketlenmemiş verilerin kullanılmasını zor hale getirmektedir. Bu tez, geleneksel yöntemlerin ve derin öğrenme yaklaşımlarının sorunlarını, denetimsiz öğrenmenin öznitelik çıkarma ve eğitim düzenleme amaçları için sınıflandırma ve bölütleme algoritmalarına dahil edilmesiyle ele alınmaktadır. Tezin birinci katkısı olarak sunulan ilk çalışma, histopatolojik doku görüntülerinin etkili bir şekilde temsil edilmesi ve sınıflandırılması için yeni bir denetimsiz öznitelik çıkarıcı sunmaktadır. Bu çalışmada, alana özgü ön bilgilerle tanımlanan önemli alt bölgelerden öznitelikler çıkarmak amacıyla, denetimsiz bir derin inanç ağı eğitilmiş ve bu eğitim sonucunda elde edilen özniteliklerin dağılımı, görüntü gösterimi ve sınıflandırması için kullanılmıştır. Tezin ikinci katkısı olarak sunulan diğer çalışmada, histopatolojik doku görüntülerinde semantik doku bölütlemesi için, tam bağlantılı bir evrişimsel ağ eğitmek amacıyla yeni bir düzenleme yöntemi önerilmektedir. Bu çalışma, denetimsiz öğrenmeyi, önerilen ağ modelinin eğitimini düzenlemek için, girdi görüntülerinin yeniden yapılandırılması şeklinde kullanmaktadır. Bu amaçla, bölütleme haritası ile girdi görüntüsünün üst üste bindirilmesiyle oluşturulan yeni bir yerleştirme tanımlanmaktadır. Önerilen bu yerleştirme yöntemi sayesinde, semantik bölütlemeyi temsil eden ana denetimli görev ile görüntüyü yeniden yapılandırmanın temsil ettiği yardımcı denetimsiz görevin tek bir görevde birleştirilmesi ve oluşturulan bu birleşik görevin, bir üretken çekişmeli ağ ile öğrenilmesi amaçlanmaktadır. Önerilen sınıflandırma ve bölütleme yöntemleri, geleneksel makine öğrenmesi yöntemleri ve güncel derin öğrenme algoritmalarıyla, farklı histopatolojik görüntü veri kümeleri kullanılarak karşılaştırılmıştır. Deneyler sonucunda elde edilen görsel ve nicel sonuçlar, önerilen yöntemlerin histopatolojik görüntülerden gürbüz öznitelikler öğrenebildiğini ve karşılaştırılan yöntemlerden daha doğru sonuçlar ürettiğini ortaya koymaktadır.","Histopathological examination is today's gold standard for cancer diagnosis and grading. However, this task is time consuming and prone to errors as it requires detailed visual inspection and interpretation of a histopathological sample provided on a glass slide under a microscope by an expert pathologist. Low-cost and high-technology whole slide digital scanners produced in recent years have eliminated the disadvantages of physical glass slide samples by digitizing histopathological samples and relocating them to digital media. Digital pathology aims at alleviating the problems of traditional examination approaches by providing auxiliary computerized tools that quantitatively analyze digitized histopathological images. Traditional machine learning methods have proposed to extract handcrafted features from histopathological images and to use these features in the design of a classification or a segmentation algorithm. The performance of these methods mainly relies on the features that they use, and thus, their success strictly depends on the ability of these features to successfully quantify the histopathology domain. More recent studies have employed deep architectures to learn expressive and robust features directly from images avoiding complex feature extraction procedures of traditional approaches. Although deep learning methods perform well in many classification and segmentation problems, convolutional neural networks that they frequently make use of require annotated data for training and this makes it difficult to utilize unannotated data that cover the majority of the available data in the histopathology domain. This thesis addresses the challenges of traditional and deep learning approaches by incorporating unsupervised learning into classification and segmentation algorithms for feature extraction and training regularization purposes in the histopathology domain. As the first contribution of this thesis, the first study presents a new unsupervised feature extractor for effective representation and classification of histopathological tissue images. This study introduces a deep belief network to quantize the salient subregions, which are identified with domain-specific prior knowledge, by extracting a set of features directly learned on image data in an unsupervised way and uses the distribution of these quantizations for image representation and classification. As its second contribution, the second study proposes a new regularization method to train a fully convolutional network for semantic tissue segmentation in histopathological images. This study relies on the benefit of unsupervised learning, in the form of image reconstruction, for network training. To this end, it puts forward an idea of defining a new embedding, which is generated by superimposing an input image on its segmentation map, that allows uniting the main supervised task of semantic segmentation and an auxiliary unsupervised task of image reconstruction into a single one and proposes to learn this united task by a generative adversarial network. We compare our classification and segmentation methods with traditional machine learning methods and the state-of-the-art deep learning algorithms on various histopathological image datasets. Visual and quantitative results of our experiments demonstrate that the proposed methods are capable of learning robust features from histopathological images and provides more accurate results than their counterparts."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım yapıları geliştirme sürecinin yan ürünleridir. Geliştiriciler projenin hayat döngüsü boyunca kaynak dosyaları ve hata raporları gibi farklı yapılar üretirler. Biz yazılım geliştirme ve işbirliği araçlarındaki veriyi kullanarak, yapılar ve aralarındaki bağlantılar ile yapı izlenebilirlik çizgeleri oluşturduk. Geliştiriciler bir yazılım projesini geliştirme ve sürdürme sürecinde kullanılan asıl kaynaktırlar. Geliştiriciler projelerin bilgisine sahip oldukları için geliştirici devri yazılım projeleri için kritik bir risktir. Bazı geliştiriciler farklı bakış açılarından proje için değerli ve vazgeçilmez olabilir. Onlar projenin anahtar geliştiricileridir ve onları belirlemek yönetimsel kararlar için çok önemlidir. Anahtar geliştirici olsun veya olmasın, geliştiriciler projeden ayrıldığında işleri başka geliştiricilere aktarılmalıdır. Bütün geliştiriciler çalışmaya devam etse bile, geliştiriciler arasındaki bilgi dağılımı dengesiz olabilir. Bilgi dağılımını değerlendirmek gelecekteki problemler için erken bir uyarı olabileceğinden önemlidir. Biz anahtar geliştiricileri belirlemek, ayrılan geliştirici yerine geliştiriciler önermek ve takımdaki bilgi dağılımını değerlendirmek için yapı izlenebilirlik çizgeleri üzerinde algoritmalar kullandık. Hadoop, Hive, Pig, HBase, Derby ve Zookeeper isimli altı açık kaynak proje ile deneyler yaptık. Sonrasında, anahtar geliştiricileri tanımlamada %98'e varan, ayrılan geliştiriciler için geliştirici önermede %91'e varan doğrulukta sonuçlar elde ettik ve bilgi dağılımı için kullandığımız etiketler %94'e varan oranda temel yöntem ile uyumlu çıktı.","Software artifacts are the by-products of the development process. Throughout the life cycle of a project, developers produce different artifacts such as source files and bug reports. To analyze developer contributions, we construct artifact traceability graphs with these artifacts and their relations using the data from software development and collaboration tools. Developers are the main resource to build and maintain software projects. Since they keep the knowledge of the projects, developer turnover is a critical risk for software projects. From different viewpoints, some developers can be valuable and indispensable for the project. They are the key developers of the project, and identifying them is a crucial task for managerial decisions. Regardless of whether they are key developers or not, when developers leave the project, their work should be transferred to other developers. Even though all developers continue to work on the project, the knowledge distribution can be imbalanced among developers. Evaluating knowledge distribution is important since it might be an early warning for future problems. We employ algorithms on artifact traceability graphs to identify key developers, recommend replacements for leaving developers and evaluate knowledge distribution among developers. We conduct experiments on six open source projects: Hadoop, Hive, Pig, HBase, Derby and Zookeeper. Then, we demonstrate that the identified key developers match the top commenters up to 98%, recommended replacements are correct up to 91% and identified knowledge distribution labels are compatible with the baseline approach up to 94%."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Metin madenciliği, üretilen metin miktarının düzenli olarak artması ile birlikte geniş ve önemli bir araştırma konusu haline gelmiştir. Türkçede metin madenciliği araştırmaları ise, diğer diller ile yapılan çalışmalar ile karşılaştırıldığında, hala yeterli değildir. Bu tez çalışmasında, farklı türlerdeki Türkçe metinleri, farklı açılardan ele alınarak çeşitli metin madenciliği yöntemleri ile analiz edilmişlerdir. İlk çalışmada, Benim Adım Kırmızı romanının metni ve bu romanın İngilizce, Fransızca ve İspanyolca çevirileri kullanılarak çevirilerin aslına sadakatinin değerlendirilmesi yapılmıştır. Önerilen metod ile çevirilerin aslına bağlılığı, doğrudan cümle çevirilerine ihtiyaç duymadan değerlenlendirilebilir. Tezin ikinci kısımında, 98 kişinin konuşma metinleri, bu kişilerin cinsiyet ve yaş özellikleri yönünden analiz edilmiştir. Sonuçlar, kişilerin konuşma metinlerinden cinsiyet ve yaş tahmini yapılabileceğini doğrular niteliktedir. Daha sonra çok-dilli yapay ağların çapraz dil aktarımı performanslarını, metin sınıflandırma için ölçen bir çalışma yapılmıştır. Deney sonuçları, başka dillerden Türkçeye aktarım olabileceğini gösteren niteliktedir fakat h\^al\^a eğitim için hazırlanmış veri olmadan, doğrudan başka dillerden taşıma yeterince başarılı değildir. Son olarak Ahmet Hamdi Tanpınar'ın eserlerinin zamansal olarak üslup analizi yapılmıştır. Analiz sonuçları, Ahmet Hamdi Tanpınar'ın üslup değişimi açısından, çağdaşlarından farklılık gösterdiğini ortaya koymuştur.","Text mining is an important research area considering the increase in text generation and the need for analysis. Text mining in Turkish is still not a well-invested research area, compared to the other languages. In this thesis, we analyze different types of Turkish text from different points of views, having an overall review on text mining in Turkish at the end. First, we analyze the translation quality of a Turkish novel, My Names is Red novel, to English, French, and Spanish with the features generated for each chapter. With the proposed method, translation loyalties to the original text can be quantified without any parallel comparisons. Then, we analyze the Turkish spoken texts of 98 people in different age groups in terms of gender and age attributes of the speakers. We also analyze the difference between written and spoken texts in Turkish. Results show that it is possible to predict the attributes of the speaker from the spoken text and written and spoken texts are significantly different in terms of stylometric measures. Later on, we make an assessment on cross-lingual transferring performances of multilingual networks from English to Turkish. We see that transferring is possible; however zero-shot cross-lingual transferring still has its way to be competitive with monolingual networks for Turkish. Lastly, we conduct a time-based stylometric analysis of Ahmet Hamdi Tanpınar's works. We see that Ahmet Hamdi Tanpınar shows some differences compared to his contemporaries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otizm Spektrum Bozukluğu (OSB), genetik sebeplerle ortaya çıkabilen, zihinsel gelişimi olumsuz etkileyen karmaşık bir hastalıktır. Karmaşık doğasından dolayı, bu hastalığa sebep olan risk genlerinin sadece küçük bir yüzdesi, gen dizileme çalışmaları sayesinde tespit edilebilmiştir. Bu hastalığa sebep olan etmenleri anlamak için, mutasyon yükü verisini gen ortak ifade çizgeleri ̈uzerinde kullanabilen bir derin öğrenme mimarisi tasarlandı. Ek olarak, hastalık için önem arz eden sinirsel gelişim periyotlarını tespit edebilmek için derin öğrenme modeline uzmanların karışımı modeli de eklendi. Bu uçtan uca eğitilebilen sistem, ̧cizge başına bir ağırlık öğrenerek bütün genler için bir olasılık atayabilmektedir. Modelimizin sonuçları, otizm geni risk tahmin gücünün en gelişmiş modellere kıyasla arttığını göstermektedir. En yüksek risk penceresi olarak talamus ve serebellum beyin bölgesinin mediyodorsal çekirdeğini ve yenidoğan/erken bebeklikten orta/geç çocukluk dönemine kadar olan periyot (0 ay - 12 yaş) belirlenmiştir. Sonuçlarımız, otizm ile alakalı bilinen anahtar biyolojik yollar ve gen hedefleri için iyi bir zenginleşmeye de işaret etmektedir. OSB ile ilişkili bir etiketi olmayan kopya sayısı değişikliği bölgelerinde risk geni olmaya aday birkaç gen gözlemlenmiştir. Yalancı-pozitif kesin referans genler, etiketlenmemiş olmasına rağmen OSB ile ilişkili olma olasılığı yüksek genler ve yüksek sıralamalı yalancı-negatif kesin referans genler incelenmiştir.","Autism Spectrum Disorder (ASD) is a complex neurodevelopmental disorder with a strong genetic basis. Due to its intricate nature, only a fraction of the risk genes were identified despite the effort spent on large-scale sequencing studies. To perceive underlying mechanisms of ASD and predict new risk genes, a deep learning architecture is designed which processes mutational burden of genes and gene co-expression networks using graph convolutional networks. In addition, a mixture of experts model is employed to detect specific neurodevelopmental periods that are of particular importance for the etiology of the disorder. This end-to-end trainable model produces a posterior ASD risk probability for each gene and learns the importance of each network for this prediction. The results of our approach show that the ASD gene risk prediction power is improved compared to the state-of-the-art models. We identify mediodorsal nucleus of thalamus and cerebellum brain region and neonatal & early infancy to middle & late childhood period (0 month - 12 years) as the most informative neurodevelopmental window for prediction. Top predicted risk genes are found to be highly enriched in ASD-associated pathways and transcription factor targets. We pinpoint several new candidate risk genes in CNV regions associated with ASD. We also investigate confident false-positives and false negatives of the method and point to studies which support the predictions of our method."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Histopatoloji görüntülerinin analizinde derin öğrenme, gösterim öğreniminin elle seçilen özniteliklere göre daha yüksek başarı göstermesiyle birlikte büyük katkıda bulunmuştur. Fakat derin öğrenmenin histopatoloji uygulamalarında iki zorluk etkisini sürdürmektedir. Bu zorluklardan ilki, histopatoloji alanında büyük veri kümelerinin bulunmamasıdır. Diğer alanlarda yaygın olarak kullanılan derin öğrenme tabanlı yöntemler, derin ve geniş modellerin büyük ve işaretli veri kümeleri üzerinde eğitilmesi üzerine kurulmuştur. Histopatoloji görüntülerinin elle işaretlenmesi ise çok zaman alan bir işlemdir. Büyük halka açık veri kümelerinin oluşturulmasında da, tıbbi verilerin mahremiyeti konusundaki çekinceler sebebiyle zorluklar yaşanmaktadır. İkinci zorluk ise, histopatoloji görüntülerinde bulunan ilgi alanlarının değişken boyut ve şekillere sahip ve farklı tanı sınıflarına ait olabilmesidir. Bu zorluĞa karşı geliştirilen alışıldık çözüm, ilgi alanlarından kesilerek elde edilen sabit boyuttaki görüntü parçaları üzerinde analiz yapılıp, elde edilen sonuçların ilgi alanı için bir araya getirilmesidir. Bu yöntemler hücre seviyesindeki öznitelikleri çıkarmada başarılı olsalar da, doku seviyesindeki bağlamsal ilişkileri modellemekte etkili olamamışlardır. Bu zorlukların ışığında, çizgesel sinir ağları ve karşılaştırmalı kendinden gözetimli öğrenme yöntemlerini kullanan iki aşamalı bir öğrenme yöntemi önermekteyiz. Bu yöntemde ilgi alanları, düğümleri küçük ve sabit boyutlu görüntü parçalarına karşılık gelen çizgeler olarak modellenir. İlk aşamada, ilgi alanlarından kesilmiş küçük ve sabit boyutlu görüntü parçaları üzerinde, hücre seviyesi niteliklere odaklanan bir evrişimli sinir ağı eğitilir. İkinci aşamada, ilgi alanı seviyesi öznitelik gösterimleri, dokudaki yapıları öĞrenebilen bir çizgesel sinir ağı ile öğrenilir. Çizgesel sinir ağları, değişken boyutlu ilgi alanlarının çizgeler olarak modellenmesinin ve bağlamsal özniteliklerin öğrenilmesinin yolunu açmaktadır. Karşılaştırmalı kendinden gözetimli öğrenme yöntemi ise, etiketli verinin az olduğu durumlarda veriden öğrenilen özniteliklerin kalitesini artırmaktadır. Kendinden gözetimli öğrenme ilk kez rastgele düğüm silme yöntemiyle çizgesel sinir ağlarına uyarlanmıştır. Çalışma kapsamında oluşturduğumuz meme histopatolojisi veri kümesi üzerinde yapmış olduğumuz deneyler, önerilen derin gösterim öğrenimi yönteminin, görüntü sınıflandırma ve görüntü tabanlı içerik erişimi görevlerinde önceki yöntemlerden daha başarılı olduğunu göstermiştir.","Deep learning has made a major contribution to histopathology image analysis with representation learning outperforming hand-crafted features. However, two notable challenges remain. The first is the lack of large histopathology datasets. The commonly used setting in deep learning-based approaches is supervised training of deep and wide models using large labeled datasets. Manually labeling histopathology images is a time-consuming operation. Assembling a large public dataset is also proven difficult due to privacy concerns. Second, the clinical practice in histopathology necessitates working with regions of interest of multiple diagnostic classes with arbitrary shapes and sizes. The typical solution to this problem is to aggregate the representations of fixed-sized patches cropped from these regions to obtain region-level representations. However, naive methods cannot sufficiently exploit the rich contextual information in the complex tissue structures. To tackle these two challenges, this thesis proposes a generic method that utilizes graph neural networks, combined with a self-supervised training method using a contrastive loss function. The regions of interest are modeled as graphs where vertices are fixed-sized patches cropped from the region. The proposed method has two stages. The first stage is patch-level representation learning using convolutional neural networks which concentrates on cell-level features. The second stage is region-level representation learning using graph neural networks which can learn the tissue structure. Graph neural networks enable representing arbitrarily-shaped regions as graphs and encoding contextual information through message passing between neighboring patches. Self-supervised contrastive learning improves quality of learned representations without requiring labeled data. We propose using self-supervised learning to train graph neural networks using vertex dropout augmentation. The experiments using a challenging breast histopathology dataset show that the proposed method achieves better performance than the state-of-the-art in both classification and retrieval tasks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tam evrişimsel sinir ağları (TEA'lar), mikroskop görüntülerinde hücre bölütlemesi için en gelişmiş modeller haline gelmiştir. Bu ağlar, tipik olarak her pikselin kaybını ayrı ayrı tanımlayan ve bu piksel kayıplarınının ortalamasını veya toplamını alan bir kayıp fonksiyonunu küçülterek eğitilir. Kayıp fonksiyonunun bu piksel bazlı tanımı, piksellerin tahminleri arasındaki uzamsal ilişkileri dikkate almadığından, ağı belirli bir şekli (şekilleri) öğrenmeye yetecek kadar eğitemez. Öte yandan, ağın bu yeteneği, doğaları gereği yaygın olarak benzer morfolojik özellikler gösteren hücreleri daha iyi bölütlemek için önemli olabilir. Bu soruna yanıt aramak adına, bu tez, hücre bölütlemesinde bir TEA'yi eğitmek için yeni bir dinamik şekil koruyan kayıp fonksiyonu ortaya koyar. Bu kayıp fonksiyonu, piksel ağırlıklarını öncül şekil bilgisine duyarlı olarak tanımlayan ağırlıklı bir çapraz düzensizliktir. Bu amaçla, ağırlıkları piksellerin ait olduğu bölütlenmiş nesnelerin şekli ile gerçek bölütleme (ground truth) hücrelerinde tahmin edilen şekil öncülleri arasındaki benzerlik temelinde hesaplar. Bu tez, bir hücrenin şeklini ölçmek için Fourier tanımlayıcılarını kullanır ve bu Fourier tanımlayıcılarının dağılımı üzerine bir benzerlik ölçütü tanımlamayı önerir. Dört farklı medikal görüntü veri seti üzerinde alınan deneysel sonuçlar, önerilen bu kayıp fonksiyonunun, bu veri setlerindeki örneklerin bölütlemesinde karşıtından daha iyi performans gösterdiğini ortaya koymuştur.","Fully convolutional networks (FCNs) have become the state-of-the-art models for cell instance segmentation in microscopy images. These networks are trained by minimizing a loss function, which typically defines the loss of each pixel separately and aggregates these pixel losses by averaging or summing. Since this pixel-wise definition of a loss function does not consider the spatial relations between the pixels' predictions, it does not sufficiently impose the network to learn a particular shape(s). On the other hand, this ability of the network might be important for better segmenting cells, which commonly show similar morphological characteristics due to their natures. In response to this issue, this thesis introduces a new dynamic shape-preserving loss function to train an FCN for cell instance segmentation. This loss function is a weighted cross-entropy whose pixel weights are defined as prior-shape-aware. To this end, it calculates the weights based on the similarity between the shape of the segmented objects that the pixels belong to and the shape-priors estimated on the ground truth cells. This thesis uses Fourier descriptors to quantify the shape of a cell and proposes to define a similarity metric on the distribution of these Fourier descriptors. Working on four different medical image datasets, the experimental results demonstrate that this proposed loss function outperforms its counterpart for the segmentation of instances in these datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Konvolüsyonel sinir ağları son yıllarda çok popüler ve başarılı bir hale geldiler. Konvolüsyonel sinir ağlarının bu başarıyı elde etmesinde derinlikleri ve içerdikleri parametre sayıları önemli bir faktördür. Fakat, derin konvolüsyonel sinir ağlarını tek bir makinenin hafızasına sığdırmak zor bir hale gelmiştir ve bu sinir ağlarını eğitmek çok uzun süreler gerektirir. Bu problemi çözmek için iki adet paralelleştirme yöntemi mevcuttur: veri paralelleştirmesi ve model paralelleştirmesi. Veri paralelleştirmesinde sinir ağları modeli bir çok farklı makineye kopyalanmaktadır ve veri bu makineler arasında bölüntülenmektedir. Her bir kopya, kendisine atanmış veriyi eğitir ve modelin parametrelerini ve parametrelerin değişimlerini diğer kopyalara gönderir. Bu süreç veri paralelleştirmesinde çok büyük bir iletişim yoğunluğuna sebep olur. Bu yoğunluk eğitim sürecini yavaşlatır ve derin sinir ağlarının sonuca yakınsamasını geciktirir. Model paralelleştirmesinde ise derin bir sinir ağı modeli farklı makinelere bölüntülenmektedir ve her bir bölüntü peşi sıra şekilde çalışmaktadır. Fakat, modelin nasıl bölüntüleneceğine karar vermek için bir uzman kişi gereklidir ve bu bölüntüleme işleminde var olan bölüntüleme yöntemlerini kullanarak düşük iletişim yoğunluğu ile birlikte düşük iş dengesizliği oranı elde etmek zordur. Bu tezde yeni bir model paralelleştirme yöntemi olan hipergrafik bölüntülenmiş model paralelleştirme önerilmiştir. Bu yöntem bölüntüleme işlemi için uzman bir kişi gerektirmez ve var olan model paralelleştirme yöntemlerine göre daha iyi iş dengesizliği oranı ile birlikte daha iyi iletişim yoğunluğu elde etmektedir. Ek olarak, bu yeni önerilen yöntem veri paralelleştirme yönteminde ortaya çıkan iletişim yoğunluğunu %93 oranında azaltmaktadır. Son olarak ise önerilen yöntemin var olan paralelleştirme yöntemlerinden daha hızlı bir şekilde sonuca yakınsadığı gösterilmiştir.","Convolutional Neural Networks (CNNs) have become very popular and successful in recent years. Increasing the depth and number of parameters of CNNs has crucial importance on this success. However, it is hard to fit deep convolutional neural networks into a single machine's memory and it takes a very long time to train these deep convolutional neural networks. There are two parallelism methods to solve this problem: data parallelism and model parallelism. In data parallelism, the neural network model is replicated among different machines and data is partitioned among them. Each replica trains its data and communicates parameters and their gradients with other replicas. This process results in a huge communication volume in data parallelism, which slows down the training and convergence of the deep neural network. In model parallelism, a deep neural network model is partitioned among different machines and trained in a pipelined manner. However, it requires a human expert to partition the network and it is hard to obtain low communication volume as well as a low computational load balance ratio by using known partitioning methods. In this thesis, a new model parallelism method called hypergraph partitioned model parallelism is proposed. It does not require a human expert to partition the network and obtains a better computational load balance ratio along with better communication volume compared to the existing model parallelism techniques. Besides, the proposed method also reduces the communication volume overhead in data parallelism by 93\%. Finally, it is also shown that distributing a deep neural network using the proposed hypergraph partitioned model rather than the existing parallelism methods causes the network to converge faster to the target accuracy."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gauss-Seidel doğrusal denklem sistemlerinin çözümünde iyi bilinen bir özyinelemeli yöntemdir. Gauss-Seidel taramalarında yapılan hesaplamalar, yeni yinelemelerin her bir bileşeni önceden hesaplanan sonuçlara bağlı olduğundan doğası gereği ardışıktır. Çizge renklendirme, hesaplamalarda önceliğin neden olduğu veri bağımlılığını ortadan kaldırarak paralellik elde etmek için yaygın olarak kullanılır. Bu tezde, dağıtık-bellek-paralel Gauss-Seidel algoritması için daha iyi bir renklendirme sağlayan bir method sunuyoruz. Metodumuz, renk sınıfları arasındaki hesaplama yük dengesini korurken renk sayısını azaltmak için çizge bölümleme ve dengeli çizge renklendirme gibi çeşitli kombinasyonel yaklaşımları kullanır. Çeşitli bilimsel uygulamalardan doğan düzensiz seyrek problemler üzerinde yapılan deneyler, modelimizin Gauss-Seidel algoritmasındaki gerekli renk sayısını ve dolayısıyla paralel taramaları etkili bir şekilde azalttığını göstermektedir.","Gauss-Seidel is a well-known iterative method for solving linear system of equations. The computations performed on Gauss-Seidel sweeps are sequential in nature since each component of new iterations depends on previously computed results. Graph coloring is widely used for extracting parallelism in Gauss-Seidel by eliminating data dependencies caused by precedence in the calculations. In this thesis, we present a method to provide a better coloring for distributed-memory-parallel Gauss-Seidel algorithm. Our method utilizes combinatorial approaches including graph partitioning and balanced graph coloring in order to decrease the number of colors while maintaining a computational load balance among the color classes. Experiments performed on irregular sparse problems arising from various scientific applications show that our model effectively reduces the required number of colors thus the number of parallel sweeps in the Gauss-Seidel algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gen verisi kişiye özel hassas bilgiler içerir. Kişinin gen verisi kullanılarak aile üyelerinin gen dizilimleri yüksek olasılıkla doğru olacak şekilde tahmin edilebilir veya kişi hastalık yatkınlıkları nedeniyle ayrımcılığa maruz kalabilir. Gen verilerini korumak ve bu verilerin kullanımını standart ve güvenli bir hale getirmek için ""Beacon"" sistemleri geliştirildi. Yapılan araştırmalar, beacon sistemlerinin bir bireyin veri setinde olup olmadığını anlamak için yapılan kimlik tespiti ataklarına karşı yetersiz olduğunu göstermiştir. Beacon sistemleri genelde belirli bir hastalığı içeren bireylerden oluştuğu için, bir bireyin veri bankasında olup olmadığının tespit edilmesi bireyin kişisel gizliliğini tehdit eden önemli bir unsurdur. Bunun yanı sıra gen verileri hastalıkların nedeni olan genlerin tespit edilmesi ve kişiselleştirilmiş tıp alanlarında yapılan çalışmalar için önemli bilgi kaynaklarıdır. Beacon sistemlerini kimlik tespiti ataklarına karşı korumak adına şu ana kadar alınan önlemlerin yetersiz kaldığı araştırmalar sonucunda gösterilmiştir. Bu tezde akrabalık ilişkilerinin gen verisi paylaşan beacon sistemleri üzerindeki etkisi araştırıldı. Çalışmamız, aynı nokta mutasyonları birden fazla aile üyesinde bulunabileceği için, akrabalığın kimlik tespiti atakları için yanıltıcı olabileceği gerçeğine dayanmaktadır. Çalışmamız sonucunda ebeveynlerden en az birinin beacon sistemine eklenmesinin (i) atakların başarı oranında önemli bir düşüşe yol açtığını ve (ii) bir bireyin veri setinde olup olmadığını anlamak için gereken sorgu sayısında artışa sebep olduğunu gösterdik. Akrabalık ilişkilerinin beacon sistemlerinde bir savunma mekanizması olarak kullanılmasının beacon sistemini nasıl etkileyeceğini araştırmak adına sistemdeki fayda azalması hesaplamaları da yapıldı. Son olarak büyük anne ve büyük baba gibi bireye daha uzak akrabalar eklemenin beacon sistemleri üzerindeki etkilerini analiz edildi.","Genomic data contains sensitive information about an individual. Family members' genome sequence can be re-constructed with high confidence or individuals' may face discrimination because of predisposition of a disease if genome sequence of a person is obtained. To protect the genomic information and provide a standardize and secure way for using this data the ""Beacon project"" initiated. Studies show that the genomic data sharing beacons are vulnerable to re-identification attacks. Since beacons generally constructed based on types of diseases, re-identification creates a significant risk for individuals. On the other hand, genomic data enables researchers to find the cause of diseases and improves personalized medicine. Previously proposed counter measures against re-identification attacks proved to be not effective as earlier researches show. In this thesis, we analyze the kin relationships' effect on the genomic data sharing beacons. Our study is based on the fact that kinship may be misleading for re-identification attacks since same SNPs can be appear in multiple family members. We showed that adding at least one of the parents to the beacon (i) cause significant decrease in the power of attacks and (ii) increase in the number of queries needed to confirm an individual's beacon membership. To investigate the suitability of using kinship as a counter measure for beacons we also calculate the utility decrease. We further show the effects of adding more distant relatives to the beacon such as grandparents."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Klinik denemelerde ortaya çıkan ve öngörülemeyen yan etkilerden kaynaklanan ilaç başarısızlıkları, katılımcılar için ciddi sağlık riskleri oluşturur ve önemli finansal kayıplara neden olmaktadır. Öte yandan yan etki tahmin algoritmaların ilaç tasarım sürecini yönlendirme potensiyeli vardır. LINCS L1000 veri kümesi, farklı hüçre hatlarında farkı zamana noklarında bir çok ilacın farklı dozda uygulandığı, geniş bir gen ifade veri seti içermektedir. Literatürdeki en güncel yaklaşım, sadece LINCS L1000'deki yüksek kaliteli deneylere dayanmakta ve farklı deneylerin büyük bir kısmını model dışında bırakmaktadır. Bu çalışmada, derin öğrenme temelli bir yaklaşımla geriye kalan kayıtlı deneylere ait verilerden daha fazla bilginin elde edilip edilemeyeceğini araştırmaktayız. (i) LINCS L1000 projesinden gen ifade verilerini, (ii) ilaçların kimyasal yapı parmak izlerini, (iii) SMILES ilaç yapısının dizi gösterimini ve (iv) ilaçların atom çözünürlüğündeki moleküler yapılarını kullanan 6 farklı derin öğrenme mimarisi ile deneyler yapmaktayız. Ilaç moleküler yapısı, kimyasal yapıları ve gen ifade özelliklerini kullanan çok katmanlı algılayıcı (MLP) tabanlı model, %88'lik mikro-AUC ve %79'luk makro-AUC'ye ulaşabilmekte ve bu sayede daha yüksek yan etki tahmini imkanı sunmaktadır. Her ne kadar farklı derin öğrenme modelleri ile güçlü gösterimler çıkarılmış olsa da, kimyasal yapının gen ifade profillerinden daha iyi bir tahmin güçlü olduğunu da gözlemlemekteyiz. Son olarak, sadece ilaçların SMILES dizilerini kullanan evrişimli sinir ağı tabanlı model, hem gen ifadesi hem de kimyasal yapı özelliklerini aynı anda kullanan modellerden daha iyi bir performans sunmaktadır.","Drug failures due to unforeseen adverse effects at clinical trials pose health risks for the participants and cause substantial financial losses. Side effect prediction algorithms, on the other hand, have the potential to guide the drug design process. LINCS L1000 dataset provides a vast resource of gene expression profiles across different cell lines that are induced with different dosages taken at different time points. The state-of-the-art approach in the literature relies on high-quality experiments in LINCS L1000 and discards a large portion of the recorded experiments. In this study, we investigate whether more information can be extracted from this remaining set of experiments with a deep learning-based approach. We experiment with 6 different deep learning architectures that use (i) gene expression data from the LINCS L1000 project, (ii) chemical structure fingerprints of drugs, (iii) SMILES string representation of drug structure, and (iv) the atomic structure of the drug molecules. The multilayer perceptron (MLP) based model which uses chemical structures and gene expression features achieve 88% micro- AUC and 79% macro-AUC, thus offering better performance in comparison to the state-of-the-art studies on side effect prediction. We observe that the chemical structure is more predictive than the gene expression profiles despite the fact that the features are extracted with different deep learning models. Finally, the convolutional neural network-based model that uses only SMILES strings of the drugs provides 82% macro-AUC, and 88%micro-AUC improvements, better performing than the models that use gene expression and chemical structure features simultaneously."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge görselleştirme, birbiriyle ilişkili bilgileri grafiksel biçimde çizge ya da ağlar şeklinde gösteren bir araştırma alanıdır. Bu gösterim, ağ sistemleri, yazılım mühendisliği ve biyoloji gibi birçok alanda uygulanabilmektedir. Bu alanlarda, çizge görselleştirme teknikleri çizge tabanlı spesifik verilerin etkili görsel analizini sağlamaktadır. Systems Biology Graphical Notation (SBGN) biyolojik varlıkları ve bunların etkileşimlerini göstermeyi sağlayan bir standardı çizge görselleştirme kullanarak tanımlamaktadır. SBGN-ML, SBGN haritaları hakkındaki bilgileri tutan bir XML tabanlı formattır. libSBGN ise SBGN-ML dosyaları üzerinde okuma ve yazma işlemini kolay bir şekilde sağlamakla birlikte SBGN haritaları için sözdizimsel ve anlamsal doğrulamayı da mümkün kılmaktadır. Bu üst seviye anlamsal doğrulama daha önce bahsedildiği gibi Java/C++ (libSBGN) ve JavaScript (libSBGN.js) programlama dillerinde farklılık gösteren seviyelerde desteklenmektedir. libSBGN, SBGN haritaları üzerinde değişiklik yapmak ve başka birçok formata dönüştürmek için önemli sözdizimsel ve anlamsal doğrulama konseptlerine olanak sağlamaktadır. SBGN-ML dosyalarının sözdizimsel doğrulaması basit bir XSD dosyasını kullanmayı içermektedir. Bu doğrulama dosyaların doğru formatta olup olmadığını kontrol etmektedir. Fakat, bu XSD dosyası SBGN içerisindeki anlamsal kuralları kontrol etmeye olanak sağlamamaktadır. Üst seviye anlamsal kontrolü sağlayan Schematron dili böyle dosyaların anlamsal doğrulaması için geliştirildi. Bu tezimizle birlikte, ilk olarak SBGN haritalarının XSLT ve süreç tanımlama haritalarının dönüşümünü kullanan üst düzey anlamsal doğrulaması libSBGN kütüphanesinin JavaScript versiyonu içerisinde sağlandı. XPath formatında yazılan, doğrulama hataları için okunabilir mesajları ve hataların kaynaklarını gösteren Schematron kuralları kullanarak XSLT stil sayfası üretildi. Bu stil dosyası kullanılarak SBGN-ML dosyalarını dönüştüren doğrulama sonuç dosyası elde edildi. libSBGN kütüphanesinin JavaScript versiyonunda, dönüşüm için Web tabanlı XSLT işleyicisi kullanıldı ve bu nedenle kütüphane diğer ihtiyaç duyulan SBGN yazılımlarında da Schematron doğrulama sağlamak için kullanılabilir duruma geldi. Buna ek olarak, Schematron doğrulama kontrolleri, değişen libSBGN.js kütüphanesini kullanarak SBGN editörü Newt aracına eklendi. Bu eklemeyle birlikte Newt, seçilen haritalar için doğrulama sonucunu okunabilir bir mesaj şeklinde gösterebilmenin yanı sıra, doğrulama açısından sorunlu objeleri renklendirerek gösterebilir ve uygun bir şekilde doğrulama problemlerini otomatik olarak çözen yollar önerebilir hale getirildi.","Graph visualization is a research field where relational information is graphically represented in the form of graphs or networks. It is applicable in numerous areas from computer network systems, to biology, to software engineering. In such areas, graph visualization techniques provide effective visual analysis of graph based data. Systems Biology Graphical Notation (SBGN) facilitates a standard model for representing biological entities and their interactions by using graph visualization. SBGN-ML is an XML based format for keeping information about SBGN maps. libSBGN enables writing and reading SBGN-ML files in an easy manner and is meant to bring syntactic and semantic validation to SBGN maps. It is currently available in Java/C++ (libSBGN) and JavaScript (libSBGN.js) programming languages with varying support for aforementioned. libSBGN enables important syntactic and semantic correctness concepts for manipulating SBGN maps and converting SBGN-ML files into several other formats. Syntactic validation of SBGN-ML files involves using a simple XML Schema Definition (XSD) file. This validation checks whether files are in correct form or not. However, this XSD file does not enable checking against semantic rules. For semantic validation of such files, the Schematron language was developed providing higher level semantic rule controls. With this thesis, we first enabled high level semantic validation (schematron validation) of SBGN maps in libSBGN.js, which uses XSLT and transformation of process description maps in SBGN-ML files. By using Schematron rules which are written in XPath syntax and enabling human-readable messages of validation errors and source of errors, we developed an XSLT stylesheet. We obtained validation result report by transforming SBGN-ML files using this XSLT stylesheet. In the JavaScript version of libSBGN library, we used a web based XSLT processor for transformation; hence, this library is now available for providing schematron validation in any SBGN related software. Furthermore, we added schematron validation checks to Newt, a web based SBGN pathway editor, using the updated libSBGN.js library. With this addition, Newt is now able to show validation results not only in a human-readable message text for the current map but also highlights the invalid map objects graphically, and, where appropriate, suggests a way to fix the problem automatically."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezin odak noktası, dağıtık bellekli sistemlerde paralel çizge hesaplamalarının performansını ölçeklendirmek için akıllı bölümleme modelleri ve yöntemleridir. Bu kapsamda dağıtık veritabanları, sunucular arasında veri-yerelliği ve iş-yükü dengesi sağlamak için grafik bölümlemeden yararlanır. Veritabanında gerçekleştirilen bazı sorgular, birbirlerini tetikleyen sorgular nedeniyle ardışıklık gösterebilir. Mevcut bölümleme yöntemleri, grafik yapısını ve sorgu-iş yükünün kayıtlarını dikkate almaktadır. Bu çalışmada ardışıklık gösteren işlemler sırasında sunucular arasındaki toplam iletişim maliyetini en aza indirgemek amacıyla ardışıklığa duyarlı grafik bölümleme problemi ortaya konmaktadır. Bu haliyle tarafımızdan büyük ölçekli bölümleme için girdi olarak kullanmak üzere grafik yapısını ve ardışık işlemleri birlikte değerlendiren rastsal bir algoritma önerilmektedir. Gerçek sosyal ağları temsil eden çizgeler üzerinde yapılan deneyler, önerilen çözümün bölümlendirme hedefleri açısından etkinliğini göstermektedir. Seyrek-genelleştirilmiş-matris çarpımı~(SyGEMM), bilimsel hesaplamalarda ve yüksek performanslı çizge hesaplarında kullanılan anahtar bir hesaplama çekirdeğidir. Yineleyici çerçevesi sayesinde yüksek performanslı dağıtık hesaplama yapabilen Accumulo veritabanı için tarafımızdan bir SyGEMM algoritması önerilmektedir. Önerilen algoritma, Accumulo'nun toplu tarama özelliğini ve sunucu düzeyinde paralellik yapılarını kullanarak yazma-yerelliği sağlar ve matrislerini birden fazla kez taranmasına neden olmaz. Ayrıca bu çalışmada sunucular arasında toplam iletişim hacmini azaltan ve iş yükü dengesi sağlayan bir matris bölümleme şeması önerilmektedir. Konuya dönük olarak gerçek problemlerde ortaya çıkan ve sentetik olarak üretilen seyrek matrisler üzerinde yapılan kapsamlı deneyler, önerilen algoritmanın ve matris bölümleme şemasının önemli performans iyileştirmeleri sağladığını göstermektedir. Paralel SyGEMM algoritmalarının ölçeklendirilebilirliği yoğun bir şekilde iletişim işlemlerine bağlıdır. SyGEMM'in iş yükünün çok boyutlu bölümlenmesi daha yüksek ölçeklenebilirlik elde etmek için şarttır. Bu itibarla işlemcilerin dizilimini dikkate alarak ve aynı zamanda SyGEMM'in iş yükü üzerinde çok boyutlu bölümleme elde eden hiperçizge modelleri tarafımızdan önerilmektedir. Yapılan kapsamlı deneyler, önerilen bölümleme modellerinin etkinliğini göstermektedir.",The focus of this thesis is intelligent partitioning models and methods for scaling the performance of parallel graph computations on distributed-memory systems. Distributed databases utilize graph partitioning to provide servers with data-locality and workload-balance. Some queries performed on a database may form cascades due to the queries triggering each other. The current partitioning methods consider the graph structure and logs of query workload. We introduce the cascade-aware graph partitioning problem with the objective of minimizing the overall cost of communication operations between servers during cascade processes. We propose a randomized algorithm that integrates the graph structure and cascade processes to use as input for large-scale partitioning. Experiments on graphs representing real social networks demonstrate the effectiveness of the proposed solution in terms of the partitioning objectives. Sparse-general-matrix-multiplication~(SpGEMM) is a key computational kernel used in scientific computing and high-performance graph computations. We propose an SpGEMM algorithm for Accumulo database which enables high performance distributed parallelism through its iterator framework. The proposed algorithm provides write-locality and avoids scanning input matrices multiple times by utilizing Accumulo's batch scanning capability and node-level parallelism structures. We also propose a matrix partitioning scheme that reduces the total communication volume and provides a workload-balance among servers. Extensive experiments performed on both real-world and synthetic sparse matrices show that the proposed algorithm and matrix partitioning scheme provide significant performance improvements. Scalability of parallel SpGEMM algorithms are heavily communication bound. Multidimensional partitioning of SpGEMM's workload is essential to achieve higher scalability. We propose hypergraph models that utilize the arrangement of processors and also attain a multidimensional partitioning on SpGEMM's workload. Thorough experimentation performed on both realistic as well as synthetically generated SpGEMM instances demonstrates the effectiveness of the proposed partitioning models.
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge görselleştirme, çizgeleri daha anlaşılır ve temsil edilen verilerin analizini daha kolay hale getirmeye çalışan önemli bir araştırma alanıdır. Çeşitli alanlarda, temel çizge tabanlı verilerin etkin bir şekilde analiz edilmesi için çizge görselleştirme teknikleri ve standartları geliştirilmiştir. Systems Biology Graphical Notation (SBGN), biyolojik süreçleri ve yolakları çizge görselleştirme yöntemiyle modellemek için standart bir dildir. SBGN haritaları hakkındaki bilgiler XML tabanlı SBGN-ML dosyalarında saklanabilir. libSBGN, SBGN-ML dosyalarını okumak, yazmak ve nesne yönelimli bir şekilde SBGN haritalarını değiştirmek için kullanılan bir Java/C++ kütüphanesidir. Çizge veritabanları, verileri düğümlerden (köşe) ve bunların ilişkilerinden (kenar) oluşan bir çizge yapısı olarak saklar. Bir çizge veritabanında çizge dolaşma yöntemiyle depolanan verilere erişmek, maliyetli birleşim işlemleri yoluyla ilişkisel veritabanlarındaki tablolanmış verilere erişmekten daha etkilidir. Neo4j, depolanmış çizge verilerini sorgulamak için Cypher adlı tescilli bir dil sağlayan seçkin bir çizge veritabanıdır. Neo4j, üçüncü taraf Java kütüphaneleriyle Neo4j'nin yeteneklerini geliştirmek için Java'da yazılmış kullanıcı tanımlı prosedürleri eklenti olarak geliştirme olanağı sağlar. Bu tez ile, Neo4j çizge veritabanında, SBGN haritalarının, bileşik yapılı düğümleri de destekleyerek modellenmesini sağlıyoruz. Neo4j'de oluşturduğumuz SBGN veri modelini ve libSBGN kütüphanesini kullanarak, Java'da Neo4j eklentisi olarak, çizge tabanlı kullanıcı tanımlı prosedürler geliştirdik. Bu prosedürler, çizge sorgulama algoritmaları yanı sıra bir SBGN haritasından bir veritabanını oluşturmak, bir çizge veritabanından bir SBGN haritasını yüklemek gibi yardımcı işlevler sağlamak için geliştirilmiştir. Bu kullanıcı tanımlı prosedürler SBGN-ML üretmek veya tüketmek için tasarlanmıştır; bu nedenle, SBGN-ML metnini içe/dışa aktarabilen herhangi bir görselleştirme aracı tarafından kullanılabilirler. SBGN haritalarını görüntülemek ve düzenlemek için web tabanlı bir editör olan Newt, bu prosedürleri Cypher ifadeleri ile koşturmak için bir web servisi kullanan ve yerel bir Neo4j veritabanına ev sahipliği yapan bir araç haline getirilmiştir.","Graph visualization is an important research area that endeavors to make graphs more understandable and easier to analyze. In various domains, graph visualization techniques and standards are developed to effectively analyze underlying graph based data. Systems Biology Graphical Notation (SBGN) is a standard language for modeling biological processes and pathways through graph visualization. Information about SBGN maps can be stored in XML based SBGN-ML files. libSBGN is a Java/C++ library for reading, writing SBGN-ML and manipulating SBGN maps in an object-oriented manner. Graph databases store data in terms of a graph structure consisting nodes and their relationships. Performing a computation on graph data stored in a graph database by traversals is more efficient than accessing tabled data in relational databases through costly join operations. Neo4j is a prominent graph database that provides a proprietary language named Cypher for querying stored graph data. Neo4j allows writing user defined procedures in Java as plugins to improve capabilities of Neo4j with third party Java libraries. With this thesis, we enable modeling SBGN maps in Neo4j graph database with support for compound structures. Using this SBGN data model in Neo4j, we developed graph based user defined procedures in Java using libSBGN as a plugin to Neo4j. These procedures were used to implement graph query algorithms, such as neighborhood, common stream, and paths between, along with helper functions such as populating a database from an SBGN map and loading an SBGN map from a graph database. These user defined procedures are designed to produce or consume SBGN-ML; hence, they can be used by any visualization tool which can import/export SBGN-ML text. Newt, a web based editor for viewing and editing SBGN maps, is such a tool making use of these procedures and hosting a local Neo4j instance by providing a web service to execute Cypher statements."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri akışları yaygınlaştıkça, bu geçişken ve dinamik verilerin madenciliği için çevrimiçi algoritmalara gereksinim gittikçe daha belirgin hale gelmektedir. Çok-etiketli veri akışı sınıflandırması, veri akışındaki her bir veri örneğinin etiket kümesindeki bir ya da birden fazla etiketle sınıflandırıldığı bir gözetimli sınıflandırma problemidir. Bu problemin çözümü için içinde çoklu-sınıflandırıcıların da bulunduğu birçok yöntem geliştirilip öne sürülmüştür. Bu çoklu-sınıflandırıcılardan bazıları yalnızca belirli birtakım çok-etiketli temel sınıflandırıcılarla çalışabilecek şekilde tasarlanmış, diğerleri ise çevrimiçi \textit{bagging} gibi yöntemlerle çoklu-sınıflandırıcılarını meydana getiren sınıflandırıcılarını seçmiştir. Bu çalışmada, çok-etiketli sınıflandırma problemi için GOOWE-ML adında yeni bir çevrimiçi, dinamik-ağırlıklı bir çoklu-sınıflandırıcı sunulmuştur. GOOWE-ML, uzamsal modelleme kullanarak içindeki sınıflandırıcılara en-iyileştirilmiş (optimal) ağırlıklar atayabilmektedir ve artımlı herhangi bir çok-etiketli sınıflandırıcıyı kendisi için bir temel sınıflandırıcı olarak kullanılabilecek niteliktedir. Bu çalışmada, 4 adet GOOWE-ML-bazlı çoklu-sınıflandırıcı ile, 7 adet rakip modele karşı çeşitli alanlardan 7 veri kümesi üzerinde deneyler yapılmıştır. Bu deneyler, GOOWE-ML-bazlı çoklu-sınıflandırıcıların neredeyse tüm veri kümelerinde, tahmin performansı bakımından rakip çoklu-sınıflandırıcılardan istikrarlı bir biçimde daha iyi sonuçlar verdiğini göstermektedir.","As data streams become more prevalent, the necessity for online algorithms that mine this transient and dynamic data becomes clearer. Multi-label data stream classification is a supervised learning problem where each instance in the data stream is classified into one or more pre-defined sets of labels. Many methods have been proposed to tackle this problem, including but not limited to ensemble-based methods. Some of these ensemble-based methods are specifically designed to work with certain multi-label base classifiers; some others employ online bagging schemes to build their ensembles. In this study, we introduce a novel online and dynamically-weighted stacked ensemble for multi-label classification, called GOOWE-ML, that utilizes spatial modeling to assign optimal weights to its component classifiers. Our model can be used with any existing incremental multi-label classification algorithm as its base classifier. We conduct experiments with 4 GOOWE-ML-based multi-label ensembles and 7 baseline models on 7 real-world datasets from diverse areas of interest. Our experiments show that GOOWE-ML ensembles yield consistently better results in terms of predictive performance in almost all of the datasets, with respect to the other prominent ensemble models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Konumlandırma sistemleriyle entegre akıllı telefonların yaygınlaşması ve nesnelerin internetinin (Internet of Things - IoT) günlük hayatımızdaki etkisinin artmasıyla birlikte, mobil veri kümeleri yaygın bir şekilde erişilebilir oldu. Günümüzde birçok mobil servis ve uygulama, ya lokasyon bazlı bir içeriğe sahip ya da yan ürün olarak mekan-zaman bilgisi içeren kayıtlar üretmektedir. Bu kayıtlar, hem kendilerini üreten varlıklar veya kullanıcılar, hem de üretildikleri çevre hakkında bilgiler içerir. Bu kayıtların kullanılabilirliği sağlık hizmetleri, hesaplamalı sosyal bilimler ve konum tabanlı pazarlama gibi alanlarda akıllı hizmetleri destekler. Bu çalışma, farklı servislerin kullanımı sonucu elde edilen, gerçek dünyada aynı varlık tarafından üretilen ve mekan-zaman bilgisi içeren kayıtların eşleştirilebileceğini öne sürmektedir. Bu eşleştirme, güvenlik için kullanıcı kimliklerini bağlama, konum tabanlı hizmetlerin gizlilik sınırlamalarını anlama ve kentsel planlama ve trafik yönetimi için birden fazla kaynaktan birleşik bir veri kümesi oluşturma gibi birçok uygulamada temel bir zorunluluktur. Bu tür birleştirilmiş mobil veri kümeleri, servis sağlayıcıların hizmetlerini optimize etmeleri ve iş zekasını geliştirmeleri için de önemlidir. Dolayısıyla, bu çalışma, iki mobil veri kümesindeki varlıkları birbirine bağlamak ve mobil veri kümelerini birleştirmeye giden yolda bir adım daha ilerleyebilmek amacıyla, yalnızca mekansal-zamansal bilgileri kullanarak ölçeklenebilir çözümler araştırmak için yapılmıştır ve sonuç olarak bu eşleştirmeye iki farklı yaklaşım önermektedir. Önerilen ilk yaklaşım, kullanım kayıtları arasındaki yakınlığın hem mekansal hem de zamansal yönlerini kapsamak üzere geliştirilen, k - l çeşitleme kavramına dayanan kurala dayalı eşlemedir. Bu modelin etkinliği ve ölçeklenebilirliği, eşleşen varlıklar için arama alanını önemli ölçüde azaltan etkili mekansal ve zamansal filtreleme mekanizmalarını kullanan ST-LINK adlı ölçeklenebilir bir eşleme algoritması geliştirilerek ölçülmektedir. Bu algoritma, mekansal ve zamansal filtreleme adımlarına ek olarak rastgele disk erişimden kaçınan sıralı tarama prosedürlerini kullanarak büyük veri kümelerine ölçeklenmeyi arttırır. İkinci yaklaşım, varlıkların mekan-zaman bilgisi içeren kullanım geçmişlerinin gösterimi ve bu gösterimler arasındaki benzerliğin tanımlanmasına bağlı, benzerliğe dayalı eşleştirmedir. Bu yaklaşım aynı zamanda eşleştirme işleminin ne zaman durduracağına otomatik olarak karar veren bir durma mekanizması ve eşleşen varlıkları tespit edebilmek için etkili bir eşleştirme sistemi geliştirmektedir. Büyük veri kümelerine ölçeklenebilirlik, eşleştirme sisteminin işleyeceği aday varlık çiftlerini önemli oranda azaltan yakınlığa-duyarlı-karım (Locality-Sensitive-Hashing LSH) sayesinde yapılmaktadır. Çalışma bu modelin ve yakınlığa-duyarlı-karım tabanlı ölçeklenebilirliğin etkinliğini ve verimliliğini ölçmek için SLIM adlı bir algoritma da içermektedir. Çalışma son kısmında, hem kural tabanlı, hem de benzerlik tabanlı eşleme yaklaşımlarını çeşitli veri setleri kullanarak doğruluk ve performans açısından inceleyen deneysel değerlendirmeyi sunmaktadır. Bu deneyler, hem ST-Link hem de SLIM algoritmalarının, mekansal-zamansal eşleme için pratikte etkili olduğunu ve büyük veri kümelerine ölçeklenebileceğini göstermektedir. Dahası, yakınlığa-duyarlı-karım tabanlı ölçeklenebilirlik adımının eşleştirme işlemini 10 üzeri 2 ila 10 üzeri 4 kat hızlandırdığı gözlemlenmiştir.","With the proliferation of smart phones integrated with positioning systems and the increasing penetration of Internet-of-Things (IoT) in our daily lives, mobility data has become widely available. A vast variety of mobile services and applications either have a location-based context or produce spatio-temporal records as a byproduct. These records contain information about both the entities that produce them, as well as the environment they were produced in. Availability of such data supports smart services in areas including healthcare, computational social sciences and location-based marketing. We postulate that the spatio-temporal usage records belonging to the same real-world entity can be matched across records from different location-enhanced services. This is a fundamental problem in many applications such as linking user identities for security, understanding privacy limitations of location based services, or producing a unified dataset from multiple sources for urban planning and traffic management. Such integrated datasets are also essential for service providers to optimise their services and improve business intelligence. As such, in this work, we explore scalable solutions to link entities across two mobility datasets, using only their spatio-temporal information to pave to road towards unifying mobility datasets. The first approach is rule-based linkage, based on the concept of k-l diversity --- that we developed to capture both spatial and temporal aspects of the linkage. This model is realized by developing a scalable linking algorithm called ST-Link, which makes use of effective spatial and temporal filtering mechanisms that significantly reduce the search space for matching users. Furthermore, ST-Link utilizes sequential scan procedures to avoid random disk access and thus scales to large datasets. The second approach is similarity based linkage that proposes a mobility based representation and similarity computation for entities. An efficient matching process is then developed to identify the final linked pairs, with an automated mechanism to decide when to stop the linkage. We scale the process with a locality-sensitive hashing (LSH) based approach that significantly reduces candidate pairs for matching. To realize the effectiveness and efficiency of our techniques in practice, we introduce an algorithm called SLIM. We evaluated our work with respect to accuracy and performance using several datasets. Experiments show that both ST-Link and SLIM are effective in practice for performing spatio-temporal linkage and can scale to large datasets. Moreover, the LSH-based scalability brings two to four orders of magnitude speedup."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tensörler veri bilimi uygulamalarında ve bilimsel çalışmalarda sıkça kullanılmakta olan çok boyutlu matrislere verilen isimdir. Paralel Faktör Analizi (PARAFAC) adıyla bilinen ayrışım şekli, yaygın olarak kullanılan alternatif kökler (ALS) tensör ayrıştırma algoritması sayesinde veri üzerindeki örtük özellikler ve faktör matrisleri ortaya çıkarabilmektedir. Günümüzde gelişmiş teknolojiler ve büyük veri biliminin yaygınlaşmasıyla birlikte ortaya çıkan tensörler milyarlarca satır veri içerebilmektedir. Bu algoritmanın basit bir uygulaması çok büyük boyutlarda ara matris ve veri iletişimi gerektirdiğinden, paralel sistemlerde etkin bir biçimde uygulanabilmesi, büyük veri biliminin gelişimi için önem kazanmaktadır. PARAFAC-ALS'in paylaşımlı veya dağıtık bellekli sistemlerde uygulamaları mevcuttur fakat bu sistemler maliyetli sistem yatırımları ve düşük seviye kodlama gerektirir, çağdaş programlama araçlarıyla uyumsuzdur ve olağan sistem ve altyapı arızalarına dayanıklı da değillerdir. Apache Spark önbellek destekli çağdaş ve dağıtık bir programlama platformudur ve Apache Hadoop ekosistemi ile birlikte birçok şirket ve veri bilimcisi tarafından uyumlu, ekonomik ve hataya dayanıklı olmaları sebebiyle tercih edilmektedir. Spark üzerinde Scala diliyle geliştirdiğimiz paralel PARAFAC-SPARK uygulaması, üç boyutlu tensörleri düşük bellek tüketimiyle ayrıştırabilmektedir. İşlem sırasında tensörler daha hızlı ve dağıtık şekilde işlenebilmeleri için sıkıştırılmış seyrek satırlar (CSR) formatına dönüştürülür ve tensör küp şeklinde parçalara ayırılıp dağıtılarak işlenir. Bu çalışmada, önceki dağıtık bellek ve Hadoop uygulamalarındaki algoritmik ve yöntemsel geliştirmeler derlenip en uygun şekilde Spark için yeniden uyarlanmıştır. Ayrıca, ana Matrisleştirilmiş Tensör ile Khatri-Rao Çarpımı (MTTKRP) operasyonu sırasında çok boyutlu dinamik paylaştırma tekniği uygulanmış, bu sayede MTTKRP operasyonunun bellek tüketiminde dinamik paylaştırma katsayısı oranında azalma ve operasyonun son indirgeme aşamasında da işlemci kapasite kullanım oranında artma sağlanmıştır. PARAFAC-SPARK uygulamasını 11 gerçek veri içeren tensör ve ölçeklenebilirliği test etmek adına sentetik olarak üretilmiş tensörler için çalıştırdık. En ileri varyasyonumuzun (PS-CSRSX), temel Spark varyasyonuna (PS-COO) göre % 67'e kadar daha hızlı olduğunu ve en ileri Hadoop uygulamalarına göre 10 kata kadar daha hızlı olduğunu gördük.","Tensors are higher order matrices, widely used in many data science applications and scientific disciplines. The Canonical Polyadic Decomposition (also known as CPD/PARAFAC) is a widely adopted tensor factorization to discover and extract latent features of tensors usually applied via alternating squares (ALS) method. Developing efficient parallelization methods of PARAFAC on commodity clusters is important because as common tensor sizes reach billions of nonzeros, a naive implementation would require infeasibly huge intermediate memory sizes. Implementations of PARAFAC-ALS on shared and distributed-memory systems are available, but these systems require expensive cluster setups, are too low level, not compatible with modern tooling and not fault tolerant by design. Many companies and data science communities widely prefer Apache Spark, a modern distributed computing framework with in-memory caching, and Hadoop ecosystem of tools for their ease of use, compatibility, ability to run on commodity hardware and fault tolerance. We developed PARAFAC-SPARK, an efficient, parallel, open-source implementation of PARAFAC on Spark, written in Scala. It can decompose 3D tensors stored in common coordinate format in parallel with low memory footprint by partitioning them as grids and utilizing compressed sparse rows (CSR) format for efficient traversals. We followed and combined many of the algorithmic and methodological improvements of its predecessor implementations on Hadoop and distributed memory, and adapted them for Spark. During the kernel MTTKRP operation, by applying a multi-way dynamic partitioning scheme, we were also able to increase the number of reducers to be on par with the number of cores to achieve better utilization and reduced memory footprint. We ran PARAFAC-SPARK with some real world tensors and evaluated the effectiveness of each improvement as a series of variants compared with each other, as well as with some synthetically generated tensors up to billions of rows to measure its scalability. Our fastest variant (PS-CSRSX) is up to 67% faster than our baseline Spark implementation (PS-COO) and up to 10 times faster than the state of art Hadoop implementations."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Teknolojik gelişmeler makineleri, eşyaları ve cihazları daha hızlı, daha yetenekli ve birbirleriyle daha bağlı hale getirmeye devam etmektedir. Birbirlerine bağlı bu akıllı cihazların oluşturduğu ağ yapısı Nesnelerin Interneti (IoT) olarak adlandırılmaktadır. Bu milyarlarca cihazın, birden fazla IoT uygulamasının ihtiyaç duyduğu ve duyabileceği petabaytlarca veriyi üretme ve/veya kullanma yeteneklerine sahip olacağı öngörülmektedir. Bu durum, büyük miktardaki bu verinin etkin ve verimli bir şekilde depolanması ve işlenmesi gibi zorlukları da beraberinde getirmektedir. Bulut bilişim ve onun son kullanıcılara yakınlaştırılmış versiyonu olan sis bilişim, IoT verilerinin verimli ve etkili bir şekilde nasıl taşınacağı, yerleştirileceği, saklanacağı ve işleneceği ile ilgili bu zorlukların bazılarının üstesinden gelebilmek için yeni yöntemler ortaya koymaktadır. Bu tezde; coğrafi olarak dağıtık, hiyerarşik bulut ve sis bileşenlerini içeren bir IoT mimarisi ve oluşturulan büyük miktardaki IoT verisinin bu mimarinin bileşenlerine, bulut ve sis veri merkezlerine, yerleştirmek için yeni teknikler öneriyoruz. Verilerin sınıflandırılabileceği göz önünde bulundurulduğunda, birden fazla uygulama bu verileri kullanabilir. Bu bilgiye dayanarak, coğrafi olarak dağıtık IoT cihazlarının oluşturduğu ve kullandığı verileri, gerçeklenebilir ağ modellerinde etkin ve verimli şekilde veri merkezlerine yerleştirecek algoritmalar tasarlayıp, bunları tamsayı doğrusal modelleme yöntemi kullanılarak elde edilen optimum sonuçlarla karşılaştırıyoruz. Birden fazla uygulama tarafından kullanılan verileri, kopyalamadan ve o veriyi kullanan bütün uygulamalar tarafından rahatlıkla erişilebilecek bir merkezde saklıyoruz. Önerdiğimiz ağ mimarisi ve algoritmalar sayesinde; verilerin etkin ve verimli şekilde yerleştirilebileceğini, uygulamaların bu verilere bant genişliğini arttırmadan daha hızlı şekilde erişebilmesine olanak sağladığını yaptığımız kapsamlı simülasyon deneylerinin sonuçlarıyla doğruluyoruz.","Technological advancements keep making machines, devices, and appliances faster, more capable, and more connected to each other. The network of all interconnected smart devices is called Internet of Things (IoT). It is envisioned that there will be billions of interconnected IoT devices producing and consuming petabytes of data that may be needed by multiple IoT applications. This brings challenges to store and process such a large amount of data in an efficient and effective way. Cloud computing and its extension to the network edge, fog computing, emerge as new technology alternatives to tackle some of these challenges in transporting, storing, and processing petabytes of IoT data in an efficient and effective manner. In this thesis, we propose a geographically distributed hierarchical cloud and fog computing based IoT storage and processing architecture, and propose techniques for placing IoT data into its components, i.e., cloud and fog data centers. Data is considered in different types and each type of data may be needed by multiple applications. Considering this fact, we generate feasible and realistic network models for a large-scale distributed storage architecture, and propose algorithms for efficient and effective placement of data generated and consumed by large number of geographically distributed IoT nodes. Data used by multiple applications is stored only once in a location that is easily accessed by applications needing that type of data. We performed extensive simulation experiments to evaluate our proposal. The results show that our network architecture and placement techniques can be used to store IoT data efficiently while providing reduced latency for IoT applications without increasing network bandwidth consumed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapılan araştırmalarda tek nükleotid polimorfizmi (TNP), baz çifti ekleme/çıkarmaları (indel) ve yapısal varyasyonlardan (YV) oluşan genetik varyasyonların bireylerde birçok fenotipik etkileri olduğu gözlemlenmiştir. Bu genetik varyasyonlar arasından 50 nükleotid ve daha fazlasını etkileyenler YV olarak adlandırılır. YV'ler otizm, şizofreni ve miyeloid lösemi gibi birçok kalıtsal hastalığa yol açmaktadır. Bu varyasyonların doğru ve hassas bir şekilde karakterize edilmesi sebep oldukları hastalıkların teşhisine olanak sağlarken aynı zamanda genomik araştırmaların daha üst seviyede yapılması için güvenilir bilgi sağlar. YV'lerin genomdaki yaklaşık yerini bulmayı amaçlayan birçok YV tespit algoritması bulunmaktadır. Genomik araştırmalardaki bir sonraki adım daha odaklı ve yakından bir incelemeyle YV'lerin kopma (başlangıç/bitiş) noktalarının rafine edilmesidir. Yaklaşık 300 - 500 baz çifti uzunluğunda güven aralıkları yerine kopma noktaları 1 - 5 baz çifti çözünürlükte bilindiğinde, YV'lerin genotipleme aşaması çok daha hızlı ve hatasız olacaktır. Böylece YV tespit algoritmalarının sonuçlarını baz alan genomik araştırmaların daha sonraki aşamaları da çalışmalarını daha kesin ve güvenilir bir veri ile sürdürebileceklerdir. Bu tezde, kısa okuma teknolojisi ve ayrık okuma yöntemini kullanarak YV kopma noktası rafine eden BROSV algoritmasını sunuyoruz.","Genomic variations that vary from single nucleotide polymorphisms (SNPs), small INDELs to structural variations (SVs) are discovered to have significant phenotypic effects on individuals. Among these genomic variations, SVs are changes that affect more than 50 nucleotides of DNA. SVs are linked to the sources of many genetic diseases such as autism, schizophrenia and chronic myelogenous leukemia. Accurate and precise characterization of these structural variants not only enables us to diagnose genetic diseases that are previously correlated with them but also it provides more reliable information to pursue higher levels of research in the genomic research pipelines. There are many SV detection tools that aim to find the approximate locations of SVs in genome, a further step in the pipeline is to refine those breakpoints of variants by a closer and more focused examination. By this means, genotyping step of structural variations would be faster using k-mer based alignment-free methods and more accurate since locations of SVs will be known with 1-5 base pair resolution compared to 300 - 500 base pair long confidence intervals. Moreover, further steps in the genomic pipelines based on the results of SV detection algorithms would have more definite data to build up on. In this thesis, we propose BROSV (Breakpoint Refinement of Structural Variation), a breakpoint refinement algorithm to obtain better resolution on SV breakpoints with split read analysis and local assembly methods using Illumina short reads and BWA alignment tool. Implementation is available at https://github.com/BilkentCompGen/brosv."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek çıktılı dizilemeye dair veri analizlerinde en temel sayısal sıkıntı okumaların referans genoma haritalanmasıdır ve bu iş için genelde bilgisayar kümeleri kullanılmaktadır. Lakin, petabaytlara varan bu veriyi işleyecek büyüklükte bilgisayar kümeleri kurmak imkansıza yakındır. Ayrıca, hataları gidermek ve yeni dizilenmiş insersiyonları eklemek için referans genom periyodik olarak güncellenmekte olup, bu durum bir çok büyük projede okumaların tekrar haritalanmasına ihtiyaç doğurmaktadır. Bu yüzden, programlama kümelerine olan ihtiyacı azaltmak için gönüllü örgü programlama teknolojilerini araştırmamız gerekmektedir. Ancak, okuma haritalama probleminin hesaplama gereksinimleri hatrı sayılır olması, analizin geri dönüşünün hızlı olması gerekliklerinden dolayı gönüllüleri motive edecek bir yönteme ihtiyaç duymaktayız. Bu sebepten dolayı, dağıtık okuma haritalama tekniklerini popüler blokzincir teknolojisi ile buluşturmayı öneriyoruz. Bitcoin gibi kripto paralar yeni blok üretimini zorlaştırmak ve kontrolde tutmak için ""nonce"" adı verilen bir değeri hesaplamayı mecburi kılar ancak bu sürecin başka herhangi bir kullanışlı amacı yoktur. Bizim çözümümüz (Coinami) yeni bir kripto para birimi olarak Halocoin'i sunuyor ve bu kripto para birimi bilimsel çalışmaları ödüllendirmeyi alternatif bir para basma yöntemi olarak sunuyor. Coinami, okuma haritalama problemlerini merkezi olmayan şekilde yayınlayıp dağıtarak gönüllüleri yaptıkları işten dolayı ödüllendirmeyi esas alır. Yetkili kurumlar sistemimizde iki temel şeyden sorumludur; 1) yeni problem kümeleri yayınlamak, 2) sonuçların kontrolünü sağlayıp sahte para basılmasının önüne geçmek.","The main computational bottleneck of High Throughput Sequencing (HTS) data analysis is to map the reads to a reference genome, for which clusters are typically used. However, building clusters large enough to handle hundreds of petabytes of data is infeasible. Additionally, the reference genome is also periodically updated to fix errors and include newly sequenced insertions, therefore in many large scale genome projects the reads are realigned to the new reference. Therefore, we need to explore volunteer grid computing technologies to help ameliorate the need for large clusters. However, since the computational demands of HTS read mapping is substantial, and the turnaround of analysis should be fast, we also need a method to motivate volunteers to dedicate their computational resources. For this purpose, we propose to merge distributed read mapping techniques with the popular blockchain technology. Cryptocurrencies such as Bitcoin calculate a value (called nonce) to ensure new block (i.e., ""money"") creations are limited and difficult in the system, however, this calculation serves no other practical purpose. Our solution (Coinami) introduces a new cryptocurrency called Halocoin, which rewards scientific work with alternative minting. In Coinami, read alignment problems are published and distributed in a decentralized manner while volunteers are rewarded for their work. Authorities have two main tasks in our system: 1) inject new problem sets (i.e., ""alignment problems"") into the system, and 2) check for the validity of the results to prevent counterfeit."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom çapında ilişkilendirme çalışmaları (Genome-Wide Association Studies - GWAS) genetik hastalıkların temelini teşkil eden kalıtsallığın altında yatan sebeplerin sadece bir kısmını açıklayabilmektedir. İki ya da daha fazla lokusun arasındaki epistatik etkileşimler açıklama gücündeki boşluğu kapatmaya yardımcı olduğu gibi kompleks etkileşimleri de tespit ederek kompleks karakterlerin daha iyi çözümlenebilmesi için gelecek vaat etmektedir. Fakat değerlendirilmesi ve hipotez için test edilmesi gereken çok sayıdaki lokus kombinasyonları, hem algoritma karmaşıklığı hem de istatiksel olarak çalışmaları engellemektedir. Sadece ikili etkileşimler göz önüne alındığında dahi bu durum düzelmemektedir. Epistasis önceliklendirme algoritmalarının hem hesaplama yükünü hem de yapılması gereken test sayısını azalttığı kanıtlanmıştır. Güncel metotlar bağlantı dengesizliğinden kaçınmayı ve vaka kohortunu kapsamayı amaçlasa da, metotların hiçbiri seçilen lokusların topolojik düzenini çeşitlendirmeyi amaçlamamıştır. Bu tezde, epistatik testleri önceliklendirmek için iki aşamalı ardışık düzen algoritması önerilmiştir. İlk aşamada çeşitli lokusları seçmek için altmodüler bir fonksiyon optimize edilmiştir. Bu aşama (i) bağlantı dengesizliğinden kaçınmayı ve (ii) birbirini fonksiyonel olarak tamamlayan lokus ikilileri seçmeyi amaçlamaktadır. İkinci aşamada, seçilen lokuslar hızlı epistatik etkileşim tespit eden bir algoritmada girdi olarak kullanılmıştır. Deneylerimizde, metot modern yöntemlerden biri olan LinDen ile Wellcome Trust Case Control Consortium'dan alınan tip 2 diyabet, hipertansiyon, bipolar bozukluk olmak üzere üç veriseti üzerinde karşılaştırılmıştır. Sonuçlar göstermektedir ki epistatik çiftleri bulmak için yapılan testlerin sayısında önemli bir düşüş gözlenirken aynı zamanda keşfedilen istatiksel olarak önemli epistatik çift sayısı da artmıştır.","Genome-wide association studies explain a fraction of the underlying heritability of genetic diseases. Epistatic interactions between two or more loci help closing the gap and identifying those complex interactions provides a promising road to a better understanding of complex traits. Unfortunately, sheer number of loci combinations to consider and hypotheses to test prohibit the process both computationally and statistically. This is true even if only pairs of loci are considered. Epistasis prioritization algorithms have proven useful for reducing the computational burden and limiting the number of tests to perform. While current methods aim at avoiding linkage disequilibrium and covering the case cohort, none aims at diversifying the topological layout of the selected SNPs which can detect complementary variants. In this thesis, a two stage pipeline to prioritize epistasis test is proposed. In the first step, a submodular set function is optimized to select a diverse set of SNPs that span the underlying genome to (i) avoid linkage disequilibrium and (ii) pair SNPs that relate to complementary function. In the second step, selected SNPs are used as seeds to a fast epistasis detection algorithm. The algorithm is compared with the state-of-the-art method LinDen on three datasets retrieved from Wellcome Trust Case Control Consortium: type two diabates, hypertension and bipolar disorder. The results show that the pipeline drastically reduces the number of tests to perform while the number of statistically significant epistatic pairs discovered increases."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ökaryotik transkriptomların büyük çoğunluğu, proteinlere çevrilmeyen kodlayıcı olmayan RNA'ları (ncRNA'lar) içerir. NcRNA'ların fonksiyonel rolleri hakkında biriken bilgilere rağmen, ncRNA'ların üstlenebileceği tüm moleküler fonksiyon spektrumunu ve bunları nasıl başarabileceklerini anlamaktan hala uzağız. Bu tez çalışmasında ncRNA'lar arasındaki etkileşimleri keşfetmek için hesaplamalı yöntemler ve bu etkileşimleri fonksiyonel olarak analiz etmek için araçlar geliştiriyoruz. Tezin ilk bölümünde, kodlayıcı olmayan uzun RNA (lncRNA) aracılı sünger etkileşimlerini keşfetmek için bütüncül bir yaklaşım sunuyoruz. LncRNA'lar, mikroRNA'lara (miRNA'lara) el koyarak mRNA'ların ekspresyon seviyelerini dolaylı olarak düzenleyebilirler ve miRNA süngeri gibi davranabilirler. Hasta gen ekspresyon profillerinde kısmi korelasyon analizi veçekirdek bağımsızlık testleri yapıyoruz ve miRNA hedef bilgisi ile aday etkileşimlerini daha da geliştiriyoruz. Bu yaklaşımı meme kanseri alt tiplerine özgü sünger etkileşimlerini bulmak için kullanıyoruz. Birden fazla alt tipte ortak süngerleri bulmamıza rağmen, yüksek prognostik potansiyeli olan farklı alt tip-spesifik etkileşimler olduğunu da görüyoruz. İkincisi, sinerjik olarak etkili miRNAçiftlerini tanımlamak için bir yöntem geliştiriyoruz. Bu çiftler ayrı ayrı hareket ettiklerinde hedef mRNA üzerinde zayıf bir baskıya sahiptirler veya hiç baskı yapmazlar, ancak birlikte olduklarında hedef gen ifadelerinin kuvvetli bir şekilde baskılanmasına neden olurlar. Parametrik olmayançekirdek bazlı etkileşim testleri kullanarak RNA üçüzlerinin kombinasyonlarını test ediyoruz. Test edilecek üçüzleri oluştururken, miRNA'lar ve mRNA arasındaki hedef tahminleri göz önünde bulunduruyoruz. Yaklaşımımızı böbrek tümör örneklerine uygularız. Keşfedilen üçlülerin, aralarındaki fonksiyonel bir ilişki veya böbrek tümörleriyle olan ilgileri hakkındaçeşitli biyolojik kanıtlar vardır. Tezin üçüncü bölümünde, biyolojik işlemlerde kritik düzenleyici roller oynadıkları halde hala pekçoğunun fonksiyonel özellikleri bilinmeyen kodlayıcı olmayan RNA'ların fonksiyonel zenginleştirme analizine odaklandık. Bu problem, ilginç bir ncRNA setinin fonksiyonel bir bağlamda analiz edilmesi gerektiğinde bir meydan okuma sunar. Biz de bu amaçla, belirli bir ncRNA kümesi için gen yakınlığına bağlı zenginleştirme analizi yapan bir yöntem geliştiriyoruz. Zenginleştirme, girilen ncRNA'lara yakın olan mRNA genlerinin fonksiyonel açıklamaları kullanılarak gerçekleştirilir. Bu yöntemin ilginç ncRNA'ların bir listesinin işlevsel önemine dair bir içgörü kazanmada nasıl kullanılabileceğini göstermek için, kanser ve psikiyatrik bozuklukların veri setleri hakkında farklı biyolojik sorular ele alıyoruz. Ayrıca, 28 farklı tipteki kanseri, bozulan ve değiştirilmiş lncRNA ekspresyonuna bağlı moleküler işlemler açısından analiz ediyoruz. Burada geliştirilen yöntemlerin, ncRNA'ların fonksiyonel rollerini aydınlatmaya yardımcı olacağını ve ncRNA'lara dayanan tedavilerin geliştirilmesine yardımcı olacağını umuyoruz.","The vast majority of eukaryotic transcriptomes comprise noncoding RNAs (ncRNAs) which are not translated into proteins. Despite the accumulating evidence on the functional roles of ncRNAs, we are still far from understanding the whole spectrum of molecular functions ncRNAs can undertake and how they accomplish them. In this thesis we develop computational methods for discovering interactions among ncRNAs and tools to analyze them functionally. In the first part of the thesis, we present an integrative approach to discover long non-coding RNA (lncRNA) mediated sponge interactions where lncRNAs can indirectly regulate mRNAs expression levels by sequestering microRNAs (miRNAs), and act as sponges. We conduct partial correlation analysis and kernel independence tests on patient gene expression profiles and further refine the candidate interactions with miRNA target information. We use this approach to find sponge interactions specific to breast-cancer subtypes. We find that although there are sponges common to multiple subtypes, there are also distinct subtype-specific interactions with high prognostic potential. Secondly, we develop a method to identify synergistically acting miRNA pairs. These pairs have weak or no repression on the target mRNA when they act individually, but when together they induce strong repression of their target gene expression. We test the combinations of RNA triplets using non-parametric kernel-based interaction tests. In forming the triplets to test, we consider target predictions between the miRNAs and mRNA. We apply our approach on kidney tumor samples. The discovered triplets have several lines of biological evidence on a functional association among them or their relevance to kidney tumors. In the third part of the thesis, we focus on functional enrichment analysis of noncoding RNAs while some non-coding RNAs (ncRNAs) have been found to play critical regulatory roles in biological processes, most remain functionally uncharacterized. This presents a challenge whenever an interesting set of ncRNAs set needs to be analyzed in a functional context. We develop a method that performs cis enrichment analysis for a given set of ncRNAs. Enrichment is carried out by using the functional annotations of the coding genes located proximally to the input ncRNAs. To demonstrate how this method could be used to gain insight into the functional importance of a list of interesting ncRNAs, we tackle different biological questions on datasets of cancer and psychiatric disorders. Particularly, we also analyze 28 different types of cancers in terms of molecular process perturbed and linked to altered lncRNA expression. We hope that the methods developed herein will help elucidate functional roles of ncRNAs and aid the development of therapies based on ncRNAs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Geleneksel olarak, bir doku numunesinin incelenmesi, o örneğin bir patolog tarafından mikroskop yardımıyla taranmasını içermekteydi. Tüm slayt görüntüleme teknolojisi, meme biyopsi slaytlarının bilgisayar ortamına aktarılması ile, mikroskopik inceleme sürecinin bilgisayar ekranıyla desteklenmesine olanak sağlamıştır. Bu teknoloji, slaytları çok yüksek çözünürlüklerde taramayı mümkün kılarak, bu slaytların 100,000 piksele 100,000 piksellik boyutlarda incelenmesine olanak tanımıştır. Görüntüleme teknolojisindeki gelişmeler, tüm slayt histopatoloji görüntüleri üzerinde analizler yaparak, teşhis sürecinde patologların iş yükünü azaltmaya yardımcı olabilecek otomatik araçların geliştirilmesini sağlamıştır. Tüm slayt görüntü analizinin zorluklarından biri, patolog tarafından bir slayt ile ilişkilendirilmiş olan teşhisler ile slaytta bulunan bölgelerin arasındaki bağlantıların bilinmemesidir. Bunun nedeni, patologların doldurdukları patoloji formlarındaki teşhislerin slayt seviyesinde bilgi içermesidir. Diğer bir zorluk ise, tüm meme histopatolojisi görüntülerinde değişken sayıda bulunan değişken büyüklükteki ilgi bölgelerinin (İB) temsilidir. Çünkü modern evrişimli ağlar histopatoloji görüntülerindeki yapısal ve çevresel bilgiyi kodlayamayacak kadar küçüklükteki sabit boyutlu küçük pencereler üzerinde çalışmaktadır. Meme histopatolojisi görüntülerinin incelenmesindeki en büyük zorluk ise bu alanın içerdiği klinik önemden dolayıdır çünkü bir vakanın yanlış sınıflandırılması gereksiz radyasyon tedavisine, cerrahi ve hormonal tedaviye sebebiyet verebilmektedir. Bu zorlukların ışığında, tüm slayt meme histopatolojisi görüntülerinin incelenmesini şu şekilde ele almaktayız. İlk katkı, tüm slayt meme histopatolojisinde görüntü analizi probleminin, çok-örnekli çok-etiketli bir öğrenme (ÇÖÇE) görevi olarak tanımlanması şeklindedir. Bu bağlamda, bir torba bir slayta tekabül etmektedir ve patoloji formunda bulunan slayt etiketleriyle ilişkilendirilmektedir. Benzer şekilde, torbadaki örnekler de slayt içerisinde bulunan İB'ye tekabül etmektedir. İkinci katkı, derin evrişimli ağların özelliklerini kullanarak, değişken sayıdaki ve değişken büyüklükteki İB'nin temsili için yeni bir öznitelik gösterim yöntemini barındırmaktadır. Nihai katkımız ise, eşzamanlı olarak slayt seviyesinde çok-sınıflı sınıflandırması yapan ve İB seviyesinde tanı etiketi çıkarsayan gelişmiş bir ÇÖÇE modeli içermektedir. Bu çalışma kapsamında geliştirilen ÇÖÇE yöntemlerinin sadece slayt düzeyinde bilgi kullanarak tüm slayt meme histopatolojisi görüntülerinin çok-sınıflı sınıflandırılması probleminde öğrenme ve genelleme yeteneğine sahip olduğunu ve ek olarak, derin öznitelik gösterimlerinin tam denetimli ve zayıf denetimli senaryolarda geleneksel öznitelik gösterimlerine kıyasla daha yüksek başarım verdiğini göstermekteyiz.","The examination of a tissue sample has traditionally involved a pathologist investigating the case under a microscope. Whole slide imaging technology has recently been utilized for the digitization of biopsy slides, replicating the microscopic examination procedure with the computer screen. This technology made it possible to scan the slides at very high resolutions, reaching up to 100,000 x 100,000 pixels. The advancements in the imaging technology has allowed the development of automated tools that could help reduce the workload of pathologists during the diagnostic process by performing analysis on the whole slide histopathology images. One of the challenges of whole slide image analysis is the ambiguity of the correspondence between the diagnostically relevant regions in a slide and the slide-level diagnostic labels in the pathology forms provided by the pathologists. Another challenge is the lack of feature representation methods for the variable number of variable-sized regions of interest (ROIs) in breast histopathology images as the state-of-the-art deep convolutional networks can only operate on fixed-sized small patches which may cause structural and contextual information loss. The last and arguably the most important challenge involves the clinical significance of breast histopathology, for the misdiagnosis or the missed diagnoses of a case may lead to unnecessary surgery, radiation or hormonal therapy. We address these challenges with the following contributions. The first contribution introduces the formulation of the whole slide breast histopathology image analysis problem as a multi-instance multi-label learning (MIMLL) task where a slide corresponds to a bag that is associated with the slide-level diagnoses provided by the pathologists, and the ROIs inside the slide correspond to the instances in the bag. The second contribution involves a novel feature representation method for the variable number of variable-sized ROIs using the activations of deep convolutional networks. Our final contribution includes a more advanced MIMLL formulation that can simultaneously perform multi-class slide-level classification and ROI-level inference. Through quantitative and qualitative experiments, we show that the proposed MIMLL methods are capable of learning from only slide-level information for the multi-class classification of whole slide breast histopathology images and the novel deep feature representations outperform the traditional features in fully supervised and weakly supervised settings."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mu ̈ ̧sterilere daha iyi hizmet sag ̆lamak ve bunu yaparken de karlarını arttırmak ̧sirketlerin iki ana hedefidir. S ̧irketlerin mu ̈ ̧steriye servislerinin kalitesini arttırmanın bir yolu ise mu ̈ ̧sterilerin konum bilgisini kullanmaktır ( ̈orne ̆gin, ̧sirket mu ̈ ̧sterilerinin en yakın ̧subelerine olan ortalama mesafeyi azaltcak ̧sekilde tesisleri konumlandırmak). Ancak, mu ̈ ̧sterilerin konum bilgisi olmadan ̧sirketlerin hedeflerine ula ̧smalrı mu ̈mku ̈n de ̆gildir. Neyse ki, bugu ̈nu ̈n du ̈nyasında, telekomu ̈nikasyon operat ̈orleri ve Swarm uygulamasi gibi servis sag ̆layacıları yu ̈ksek miktarda veri toplamaktadır. Servis sag ̆layıcılar ellerinde bulunan ver- ileri, bu tip ̧sirketlerle payla ̧smaya istekli ancak, bu payla ̧sımı mu ̈ ̧sterilerin gi- zlilig ̆ini ihlal etmeden yapmak sorunlara yol a ̧cabilir. Bu ̧calı ̧smada biz, ̧sirketler tarafından kullanıması i ̧cin, servis sag ̆layıcıların topladıg ̆ı konum verilerinin gi- zlilig ̆ini koruyan iki yeni protokol tasarladık. Yeni protokollerimiz ic ̧in ̈orgu ̈ ta- banlı homomorfik sifreleme ve ̧cok partili hesaplamayı kullanmaktayız. Aynı za- manda daha ̈once tasarladı ̆gımız kısmi homomorfik ̧sifreleme tabanlı protokol ile iki yeni protokollerimizi kar ̧sıla ̧stırıyoruz. Protokollerimizde, ̧sirketlerin mu ̈ ̧steri listesini servis sag ̆layıcılarından, mu ̈ ̧sterilerin konum bilgilerini ̧sirketlerden ve sorgu sonucunu ise servis sag ̆layıcılarından saklıyoruz. Protokollerimizi deney- sel ortamda deg ̆erlendirip onların pratik oldu ̆gunu go ̈steriyoruz. Sonrasında, bu u ̈ ̧c protokolu ̈ kendi aralarında kar ̧sıla ̧stırarak her birinin yararları ve zararları hakkında tartı ̧sıyoruz ve her protokol i ̧cin birer kullanım ̈orneg ̆i veriyoruz. Tasar- ladıg ̆ımız protokeller veri payla ̧sımını gizlilig ̆i koruyan ̧sekilde ger ̧cekle ̧stiriyor ve aynı zamanda gelecekteki karma ̧sık sorgular i ̧cin de bir temel olu ̧sturuyoruz.","Two main goals of the businesses are to serve their customers better and in the meantime, increase their profit. One of the ways that businesses can improve their services is using location information of their customers (e.g., positioning their facilities with an objective to minimize the average distance of their cus- tomers to their closest facilities). However, without the customer's location data, it is impossible for businesses to achieve such goals. Luckily, in today's world, large amounts of location data is collected by service providers such as telecom- munication operators or mobile apps such as Swarm. Service providers are willing to share their data with businesses, doing this will violate the privacy of their customers. Here, we propose two new privacy-preserving schemes for businesses to utilize location data of their customers that is collected by location-based ser- vice providers (LBSPs). We utilize lattice based homomorphic encryption and multiparty computation for our new schemes and then we compare them with our existing scheme which is based on partial homomorphic encryption. In our proto- cols, we hide customer lists of businesses from LBSPs, locations of the customers from the businesses, and query result from LBSPs. In such a setting, we let the businesses send location-based queries to the LBSPs. In addition, we make the query result only available to the businesses and hide them from the LBSPs. We evaluate our proposed schemes to show that they are practical. We then compare our three protocols, discussing each one's advantages and disadvantages and give use cases for all protocols. Our proposed schemes allow data sharing in a private manner and create the foundation for the future complex queries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tümör tomurcuklanmasının gözlenmesi, kolorektal karsinomların evrelenmesinde ümit veren bir biyobelirteç olarak kabul edilmektedir. Mevcut tıp uygulamalarında, bu tümör tomurcukları, immünohistokimyasal (IHK) olarak boyanmış doku örneğinin mikroskop altında manuel olarak incelenmesi ile tespit edilmektedir. Öte yandan bu manuel inceleme zaman kaybına ve aynı zamanda gözlemciler arası değişkenliğe yol açabilmektedir. Hızlı ve tekrarlanabilir incelemeler için, bilgisayar destekli çözümler geliştirmek giderek daha fazla önem kazanmaktadır. Bu motivasyon ile, bu tez, otomatik tümör tomurcuklanma tespiti amacıyla ilk defa bir tam evrişimsel ağ tasarımı sunmaktadır. Bu ağ tasarımı, güncel öğrenme mekanizmaları dikkate alınarak U-net mimarisi üzerine geliştirilmiştir. Bu mekanizmalar, kodlayıcı evresinde veri besleme bağlantılarının kullanılmasını, hem ELU hem de ReLU aktivasyon fonksiyonlarının ağın farklı katmanlarında kullanılmasını, ağın Tversky hata hesaplama fonksiyonuyla eğitilmesini ve son bölütleme haritasının geri çatılması için geri kodlayıcının farklı katmanlarında elde edilen çıktıların birleştirilmesini içerir. IHK ile boyanmış kolorektal karsinom örneklerinin 23 tam biyopsi slayt görüntüsünden alınan 3295 görüntü üzerinde yaptığımız deneyler, bu genişletilmiş versiyonun gradyan sıfırlanması problemini ve çok büyük sınıf dengesizliği durumuna sahip veri setindeki olumsuzlukları hafifletmeye yardımcı olduğunu göstermiştir. Sonuç olarak, bu ağ tasarımının, yaygın kullanılan iki başka ağa kıyasla daha iyi bölütleme sonuçları verdiğini göstermiştir.","The existence of tumor buds is accepted as a promising biomarker for staging colorectal carcinomas. In the current practice of medicine, these tumor buds are detected by the manual examination of a immunohistochemically (IHC) stained tissue sample under a microscope. This manual examination is time-consuming as well as it may lead to inter-observer variability. In order to obtain fast and reproducible examinations, developing computational solutions has been becoming more and more important. With this motivation, this thesis presents a fully convolutional network design for the purpose of automatic tumor bud detection, for the first time. This network design extends the U-net architecture by considering up-to-date learning mechanisms. These mechanisms include using residual connections in the encoder path, employing both ELU and ReLU activation functions in different layers of the network, training the network with a Tversky loss function, and combining outputs of different layers of the decoder path to reconstruct the final segmentation map. Our experiments on 3295 image tiles taken from 23 whole slide images of IHC stained colorectal carcinomatous samples show that this extended version helps alleviate the vanishing gradient problem and those related with having a high class-imbalance dataset. And as a result, this network design yields better segmentation results compared with those of the two state-of-the-art networks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Heyet yaklaşımlı sınıflandırıcılarda budama, bellek kullanımını ve işlem zamanını azaltırken en azından orijinal heyet kadar başarılı bir alt küme seçme işlemidir. Heyet yaklaşımlı sınıflandırıcılarda budama konusu veri akışlarında üzerine düşülmemiş bir araştırma konusudur. Heyet bileşenlerinin veri akışı devam ederken analiz edilmesini ve faydalı bileşenlerin gereksiz olanlardan ayrıştırılmasını gerektirir. Bu tezde iki anında heyet budama metodu önerilmiştir; CCRP ve CCP. CCRP seçilen alt kümede her bir hedef sınıf için en iyi sınıflandırmayı yapan bileşenlerin bulunmasını hedefliyerek veri akışındaki sınıf dağılımlarının dengesizliğinin etkilerini azaltır. Öte yandan, CCP farklı veri noktalarında hatalar yapan bileşenleri seçer. Gerçek ve sentetik veri akışları üzerinde farklı sınıflandırıcılar ile yapılan deneyler, budama işleminin kaynaştırıldığı heyet sınıflandırıcılarının bellek kullanımını ve harcadıkları zamanı istatistiksel açıdan önemli derecede azaltırken sınıfların tahminsel başarına zarar vermediğini göstermiştir.","Ensemble pruning is the process of selecting a subset of component classifiers from an ensemble which performs at least as well as the original ensemble while reducing storage and computational costs. Ensemble pruning in data streams is a largely unexplored area of research. It requires analysis of ensemble components as they are running on the stream and differentiation of useful classifiers from redundant ones. We present two on-the-fly ensemble pruning methods; Class-wise Component Ranking-based Pruner (CCRP) and Cover Coefficient-based Pruner (CCP). CCRP aims that the resulting pruned ensemble contains the best performing classifier for each target class and hence, reduces the effects of class imbalance. On the other hand, CCP aims to select components that make misclassification errors on different instances. The conducted experiments on real-world and synthetic data streams demonstrate that different types of ensembles that integrate pruners consume significantly less memory and perform significantly faster without hurting the predictive performance."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein-protein etkileşimi (PPE) ağları, proteinleri ve dolayısı ile onları kodlayan genler arasındaki olası etkileşimler kümesini temsil eder. Mutasyonlar veya değişken ifade örüntüleri gibi tek tek genlerden gelen sinyalleri entegre edilmesini olanaklı kılarak PPE ağları günüze dek çeşitli biyolojik keşiflere vesile olmuştur. Ayrıca, bu tür ağlardaki proteinlerin bağlantı örüntülerinin, proteinleri veya genleri içeren çeşitli tahmin görevleri için oldukça bilgilendirici olduğu kanıtlanmıştır. Ancak, bu görevler göreve özel öznitelik mühendisliği gerektirmektedir. Ağdaki düğümlerin derin bir gösterimini öğrenen çizge gömülüm teknikleri, bu konuda güçlü bir alternatif sağlamakta ve söz konusu ağ için duyulan kapsamlı öznitelik mühendisliği ihtiyacını ortadan kaldırmaktadır. Bu çalışmada, biz çizge gömülme tekniklerini iki bağımsız makine öğrenmesi görevinde kullanıyoruz. Mevcut çalışmanın ilk kısmı, gen esaslılığını tahmin etmeye odaklanıyor. Bu bölümde, iki farklı düğüm gömülme tekniği, node2vec ve DeepWalk kullanarak, girdi olarak yalnızca düğüm gömülme kullanıldığında, insan genlerinin gerekliliğini tahmin etmede % 88'e varan AUC alabileceğini gösteriyoruz. Tezin ikinci kısmı, protein ifade değerlerinin çiftli sıralamaları ve protein etkileşimlerine dayalı, açılımını PRER olarak kısalttığımız özgün bir hasta gösterimi önermektedir. Daha spesifik olarak, proteinlerin ifade değerlerini kullanıyor ve bir proteinin kendi komşuluk bölgesindeki diğer proteinlerle nispi ifadesini temsil eden hastaya özgü bir gen gömülmesi üretiyoruz. Komşuluk bölgesi PPE ağında yanlı rastgele yürüme stratejisi kullanılarak türetiliyor. öncelikle, belirli bir proteinin spesifik bir tümör için komşuluk bölgesindeki diğer proteinlere kıyasla daha az veya daha fazla ifade edilip edilmediğini kontrol ediyoruz. Buna dayanarak, sadece proteinler arasındaki düzensizlik örüntülerini yakalayan değil, aynı zamanda moleküler etkileşimleri de hesaba katan bir gösterim üretiyoruz. Bu gösterimin etkinliğini test etmek için, PRER'i hasta sağkalım tahmin problemi için kullanıyoruz. Hastaların bireysel protein ifade özellikleriyle gösterimine kıyasla, PRER gösterimi 10 kanser türünden 8'inde istatiski olarak anlamlı bir şekilde üstün tahmin performansı gösteriyor. Bireysel ifade değerlerinin aksine PRER'de önemli olarak ortaya çıkan proteinler, yüksek prognostik değeri olan değerli bir biyobelirteç seti sağlıyor. Ek olarak, düzensizlik desenleri için daha fazla araştırılması gereken diğer proteinleri de vurguluyor.","Protein-protein interaction (PPI) networks represent the possible set of interactions among proteins and thereby the genes that code for them. By integrating isolated signals on single genes such as mutations or differential expression patterns, PPI networks have enabled various biological discoveries so far. Furthermore, even the connectivity patterns of proteins in such networks have been proven to be highly informative for various prediction tasks involving proteins or genes. These tasks; however, require task specific feature engineering. Graph embedding techniques that learn a deep representation of the nodes on the network, provides a powerful alternative and obviate the need for this extensive feature engineering on the network. In this study we use graph embedding techniques on PPI networks in two independent machine learning tasks. The first part of the present work focuses on predicting gene essentiality. Using two different node embedding techniques, node2vec and DeepWalk, we present a classifier which only uses node embeddings as input and show that it can achieve up to 88 % AUC score in predicting human gene essentiality. The second part of the thesis proposes a novel representation of patients based on pairwise rank order of patient protein expression values and protein interactions, which we abbreviate as PRER. Specifically, we use the protein expression values of proteins, and generate a patient specific gene embedding to represent relative expression of a protein with other proteins in the neighborhood of that protein. The neighborhood is derived using a biased random-walk strategy. We first check whether a given protein is less or more expressed compared to the other proteins in their neighborhood for a specific tumor. Based on this we generate a representation that not only captures the dysregulation patterns among the proteins but also accounts for the molecular interactions. To test the effectiveness of this representation, we use PRER for the problem of patient survival prediction. When compared against the representation of patients with their individual protein expression features, PRER representation demonstrates significantly superior predictive performance in 8 out of 10 cancer types. Proteins that emerge as important in the PRER as opposed to individual expression values provide a valuable set of biomarkers with high prognostic value. Additionally, they highlight other proteins that should be further investigated for the dysregulation patterns."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilişim sistemlerinin performansı son yıllarda, özellikle transistör boyutunun azalması ve saat hızının artması nedeniyle muazzam bir gelişme göstermiştir. Tek bir yonganın üzerine yerleştirilmiş milyarlarca transistör ve yüksek saat hızında anahtarlama, yonganın aşırı ısınmasına neden olmaktadır. Isı dağılımını artırmadan performans iyileştirme gereksinimi, çoklu çekirdeklerin ve/veya belleklerin yonga üzerindeki bir ağ üzerinden iletişim kurduğu çoklu çekirdek tasarımlarının kullanılmasına yol açmıştır. Ne yazık ki, bellek performansı, işlemci performansı ile aynı hızda gelişememiştir ve bu nedenle bellekler bir performans darboğazı haline gelmiştir. Diğer yandan, gerçek uygulamalarda değişen trafk düzenleri, bir rotalama algoritması tarafından gözetilen ağ verimliliğini kısıtlamaktadır. Bu tezde, işlemci-bellek performans farkı sorununu iki şekilde ele alıyoruz: Birincisi, gelişmiş ve yeni geliştirilen bellek teknolojilerini bir bilgisayar sisteminin bellek hiyerarşisinde birleştiriyoruz. İkincisi, yürütme platformunu gerekli mimari özelliklerle donatıyor ve derleyicinin bellek erişim talimatlarını paralel hale getirmesini sağlıyoruz. Ayrıca, bir Yonga üzeri Ağ (YüA)'nın rotalama algoritmasını, bir uygulamanın değişen trafk düzenine göre değiştiren bir seçim yöntemi önererek, ağ verimliliğinin kısıtlanması sorununu da ele alıyoruz. Yeni gelişen kalıcı bellek (Non-volatile memory - NVM) cihazlarının bir bilgisayar sisteminin bellek hiyerarşisinde, veri tabanı yönetim sistemleri (DBMS) bağlamında entegrasyonunu sunuyoruz. Bu amaçla, bir DBMS'nin depolama motorunda (SE), DBMS'nin fonksiyonlarının doğru çalışmasını etkilemeden, disk arayüzlerini atlayarak verilere hızlı erişim sağlayan değişiklikler öneriyoruz. Bir uygulama çalışması olarak, PostgreSQL'in SE'sini değiştiriyoruz ve bu değişikliğin gereksinimlerini ve zorlukları detaylandırıyoruz. Önerdiğimiz yaklaşımı kapsamlı bir emülasyon platformu kullanarak değerlendiriyoruz. Sonuçlar, disk ve NVM deposuyla karşılaştırıldığında, değiştirilmiş SE'mizin sorgu sürelerini sırasıyla %19 ve %4'lük bir ortalama azalışla, %45 ve %13'e kadar azalttığını göstermektedir. Bu sonuçların detaylı analizi, değiştirilmiş SE'mizin veri hazırlığı sorununa tâbi olduğunu göstermektedir. Bu sorunu çözmek için, basit bir uygulama programı arayüzü (API) yoluyla NVM donanımından belleğe önceden veri almak için yardımcı iş parçacıkları kullanan genel amaçlı bir kütüphane geliştirdik. Bu kütüphanemiz ile, değiştirilmiş SE için disk ve NVM depolamaya kıyasla sorgu sürelerinin sırasıyla ortalama %23 ve %8'lik bir düşüşle, %54 ve %17'ye kadar azaltılabildiği görülmüştür. İşlemci bellek performansı farkını azaltmak üzere ikinci bir yol olarak, belleğe bağlı duraklamaların azaltılmasını amaçlayan bir derleyici optimizasyonu öneriyoruz. Önerilen optimizasyon, bellek referanslarının sınıﬂandırılması yoluyla etkili bir sıralama oluşturmaktadır ve iki adımdan oluşmaktadır: ilginlik analizi ve ilginlik duyarlı sıralama. İlginlik analizi için iki farklı yaklaşımı öneriyoruz; kaynak kod ek açıklaması ve otomatik analiz. Deneysel sonuçlarımız, ek açıklama tabanlı yaklaşımın uygulanmasının, bellek erişimi yoğun bir programda duraklama döngülerini %67,44 azaltarak çalışma süresinde %25,61 kadar iyileşme sağladığını göstermektedir. Ayrıca 11 farklı görüntü işleme değerlendirme deneyi ile otomatik analiz yaklaşımımızı değerlendirdik. Deneysel sonuçlar, otomatik analizin duraklama döngülerini ortalama %69,83 oranında azalttığını göstermektedir. Tüm deneylerde hem hesaplama hem de bellek yoğun işlemler olduğu için, çalışma süresinde ortalama %5.79 iyileşme olmakla birlikte bu oran %30'a kadar çıkmaktadır. Ağ verimliliğini arttırmak için, rotalama algoritmasını değişen trafk düzenine göre değiştiren bir seçim yöntemi öneriyoruz. İki seçim stratejisi kullanıyoruz: statik ve dinamik seçim. Statik seçim devre dışı bırakıldığında, dinamik yaklaşım, rotalama algoritmasının seçimi için ağ tıkanıklığına ilişkin koşum zamanı bilgilerini kullanmaktadır. Deneysel sonuçlar, yöntemimizin gerçek uygulamalar için %37,49'a kadar verimliliği artırdığını göstermektedir. Bu tezin temel sonucu, bir bilgisayar sisteminin performansında iyileşme elde etmek için çok yönlü bir yaklaşıma ihtiyaç duyulduğudur, yani aynı anda hem bellek, hem de iletişim alt sisteminin performansının iyileştirilmesi gerektiğidir. İşlemciler ve bellekler arasındaki performans farkının azaltılması için yalnızca gelişmiş bellek teknolojilerinin sisteme entegrasyonu değil, aynı zamanda yazılım/derleyici desteği de gerekmektedir. Ayrıca, bir uygulamanın değişen trafk düzenine göre rotalama algoritmasının değiştirilmesinin, YüA veriminin artmasını sağladığı sonucuna varılmıştır.","Performance of computing systems has tremendously improved over last few decades primarily due to decreasing transistor size and increasing clock rate. Billions of transistors placed on a single chip and switching at high clock rate result in overheating of the chip. The demand for performance improvement without increasing the heat dissipation lead to the inception of multi/many core design where multiple cores and/or memories communicate through a network on chip. Unfortunately, performance of memory devices has not improved at the same rate as that of processors and hence become a performance bottleneck. On the other hand, varying trafc pattern in real applications limits the network throughput delivered by a routing algorithm. In this thesis, we address the issue of reducing processor-memory performance gap in two ways: First, by integrating improved and newly developed memory technologies in memory hierarchy of a computing system. Second, by equipping the execution platform with necessary architectural features and enabling its compiler to parallelize memory access instructions. We also address issue of improving network throughput by proposing a selection scheme that switches routing algorithm of an NoC with changing trafc pattern of an application. We present integration of emerging non-volatile memory (NVM) devices in memory hierarchy of a computing system in the context of database management systems (DBMS). To this end, we propose modifcations in storage engine (SE) of a DBMS aiming at fast access to data through bypassing the slow disk interfaces while maintaining all the functionalities of a robust DBMS. As a case study, we modify the SE of PostgreSQL and detail the necessary changes and challenges such modifcations entail. We evaluate our proposal using a comprehensive emulation platform. Results indicate that our modifed SE reduces query execution time by up to 45% and 13% when compared to disk and NVM storage, with average reductions of 19% and 4%, respectively. Detailed analysis of these results shows that our modifed SE suﬀers from data readiness problem. To solve this, we develop a general purpose library that employs helper threads to prefetch data from NVM hardware via a simple application program interface (API). Our library further improves query execution time for our modifed SE when compared to disk and NVM storage by up to 54% and 17%, with average reductions of 23% and 8%, respectively. As a second way to reduce processor-memory performance gap, we propose a compiler optimization aiming at reduction of memory bound stalls. The proposed optimization generates efcient instruction schedule through classifcation of memory references and consists of two steps: afnity analysis and afnity-aware instruction scheduling. We suggest two diﬀerent approaches for afnity analysis, i.e., source code annotation and automated analysis. Our experimental results show that application of annotation-based approach on a memory intensive program reduces stall cycles by 67.44%, leading to 25.61% improvement in execution time. We also evaluate automated-analysis approach using eleven diﬀerent image processing benchmarks. Experimental results show that automated-analysis reduces stall cycles, on average, by 69.83%. As all benchmarks are both compute and memory-intensive, we achieve improvement in execution time by up to 30%, with a modest average of 5.79%. In order to improve network throughput, we propose a selection scheme that switches routing algorithm with changing trafc pattern. We use two selection strategies: static and dynamic selection. While static selection is made oﬀ-line, dynamic approach uses run-time information on network congestion for selection of routing algorithm. Experimental results show that our proposal improves throughput for real applications up to 37.49 %. They key conclusion of this thesis is that improvement in performance of a computing system needs multifaceted approach i.e., improving the performance of memory and communication subsystem at the same time. The reduction in performance gap between processors and memories requires not only integration of improved memory technologies in system but also software/compiler support. We also conclude that switching routing algorithm with changing trafc pattern of an application leads to improvement of NoC throughput."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tensör ayrıştırımının yapısal hasar algılama, ağlarda anormallik saptanması gibi bir çok uygulaması vardır. Tensör ayrıştırımının en çok zaman alan parçası ma- trisleştirilmiş tensör ile Khatri-Rao çarpımı (MTTKRP) adı verilen çekirdek kod parçasıdır. Bu tezde, FPGA kullanarak MTTKRP kod parçasının çalıştırılması üzerine güncel uygulamalarla karşılaştırılabilir bir uygulama yapılabileceği göster- ilmiştir. Bu hedefe ulaşmak için Vivado HLS kullanılarak tek bir döngüden oluşan düz bir tasarım geliştirilmiştir. Büyük tensörlerin kısıtlı BRAM kapasitesi ile işlenebilmesi için parçalara bölme yöntemi kullanılmıştır. Parçalara bölme işlem- inin performansı olumsuz etkilediğinin gösterilmesine rağmen, tensörleri işlemek için yeterli en az bölme kullanıldığında 3.40'a kadar hızlanma gözlemlenmiştir.","Tensor factorization has many applications such as network anomaly detection, structural damage detection and music genre classification. Most time consuming part of the CPD-ALS based tensor factorization is the Matricized Tensor Times Khatri-Rao Product (MTTKRP). In this thesis, the goal was to show that an FPGA implementation of the MTTKRP kernel can be comparable with the state of the art software implementations. To achieve this goal, a flat design consisting of a single loop is developed using Vivado HLS. In order to process the large ten- sors with the limited BRAM capacity of the FPGA board, a tiling methodology with optimized processing order is introduced. It has been shown that tiling has a negative impact on the general performance because of increasing DRAM access per subtensor. On the other hand, with the minimum tiling possible to process the tensors, the FPGA implementation achieves up to 3.40 speedup against the single threaded software."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğal dil işlemede (DDİ) yaygın bir yöntem olan kelime temsilleri, kelimelerin anlamsal özelliklerini yoğun vektörler kullanarak temsil etmek için sıklıkla kullanılmaktadır. Çok sayıda DDİ uygulamasında elde edilen en iyi performansları sağladıklarından popülerlikleri giderek artmıştır. Kelime temsilleri özellikle kelimeler arasındaki anlamsal ilişkileri yakalamakta başarılı olduklarından, bu temsil uzayları içlerinde anlamlı bir semantik yapı barındırmalıdırlar. Ancak genellikle bu anlamsal yapı uzayın boyutları arasında heterojen bir şekilde dağılmaktadır. Başka bir ifadeyle, kelimelere karşılık gelen vektörler sadece birbirlerine göre anlam taşırlar. Bir kelime vektörünün ve bu vektörün boyutlarının tek başına mutlak bir anlamı yoktur ve bu durum boyutların yorumlanmasını zorlaştırmaktadır. Bu tezde, yoğun kelime temsil uzaylarında altta yatan saklı anlamsal yapıyı ortaya çıkarmak için istatistiksel bir yöntem önerilmiştir. Buna ek olarak, kelime temsil uzaylarının yorumlanabilirlik düzeylerini sayısal olarak ölçmeye yarayan bir yöntem önerilmiştir. Önerilen yöntem, literatürde yorumlanabilirliği ölçmek için kullanılan ve insan değerlendirmesine gereksinim duyan kelime ihlal testine pratik bir alternatif olma potansiyeline sahiptir. Ayrıca, orijinal öğrenme mekanizmasını etkilemeden kelime temsillerinin yorumlanabilirliklerini arttırmak amacıyla, GloVe kelime temsil algoritmalasının amaç fonksiyonuna yeni bir terim eklenmiştir. Eklenen terim, önceden tanımlanan konular ile anlamsal olarak ilişkili olan kelimelerin vektörlerinin temsil uzayının belirli boyutlarında yüksek değerler almasını sağlamaktadır. Kavram gruplarını oluşturmak amacıyla Roget's Thesaurus kaynak olarak kullanılmıştır. Elde edilen kavram gruplarının içerisindeki kelimelerin vektörlerinin temsil uzayının belirli boyutlarında yüksek değerler almaları sağlanmıştır. Önerilen yöntemin kelime temsil uzayının yorumlanabilirliğini, uzayın anlamsal yapısına zarar vermeden, önemli derecede arttırdığı yapılan ayrıntılı değerlendirme ve ölçümler ile gösterilmiştir. Ayrıca önerilen yöntemin uygun kavram grupları ile beraber kullanıldığında denektaşı sınamalarında önemli performans artışı sağladığı ve kelime temsillerinde bulunan cinsiyet önyargısını düşürdüğü gösterilmiştir.","As an ubiquitous method in natural language processing, word embeddings are extensively employed to map semantic properties of words into a dense vector representations. They have become increasingly popular due to their state-of-the-art performances in many natural language processing (NLP) tasks. Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces. However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions. In other words, vectors corresponding to the words are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute meaning, making interpretation of dimensions a big challenge. We propose a statistical method to uncover the underlying latent semantic structure in the dense word embeddings. To perform our analysis, we introduce a new dataset (SEMCAT) that contains more than 6,500 words semantically grouped under 110 categories. We further propose a method to quantify the interpretability of the word embeddings that is a practical alternative to the classical word intrusion test that requires human intervention. Moreover, in order to improve the interpretability of word embeddings while leaving the original semantic learning mechanism mostly unaffected, we introduce an additive modification to the objective function of the embedding learning algorithm, GloVe, that promotes the vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension. We use Roget's Thesaurus to extract concept groups and align the words in these groups with embedding dimensions using modified objective function. By performing detailed evaluations, we show that proposed method improves interpretability drastically while preserving the semantic structure. We also demonstrate that imparting method with suitable concept groups can be used to significantly improve performance on benchmark tests and to measure and reduce gender bias present in the word embeddings."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom dizileme ve analizinde gelişen teknolojiyle birlikte, insan genomundaki değişimlerin ana kaynağının yapısal varyasyonlar olduğu belirginleşti. Çeşitli hastalıklara yol açması bakımından önemli olmalarına rağmen, halen bütün tip ve uzunluktaki yapısal varyasyonları bulan bir algoritma bulunmamaktadır. Yapısal varyasyon keşfi problemli bir alan olarak kalmıştır çünkü sıklıkla segmental duplikasyonların, DNA üzerinde birden fazla bulunan yüksek oranda benzerlik gösteren segmentler, üzerinde bulunmaktadır. Araştırmacılar bütün genomun yaklaşık %5'ine karşılık gelen bu bölgeleri araştırmalarda karmaşıklık getireceği için kapsam dışında bırakmıştır. Sadece birazı bu alanlarda çalışma yapmıştır, ancak onlar da okumaların birden fazla lokasyonla eşleştirildiği özel bir dizi hizalama dosyası kullanmıştır. Bu çalışmada okumaların tek bir lokasyonla eşleştirildiği dizi hizalama dosyası kullanarak segmental duplikasyonlarda paralog-özgü kopya sayısı varyasyonu keşfi yapan ParaCoND algoritmasını sunuyoruz. Tekrarlı bölgelerdeki paralogları birbirinden ayıran tekli özel nükleotidleri (TÖN) kullanıyoruz. Metodumuz okuma derinliğine dayanmaktadır ve sadece duplikasyon ve silme işlemlerini tespit etmekle sınırlıdır. Genlerin mutlak kopya sayılarını sadece TÖN okuma derinliğini kullanarak hesapladık. Ayrıca, aynı segmental kopyada bulunan genler için paraloga özgü mutlak kopya numaralarını da hesapladık.","With the advancing technology in genome sequencing and analysis, it has become evident that the structural variations are the main source of alteration in human genome. Despite their significance in understanding disease susceptibility, there is no algorithm yet to find all types and sizes of structural variations at once. Structural variation discovery remained problematic since they often overlap with the segmental duplications, nearly identical segments of DNA that appear more than once in the genome. Researchers often excluded these regions that made up ~5% of the genome because of the complexity it brings to their studies. Only few of them are working in these regions, however, they require a special sequence alignment file where reads are mapped to multiple locations. Here, we present ParaCoND to discover paralog specific gene copy number within segmental duplications using a sequence alignment file with unique mapping. We utilize the singly unique nucleotides (SUN) that distinguish paralogs from each other in the sequence alignment of the duplicated regions. Our method is based on read depth and is limited to detect only duplications and deletions. We computed the absolute copy numbers of genes using only read depth of SUN. Furthermore, we also computed the paralog specific absolute copy numbers for genes residing in the same segmental duplication."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kestirimci Bakım, arıza kaynaklı sistem kesintilerini en aza indirgeyerek bakım maliyetlerinin azaltılmasını amaçlamaktadır. Erken tanı sistemleri, arızalar konusunda önlem almak için zamanında alarm oluşturulmasına olanak sağlamaktadır. Arıza teşhisi için önemli bir zorluk, yetersiz veri örneğine sahip olmaktır, ancak Transfer Öğrenimi yaklaşımları kısıtlı bir eğitim verisi olması sorununu hafifletebilmektedir. Bu çalışmada, hedeflenen ekipman ve kaynak ekipman arasında ilişki kurularak, kaynak ekipman üzerinde öğrenilen bozulma bilgileri hedef ekipmana transfer öğrenimi ile aktarılmaktadır. Aktarım yapılabilmesi için ekipmanlar arasında ortak benzerlik kümesi oluşturulması gerekmektedir. Bu küme, ekipmanların Kalan Yararlı Ömür niteliği üzerinden elde edilmektedir. Ortak benzerlik kümesinde; hedef ekipmandan farklı ölçümlere sahip olan bir kaynaktan bilgi aktarılmaktadır. Öğrenilen ortak nitelik kümelerinde, model farklı ekipmanların geniş miktarda verisine göre eğitmektedir ve öğrenilen bilgi, hedef ekipmanın arıza teşhisi için kullanılmaktadır. Çalışmada, kısıtlı veri olması durumunda erken arıza tahmini için transfer öğrenme ile güvenilir bir model elde edilmesi amaçlanmaktadır.","Breakdown prediction of equipment is an essential task considering the management of resources and maintenance operations. Early diagnosis systems allow creating alerts on time for taking precautions on production. A significant challenge for diagnosis is to have an insufficient size of data, yet, transfer learning approaches can alleviate such an issue when there is a constrained supply of training data. We intend to improve the reliability of breakdown prediction when there is a limited quantity of training data. We recommend similarity correlation on Remaining Useful Life of these equipment. To do this, we offer learning a common feature space between the target and the source equipment, where we acquire prior knowledge from the source that has different measurements than the target. Within the learned joint feature matrices, we train our model on the vast amount of data of different equipment and finetune it using the data of our target equipment. In this way, we aim to obtain an accurate and reliable model for early breakdown prediction."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Matris çarpanlarına ayırma gerçek dünya verilerinin gizli özelliklerini ortaya çıkarmak için kullanılan verimli bir tekniktir. Bu teknik, metin madenciliği, görüntü analizi, sosyal ağlar ve son zamanlarda yaygın olarak öneri sistemleri gibi alanlarda uygulanmaktadır. Birbirini izleyen en küçük karaler (ALS), olasılıksal egim iniş (SGD) ve koordinat iniş (CD) genis matrisleri çarpanlarına ayırırken kullanılan yöntemler arasındadır. Bu üç yöntem arasında, SGD'ye dayalı çarpanlarına ayırma yöntemi, Netflix ve KDDCup yarısmalarından sonra en basarılı yöntem olarak ispatlanmıştır. Sonrasında, SGD'nin paralelleştirilmesi yaygınlasmış ve literatürde geniş bir biçimde çalışılmıştır. Biz paylaşımlı ve dağıtık bellek sistemleri için geliştirilmiş paralel SGD algoritmalarına odaklanıyoruz. Paylaşımlı bellek paralelleştirmeleri HogWild, FPSGD ve MLGF-MF gibi çalışmalar içerirken dağıtık bellek paralelleştirmeleri DSGD, GASGD ve NOMAD gibi çalısmalar içermektedir. Biz bu çalışmaların detaylı analizini içeren bir araştırma metni oluşturuyoruz, sonrasında ayrıntılı olarak DSGD'ye odaklanıp bu algoritmayı mesaj aktarma yaklaşımı ile uyguluyoruz ve performansını yakınsama ve hızlanma yönünden test ediyoruz. Mevcut çalışmaların aksine deneylerde kendi ürettiğimiz çok sayıda gerçek dünya veri kümeleri kullanıyoruz. DSGD'nin geniş ölçekli veri kümeleri için dirençli bir algoritma olduğunu ve hızlı yakınsama değerleri ile birlikte doğrusala yakın hızlanmayı başardığını gösteriyoruz.","Matrix factorization is an efficient technique used for disclosing latent features of real-world data. It finds its application in areas such as text mining, image analysis, social network and more recently and popularly in recommendation systems. Alternating least squares (ALS), stochastic gradient descent (SGD) and coordinate descent (CD) are among the methods used commonly while factorizing large matrices. SGD-based factorization has proven to be the most successful among these methods after Netflix and KDDCup competitions where the winners' algorithms relied on methods based on SGD. Parallelization of SGD then became a hot topic and studied extensively in the literature in recent years. We focus on parallel SGD algorithms developed for shared memory and distributed memory systems. Shared memory parallelizations include works such as HogWild, FPSGD and MLGF-MF, and distributed memory parallelizations include works such as DSGD, GASGD and NOMAD. We design a survey that contains exhaustive analysis of these studies, and then particularly focus on DSGD by implementing it through message-passing paradigm and testing its performance in terms of convergence and speedup. In contrast to the existing works, many real-wold datasets are used in the experiments that we produce using published raw data. We show that DSGD is a robust algorithm for large-scale datasets and achieves near-linear speedup with fast convergence rates."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Konuşabilen insan karakterler bilgisayar animasyonunda çeşitli mesajları iletme amacıyla sıklıkla kullanılmaktadır. Bu karakterlerin görünüşü, hareketi ve sesi kişiliklerinin algılanmasında etkili olmaktadır. Beden dili, yüz ifadesi ve seslendirme gibi insan iletişiminin farklı kanallarını analiz ederek, tutarlı bir kişilik sergileyen animasyon tasarlamak mümkündür. Bu işlem mesajı pekiştirip sanal karakterin gerçekçiliğini artıracaktır. OCEAN kişilik modelini kullanarak, sırasıyla son animasyonu oluşturan, hareket ve ses dönüştürücülerine eşlenen dahili aracı parametreleri tasarlıyoruz. Laban Hareket Analizi ve Sözsüz İletişim İşaretleri, her karede kemik dönme açıları ve yüz şekil anahtar değerlerini üreten işlemler için kullanılır. Kişilik ve konuşulan metin arasındaki ilişkiler ve kişilik ile ses özellikleri arasındaki bağlantılar bütüncül aracı davranışı için ilave edilmiştir. Birden fazla animasyon dönüştürme algoritması ve kişilik tabanlı bir diyalog seçme metodu tanıtıldı. Geliştirilen konuşabilen aracı pasaport kontrolü ve hazır yemek siparişi dahil farklı senaryolarda test edildi. Kullanıcı sesi yazıya dönüştüren bir uygulama programlama arayüzü sayesinde diyalog akışını kontrol eder. Kayıt edilen etkileşimler Amazon Mechanical Turk sistemi kullanılarak değerlendirildi. Aracı kişiliğini hakkında çoklu ifadeler toplum tarafından derecelendirildi. Her deneyde, bir kişilik parametresi uç noktaya ayarlanırken diğerleri nötr kalır ve algı üzerinde bir etki beklenir.","Conversational human characters are heavily used in computer animation to convey various messages. Appearance, movement and voice of such characters influence their perceived personality. Analyzing different channels of human communication, including body language, facial expression and vocalics, it is possible to design animation that exhibit consistent personality. This would enhance the message and improve realism of the virtual character. Using OCEAN personality model, we design internal agent parameters that are mapped into movement and sound modifiers, which in turn produce the final animation. Laban Movement Analysis and Nonverbal Communication Cues are used for the operations that output bone rotations and facial shape key values at each frame. Correlations between personality and spoken text, and relations between personality and vocal features are integrated to introduce compherensive agent behavior. Multiple animation modification algorithms and a personality based dialogue selection method is introduced. Resulting conversational agent is tested in different scenarios, including passport check and fastfood order. Using a speech to text API user controls the dialog flow. Recorded interactions are evaluated using Amazon Mechanical Turk. Multiple statements about agent personality are rated by the crowd. In each experiment, one personality parameter is set to an extreme while others remain neutral, expecting an effect on perception."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin öğrenme modelleri, düşük ve orta seviye desenleri ve üst seviye anlambilimden oluşan zengin kümeleri öğrenmek için büyük ölçekli veri kümelerini gerektirir. Bu nedenle, yüksek kapasiteli bir sinir ağı göz önüne alındığında, bir modelin performansını iyileştirmenin bir yolu, modelin üzerinde çalıştığı veri kümesinin boyutunu artırmaktır. Bir ağı eğitmek için gereken hesaplama gücü miktarını elde etmenin kolay olduğu göz önüne alındığında, veriler mevcut makine öğrenme çözümlemelerinin yükseltilmesinde ciddi bir tıkanıklık haline gelir. Bu tezde, bilgisayar görme uygulamalarında yükselen iki ana veri darboğazını inceliyoruz: (I) çeşitli nesne kategorileri kümesinde eğitim verisi bulma zorluğu, (II) sinir ağı modellerini eğitim amacıyla hassas kullanıcı bilgisi içeren verilerin kullanılmasının zorluğu. Bu konuları ele almak için sırasıyla sıfır atış öğrenme ve merkezi olmayan öğrenme şemalarını inceliyoruz. Sıfır atış öğrenme, denetimli ve sıfır atış sınıfları arasındaki dağılım farklılıkları nedeniyle denetimsiz öğrenme yoluyla önemli ilerlemenin potansiyel olarak gerçekleştirilebileceği en umut verici sorunlardan biridir. Bu nedenle, birkaç çalışma, ayrımcı alan uyarlama tekniklerinin sıfır atış öğrenmeye dahil edilmesini araştırmaktadır; ancak bu, sıfır atış öğrenmenin doğruluğunda mütevazi gelişmelere yol açmaktadır. Buna karşılık, denetlenmeyen örneklerden doğal olarak öğrenebilecek ve görünmeyen sınıflara yönelik eğitim örneklerini yalnızca sınıf gömülerine dayanarak sentezleyen bir üretici modeli öneriyoruz. Bu nedenle, sıfır atış öğrenme problemini denetlenen bir sınıflandırma görevine indirgiyoruz. Önerilen yaklaşım iki önemli bileşenden oluşmaktadır: (I) denetlenmemiş veri örneklerinin özelliklerini taklit eden örnekler üretmeyi öğrenen koşullu bir üretici grup ağı ve (II) Sentezlenen örneklerden elde edilen gradyan sinyalinin kalitesini ölçen gradyan eşleşmesi kaybı. Gradyan eşlemesi kayıp formülasyonumuzu kullanarak, üreticiyi doğru sınıflandırıcıların eğitilebileceği örnekler üretmesi için zorlarız. Çeşitli sıfır atış öğrenme ölçütü veri seti üzerinde elde edilen deneysel sonuçlar, yaklaşımımızın gelişmiş sıfır atış öğrenme modellerine göre genelleştirilmiş sıfır atış sınıflandırmada önemli gelişmelere yol açtığını gösteriyor. İşbirlikçi öğrenme teknikleri, sahipleri tarafından paylaşılmayan bazı özel veri setleri üzerinde eğitim sağlayarak gizlilik koruma çözümü sunar. Bununla birlikte, son zamanlarda, mevcut işbirlikçi öğrenme çerçevelerinin üretken bir çekişmeli ağ saldırısı yapan aktif bir düşmana karşı savunmasız olduğu gösterilmiştir. Bu çalışmada, bu tür saldırılara karşı tasarım açısından dayanıklı yeni bir sınıflandırma modeli öneriyoruz. Daha spesifik olarak, bir çekişmeli üretken ağ saldırısı için gerekli bilgileri etkin bir şekilde saldırgandan saklayan, sınıfa özgü özel anahtarlar kullanarak sınıf puanlarını koruyan bir anahtar tabanlı sınıflandırma modeli ve ilkeli bir eğitim programı sunuyoruz. Ayrıca, model karmaşıklığını arttırmadan saldırılara karşı sağlamlığı artırmak için yüksek boyutlu anahtarların nasıl kullanılacağını gösteriyoruz. Detaylı deneylerimiz önerilen tekniğin etkinliğini göstermektedir.","Deep learning models require large-scale datasets to learn rich sets of low and mid-level patterns and high-level semantics. Therefore, given a high-capacity neural network, one way to improve the performance of a model is increasing the size of the dataset which the model is trained over. Considering that it is easy to get the amount of computational power required to train a network, data becomes a serious bottleneck in scaling up the existing machine learning pipelines. In this thesis, we look into two main data bottlenecks rise in computer vision applications: I. the difficulty of finding training data for diverse sets of object categories, II. the complication of utilizing data containing sensitive user information for the purpose of training neural network models. To address these issues, we study zero-shot learning and decentralized learning schemes, respectively. Zero-shot learning (ZSL) is one of the most promising problems where substantial progress can potentially be achieved through unsupervised learning, due to distributional differences between supervised and zero-shot classes. For this reason, several works investigate the incorporation of discriminative domain adaptation techniques into ZSL, which, however, lead to modest improvements in ZSL accuracy. In contrast, we propose a generative model that can naturally learn from unsupervised examples, and synthesize training examples for unseen classes purely based on their class embeddings, and therefore, reduce the zero-shot learning problem into a supervised classification task. The proposed approach consists of two important components: I. a conditional Generative Adversarial Network that learns to produce samples that mimic the characteristics of unsupervised data examples, and II. the Gradient Matching (GM) loss that measures the quality of the gradient signal obtained from the synthesized examples. Using our GM loss formulation, we enforce the generator to produce examples from which accurate classifiers can be trained. Experimental results on several ZSL benchmark datasets show that our approach leads to significant improvements over the state of the art in generalized zero-shot classification. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hides the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mevcut literatürde, tam evrişimsel sinir ağları (FCN'ler), bez bölütleme de dahil olmak üzere yoğun tahmin işleri için en çok tercih edilen mimarilerdir. Öte yandan, öğrenmesi zor pikselleri doğru şekilde tahmin etmek için bu ağları yeterince eğitmek önemli bir zorluktur. Bu amaçla geliştirilmiş ek stratejiler olmadan, ağlar veri setinin zayıf genellemelerini öğrenmeye meyillidir. Buna neden eğitim sırasında ağların kayıp fonksiyonlarına veri setindeki en yaygın ve öğrenmesi kolay piksellerin hakim olabilmesidir. Bez örneği bölütleme işindeki sınır ayrımı problemi bu duruma tipik bir örnektir. Bezler birbirine çok yakın olabilir ve sınır bölgeleri nispeten az piksel içerdiğinden, bu bölgeleri öğrenmek ve bez örneklerini ayırmak daha zordur. Bu ayırma, bez örneği bölütleme işi için esas olduğundan; bu durum, sonuçlarda büyük dezavantajlara yol açar. Bahsedilen sınır ayırma problemi için, bu piksellerin bağıl kayıp katkısını artırarak veya sınır algılamayı mimariye ek bir görev olarak ekleyerek ağ eğitimi sırasında sınır piksellerine verilen dikkatin artırılması önerilmiştir. Her ne kadar bu teknikler bezlerin sınırlarını daha iyi ayırmaya yardımcı olsa da, görüntülerde çoğunlukla gürültü ve artifaktlara bağlı başka öğrenmesi zor piksel türleri (ve bundan dolayı başka hata türleri) olabilir. Ancak, ağların her türlü hataya karşı eğitilmesi için uygun dikkatin açıkça ayarlanması mümkün değildir. Bunu motivasyonla, daha etkili bir çözüm olarak, bu tez uyarlamalı güçlendirmeye dayanan yinelemeli bir dikkat öğrenme modeli önermektedir. Önerilen bu AttentionBoost modeli; önceden bir varsayımda bulunulmadan, doğrudan görüntü verileri üzerinde eğitilen çok aşamalı bir yoğun tahmin ağıdır. Bu ağın uçtan uca eğitimi sırasında, her aşama, önceki aşamaların hatalarına bağlı olarak, her görüntüdeki her piksel için tahminin önemini ayarlar. Bu şekilde, her aşama, kendisini önceki aşamaların hatalarını öğrenmeye zorlayan farklı bir dikkatle ilgili işi öğrenir. Bez örneği bölütleme işi üzerinde yapılan deneyler, modelimizin literatürdeki yaklaşımlardan daha iyi sonuçlar elde edebildiğini göstermiştir.","In the current literature, fully convolutional neural networks (FCNs) are the most preferred architectures for dense prediction tasks, including gland segmentation. However, a significant challenge is to adequately train these networks to correctly predict pixels that are hard-to-learn. Without additional strategies developed for this purpose, networks tend to learn poor generalizations of the dataset since the loss functions of the networks during training may be dominated by the most common and easy-to-learn pixels in the dataset. A typical example of this is the border separation problem in the gland instance segmentation task. Glands can be very close to each other, and since the border regions contain relatively few pixels, it is more difficult to learn these regions and separate gland instances. As this separation is essential for the gland instance segmentation task, this situation arises major drawbacks on the results. To address this border separation problem, it has been proposed to increase the given attention to border pixels during network training either by increasing the relative loss contribution of these pixels or by adding border detection as an additional task to the architecture. Although these techniques may help better separate gland borders, there may exist other types of hard-to-learn pixels (and thus, other mistake types), mostly related to noise and artifacts in the images. Yet, explicitly adjusting the appropriate attention to train the networks against every type of mistake is not feasible. Motivated by this, as a more effective solution, this thesis proposes an iterative attention learning model based on adaptive boosting. The proposed AttentionBoost model is a multi-stage dense segmentation network trained directly on image data without making any prior assumption. During the end-to-end training of this network, each stage adjusts the importance of each pixel-wise prediction for each image depending on the errors of the previous stages. This way, each stage learns the task with different attention forcing the stage to learn the mistakes of the earlier stages. With experiments on the gland instance segmentation task, we demonstrate that our model achieves better segmentation results than the approaches in the literature."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge uygulamaları, yaygın kullanım alanları ve ele aldıkları veri miktarları ile gittikçe daha fazla önem kazanmaktadır. Biyolojik ve sosyal web çizgeleri, çizge analitik uygulamalarının ve problemlerinin verimli işlenmesinin önemini gösteren bilinen örneklerdir. Bu sorunların verimli bir şekilde ele alınması kolay bir iş değildir. Hesaplamanın dağıtılması ve paralel hale getirilmesi ve donanım hızlandırıcılarının eklenmesi, son on yılda denenen ana yaklaşımlardır. Bununla birlikte, bu yaklaşımlar temel olarak belirli eski algoritmalara odaklanır ve sorunları tamamen çözemeyebilir. Bu nedenle, belirli bir sorunu hedefleyen yeni bir algoritmaya ihtiyaç duyulduğunda, geliştirici, uygulamayı paralelleştirmek ve hızlandırmak için dağıtım, paralelleştirme tekniklerinin ve donanım özelliklerinin üstesinden gelmek zorundadır. Önerilen kaynaktan kaynağa temelli metodolojimiz, düğüm merkezli herhangi bir C++ çizge uygulamasını boruhattı bazlı SystemC modeline çevirerek paralellik ve dağıtımın düşük seviyeli ayrıntılarını bilmeme özgürlüğünü verir. Farklı çizge uygulama türlerini desteklemek için standart dışı uygulama desteği, etkin set fonksiyonu, çoklu boru hattı desteği gibi çeşitli özellikleri uyguladık. Üretilen SystemC modeli, Üst Düzey Sentez (HLS) araçları ile sentezlenebilir; FPGA programlama görüntüsü, yani bitstream oluşturabilir. Hızlandırıcı geliştirme akışımız, yüksek verimli (HT) ve iş verimli (WE) olmak üzere iki farklı uygulama modeli üretebilir. Algoritmalar OpenCL benzerleri ile karşılaştırıldığında, HT ve WE modellerinin yürütme süresi ve verimi bakımından biraz daha iyi performans gösterdiği görülmektedir. WE modeli, yapılan iş ve uygulama süresi açısından OpenCL'den yaklaşık %40 daha iyi performans göstermiştir. Bu nedenle, önerilen kaynaktan kaynağa temelli metodoloji, kullanıcıdan sadece basit bir üst düzey dil tanımı gerektirerek daha verimli donanım tasarımları sağlayabilmektedir.","Graph applications are becoming more and more important with their widespread usage and the amounts of data they deal with. Biological and social web graphs are well-known examples which show the importance of efficient processing of the graph analytic applications and problems. Addressing those problems in an efficient manner is not a straightforward task. Distributing and parallelizing the computation, and integrating hardware accelerators are the main approaches that were tried during the last decade. However, these approaches mainly focus on specific legacy algorithms and may not completely solve the problems. Therefore, when there is an emerging need for a non-legacy algorithm targeting a specific problem, the developer has to cope with the adversaries of the distribution, parallelization techniques, and hardware specifications to parallelize and accelerate the application. Our proposed source-to-source based methodology gives the freedom of not knowing the low-level details of parallelization and distribution by translating any vertex-centric C++ graph application into pipelined SystemC model. In order to support different types of graph applications, we have implemented several features like non-standard application support, active set functionality, multi-pipeline support, etc. The generated SystemC model can be synthesized by High-Level Synthesis (HLS) tools to obtain the FPGA programming image, i.e., the bitstream. Our accelerator development flow can generate two different execution models, high-throughput (HT) and work-efficient (WE). Compared to OpenCL counterparts of the algorithms, HT and WE models perform slightly better in terms of execution time and throughput. WE model performed approximately 40% better than OpenCL in terms of work done and execution time. Therefore, the proposed source-to-source based methodology is able to provide more efficient hardware designs by only requiring a simple high-level language description from the user."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kişilik özelliklerinin tanınmasının gerekli olduğu kişilik hesaplama ve duygusal hesaplama, son zamanlarda bir çok araştırma alanında artan ilgi ve dikkate sahip olmuştur. Kişilik özellikleri, Beş Faktörlü Model tarafından beş boyutta tanımlanmaktadır: açıklık, sorumluluk, dışadönüklük, uyumluluk, ve duygusallık. Biz, insanların bu beş kişilik özelliklerini videolardan tanımak için yeni bir yaklaşım öneriyoruz. Kişilik ve duygu, konuşma tarzını, yüz ifadelerini, vücut hareketlerini ve sosyal bağlamdaki dilsel faktörleri etkiler ve ayrıca çevresel unsurlardan etkilenir. Bu nedenle, yüz, çevre, ses ve çevriyazı özellikleri gibi çeşitli kiplere dayanan belirgin kişilik özelliklerini tanımak için çok kipli bir sistem geliştiriyoruz. Yöntemimizde, özellikleri bağımsız olarak tanımayı öğrenen kipe özgü sinir ağları kullanıyoruz ve son bir belirgin kişilik tahminini bu ağların öznitelik düzeyinde bir kaynaşımı ile elde ediyoruz. Yüksek düzey öznitelikleri bulmak için ResNet ve VGGish ağları gibi önceden eğitilmiş derin evrişimli sinir ağlarını ve zamansal bilgiyi bütünleştirmek için uzun kısa-süreli bellek ağlarını kullanıyoruz. Kipe özgü alt ağlardan oluşan büyük modeli, iki-aşamalı bir eğitim yöntemi ile eğitiyoruz. İlk önce alt ağları ayrı olarak eğitiyoruz, ardından, genel modele bu eğitilmiş ağları kullanarak ince ayar yapıyoruz. Önerilen yöntemi ""ChaLearn First Impressions V2 challenge"" veri setini kullanarak değerlendiriyoruz. Yaklaşımımız, beş kişilik özelliklerinin ""ortalama doğruluk"" puanlarının ortalaması alındığında literatürdeki yöntemlere göre en iyi sonuçları elde etmektedir.","Personality computing and affective computing, where recognition of personality traits is essential, have gained increasing interest and attention in many research areas recently. The personality traits are described by the Five-Factor Model along five dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. We propose a novel approach to recognize these five personality traits of people from videos. Personality and emotion affect the speaking style, facial expressions, body movements, and linguistic factors in social contexts, and they are affected by environmental elements. For this reason, we develop a multimodal system to recognize apparent personality traits based on various modalities such as the face, environment, audio, and transcription features. In our method, we use modality-specific neural networks that learn to recognize the traits independently and we obtain a final prediction of apparent personality with a feature-level fusion of these networks. We employ pre-trained deep convolutional neural networks such as ResNet and VGGish networks to extract high-level features and Long Short-Term Memory networks to integrate temporal information. We train the large model consisting of modality-specific subnetworks using a two-stage training process. We first train the subnetworks separately and then fine-tune the overall model using these trained networks. We evaluate the proposed method using ChaLearn First Impressions V2 challenge dataset. Our approach obtains the best overall ""mean accuracy"" score, averaged over five personality traits, compared to the state-of-the-art."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalabalık benzetimleri, kalabalıkların ve içerdikleri ajanların bireysel davranışlarını, kişilik ve görünüşlerini örnek alarak, bir çoklu-ajan sisteminin genel modelini tanımlar. Bu tür çalışmalarda, modeller değerlendirilirken genellikle gerçek hayattaki senaryolarla karşılaştırılır. Ancak yan yana karşılaştırma ve güzergah analizleri dışında, verilen herhangi bir modelin gerçek bir senaryonun ne derece başarılı şekilde benzetimini yaptığını doğrulayan, pratik ve doğrudan kullanılabilir araçlar bulunmamaktadır. Bu çalışmada, yapay ajanları gerçek kalabalık videolarına eklemek için bir sistem öneriyoruz. Önerilen sistem ilk olarak kalabalık videosundan otomatik olarak elde ettiği yaya tespit bilgilerini kullanarak, yer yüzeyindeki gezilebilir alanları bulmaktadır. Sonrasında gerçek yayaların üç-boyutlu (3B) modellerini 3B sahneye yerleştirmektedir. Tespit edilen gerçek yayalarla çarpışma önleme algoritmaları kullanılarak beraber benzetimi yapılan yapay ajanlar, kullanıcılara sunulan etkileşimli bir kullanıcı arayüzü aracılığıyla eklenip, kontrol edilebilmektedir.","Crowd simulations imitate the behavior of crowds and individual agents in the crowd with personality and appearance, which determines the overall model of a multi-agent system. In such studies, the models are often compared with real-life scenarios for assessment. Yet apart from side-by-side comparison and trajectory analysis, there are no practical, out-of-the-box tools to test how a given arbitrary model simulate the scenario that takes place in the real world. We propose a framework for augmenting virtual agents in real-life crowd videos. The framework locates the navigable areas on the ground plane using the automatically-extracted detection data of the pedestrians in the crowd video. Then it places the three-dimensional (3D) models of real pedestrians in the 3D model of the scene. An interactive user interface is provided for users to add and control virtual agents, which are simulated together with detected real pedestrians using collision avoidance algorithms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birbiriyle yakından ilişkili iki probleme odaklanıyoruz: Kolları olasılıksal olarak tetiklenen Kombinatorik Çok Kollu Haydut Problemi (KÇKHP) ve bir KÇKHP yaklaşımı getirdiğimiz Kontekst içeren Gerçek zamanlı Etki Maksimizasyonu Problemi (KGEMP). Kolları tetikleme olasılıklarının (KTO) pozitif olduğunu varsaydığımız KÇKHP probleminde, bu konularda sıklıkla çalışılan bir grup algoritmanın performans garantilerini bularak literatürdeki, KTOlar daha zayıf varsayımlar altındayken bulunan performans garantilerini geliştiriyoruz. Sonrasında ise bir gerçek hayat film önerisi sisteminde bu algoritmaları numerik olarak değerlendiriyoruz. KGEMP problemi için ise, öğrenicinin etki yayılımını bir bedel ödeyerek öğrendiği ve bu yolla, bütün dönemlerdeki toplam etkilenen düğüm sayısından gözlem bedel değeri çıkarılarak elde edilen amaç fonksiyonunu maksimize etmeye çalışan bir problem durumunu çalışıyoruz. Gerçek zamanlı olmayan Etki Maksimizasyonu problemi NP-Zor olduğundan, bir yaklaşım algoritmasını alt-rutin olarak kullanan bir KÇKHP problemi öneriyoruz. Etkileme olasılıklarının kontekstin Hölder-sürekli bir fonksiyonu olduğu durumda, pişmanlığın zamanın lineer bir fonksiyonunun altında kalacağını ispatlıyoruz. Dahası, pişmanlık için bir alt sınır bularak bulduğumuz üst sınırın elde edilebilecek en iyi üst sınır olduğunu kanıtlıyoruz. Numerik sonuçlarımız ile ise önerdiğimiz algoritmaların, gözlemlerin bedelsiz olduğu durumlarda bile literatürdeki en iyi algoritmalar ile başabaş bir performans gösterdiğini görüyoruz.","We focus on two related problems: Combinatorial multi-armed bandit problem (CMAB) with probabilistically triggered arms (PTAs) and Online Contextual Influence Maximization Problem with Costly Observations (OCIMP-CO) where we utilize a CMAB approach. Under the assumption that the arm triggering probabilities (ATPs) are positive for all arms, we prove that a class of upper confidence bound (UCB) policies, named Combinatorial UCB with exploration rate κ (CUCB-κ), and Combinatorial Thompson Sampling (CTS), which estimates the expected states of the arms via Thompson sampling, achieve bounded gapdependent and O( √ T) gap-independent regret improving on previous works which study CMAB with PTAs under more general ATPs. Then, we numerically evaluate the performance of CUCB-κ and CTS in a real-world movie recommendation problem. For the Online Contextual Influence Maximization Problem with Costly Observations, we study a case where the learner can observe the spread of influence by paying an observation cost, by which it aims to maximize the total number of influenced nodes over all epochs minus the observation costs. Since the offline influence maximization problem is NP-hard, we develop a CMAB approach that use an approximation algorithm as a subroutine to obtain the set of seed nodes in each epoch. When the influence probabilities are Hölder continuous functions of the context, we prove that these algorithms achieve sublinear regret (for any sequence of contexts) with respect to an approximation oracle that knows the influence probabilities for all contexts. Moreover, we prove a lower bound that matches the upper bound with respect to time and cost order, suggesting that the upper bound is the best possible. Our numerical results on several networks illustrate that the proposed algorithms perform on par with the state-of-the-art methods even when the observations are cost-free."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otizm Spektrum Bozukluğu'nun (OSB) kalıtsal yapısının karmaşıklığından dolayı Tüm Ekzom Dizileme (Whole Exome Sequencing ya da WES) çalışmaları ile günümüze değin sadece altı düzine kadar risk geni belirlenebilmiştir. Gen keşif sürecini hızlandırabilmek amacıyla ağ temelli birkaç yöntem geliştirilmiştir. Bu yöntemlerin kullandıkları ağlar, durağan türden gen-gen etkileşim ağlarındandır. Gelgelelim, genlerin işlevsel kümelenmeleri sinir sisteminin gelişimiyle evrilir. Ayrıca, gen işleyişlerindeki aksaklıklar kimi zaman sonraki gen-gen etkileşimleri üzerinde katlanarak artan bozulmalara neden olur. Bu nedenle, sinir sistemi gelişiminin değişken ve devingen doğasını göz önünde bulundurmayan yaklaşımlar sınırlı kalacaktır. Çalışmamızda sinir sistemi gelişimi bağlamında evrimleşen gen-gen ortak ifade (coexpression) ağlarının zaman-mekansal bilgisini kullanan ST-Steiner adını verdiğimiz bir gen keşif algoritması sunulmaktadır. Bu algoritma, sinir sistemi gelişimini modelleyecek şekilde uyarlanmış ödül toplayan Steiner ormanı (prize-collecting Steiner forest) temelli bir problemi, öncül sinir-gelişimsel pencerelerdeki bilgiyi taşıyarak, ortak ifade ağlarında çözmektedir. Algoritmanın verdiği kararların izleri geriye doğru sürülebilmekte; bu da sonuçların yorumlanabilirliğini arttırmaktadır. Çalışmamızda ST-Steiner, 3871 örnekten oluşan WES verisine uygulanmakta; erken ve orta cenin dönemlerinin BrainSpan ortak ifade ağlarından risk geni kümeleri belirlenmektedir. Ayrıca, bağımsız bir veri kümesinde, zamansal bilgiyi eklemenin öngörü gücünü arttırdığı gösterilmektedir: Belirlenen kümeler en gelişkin yöntemler (state of the art) ile karşılaştırıldığında hem daha fazla isabet görmekte ,yani daha fazla yıkıcı değişinim (mutasyon) içeren genlerden oluşmakta, hem de OSB ile ilişkili işlevlerde daha çok zenginleşme (enrichment) göstermektedir.","Whole Exome Sequencing (WES) studies for Autism Spectrum Disorder (ASD) could identify only around six dozen risk genes to date, because the genetic architecture of the disorder is highly complex. To speed the gene discovery process up, a few network-based ASD gene discovery algorithms were proposed. Although these methods use static gene interaction networks, functional clustering of genes is bound to evolve during neurodevelopment and disruptions are likely to have a cascading effect on the future associations. Thus, approaches that disregard the dynamic nature of neurodevelopment are limited. Here, we present a spatio-temporal gene discovery algorithm for ASD, which leverages information from evolving gene coexpression networks of neurodevelopment. The algorithm solves a prize-collecting Steiner forest based problem on coexpression networks, adapted to model neurodevelopment and transfer information from precursor neurodevelopmental windows. The decisions made by the algorithm can be traced back, adding interpretability to the results. We apply the algorithm on WES data of 3,871 samples and identify risk clusters using BrainSpan coexpression networks of early- and mid-fetal periods. On an independent dataset, we show that incorporation of the temporal dimension increases the predictive power: Predicted clusters are hit more (i.e. they contain genes with more disruptive mutations on them) and show higher enrichment in ASD-related functions compared to the state of the art."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgi görselleştirme insan algısını güçlendirmek için soyut veriyi görsel olarak sunmaya odaklanır. Çizge görselleştirme büyük miktarda veriyi anlaşılır ve anlamlı bir şekilde sunma becerisinden dolayı bilgi görselleştirme alanındaki en yaygın türlerden biridir. Çizgeler ilişkisel bilgiyi sunmak için uygun bir veri yapısıdır ve bu nedenle çizge görselleştirme biyolojik yolak görselleştirmede de geniş bir kullanıma sahiptir. Bu tez çalışmasında, kanser yolaklarının ortak çalışmaya dayalı inşasına ve bu tür ağlar üzerinde bulunan kanser genomik verisinin görselleştirmesine odaklanıyoruz. Geçmişte biologların kanser genomik verisini incelemelerine yardım etmek amacıyla, SBGN gibi standart formatları da kapsayan çeşitli formatlar kullanan birkaç biyolojik yolak görselleştirme aracı geliştirilmiştir. Bununla birlikte, pek çok biyolog daha basit bir gösterim kullanan The Cancer Genome Atlas (TCGA) taslaklarında yayınlananlara benzer düzenlenmiş yolak diyagramlarını tercih ederler. Bu yolak diyagramları farklı kanser türleri için olan yolaklarda meydana gelen değişiklikleri özetler. Bu gereksinimi karşılamak amacıyla daha önceden PathwayMapper isimli ağ tabanlı bir araç geliştirilmişti. PathwayMapper önceden düzenlenmiş kanser yolaklarını görmek ya da yeni yolakları sıfırdan oluşturmak için kullanılabilir. Bu araç cBioPortal'dan genomik başkalaşım verisinin yüklenmesini de içeren pek çok özelliğe sahiptir. Ayrıca kullanıcıların etkileşimli şekilde kanser yolakları oluşturmalarını ve düzenlemelerini sağlamak amacıyla bir ortak çalışma modu da içermektedir. Bu tez çalışmasında, PathwayMapper'ı daha iyi bir kullanıcı arayüzü ile birlikte, daha eksiksiz ve güçlü bir araç haline getirmek amacıyla bir kaç yoldan geliştiriyoruz. Yeni özellikler karmaşıklık yönetimi işlemlerini, kenar büküm desteğini, etkileşimli köşe boyutlandırmayı ve çeşitli vurgulama imkanlarını içerir. Bunun yanında, araç çubuğunun eklenmesiyle arayüz daha kullanıcı dostu bir hale getirilmiştir.","Information visualization focuses on visually representing abstract data to amplify human cognition. Graph visualization is one of the most common types in the field of information visualization because of its capabilities to present huge amount of data in a clear and meaningful manner. A graph is a suitable data structure for representing relational information and for this reason graph visualization has a wide usage in biological pathway visualization as well. In this thesis, we focus on collaborative construction of cancer pathways and visualization of cancer genomics data overlaid over such networks. Several biological pathway visualization tools have been developed to help biologists analyze cancer genomics data, using various formats, including standard formats like SBGN, in the past. Nevertheless, most biologists prefer curated pathway diagrams like the ones featured in The Cancer Genome Atlas (TCGA) manuscripts, using a simpler notation. These pathway diagrams outline the alterations occurring in pathways for different cancer types. To address this need, a web-based tool called PathwayMapper was previously developed. PathwayMapper can be used to view pre-curated cancer pathways or to create new pathways from scratch. It has many features including overlay of genomic alteration data from the cBioPortal. It also includes a collaborative mode so that the users can interactively create and modify the cancer pathways. With this thesis, we improve PathwayMapper in several ways to make it a more complete and powerful editor with a better user interface. New features include complexity management operations, edge bend support, interactive node resize, and various highlighting capabilities. Furthermore, the user interface has been improved to be more user friendly with the addition of a toolbar."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek verimli okuma cihazlarının gelişmesiyle yapısal varyasyonların keşfi için birçok algoritma geliştirilmiştir. Ama insan genomundaki YV'ların birçoğu henüz belirlenememiştir. Mevcut metotlar delesyon, insersiyon ve mobil elemanlar üzerinde yoğunlaşmaktadır. Dengeli, DNA miktarını değiştirmeyen varyasyonların tespiti zor bir problemdir. Uzun okuma teknolojileri ile birlikte kısa inversiyonların bulunması mümkün olduysa da, büyük genomik inversiyonların keşfi için yeni metodların geliştirilmesi gerekmektedir. Dahası, şu an büyük segmental duplikasyonların insersiyon lokusunu tahmin eden bir algoritma bulunmamaktadır. Bu tezde bağlı okuma teknolojilerini kullanarak ardışık olmayan segmental duplikasyonların ve inversiyonların karakterizasyonu için özgün algoritmalar öne sürüyoruz. Bağlı okuma teknolojisi Illumina okumalarını barkodlar ile işaretle\-yerek uzun mesafe bilgisi sağlamaktadır. Tezdeki metotlar ayrık okuma sinyallerine benzeyen ayrık molekül sinyallerine dayanmaktadır. Ayrık moleküller varyasyon kesim noktalarına denk gelen ve bu nedenle referansa hizalandığında bölünen moleküllerdir. Daha önce Havuzlanmış Klon Dizileme yöntemi için tasarlanmış olan VALOR algoritmasını, bağlı okumalar ile ardışık olmayan segmental duplikasyonları ve inversiyonları bulmak için yeniden tasarladık. Bu yeni algoritmayı, VALOR2 adıyla yeni bir yazılım paketinde uyguluyoruz.","Many algorithms aimed at characterizing genomic structural variation (SV) have been developed since the inception of high-throughput sequencing. However, the full spectrum of SVs in the human genome is not yet assessed. Most of the existing methods focus on discovery and genotyping of deletions, insertions, and mobile elements. Detection of balanced SVs with no gain or loss of genomic segments (e.g. inversions) is particularly a challenging task. Long read sequencing has been leveraged to find short inversions but there is still a need to develop methods to detect large genomic inversions. Furthermore, currently there are no algorithms to predict the insertion locus of large interspersed segmental duplications. Here we propose novel algorithms to characterize large (>40Kbp) interspersed segmental duplications and (>80Kbp) inversions using Linked-Read sequencing data. Linked-Read sequencing provides long range information, where Illumina reads are tagged with barcodes that can be used to assign short reads to pools of larger (30-50 Kbp) molecules. Our methods rely on split molecule sequence signature that we have previously described. Similar to the split read, split molecules refer to large segments of DNA that span an SV breakpoint. Therefore, when mapped to the reference genome, the mapping of these segments would be discontinuous. We redesign our earlier algorithm, VALOR, to specifically leverage Linked-Read sequencing data to discover large inversions and characterize interspersed segmental duplications. We implement our new algorithms in a new software package, called VALOR2."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Küresel Konumlandırma Sistemi (GPS) donanımlı mobil cihazlar ve kablosuz iletişim teknolojilerinin yaygınlaşması konum tabanlı hizmetlerde (LBS) geniş alana yayılmış bir gelişime sebep oldu. Bölgesel arama, rotalama, konum takibi, sosyal paylaşım ve bağlamsal reklam LBS'lere örnektir. Konum verisi toplama sıklıklarına göre LBS'ler anlık ve sürekli olmak üzere ikiye ayrılır. Anlık LBS söz konusu hizmeti sunabilmek için kullanıcının konum verisini bir kez iletmesine ihtiyaç duyar. Öte yandan, sürekli LBS kullanıcının konumunu periyodik olarak ya da her talep edildiğinde servis sağlayıcısı ile paylaşmasını gerektirir. Bir sürekli LBS sunan uygulamanın kullanımı sırasında, kullanıcı servis sağlayıcısına konum verilerinin birbiri ardına sıralanmasından oluşan kayıt listesini, yani uzamsal gezingesini, iletir. Sisteme saldırı düzenleyecek kötü niyetli kimseler, kullanıcılara ait uzamsal-zamansal dizi modellerinden faydalanarak şahısları tanımlamak için kullanılan bilgilere (PII) yüksek kesinlikte ulaşabilmesinden ötürü, bu servislerde tutulan gezingelerin gizliliği son derece önem taşımaktadır. Bu gibi durumların önüne geçmek amacıyla, servis sağlayıcılar genellikle uzamsal gezingeleri kullanıcı parolasıyla şifreledikten sonra veri tabanlarında kayıt altında tutmaktadırlar. Ancak, potansiyel bir saldırgan şifrelenmiş veri tabanını bir kaba kuvvet saldırı vasıtasıyla deşifre edebilir. Başka bir deyişle, kullanıcılara ait gezingelerine ulaşana kadar olabilecek tüm parola kombinasyonlarını deneyebilirler. Her ne kadar yüksek entropili parola kullanımı saldırganların işini güçleştiriyor olsa da kullanıcılar zayıf parola seçme alışkanlıkları nedeniyle servis sağlayıcıların şifreleme şemaları bu tip saldırılara karşı zaafiyet taşımaktadır. Ayrıca, hesaplama teknolojilerinin ve ilgili algoritmaların hızlı gelişimi göz önünde bulundurulduğunda ne kadar geniş bir parola aralığı seçilse de kaba kuvvet saldırıları istatistiki olarak başarıyla sonuçlanabilmektedir. Bu sebeplerden ötürü, uzamsal gezingelerin gizliliği tehdit eden unsurların incelenip gerekli güvenlik önlemlerinin alınması son derece gereklidir. Bu doğrultuda uzamsal gezingelere saldırı düzenleyen hesaplama sınırı bulunmayan şahısların neden olacağı veri ihlallerine karşı mutlak koruma sağlamak amacıyla, kaba kuvvet saldırıları limitinin ötesinde bir koruma sağlayan honey encryption (HE) ile beraber çalışan bir sistem sunuyoruz. Tekniğimiz şifrelenmiş bir uzamsal gezingenin deşifre edilmesi sonucunda her durumda makul bir görünüme sahip gezingeye ulaşılmasını garanti etmektedir. Bu demektir ki bir saldırgan şifrelenmiş bir gezingeyi yanlış bir şifre deneyerek deşifre ettiğinde, bu şifrenin yanlışlığını doğrulayamayacak, çünkü sistem bu saldırgana gerçeğinden ayırt etmenin mümkün olmadığı sahte bir gezingeyi sonuç olarak verecektir. Bir uzamsal gezingeyi etkin bir şekilde kodlama ve gezingenin kodlanmış halini geri çözmek için, ağaç tabanlı bir dağıtım dönüştürücü kodlayıcı (DTE) oluşturarak HE uygulamak için en temel gereksinimi yerine getirdik. Buna ek olarak, DTE ağacını dinamik olarak yenilememize olanak sağlayacak metotları tanıttık. Sistemimizin güvenlik garantisini ispat etmek için, potansiyel bir saldırganın ulaşmaya çalıştığı verilerle ilgili yan bilgisinin olduğu ve olmadığı çeşitli saldırı senaryolarını sistemi 537 taksiden 30 gün boyunca toplanmış gerçek bir GPS veri seti üzerinde uygulayarak analiz ettik.","The prevalence of Global Positioning System (GPS) equipped mobile devices and wireless communication technologies have resulted in widespread development of location-based services (LBS). As some typical examples of LBS, routing, tracking, local search, social networking, and context advertising can be given. In terms of update frequency of location, LBS are divided into two categories: snapshot and continuous. Snapshot LBS request a user's location only once to control features. Continuous LBS, on the other hand, require a user's location in a dynamically periodic or on-demand manner. In the course of interaction with a continuous LBS application, the user reveals a sequence of location samples, namely, spatial trajectory, to service provider. Trajectory privacy in such services is of great importance, since adversaries may use the spatio-temporal sequential pattern to disclose the user's personally identifable information (PII) with high certainty. In order to prevent this from happening, service providers generally encrypt spatial trajectory data under the user's password, and then store in their databases. However, potential adversaries may decrypt the encrypted database via a brute-force attack. In other words, they try every possible value for a password until success is achieved. Although using high-entropy passwords have caused inconvenience for adversaries, the encryption schemes of service providers are vulnerable to this type of an attack due to the tendency of users to choose weak passwords. Also, if the rapid evaluation of computing technology and algorithmic advances are taken into consideration, even the use of a large password domain with conventional encryption can lead to the success of a brute-force attack that became feasible computationally. Thus it is crucial to assess privacy threats and take security countermeasures for spatial trajectories. We present a system that incorporates honey encryption (HE) scheme that provides security beyond the brute-force bound in order to offer absolute protection for spatial trajectories against data breaches that involve computationally unbounded adversary. Our technique guarantees that decryption under any password will yield a plausible-looking trajectory. If an adversary decrypts an encrypted trajectory with a wrong password, it cannot eliminate that password, since the system returns an incorrect trajectory that is impossible to distinguish from the correct one. To efficiently encode and decode a spatial trajectory, we build a precise tree-based distribution transforming encoder (DTE) as the fundamental requirement of HE. In addition, we introduce the methods to dynamically update the proposed DTE. To prove the security guarantee of our system, we evalute it considering several attacks with and without side information using a real-life GPS sampling data set taken from 537 taxis over 30 days."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge görselleştirme, biyolojik ağlar, sosyal ağlar ve bilgisayar ağları ilişkisel biçimindeki bilgileri iletmek ve analiz etmek için güçlü bir araçtır. Statik çizgelerin otomatik yerleştirilmesi üzerinde bol miktarda araştırma yapılmış ve bunları görselleştirmek için çok sayıda araç geliştirilmiştir. Ancak, birçok uygulamada, çizgeler statik değildir; zamanla değişmekte ve ya gelişmektedir. Bu boşluğu gidermek için iki yeni dinamik çizge yerleştirme yöntemi tasarladık, dinamik bir çizge yerleştirme kütüphanesi uyguladık ve dinamik çizgeleri etkili bir şekilde görselleştirmek ve karşılaştırmak için web tabanlı bir uygulama geliştirdik. Cytoscape.js-synched, ikisi bizim tarafımızdan önerilen üç farklı dinamik çizge yerleştirme algoritması gerçekleştiren bir yazılım kütüphanesidir. Dinamik çizgelerin düzeninde, ana kriter, zihinsel haritayı korumak ve aynı çizgenin farklı versiyonlarının yerleşimi arasındaki farkları en aza indirmektir. Düğüm düğüm örtüşmeleri, kenar kenar kesişim sayısı, toplam alan ve toplam kenar uzunluğu gibi genel yerleşim ölçütlerini en aza indirmek de önemlidir. Önerilen ve uygulanan algoritmalar, bu çelişen kriterlerin her ikisini de tatmin edici bir seviyede karşılamakta ve estetik olarak tatmin edici sonuçlar vermektedir. Cytoscape.js çizge yazılım kütüphanesi üzerine kurulan EVEN, dinamik çizgelerin görselleştirilmesi için oldukça özelleştirilebilir, açık kaynaklı, web tabanlı ve etkileşimli bir uygulamadır. İki veya daha fazla dinamik çizgenin görselleştirmesini ve otomatik senkronizasyon düzenini destekler. EVEN, senkronize çizge keşif özellikleri sağlayarak dinamik çizgelerin analizini daha pratik hale getirir. Aynı zamanda dinamik çizge oluşturmak ve düzenlemek için temel desteğe sahiptir.","Graph visualization is a powerful tool to convey and analyze relational information in the form of networks such as biological networks, social networks, and computer networks. Abundant research has been conducted on the automatic layout of static graphs and numerous tools have been developed to visualize them. However, in many applications, graphs are not static; they change or evolve over time. To void this gap, we designed two new dynamic graph layout methods, implemented a dynamic graph layout library and developed a web-based application to visualize and contrast dynamic graphs effectively. Cytoscape.js-synched is a library that implements three different dynamic graph layout algorithms, two of which are proposed by us. In the layout of dynamic graphs, the main criteria is to keep the mental map and minimize the total distance among different versions of the same graphs. It is also important to minimize general layout metrics such as node to node overlaps, number of edge crossing, total area, and total edge length. Proposed and implemented algorithms address both of these conflicting criteria to a satisfactory level and result in aesthetically pleasing layouts. EVEN, which is based on Cytoscape.js graph library, is a highly customizable, open source, web-based, and interactive application for visualization of dynamic graphs. It supports visualization and automatic synchronized layout of two or more dynamic graphs. EVEN makes analysis of dynamic graphs more practical by providing synchronized graph exploration features. It also has basic support for constructing and editing dynamic graph."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet kapasitesinin ve servislerinin son zamanlardaki hızlı artışı bulut bilişim ihtiyacının da hızla artmasına sebep olmaktadır. Bulut temelli bilgi depolama ve işlemci gereksinimi bulut sağlayıcıların kendi platformlarını ve tesislerini en iyi şekilde kullanmalarını zorunlu hale getirmektedir. Tesislerin ve ekipmanların en iyi şekilde kullanılabilmesi için enerji tüketiminin azaltılması ve servis anlaşmalarının belirli bir seviyede tutulması gerekmektedir. Dinamik sanal makine yerleşmesi ve taşınması bu hedefe ulaşmada kullanılan yöntemlerden biridir. Bu yöntemin en iyi şekilde uygulanabilmesi için fiziksel makine kaynak kullanım oranlarının sürekli gözlemlenmesi ve tahmin edilmesi gerekmektedir. Bu tez çalışmasında, fiziksel makine işlemci kullanım oranlarını tahmin ederek enerji tasarrufu ve servis kalitesi sağlayan yeni bir dinamik sanal makine yerleştirme ve taşınma yöntemi sunmaktayız. Önerdiğimiz yöntem, LRAPS, fiziksel makinelerin geçmiş işlemci kullanım bilgilerine bakarak gelecekte gerekecek işlemci kullanım oranını tahmin etmeye çalışmaktadır. Daha sonra bu tahmin kullanılarak normal yükleme değerinin üstünde ya da altında olan fiziksel makineler tespit edilmektedir. Eğer bir makine aşırı yüklenmişse, o makinede bulunan bazı sanal makineler servis anlaşmasını bozmamak için uygun olan diğer fiziksel makinelere taşınır; eğer bir makine az yüklenmişse, o makinede bulunan bütün sanal makineler enerji tasarrufu sağlamak için uygun olan diğer fiziksel makinelere taşınır. Yöntemimizin performansını ve etkinliğini görmek için kapsamlı simülasyon deneyleri gerçekleştirdik. Simülasyon deneyleri yöntemimizin uygulanabilir olduğunu, enerji tasarrufu yapmanın yanı sıra servis anlaşmasını sağladığını da gösterdi.","With tremendous increase in Internet capacity and services, the demand for cloud computing has also grown enormously. This enormous demand for cloud based data storage and processing forces cloud providers to optimize their platforms and facilities. Reducing energy consumption while maintaining service level agreements (SLAs) is one of the most important issues in this optimization effort. Dynamic virtual machine allocation and migration is one of the techniques to achieve this goal. This technique requires constant measurement and prediction of usage of machine resources to trigger migrations at right times. In this thesis, we present a dynamic virtual machine allocation and migration method utilizing CPU usage prediction to improve energy efficiency while maintaining agreed quality of service levels in cloud datacenters. Our proposed method, called LRAPS, tries to estimate short-term CPU utilization of hosts based on their utilization history. This estimation is then used to detect overloaded and underloaded hosts as part of live migration process. If a host is overloaded, some of the VMs running on that host are migrated to other hosts to avoid SLA violations; if a host is underloaded, all of the VMs in that host are tried to be migrated to other machines so that the host can be powered off. We did extensive simulation experiments using CloudSim to evaluate the efficiency and effectiveness of our proposed method. Our simulation experiments show that our method is feasible to apply and can significantly reduce power consumption and SLA violations in cloud systems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnce taneli nesne tanıma, cok sayıda alt kategori arasından hedef nesnenin türünü belirleme görevi ile ilgilenir. Uzaktan algılanmış görüntülerde yeni detayların ortaya çıkmasını sağlayan uzamsal ve spektral çözünürlükteki sürekli artış ve zor algılanan farklara sahip olan daha çeşitli hedef nesne sınıflarının ortaya çıkışı bunu yeni bir uygulama haline getirmektedir. Tek bir veri kaynağından alınan görüntüleri kullanan yaklaşımlarda, denetimli algoritmalar, düşük sınıflar arası değişinti ve yüksek sınıf içi değişintiye ek olarak küçük örneklem büyüklüğü nedeniyle bu problemi tam olarak çözemez. Bu sorunların yanı sıra, daha da zorlu bir görev olarak sınıfların bazıları için hiçbir eğitim örneği bulunmayan örneksiz öğrenme problemi ele alınabilir. Örneksiz öğrenme, daha önce öğrendiği alt kategorilerle, eğitim örnekleri olmayan yeni alt kategorileri ilişkilendirerek bir tanıma modeli oluşturmayı amaçlamaktadır. Bu ilişkiyi kurmak için geliştirdiğimiz yöntem, derin bir evrişimsel sinir ağından elde edilen görüntü temsili ile sınıfların anlamsal özelliklerini tanımlayan yardımcı bilgiler arasında bir uyumluluk fonksiyonu öğrenir. Eğitim örneği olmayan sınıflar için bilgi aktarımı, çıkarım esnasında bu fonksiyonun en büyüklenmesi ile gerçekleştirilir. Örneksiz öğrenmeye ek olarak çoklu veri kaynaklarından faydalanmak, nesne tanıma performansını sınırlayan alt kategorilerin benzerliğinin yarattığı olumsuz etkilerin üstesinden gelebilir. Ancak bu durum aynı zamanda yeni sorunları ortaya çıkarmaktadır. Farklı uzamsal ve spektral çözünürlüklerde, farklı koşullar altında ve farklı sensörlerden elde edilen görüntüler; mevsimsel değişiklikler, farklı görüntüleme geometrisi, edinim gürültüsü, sensörlerin kusurları, farklı atmosfer koşulları vb. nedeniyle geometrik olarak doğru şekilde çakıştırılamayabilirler. Bu çalışmada farklı kaynaklardan edinilen görüntüleri doğru bir şekilde çakıştırmayı ve sınıflandırma kurallarını aynı anda tek bir çerçevede öğrenmeyi amaçlayan bir sinir ağı modeli önerilmiştir. Bunu yapmak için bir görüntü kaynak görüntü olarak kullanılır. Diğer görüntülerde olası bölge önerilerinin temsilleri ağırlıklandırılarak kaynak görüntü ile çakışan doğru uzamsal bölge kestirilir. Kaynak görüntüsünden çıkarılan derin özelliklerin yardımıyla gerekli ağırlıklar bulunur. Sonunda, alt kategorilerin sınıflandırılması, kaynak bölgeden ve kestirilmiş hedef bölgelerden çıkarılan temsillerin kaynaştırılması ile gerçekleştirilir. Yeni önerilen bir veri kümesi üzerinde yapılan deneysel analiz, her iki yöntemin de başarılı sonuçlar verdiğini göstermektedir.","Fine-grained object recognition aims to determine the type of an object in domains with a large number of sub-categories. The steadily increase in spatial and spectral resolution entailing new details in remote sensing image data, and consequently more diversified target object classes having subtle differences makes it an emerging application. For the approaches using images from a single domain, widespread fully supervised algorithms do not completely fit into accomplishing this problem since target object classes tend to have low between-class variance and high within-class variance with small sample sizes. As an even more arduous task, a method for zero-shot learning (ZSL), in which identification of unseen sub-categories is tackled by associating them with previously learned seen subcategories when there is no training example for some of the classes, is proposed. More specifically, our method learns a compatibility function between image representation obtained from a deep convolutional neural network and the semantics of target object sub-categories explained by auxiliary information gathered from complementary sources. Knowledge transfer for unseen classes is carried out by maximizing this function throughout the inference. Furthermore, benefitting from multiple image sensors can overcome the drawbacks of closely intertwined sub-categories that limits the object recognition performance. However, since multiple images may be acquired from different sensors under different conditions at different spatial and spectral resolutions, they may be geometrically unaligned correctly due to seasonal changes, different viewing geometry, acquisition noise, an imperfection of sensors, different atmospheric conditions etc. To address these challenges, a neural network model that aims to correctly align images acquired from different sources and to learn the classification rules in a unified framework simultaneously is proposed. In this network, one of the sources is used as the reference and the others are aligned with the reference image at representation level throughout a learned weighting mechanism. At the end, classification of sub-categories is carried out with a feature-level fusion of representations from the source region and estimated multiple target regions. Experimental analysis conducted on a newly proposed data set shows that both zero-shot learning algorithm and the multi-source fine-grained object recognition algorithm give promising results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sağlık ve genetik verileri bireylerin kişisel bilgilerini taşıması açısından çok hassas verilerdir. Genetik verilerden kalıtsal/genetik hastalık veya bunların oluşma olasılığına dair bilgi, ırk veya akrabalık ilişkilerine dair çıkarım yapılabilir. Bunun yanı sıra, çoğunlukla hastaneler veya diğer sağlık kuruluşları tarafından toplanan sağlık verileri de bireylerin şu anda veya gelecekteki hastalıklarına dair gösterge olabilir. Bu tarz verileri korurken, veriler araştırmalarda kullanıldığı için verinin yararlılığını (utility) korumak çok önemlidir. Tüm bunları göz önünde bulundurarak, bu çalışmada sağklık verilerinin korunmas için Paillier Cryptosystem kullanılarak homomorfik şifreleme ve genetik veri için yeni bir filigran şeması (watermarking scheme) kullanılmasını öneriyoruz. Homomorphic şifreleme ile sağlık verileri üzerinde yapılabilecek matematiksel işlemlerin, şifreyi bozmaksızın yapılabileceğini gerçek hayattan bir senaryo ile gösteriyoruz. Bu tezin ikinci kısmında, genetik verilerin yetkisiz servis sağlayıcıları tarafından paylaşılmasını engellemek için filigran şeması kullanıyoruz. Güçlü filigran şeması tekniği, kasıtlı şahısların yüksek olasılıkla tespit edilmesini sağlıyor ve bu tespitin olasılıksal limitlerini farklı deney düzenekleri kullanarak gösteriyoruz. Son olarak, bu şema şunları yüksek olasılıkla garanti ediyor: (i) verinin yararlılığı korunur, (ii) bir veya birden çok servis sağlayıcı tarafından saldırılara karşı dayanıklıdır, ve (iii) önerilen şema, kasıtlı servis sağlayıcılarının veriyi sızdırmadan önce filigranlı noktaları değiştirmek/kaldırmak için kullanacağı yardımcı bilgileri kullanarak filigranlı verinin yapısı ile uyumlu hale getirir.","Health and genomic data is sensitive in terms of carrying private information about individuals. One can infer inherited/genetic disorders, their occurrence probabilities, information about race, and kinship by analyzing an individual's genomic data. Furthermore, health data which is mostly collected by hospitals or other health institutions carries private information about individuals including the diseases they have at present or indicators of future diseases/disorders. While protecting such data, it is important to show that its utility is preserved and maximized since the data is used in researches. Regarding these facts, homomorphic encryption-based scheme (using Paillier cryptosystem) for the protection of health data and a novel watermarking scheme based on belief propagation algorithm for the genomic data is proposed in this work. Homomorphic encryption is used for the health data to show the ability of performing mathematical operations on the encrypted data without decrypting it with a real-life use-case. We show its practicality with the correctness and performance results. In the second part of this thesis, a watermarking scheme for genomic data is proposed to overcome the liability issues due to unauthorized sharing by service providers (SPs). Robust-watermarking techniques ensure the detection of malicious parties with a high probability and we show the probabilistic limits of this detection with different experimental setups and evaluation metrics. Lastly, this scheme guarantees the following with a high probability: (i) the utility is preserved, (ii) it is robust against single or colluding SP attacks, and (iii) watermark addition is compatible with the nature of the data as the proposed method considers auxiliary information that a malicious SP may use in order to remove/modify watermarked points before leaking the data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Zamansal ilişkiye sahip veriler, artan önemi ve çok sayıda uygulaması ile birçok doğal ve dijital süreçte ortaya çıkar. Bu çalışma hem tek başına hem de seyrek ve ağ ilişkili durumlarda zaman serilerinin analizinde veri madenciliği problemlerine çözümler sunmaktadır. İlk olarak kullanıcı derecelendirmesine dayanan, yeni zaman serisi sorguları oluşturarak zaman serisi veri depolarını taramak için bir yöntem geliştirdik. Sonuç kümesi, ilgililik geri bildirim mekanizmasının etkinliğini arttırmak için çeşitlilik içeren seçim yöntemleri kullanılarak oluşturulmuştur. Geri bildirime ek olarak, zaman dizisi verilerinin benzersiz bir yönü göz önünde bulundurularak, el ile yapılabilecek bir seçimin aksine, kullanıcı açıklamalarına dayanan çeşitli dönüşümler arasında üstün performans gösteren temsil türüne yakınsamak için temsil geri bildirim yöntemleri önerilmektedir. Bu yöntemler, her bir temsil başarımına göre sonuç kümesinin bölümlendirilmesine ve birden çok temsilin farklı özelliklerini kuvvetlendiren bir ağırlık yaklaşımına dayanmaktadır. Daha sonra, hem işlem yükünü azaltmak hem de doğruluğunu arttırmak için, zaman serilerini veri bağımlı seyrek temsillere indirgemek için oto-kodlayıcıların kullanımını önermekteyiz. çok çeşitli gerçek veri kümeleri üzerinde yapılan deneyler, önerilen yöntemlerin doğruluğu önemli ölçüde geliştirdiğini kanıtlamakta olup veriye duyarlı temsiller, verileri ve hesaplama yükünü azaltırken, benzer başarım kaydetmiştir. Daha zorlu bir durum olarak, zaman serisi veri kümesi noksan olup, veri madencilik tekniklerini uygulayabilmek için interpolasyon yaklaşımlarına ihtiyaç duyulabilir. Bu bağlamda, zamana bağlı değişen bir ağ ile ilişkili seyrek bir zaman dizisi verisini analiz ediyoruz. Gürültülü ve seyrek araç izlerini kullanarak bir yol ağı zaman serisi veri kümesi oluşturmak için bir metodoloji geliştirdik ve en kısa yol çözümlerini kullanarak değerlendirdik.","Data with temporal ordering arises in many natural and digital processes with an increasing importance and immense number of applications. This study provides solutions to data mining problems in analyzing time series both in standalone and sparse networked cases. We initially develop a methodology for browsing time series repositories by forming new time series queries based on user annotations. The result set for each query is formed using diverse selection methods to increase the effectiveness of the relevance feedback (RF) mechanism. In addition to RF, a unique aspect of time series data is considered and representation feedback methods are proposed to converge to the outperforming representation type among various transformations based on user annotations as opposed to manual selection. These methods are based on partitioning of the result set according to representation performance and a weighting approach which amplifies different features from multiple representations. We subsequently propose the utilization of autoencoders to summarize the time series into a data-aware sparse representation to both decrease computation load and increase the accuracy. Experiments on a large variety of real data sets prove that the proposed methods improve the accuracy significantly and data-aware representations have recorded similar performances while reducing the data and computational load. As a more demanding case, the time series dataset may be incomplete needing interpolation approaches to apply data mining techniques. In this regard, we analyze a sparse time series data with an underlying time varying network. We develop a methodology to generate a road network time series dataset using noisy and sparse vehicle trajectories and evaluate the result using time varying shortest path solutions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizelge bölümleme, çeşitli uygulamaların verimli bir şekilde paralelleştirilmesi için yaygın olarak kullanılmaktadır. Akış grafiği bölümleme, çevrimdışı çizelge bölümleyicilerin yüksek hesaplama maliyetlerini aşmak için sağlanan bir geçiş bölümleme çözümüdür. Bu aktarım algoritmaları, bölümleme özelliklerinde daha fazla geliştirmeyi amaçlayan ardışık olarak yeniden bölümlendirme için kullanılabilir olsa da, kalite iyileştirmeleri, birkaç geçişle sınırlıdır. Çevrimdışı çizelge bölümleme araçlarını, oluşturulan yüksek kaliteli bölümler nedeniyle çizelge bölümleme için hala istenen bir çözüm halindedir. Çizelge bölümleme probleminde kalite ve performans arasındaki dengeyi azaltabilen akış algoritmalarını kullanarak çok düzeyli bir yaklaşım öneriyoruz. Ayrıca Openmp tabanlı çok parçalı uygulamalarımız, son teknoloji ürünü çevrimdışı yüksek kaliteli çizelge bölümleme aracı olan METIS için çok iş parçacıklı bir çözüm olan \ emph {mt-metis} ile kıyaslandığında hızlı ve yüksek ölçeklenebilir çözümler üretebilir. Sonuçlarımız, yöntemimizin büyük çizelge veri setlerinde on beş kat daha hızlı ve daha ölçeklenebilir sonuçlar üretebildiğini göstermektedir. Ayrıca, yöntemin, birkaç kez yeniden bölümlendirildikten sonra en gelişmiş akış grafiği bölümleme algoritması LDG'ye kıyasla önemli ölçüde bölümlerin kalitesini artırabildiğini gösteriyoruz. Ortalama olarak LDG algoritmasından 29 % daha iyi niteliklere sahip bölümler üretiyoruz.","Graph partitioning is widely used for efficient parallelization of a variety of applications. Streaming graph partitioning is a one pass partitioning solution provided to overcome high computation costs of offline graph partitioners. Even though these streaming algorithms can be used for successively repartitioning, aiming at further improvements in partitioning qualities, quality improvements is limited to few passes that make offline graph partitioning tools still a desirable solution for graph partitioning due to the generated high-quality partitions. We propose a multilevel approach using streaming algorithms that can alleviate the tradeoff between quality and performance in graph partitioning problem. Moreover, our OpenMP based multi-threaded implementation can generate fast and highly scalable solutions compared to mt-metis, a multi-threaded solution for METIS, and the state-of-the-art offline high-quality graph partitioning tool. Our results show that our method can produce up to fifteen times faster and more scalable results in large graph datasets. We also show that our method can improve the quality of partitions significantly compared to state-of-the-art streaming graph partitioning algorithm LDG after repartitioning several times. On average we produce partitions with 29% better qualities than the LDG algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapay Sinir Ağları (YSA) sınıflandırma, kümelendirme vb. farklı makine öğrenmesi görevleri için kullanılmaktadır. Bu ağlar günlük hayatımızda önemli görevler yapıyor ve yeni hizmetler sunmaktadırlar. Bu ağların öğrenme kapasiteleri, artan hesaplama gücü ve veri miktarıyla, 2000li yıllardan beri önemli ölçüde ivmelenmektedir. Bu nedenle, bu ağlar üzerinde yapılan araştırmalar Derin Öğrenme ismiyle yeniden adlandırılmış ve, sadece sinir ağlarında değil Makine Öğrenmesi disiplini için de, önemli bir araştırma sahası olarak doğmuştur. Böylesi önemli bir araştırma alanı için, bu ağların eğitiminde kullanılan teknikler daha başarılı sonuçlar için anahtar olarak görülebilir. Bu çalışmada, bu eğitim prosedürünün her kısmı farklı ve geliştirilmiş - kimi zaman yeni - teknikler siyah-beyaz ve renkli imaj veri setlerini sınıflandıran evrişimsel sinir ağları üzerinde araştırılmıştır. İleri yöntemler, He-budanmış Gauss ön değer atama gibi literatürde var olanları içermiştir. Buna ek olarak, bizim literatüre katkımız olan SinAdaMax iyileştiricisi, Dominant Olarak Üstel ve Doğrusal Birim (ing. DELU), He-budanmış Laplasyen ve Maksimum-Bölütleme katmanları için Piramit Yaklaşımı gibileri de içermiştir. Bu tezin bölümlerinde, başarı oranları bu ileri yöntemlerin - özellikle bizim katkımız olan DELU ve SinAdaMax iyileştirilmiş metotlarının - kümülatif olarak eklenmesiyle arttırılmıştır. Sonuç olarak, farklı veri setleri için başarı oranı eşikleri - bu yöntemlerle geliştirilmiş ve önemli test başarı artışlarına ulaşmış - temel evrişimsel sinir ağları ile 15-21 saat içinde (tipik olarak bir günden daha az sürede) karşılanmıştır. Böylece, bu farklı ve ileri yöntemlerle elde edilmiş daha iyi performanslar, tanınmış sınıflandırma veri setleri kullanılarak gösterilmiştir.","Artificial Neural Networks (ANNs) are used for different machine learning tasks such as classification, clustering etc. They have been utilized in important tasks and offering new services more and more in our daily lives. Learning capabilities of these networks have accelerated significantly since 2000s, with the increasing computational power and data amount. Therefore, research conducted on these networks is renamed as Deep Learning, which emerged as a major research area - not only in the neural networks, but also in the Machine Learning discipline. For such an important research field, the techniques used in the training of these networks can be seen as keys for more successful results. In this work, each part of this training procedure is investigated by using of different and improved - sometimes new - techniques on convolutional neural networks which classify grayscale and colored image datasets. Advanced methods included the ones from the literature such as He-truncated Gaussian initialization. In addition, our contributions to the literature include ones such as SinAdaMax Optimizer, Dominantly Exponential Linear Unit (DELU), He-truncated Laplacian initialization and Pyramid Approach for Max-Pool layers. In the chapters of this thesis, success rates are increased with the addition of these advanced methods accumulatively, especially with DELU and SinAdaMax which are our contributions as upgraded methods. In result, success rate thresholds for different datasets are met with simple convolutional neural networks - which are improved with these advanced methods and reached promising test success increases - within 15 to 21 hours (typically less than a day). Thus, better performances are obtained by those different and improved techniques are shown using well-known classification datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde, genom dizilimi her zamankinden daha erişilebilir ve hesaplıdır. Ayrıca bireylerin genom verilerini servis sağlayıcıları veya kamuya açık web sitelerinde paylaşmaları da mümkündür. Genomik verilerin tıbbi araştırmalarda önemli bir etkisi ve yaygın kullanımı olmasına rağmen, genetik verilerini anonim veya kısmen paylaşsalar bile bireylerin gizliliğini tehlikeye atmaktadır. Bu çalışmada, ilk olarak, gözlemlenebilir Markov modeli, haplotipler, akrabalık ilişkileri ve fenotipik özellikler arasındaki rekombinasyon modeli kullanılarak genomik gizlilik çıkarsama saldırısı üzerinde mevcut çalışmalarını geliştiriyoruz. Daha sonra bu gizlilik konusunu ele almak için, mahremiyetlerini korurken bireylerin genomik verilerini paylaşmaya yönelik farklı bir gizlilik temelli çerçeve sunuyoruz. Genomik veriler için var olan farklı gizlilik temelli çözümlerden farklı olarak (özet istatistiklerinin gizliliğin korunmasını da göz önünde bulundurarak), gerçek genomik verilerin gizliliğini koruyan paylaşımına odaklanıyoruz. Kendi genomunda (örneğin, mutasyonlar veya tek nükleotid polimorfizmleri - bireyle ilgili hassas bilgileri açığa çıkaran SNP'ler) bazı hassas kısımları olan bir bireyi varsayıyoruz. Bireyin amaçları (i) hassas verilerinin gizliliğini korumak, (ii) birbirine bağlı verilerin gizliliğini korumak (kendi verileriyle ilişkili olan diğer bireylere ait veriler) ve (iii) çok fazla veri paylaşmak Veri paylaşımını en üst düzeye çıkarmak için mümkün olduğu kadar. Geleneksel farklı gizlilik temelli veri paylaşım şemalarının aksine, önerilen plan, verilere kasıtlı olarak gürültü eklemez; veri noktalarının seçici bir şekilde paylaşılmasına dayanır. Önceki çalışmalar, diğerlerini paylaşırken hassas SNP'leri gizlemenin, bireyin (ya da diğer birbirine bağlı halkların) gizliliğini korumadığını göstermektedir. Yardımcı bilgilerden yararlanarak, bir saldırgan etkili çıkarım saldırıları yürütebilir ve bireylerin hassas SNP'lerini çıkartabilir. Bu çalışmada, öncelikle gizlilik temelli veri paylaşımı çerçevemizde, ayrıntılı olarak tartıştığımız bu çıkarım saldırılarını ve farklı gizlilik garantileri sağlayan bireyler için bir SNP paylaşım platformu önermekteyiz. Önerilen çerçevenin, yüksek bir veri paylaşım programı sağlarken saldırganı hassas bilgiler sağlamadığını gösteriyoruz. Gerçek veriler üzerinde yapılan deneyler sayesinde, fayda ile mahremiyeti etkileyen çeşitli parametreler arasındaki ilişkiyi kapsamlı bir şekilde inceliyoruz. Ayrıca, önerilen tekniği daha öncekilerle karşılaştırıyoruz ve hem gizlilik hem de veri paylaşımı yararı açısından avantajımız olduğunu gösteriyoruz.","Today, genome sequencing is more accessible and affordable than ever. It is also possible for individuals to share their genomic data with service providers or on public websites. Although genomic data has significant impact and widespread usage on medical research, it puts individuals' privacy in danger, even if they anonymously or partially share their genomic data. In this work, first, we improve the existing work on inference attack on genomic privacy using observable Markov model, recombination model between the haplotypes, kinship relations, and phenotypic traits. Then to address this privacy concern, we present a differential privacy-based framework for sharing individuals' genomic data while preserving their privacy. Different from existing differential privacy-based solutions for genomic data (which consider privacy-preserving release of summary statistics), we focus on privacy-preserving sharing of actual genomic data. We assume an individual with some sensitive portion on his genome (e.g., mutations or single nucleotide polymorphisms - SNPs that reveal sensitive information about the individual). The goals of the individual are to (i) preserve the privacy of his sensitive data, (ii) preserve the privacy of interdependent data (data that belongs to other individuals that is correlated with his data), and (iii) share as much data as possible to maximize utility of data sharing. As opposed to traditional differential privacy-based data sharing schemes, the proposed scheme does not intentionally add noise to data; it is based on selective sharing of data points. Previous studies show that hiding the sensitive SNPs while sharing the others does not preserve individual's (or other interdependent peoples') privacy. By exploiting auxiliary information, an attacker can run efficient inference attacks and infer the sensitive SNPs of individuals. In this work, we also utilize such inference attacks, which we discuss in details first, in our differential privacy-based data sharing framework and propose a SNP sharing platform for individuals that provides differential privacy guarantees. We show that the proposed framework does not provide sensitive information to the attacker while it provides a high data sharing utility. Through experiments on real data, we extensively study the relationship between utility and several parameters that effect privacy. We also compare the proposed technique with the previous ones and show our advantage both in terms of privacy and data sharing utility."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizge analiz uygulamaları; sosyal ağlar, proteinler arası etkileşimler, güç nakli şebekeleri, taşıma ağlar gibi birçok alana uygulanabilirlikleri sayesinde gittikçe önem kazanmıştır. Çizge veri setlerinin karmaşık ve çok çeşitli yapıda olması nedeniyle, günümüz sistemlerinin sayısal işlem yapabilme kapasitesi artsa da etkili bir çizge algoritması geliştirmek son derece zordur. Büyük çizge veri setlerini işlemek amacıyla önceden hazırlanmış yapıların ve fonksiyonların bulunduğu farklı dizayn kararları alan birçok çerçeve geliştirilmiştir. Fakat bu çerçeveler arasında en iyi tasarım seçimlerinin nasıl olacağı hakkında ortak bir karar yoktur. Bu tezde; işlemlerim uygulanma sırası, veri erişim biçimleri ve iş aktivasyonu gibi farklı tasarım kararlar göz önüne alınarak, tekrarlayan çizge uygulamalarına örnek teşkil eden üç algoritmanın çeşitli paralel geliştirmeleri sunulmuştur. Bu üç algoritma şunlardır: ""PageRank"", ""Tek Kaynaklı En Kısa Yollar"" ve ""Sığ-Öncelikli Arama"". Her bir uygulamanın, hem gerçek hem de sentetik çizge veri setleri üzerinde performans, ölçeklenebilirlik ve iş verimi analizleri yapılarak bunlar arasında nasıl bir denge kurulabileceği deneysel olarak araştırılmıştır. Milyarlarca düğüm ve bunları birbirine bağlayan çok sayıda bağıntıdan oluşan çizge veri setleri her ne kadar çok büyük olsalar da modern ortak bellekli sistemlerin bellek kapasitesine sığabilirler. Bu nedenle, bu tezde, tüm uygulamalar ortak bellekli paralel çok çekirdekli sistemler üzerinde tasarlanmıştır. Aynı zamanda donanım performans sayaçları kullanılarak ortak bellekli sistemlerin performansını sınırlayabilecek noktalar belirlemek için her bir algoritmanın mikro-donanımsal değişkenleri incelenmiştir. Sonuç olarak; bu tezde, geliştiricilerin çizge analiz uygulamaları yazarken, farklı tasarım kararları arasından etkili ve bilinçli seçimler yapabilmeleri hedeflenmiştir.","Graph analytics have come to prominence due to their wide applicability to many phenomena of real world such as social networks, protein-protein interactions, power grids, transportation networks, and other domains. Despite the increase in computational capability of current systems, developing an effective graph algorithm is challenging due to the complexity and diversity of graphs. In order to process large graphs, there exist many frameworks adopting different design decisions. Nonetheless, there is no clear consensus among the frameworks on optimum design selections. In this dissertation, we provide various parallel implementations of three representative iterative graph algorithms: PageRank, Single-Source Shortest Path, and Breadth-First Search by considering different design decisions such as the order of computations, data access pattern, and work activation. We experimentally study the trade-offs between performance, scalability, work effciency of each implementation on both real-world and synthetic graphs in order to guide developers in making effective choices while implementing graph applications. Since graphs with billions of edges can fit in memory capacities of modern shared-memory systems, the applications are implemented on a shared-memory parallel/multicore machine. We also investigate the bottlenecks of each algorithm that may limit the performance of shared-memory platforms by considering the micro-architectural parameters. Finally, we give a detailed road-map for choosing design points for effcient graph processing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Literatürde, dağıtık hafıza mimarilerinde düzensiz surette seyrek uygula- maların hesaplama yükünü dengelemek için başarılı bölümleme modelleri ve metotları önerilmiştir ve kullanılmıştır. Ancak, literatür işlemcilerin hem hesaplama hem veri yükünü dengelemeyi amaçlayan bölümleme modelleri ile metotlarından yoksun bulunmaktadır. Bu tezde, işlemcilerin hesaplama ve veri yükünü eşzamanlı olarak dengelemeyi amaçlayan çizge/hiperçizge modelleri ve metotları önererek bu boşluğu kapamaya çalışıyoruz. Önerilen modellerin ve metotların geçerliliği, iki genişçe kullanılan düzensiz şekilde seyrek uygulamada, paralel çözüm ağı (mesh) simülasyonlarında ve paralel seyrek matris seyrek matris çarpımında, test edildi.","In the literature, several successful partitioning models and methods have been proposed and used for computational load balancing of irregularly sparse appli- cations on distributed-memory architectures. However, the literature lacks par- titioning models and methods that encode both computational and data load balancing of processors. In this thesis, we try to close this gap by proposing graph and hypergraph partitioning models and methods that simultaneously en- code computational and data load balancing of processors. The validity of the proposed models and methods are tested on two widely-used irregularly sparse applications: parallel mesh simulations and parallel sparse matrix sparse matrix multiplication."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tek nükleotid polimorfizmi (TNP), baz çifti ekleme/çıkarma (Indel) ve yapısal varyasyon (YV) gibi genetik varyasyonların canlılar üzerinde önemli fenotipik etkileri vardır. Bunların içinde 50'den fazla baz çiftini etkileyen YV'ler, Crohn Hastalığı, şizofreni ve otizm gibi çeşitli kalıtsal hastalıkların da temel sebebidir. Ayrıca YV'lerin etkilediği baz çifti sayısı TNP'lere göre çok daha fazladır (3,5 Mbp TNP, 15-20 Mbp YV). Bugün, yeni nesil dizileme (YND) teknolojisini kullanarak tam genom hizalama (WGS) yapabiliyor ve bu tip varyasyonları çok daha hızlı, ucuz ve yüksek doğrulukla keşfedebiliyoruz. Ancak 1000 Genom Projesi'nde de gördüğümüz gibi, YND teknolojisinin bazı yetersizlikleri vardır. En önemli sorun şu an kullanılan YND platformlarının ürettiği kısa okuma (<250 bp) boyutları ve genomların çok tekrarlı bölgeler barındırması sebebiyle bu kısa okumaların yüksek doğrulukla hizalanmasını zorlaştırmasıdır. Bu durum, keşfedilen genomik varyasyonların doğruluk oranını da etkilemektedir. Bu sebeple, bugüne kadar geliştirilmiş algoritmalar ekleme, silinme ve kısa inversiyonlar gibi görece olarak daha basit YV'leri karakterize edebilmesine rağmen birçok genetik hastalıkla bağdaştırılan daha karmaşık varyasyonları göz ardı etmiştir. Bu tip YV'lerin insan genomuna etkilerini gözlemlemek için daha farklı yaklaşımlar kullanan, yüksek doğruluk oranına sahip yeni algoritmalar gerekmektedir. Bu tezde, YND teknolojisiyle kısa okumaları kullanarak bir canlının genomundaki YV'leri bulan TARDIS algoritmasını tanıtıyoruz. TARDIS; silinme, yeni dizi ekleme, inversiyon, transpozon ekleme, mitokondriyal ekleme, ardışık kopya ve ters/düz ayrışık kopya gibi birçok YV'yi karakterize edebilmektedir. Bu varyasyonların yüksek doğrulukta keşfi için okuma çiftleri, okuma derinliği ve ayrık okumalar gibi farklı sinyalleri birarada kullanmaktadır. Ayrıca TARDIS, genomun tekrarlı yapısı sebebiyle aynı okumanın birden çok yere benzer doğrulukta hizalanmasından dolayı oluşan hataları göz önünde bulundurarak, tüm hizalanma lokasyonlarını da kullanabilme özelliğine sahiptir. Son zamanlarda kısa okumaların barındırdığı kısıtlamalar sebebiyle yeni kütüphane hazırlama protokolleri geliştirilmiştir. 10x Genomics de bunlardan biridir. Bu teknik, düşük maliyetle uzun mesafeli bitişiklik bilgisi (Long range contiguity) sağlayan, yüksek maliyetli uzun okumalara alternatif bir yöntemdir. TARDIS, kısa okumaların sebep olduğu kısıtlamaların önüne geçebilmek için 10x Genomics'in bağlantılı okumalarını da kullanabilmektedir. Geliştirdiğimiz algoritmaların doğruluk oranlarını simülasyon ve gerçek veriler kullanarak değerlendirdik. Simülasyonlarda TARDIS %97,67 hassasiyet ve %1,12 hatalı tahmin oranını yakaladı. Gerçek veri deneyleri için de iki haploid (CHM1 ve CHM13) ve bir diploid (NA12878) insan genomu kullandık. Sonuçları PacBio veri setleriyle karşılaştırdığımızda TARDIS'in literatürdeki en başarılı metotlara göre daha yüksek doğruluğa sahip olduğunu gördük. Ayrıca CHM1 genomu için TARDIS'in ardışık ve ayrışık kopya varyasyonlarında çok düşük hata oranına sahip olduğunu gösterdik (En iyi 50 tahmininde hata oranı %5'den azdır). Son olarak belirtmeliyiz ki burada tanıttığımız algoritmalar YND teknolojisini kullanarak ayrışık yapısal varyasyonları karakterize edebilen ilk algoritmalardır.","Genomic variations including single nucleotide polymorphisms (SNPs), small INDELs and structural variations (SVs) are known to have significant phenotypic effects on individuals. Among them, SVs, that alter more than 50 nucleotides of DNA, are the major source of complex genetic diseases such as Crohn's, schizophrenia and autism. Additionally, the total number of nucleotides affected by SVs are substantially higher than SNPs (3.5 Mbp SNP, 15-20 Mbp SV). Today, we are able to perform whole genome sequencing (WGS) by utilizing high throughput sequencing technology (HTS) to discover these modifications unimaginably faster, cheaper and more accurate than before. However, as demonstrated in the 1000 Genomes Project, HTS technology still has significant limitations. The major problem lies in the short read lengths (<250 bp) produced by the current sequencing platforms and the fact that most genomes include large amounts of repeats make it very challenging to unambiguously map and accurately characterize genomic variants. Thus, most of the existing SV discovery tools focus on detecting relatively simple types of SVs such as insertions, deletions, and short inversions. In fact, other types of SVs including the complex ones are of crucial importance and several have been associated with genomic disorders. To better understand the contribution of these SVs to human genome, we need new approaches to accurately discover and genotype such variants. Therefore, there is still a need for accurate algorithms to fully characterize a broader spectrum of SVs and thus improve calling accuracy of more simple variants. Here we introduce TARDIS that harbors novel algorithms to accurately characterize various types of SVs including deletions, novel sequence insertions, inversions, transposon insertions, nuclear mitochondria insertions, tandem duplications and interspersed segmental duplications in direct or inverted orientations using short read whole genome sequencing datasets. Within our framework, we make use of multiple sequence signatures including read pair, read depth and split read in order to capture different sequence signatures and increase our SV prediction accuracy. Additionally, we are able to analyze more than one possible mapping location of each read to overcome the problems associated with repeated nature of genomes. Recently, due to the limitations of short-read sequencing technology, newer library preparation techniques emerged and 10x Genomics is one of these initiatives. This technique is regarded as a cost-effective alternative to long read sequencing, which can obtain long range contiguity information. We extended TARDIS to be able to utilize Linked-Read information of 10x Genomics to overcome some of the constraints of short-read sequencing technology. We evaluated the prediction performance of our algorithms through several experiments using both simulated and real data sets. In the simulation experiments, TARDIS achieved 97.67% sensitivity with only 1.12% false discovery rate. For experiments that involve real data, we used two haploid genomes (CHM1 and CHM13) and one human genome (NA12878) from the Illumina Platinum Genomes set. Comparison of our results with orthogonal PacBio call sets from the same genomes revealed higher accuracy for TARDIS than state of the art methods. Furthermore, we showed a surprisingly low false discovery rate of our approach for discovery of tandem, direct and inverted interspersed segmental duplications prediction on CHM1 (less than 5% for the top 50 predictions). The algorithms we describe here are the first to predict insertion location and the various types of new segmental duplications using HTS data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mevcut İnternet mimarisi, internette giderek artan sayıda kullanıcı ve içerikle birlikte, içeriği bulmakta ve taşımakta daha verimli olmak için gelişmelidir. Adlandırılmış Veri Ağı (NDN) bunu başarmayı amaçlayan yeni bir internet mimarisidir. Adlandırılmış Veri Ağı, verileri almak için sunucuları ve konumlarını düşünmek yerine doğrudan veri üzerinde odaklanır. Bir paketin belirli bir hedef adrese teslim edilmesi yerine isim ile veri talep etme ve alma paradigmasını benimser. NDN'de, ara ağ düğümlerindeki verilerin önbelleğe alınması, verilerin tüketicilere daha yakın bir yere taşınmasına yardımcı olabilir ve bu şekilde uygulama yanıt süresinin ve ağ bant genişliği tüketiminin azaltılmasında çok etkili olabilir. Bunun için uygun önbellek yerleştirme ve değiştirme stratejileri gereklidir. Bu tezde, biz adlandırılmış bir veri ağındaki ara düğümlerdeki verileri verimli ve etkili bir şekilde önbelleğe almak için PCDC adı verilen, üretici merkezli ağ içi önbelleğe alma stratejisi öneriyoruz. Bizim şemamızda, çoklu tüketici yollarının kesişme noktalarındaki ara düğümler, istenen verileri önbelleğe almak için önbellekleme noktaları olarak seçilmiştir. NDN protokollerinin pratik kısıtlarını dikkate alarak, bu tür önbellekleme noktalarını seçmek için gerekli mekanizmaları ve algoritmaları öneriyoruz. Şemamızı ndnSIM / ns3'te simüle ettik ve performansını değerlendirmek için kapsamlı simülasyon deneyleri yaptık. Simülasyon sonuçları, plan çeşitliliğinin ve ortalama isabet oranının artırılması, uygulama yanıt süresinin ve ağ bant genişliği tüketiminin azaltılması açısından uygulanabilir ve uygulanmasının pratik olduğunu göstermektedir.","With growing number of users and content on Internet, the existing Internet architecture needs to evolve to be more efficient to find and carry content, and Named Data Networking (NDN) is a new Internet architecture aiming to achieve this. Named Data Networking focuses on data directly instead of considering servers and their locations to retrieve data. It adopts the paradigm of requesting and retrieving data with name instead of delivering a packet to a specific destination address. In NDN, caching of data in intermediate network nodes can help moving data closer to the consumers and in this way can be very effective in reducing application response time and network bandwidth consumption. For this, proper cache placement and replacement strategies are needed. In this thesis, we propose a producer-centric in-network caching strategy, called PCDC, to efficiently and effectively cache data in the intermediate nodes of a named data network. In our scheme, intermediate nodes that are at the intersections of multiple consumer paths are selected as caching points to cache requested data. We propose the necessary mechanisms and algorithms to select such caching points, considering the practical constrains of NDN protocols. We simulated our scheme in ndnSIM/ns3 and did extensive simulation experiments to evaluate its performance. The simulation results show that our scheme is feasible and practical to apply and is efficient in terms of increasing packet diversity and average hit-ratio, and decreasing application response time and network bandwidth consumption."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Serebral palsili hastaların rehabilitasyonuna yönelik olarak ergoterapistler tarafından derinlik kamerası tabanlı sanal rehabilitasyon uygulamalarının kullanımı ilgi çekmekte olan bir yaklaşımdır. Bu tip bir sistem geliştirirken, hedeflenen alana özel bir egzersiz tanıma yönteminin kullanılması oldukça önemlidir. Bu amaca yönelik, başarılı bir hareket tanıma çözümü geliştirmek için bazı problemlerin aşılması gerekmektedir. Bu problemler kısaca egzersiz kümesinde tanımlı olmayan hareketlerin tespit edilerek dikkate alınmaması ve hastalar tarafından fiziksel olarak yetersiz oldukları egzersizlerde bu eksiklerini telafi etme amacıyla yanlış bir şekilde tamamladıkları egzersizlerin hatalı olarak tanınabilmesidir. Saklı Markov model tabanlı olarak geliştirilen bir çözümler bütünü bahsedilen bu sorunlara çözüm olarak sunulmuştur. Geliştirilen çözüm üst ekstremite fonksiyonel egzersizleri ile çalışmaktadır. Çözümümüz, ilk olarak tanımlı olmayan hareketleri uyarlanmış eşik değeri üretebilmek üzere tasarlanan modeller aracılığıyla tespit ederek, elemektedir. Sonraki adımda ise ergoterapistler tarafından belirlenen telafi hataları ve bu hataların sınıflandırılabilmesi için gerekli kısıtlar kullanılarak öznitelik eşikleme adını verdiğimiz yöntem üzerinde çalışan özel negatif modeller aracılığıyla bu telafi hatalarının tespiti sağlanır. Geliştirilen yönteme yönelik çeşitli testler uzman ergoterapistlerin gözetimi altında ve laboratuvar ortamında tamamlanmış ve elde edilen sonuçlar sunulmuştur.","Depth camera-based virtual rehabilitation systems are gaining traction in occupational therapy for approaching patients with cerebral palsy. When developing such a system, a domain specific exercise recognition method is vital. In order to design a successful gesture recognition solution for this specific purpose, some obstacles needs to be overcome, namely; detection of gestures that are not related to the defined exercise set and recognition of incorrect exercises that are performed by the patients to compensate for their lack of ability. A combination of solutions, that are based on hidden Markov models, targeting aforementioned obstacles are proposed and elaborated on. The proposed solution works for upper extremity functional exercises and critical compensation mistakes together with restrictions for classifying these mistakes are determined with the help of occupational therapists. Afterwards, we first aim to eliminate the undefined gestures by designing two models that produce adaptive threshold values. Then, we utilize specific negative models based on an approach named feature thresholding and train them specifically for each exercise to distinguish the compensation mistakes. We conducted various tests using our method in a laboratory environment under the supervision of occupational therapists and presented the results of our proposed approach."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom çapında ilişkilendirme çalışmalarında (Genome-Wide Association Studies - GWAS) saptanan genetik varyasyonlar nadiren tek başlarına karmaşık hastalıkların kalıtsal aktarımını açıklamakta başarılı olabilmektedirler. Şimdiye kadar, fenotiple ilişkili olan varyasyonların bir alt kümesini seçmek amacıyla çeşitli yöntemler geliştirilmiştir. Bu yöntemlerden bazılarında, tekil nükleotit polimorfizmlerini (Single Nucleotide Polymorphism - SNP) bir SNP-SNP ağında bağlı şekilde ödüllendiren bir yaklaşım izlenmiştir. Bu yaklaşımın fenotipi açıklayıcı ve biyolojik anlamda yorumlanabilir SNP'leri bulmakta başarılı sonuçlar elde ettiği de gösterilmiştir. Fakat, bizim hipotezimize göre, ağ üzerinde bağlılık kısıtlaması yapmak benzer biyolojik süreçleri etkileyen, ihtiyaç fazlası SNP'lerin seçimini destekler ve bu da fenotipi açıklama gücünde potansiyel bir kayba sebep olabilir. Bu doğrultudaki çalışmamızda, birbirini tamamlayıcı etkiye sahip olması adına, ağ üzerinde yakın SNP'leri seçmekten kaçınan SPADIS adında yeni bir yöntem sunulmaktadır. SPADIS bu işlevini, altmodüler bir fonksiyonun azami değerine yakınlığını bir sabit çarpan (1-1/e) ile garanti edebilen açgözlü (greedy) bir algoritma ile yerine getirmektedir. SPADIS, deneylerimizde, modern yöntemlerden biri olan SConES ile Arabidopsis Thaliana verisinde karşılaştırılmıştır: Fenotip açıklayabilme ölçütünde ortalama olarak 17 fenotipin 15'inde daha iyi sonuçlar elde edilmekle birlikte, çeşitli ağ ve kurgular arasında istikrarlı gelişmeler de sağlanmıştır. Üstelik, SPADIS'in fenotip ile ilişki daha fazla sayıda gen saptadığı ve çalışmasını daha kısa sürede tamamladığı gösterilmiştir. Ayrıca, deneylerimizde, Hi-C verisinin SNP seçimi problemi çerçevesinde SNP-SNP ağı oluşturmadaki kullanımı incelenmiş ve bunun test edilen tüm yöntemlerin fenotipi açıklamasına katkıda bulunduğu gözlemlenmiştir.","Phenotypic heritability of complex traits and diseases is seldom explained by individual genetic variants identfied in genome-wide association studies (GWAS). Many methods have been developed to select a subset of variant loci, which are associated with or predictive of the phenotype. Selecting connected Single Nucleotide Polymorphisms (SNPs) on SNP-SNP networks has been proven successful finding biologically interpretable and predictive SNPs. However, we argue that the connectedness constraint favors selecting redundant features that affect similar biological processes and therefore does not necessarily yield better predictive performance. To this end, we propose a novel method called SPADIS that favors the selection of remotely located SNPs in order to account for their complementary effects in explaining a phenotype. SPADIS selects a diverse set of loci on a SNP-SNP network. This is achieved by maximizing a submodular set function with a greedy algorithm that ensures a constant factor (1 - 1/e) approximation to the optimal solution. We compare SPADIS to the state-of-the-art method SConES, on a dataset of Arabidopsis Thaliana with continuous flowering time phenotypes. SPADIS has better average phenotype prediction performance in 15 out of 17 phenotypes when the same number of SNPs are selected and provides consistent improvements across multiple networks and settings on average. Moreover, it identifies more candidate genes and runs faster. We also investigate the use of Hi-C data to construct a SNP SNP network in the context of SNP selection problem for the first time, which yields improvements in regression performance across all methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sis bili ̧sim, bulut bili ̧sim kavramını a ̆gın sınırlarına kadar geni ̧sletmeyi ama ̧clayan, son kullanıcı ag ̆ına ekstra depolama ve i ̧sleme gu ̈cu ̈ sag ̆layan yeni bir kavramdır. Sis bili ̧simin bu ̈yu ̈k bir katkısı Kablosuz Senso ̈r Ag ̆ları (KSA'lar) kapsamındadır. KSA, nispeten karma ̧sık su ̈re ̧clerin i ̧slenmesinde kısa kalmasını sag ̆layan ucuz, aku ̈lu ̈ ve basit i ̧sleme cihazlarından olu ̧sur. Bu nedenle, KSA'lara sis bilgisinin uygulanması bulut ile a ̆g arasındaki bo ̧slu ̆gu dolduracak ve bu sayede daha ̈once sadece bulut tarafında mu ̈mku ̈n olan, hesaplama a ̧cısından kapsamlı operas- yonların yapılmasına olanak sag ̆layacaktır. Bizim ̧calı ̧smamızda, veri madencili ̆gi teknikleri kullanılarak orman yangınlarının tespiti ic ̧in KSA'ların gu ̈ ̧c tu ̈ketimini en aza indirgemek amacıyla sis tarafından sag ̆lanan i ̧slem gu ̈cu ̈nu ̈ kullanıyoruz. Sis tabakası, orman yangınlarının olasılıg ̆ını tahmin eden bir model u ̈retmek i ̧cin sıcaklık, nem, yag ̆mur vb. gibi ag ̆ tarafından u ̈retilen verileri kullanır. Ardından, ortamın mevcut durumuna bag ̆lı olarak a ̆gın ̧calı ̧sma ̧seklini tahmin etmek ic ̧in sis tabakası bu modeli kullanır. Orman yangınının yu ̈ksek olasılıkla tahmin edilmesi, KSA'nın arttırılmı ̧s bir aktivitesi ile sonu ̧clanırken, du ̈ ̧su ̈k yangın olasılıg ̆ı, azaltılmı ̧s bir ag ̆ aktivitesi ile sonu ̧clanmaktadır. Sonu ̧c olarak, o ̈nerilen modelimiz KSA ic ̧indeki enerji tu ̈ketimini optimize eder ve orman yangınlarının tespit su ̈resini azaltır.","Fog computing is a new paradigm that aims to extend the concept of cloud com- puting to the edge of the network, providing the end users network with extra storage and processing power. One big contribution of Fog computing is in the context of Wireless Sensor Networks (WSNs). WSNs consist of cheap, battery powered and simple processing devices that make it fall short in handling rel- atively complex processes.Therefore, applying fog computing to WSNs will fill the gap between the cloud and the network and by that, it will enable computa- tionally extensive operations which were earlier possible only at the cloud side. In our work, we exploit the processing power provided by the Fog to minimize the power consumption of WSNs for forest fire detection through the use of data mining techniques. The Fog layer uses the data generated by the network such as temperature, humidity, rain, etc., to train a model that predicts the proba- bility of forest fires. Next, the Fog layer uses this model to predict the mode of operation of the network based on the current condition of the environment. While a high predicted probability of forest fire results in an increased activity of the WSN, a low fire probability results in a reduced network activity. As a result, our proposed model optimizes the energy consumption within the WSN and improves the detection time of forest fires."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Protein kinazlar, diğer proteinlerin fosforilasyonunu katalize eden büyük bir enzim ailesidir. Protein aktivitesi için moleküler anahtarlar olarak görev yaparlar ve fosforilasyon olayları vasıtasıyla hücre içi sinyal iletimini düzenlerler. Bu sebeple, bir çok hücresel mekanizmada, merkezi bir rol üstlenirler. Öte yandan, kinaz proteinlerinin fonksiyonel bozukluklarının da birçok hastalıkla ilişkili olduğu belirlenmiştir. Hücredeki normal ve arızalı sinyallerin anlaşılması için, fosforilasyon bölgelerinin tanımlanması ve bu bölgelerin fosforilasyonunda hangi kinazların görev aldığının belirlenmesi gerekir. Kütle spektrometresindeki son gelişmeler, fosforilasyon bölgelerinin proteom seviyesinde hızlı bir şekilde tanımlanmasını olanaklı kılmıştır. Alternatif olarak, protein dizisinde fosforilasyon yerlerini tahmin eden birçok hesaplamalı yöntem de mevcuttur. Bir fosforilasyon bölgesi, deneysel ya da hesaplamalı yöntemlerle belirlendikten sonra, bu bölgeyi hangi kinazın fosforile ettiğini belirlemek ise bir sonraki soruyu oluşturur. Fosforilasyon bölgelerini tahminleyen mevcut hesaplamalı metotların bir kısmı, kinaza-özgü tahminler sunsa da bu yöntemler konvensiyonel gözetimli öğrenme tekniklerine dayandıkları için, ancak bir çok fosforilasyon bölgesi bilinen kinazlar için yeni bölgeleri tahminleyebilirler. Bu zamana kadar üzerine eğinilmemiş bir problem ise daha önce fosforile ettiği hiç bir bölge tespit edilmemiş olan kinazlar için tahmin yapabilmektir. Klasik gözetimli tekniklere dayanan yöntemlerinden hiçbiri, bu tür kinazlar için bağlanma bölgelerini öngöremeyecektir. Bu çalışmada, fosforilasyon bilgisi olmayan kinazlar için sıfır-vuruşlu ögrenme yaklaşımına dayanan DeepKinZero'yu sunuyoruz. DeepKinZero fosforilasyon yerinin merkezde olduğu peptit dizisini girdi olarak alır ve çift yönlü tekrarlayan sinir ağı ile bu dizileri bir vektör uzayına yerleştirir. Kinazları da fonksiyonel özelliklerine ve protein dizilerine göre bir vektör uzayına yerleştirilir. Fosforilasyon bölgesinin çok boyutlu uzayda temsili ile kinazların temsili arasında tanımlanmış bir uyumluluk fonksiyonu aracılığıyla, DeepKinZero fosforile ettiği bölgelerin bilindiği kinazlardan bu bölgeleri bilinmeyen kinazlara bilgi aktarır. Hesaplamalı deneylerimiz, DeepKinZero'nun taban modellerine göre doğrulukta 30 kata varan artış sağladığını göstermektedir. DeepKinZero'nun önceden fosforilasyon bölgeleri bilinmeyen kinazların fosforilasyon bölgelerini tahminleyerek, önemli ilaç hedefleri olan ve az çalışılmış bu kinazlar hakkındaki mevcut bilgi birikimini artırmasını bekliyoruz.","Protein kinases are a large family of enzymes that catalyze the phosphorylation of other proteins. By acting as molecular switches for protein activity, the phosphorylation events regulate intracellular signal transduction, thereby assuming a central role in a broad range of cellular activities. On the other hand, aberrant kinase function is implicated in many diseases. Understanding the normal and malfunctioning signaling in the cell entails the identification of phosphorylation sites and the characterization of their interactions with kinases. Recent advances in mass spectrometry enable rapid identification of phosphosites at the proteome level. Alternatively, there are many computational models that predict phosphosites in a given input protein sequence. Once a phosphosite is identified, either experimentally or computationally, knowing which kinase would catalyze the phosphorylation on this particular site becomes the next question. Although a subset of available computational methods provides kinase-specific predictions for phosphorylation sites, due to the need for training data in such supervised methods, these tools can provide predictions only for kinases for which a substantial number of the phosphosites are already known. A particular problem that has not received any attention is the prediction of new sites for kinases with few or no a priori known sites. None of the current computational methods which rely on the classical supervised learning settings can predict additional sites for this kinases. We present DeepKinZero, the first zero-shot learning approach, that can predict phosphosites for kinases with no known phosphosite information. DeepKinZero takes a peptide sequence centered at the phosphorylation site and learns the embeddings of these phosphosite sequences via a bi-directional recurrent neural network, whereas kinase embeddings are based on protein sequence vector representations and the taxonomy of kinases based on their functional properties. Through a compatibility function that associates the representations of the site sequences and the kinases, DeepKinZero transfers knowledge from kinases with many known sites to those kinases with no known sites. Our computational experiments show that DeepKinZero achieves a 30-fold increase in accuracy compared to baseline models. DeepKinZero complements existing approaches by expanding the knowledge of kinases through mapping of the phosphorylation sites pertaining to understudied kinases with no prior information, which are increasingly investigated as novel drug targets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"G¨un¨um¨uzde insan genomları konusundaki anlayı¸sımız, modern bili¸sim teknolojisinin bir bireyin t¨um genomunu hızlı ve do˘gru bir ¸sekilde belirleyebilme yetene˘ginden etkilenmektedir. Ge¸cti˘gimiz on yıl boyunca, y¨uksek verimli dizileme (HTS) teknolojileri, zaman ve maliyette ¨onemli bir azalma ile birlikte, tek bir ¸calı¸smada y¨uz milyonlardan milyarlarcaya kadar DNA par¸cası ¨uretme kabiliyeti sayesinde dikkat ¸cekici biyomedikal ke¸siflere kapı a¸cmı¸stır. Ancak, bu dizileme verisi bollu˘gu mevcut algoritmaların ve donanımların i¸slem kapasitelerinin sınırlarını zorlamaya devam etmektedir. Bir hastanın genomunu analiz etmek i¸cin, ""okuma"" adı verilen bu par¸caların her biri referans genomundaki aday b¨olgelerle olan benzerliklerine bakılarak, referans genomu ¨uzerine yerle¸stirilir. Yakla¸sık karakter dizgisi e¸sle¸stirme problemi ¸seklinde form¨ule edilen ve hizalama olarak adlandırılan benzerlik hesaplaması, i¸slemsel bir darbo˘gazdır ¸c¨unk¨u: (1) ikinci dereceden devingen programlama algoritmaları kullanılarak hesaplanır ve (2) referans genomundaki aday b¨olgelerin b¨uy¨uk bir b¨ol¨um¨u ile verilen okuma par¸cası birbirlerinden y¨uksek d¨uzeyde farklılık g¨osterdiklerinden dolayı hizalanamaz. Bu ¸sekilde yanlı¸s belirlenen aday b¨olgelerin hizalanabilirli˘gin hesaplanması, g¨un¨um¨uzdeki okuma haritalandırıcı algoritmaların ¸calı¸sma s¨urelerinin b¨uy¨uk b¨ol¨um¨un¨u olu¸sturmaktadır. Bu nedenle, hesaplama olarak maliyetli bu hizalama algoritmalarını ¸calı¸stırmadan ¨once, do˘gru olmayan aday b¨olgeleri tespit edebilen ve bu b¨olgeleri aday b¨olge olmaktan ¸cıkaran, hızlı ve etkili bir filtre geli¸stirmek ¸cok ¨onemlidir. Bu tezde, ¨on hizalama a¸saması olarak i¸slev g¨oren ve yanlı¸s aday konumlarının ¸co˘gunu filtrelemeyi hedefleyen d¨ort yeni algoritma sunuyoruz. Algoritmalarımızı GateKeeper, SLIDER, MAGNET ve SneakySnake olarak adlandırıyoruz. Onerilen ¨on hizalama filtrelerinin ilk temel fikri, iki dizi arasında payla¸sılan ¨ t¨um benzer segmentleri do˘gru bir ¸sekilde tespit ederek y¨uksek filtreleme do˘grulu˘gu sa˘glamaktır. ˙Ikinci temel fikir, ¨onerilen d¨ort filtreleme algoritmamızın hızlandırılması i¸cin modern FPGA'ların ¸cok b¨uy¨uk ¨ol¸cekte paralel mimarisini kullanmaktır. SneakySnake'i esas olarak biyoinformatisyenlerin mevcut olan, donanım karma¸sıklı˘gı ile u˘gra¸smak zorunda olmadıkları emtia masa¨ust¨u ve sunucularında kullanabilmeleri i¸cin geli¸stirdik. On okuma filtreleme yakla¸sımımızın ¨ avantaj ve dezavantajlarını 12 ger¸cek veri setini, farklı okuma uzunlukları ve mesafe e¸sikleri kullanarak ayrıntılı olarak de˘gerlendirdik. De˘gerlendirmemizde, donanım ¨on hizalama filtrelerimizin e¸sde˘ger CPU uygulamalarına g¨ore iki ila ¨u¸c derece hızlı olduklarını g¨osteriyoruz. Donanım ¨on hizalama filtrelerimizi son teknoloji okuma hizalayıcılarıyla entegre etmenin hizalayıcının ¸calı¸sma s¨uresini d¨uzenleme mesafesi e¸si˘gine ba˘glı olarak 21.5x. Son olarak, ¨on hizalama filtrelerinin etkin CPU uygulamasının hala ¨onemli faydalar sa˘gladı˘gını g¨osteriyoruz. SneakySnake'in en iyi performansa sahip CPU tabanlı okuma ayarlayıcıları Edlib ve Parasail'in y¨ur¨utme s¨urelerini sırasıyla 43x ve 57,9x'e kadar azalttı˘gını g¨osteriyoruz. Bu tezin ana sonucu, hızlı ve verimli bir filtreleme mekanizması geli¸stirilmesi ve bu mekanizmanın do˘grulu˘gunun daha iyi anla¸sılması, hizalayıcıların yeteneklerinden hi¸cbir ¸sey ¨od¨un vermeden, okuma hizalamasının ¸calı¸sma s¨uresinde ¨onemli bir azalmaya yol a¸cmaktadır. Yeni mimarilerimizin ve algoritmalarımızın, mevcut ve gelecekteki genom analiz planlarında benimsenmelerini katalize etti˘gimizi umuyor ve buna inanıyoruz.","Our understanding of human genomes today is affected by the ability of modern computing technology to quickly and accurately determine an individual's entire genome. Over the past decade, high throughput sequencing (HTS) technologies have opened the door to remarkable biomedical discoveries through its ability to generate hundreds of millions to billions of DNA segments per run along with a substantial reduction in time and cost. However, this flood of sequencing data continues to overwhelm the processing capacity of existing algorithms and hardware. To analyze a patient's genome, each of these segments - called reads - must be mapped to a reference genome based on the similarity between a read and ""candidate"" locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: (1) it is implemented using quadratic-time dynamic programming algorithms, and (2) the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment algorithms. In this thesis, we introduce four new algorithms that function as a prealignment step and aim to filter out most incorrect candidate locations. We call our algorithms GateKeeper, Slider, MAGNET, and SneakySnake. The first key idea of our proposed pre-alignment filters is to provide high filtering accuracy by correctly detecting all similar segments shared between two sequences. The second key idea is to exploit the massively parallel architecture of modern FPGAs for accelerating our four proposed filtering algorithms. We also develop an efficient CPU implementation of the SneakySnake algorithm for commodity desktops and servers, which are largely available to bioinformaticians without the hassle of handling hardware complexity. We evaluate the benefits and downsides of our pre-alignment filtering approach in detail using 12 real datasets across different read length and edit distance thresholds. In our evaluation, we demonstrate that our hardware pre-alignment filters show two to three orders of magnitude speedup over their equivalent CPU implementations. We also demonstrate that integrating our hardware pre-alignment filters with the state-of-the-art read aligners reduces the aligner's execution time by up to 21.5x. Finally, we show that efficient CPU implementation of pre-alignment filtering still provides significant benefits. We show that SneakySnake on average reduces the execution time of the best performing CPU-based read aligners Edlib and Parasail, by up to 43x and 57.9x, respectively. The key conclusion of this thesis is that developing a fast and efficient filtering heuristic, and developing a better understanding of its accuracy together leads to significant reduction in read alignment's execution time, without sacrificing any of the aligner' capabilities. We hope and believe that our new architectures and algorithms catalyze their adoption in existing and future genome analysis pipelines."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tüm kolorektal kanserlerin yüzde doksanından fazlasını oluşturan kolon adenokarsinomu, kolon bezlerini oluşturan epitel hücrelerden kaynak almaktadır. Dolayısıyla, bu kanserin tanı ve derecelendirmesinde, epitel hücrelerin organizasyonlarındaki bozuklukların, bundan dolayı da kolon bezlerindeki deformasyonların incelenmesi önemlidir. Bu nedenle, kolon dokusundaki bezlerin yerlerinin tespit edilmesi ve deformasyonlarının nicelenmesi, otomatik veya yarı otomatik karar destek sistemlerinin geliştirilmesi için esastır. Bu motivasyonla, bu tez, histopatolojik doku görüntülerindeki bezleri saptamak için yeni bir yapısal bölütleme algoritması önermektedir. Bu yapısal algoritma, Voronoi diyagramı kullanarak histopatolojik görüntü üzerinde bir temel öğe kümesi yerleştirmeyi ve bu şekilde görüntüyü yeni bir gösterime dönüştürmeyi; bu yeni gösterim üzerinde kurallar tanımlayarak bez adaylarını üretmeyi; ve uygunluk skorlarına göre bu adaylar arasından alt küme seçen tekrarlı bir algoritma tasarlamayı önermektedir. Bu tezin başlıca katkısı; önerilen algoritma ile ortaya konan gösterimin, bezlerin kolon dokusundaki görünümlerine göre kural ve uygunluk skoru tanımlayarak, kolon bezlerinin daha iyi kodlanmasına olanak sağlamasıdır. Bu gösterim ve kodlama daha önceki çalışmalarda kullanılmamıştır. Algoritmamızın deneysel sonuçları, önerilen bu algoritmanın, ek bir işlem uygulamadan, piksel tabanlı ve yapısal benzerlerinin bölütleme sonuçlarını iyileştirdiğini göstermiştir.","Colon adenocarcinoma, which accounts for more than 90 percent of all colorectal cancers, originates from epithelial cells that form colon glands. Thus, for its diagnosis and grading, it is important to examine the distortions in the organizations of these epithelial cells, and hence, the deformations in the colon glands. Therefore, localization of the glands within a tissue and quantification of their deformations is essential to develop an automated or a semi-automated decision support system. With this motivation, this thesis proposes a new structural segmentation algorithm to detect glands in a histopathological tissue image. This structural algorithm proposes to transform the histopathological image into a new representation by locating a set of primitives using the Voronoi diagram, to generate gland candidates by defining a set of rules on this new representation, and to devise an iterative algorithm that selects a subset of these candidates based on their fitness scores. The main contribution of this thesis is the following: The representation introduced by this proposed algorithm enables us to better encode the colon glands by defining the rules and the fitness scores with respect to the appearance of the glands in a colon tissue. This representation and encoding have not been used by the previous studies. The experimental results of our algorithm show that this proposed algorithm improves the segmentation results of its pixel-based and structural counterparts without applying any further processing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dijital patolojide, hücre görüntüleme sistemleri, histopatolojik olayları hücresel düzeyde anlamamıza izin verir. Bu sistemlerde genellikle ilk adım, histopatolojik görüntülerin etkili ve güvenilir bir analizi için sonraki aşamaları büyük ölçüde etkileyen hücre bölütlemesidir. Diğer taraftan, hücre bölütlemesi, farklı piksel yoğunluklarına ve morfolojik özelliklere sahip hücrelerin bulunduğu histopatolojik görüntüler için zor bir iştir. Hücrelerin hem piksel yoğunluğunu hem de morfolojik özelliklerini bütünleştiren yaklaşımların, başarılı bölütleme sonuçlarına ulaşması muhtemeldir. Bu tez, rutin olarak kullanılan hematoksilen ve eosin tekniği ile boyanmış histopatolojik doku örnek görüntülerinde hücrelerin güvenilir bölütlemesi için derin öğrenme tabanlı bir yaklaşım önermektedir. Bu yaklaşım, ilk aşamada piksel yoğunluğunu ve ikinci aşamada morfolojik hücre özniteliklerini kullananan iki aşamalı konvolüsyonel sinir ağlarını ortaya koymaktadır. Önerilen yöntem, hücre morfolojisi ile ilgili hücre özniteliklerinin, birinci aşamada üretilen sınıf etiketlerinden ve olasılıklarından çıkarılmasına ve son bölütleme için ikinci aşamada morfolojik hücre özniteliklerinin kullanılmasına dayanmaktadır. Önerilen yaklaşım 3428 hücre üzerinde test edilmiş ve deneysel sonuçlar, yaklaşımımızın farklı bölütleme teknikleriyle karşılaştırıldığında daha iyi bölütleme sonuçları verdiğini göstermiştir.","In digital pathology, cell imaging systems allow us to comprehend histopathological events at the cellular level. The first step in these systems is generally cell segmentation, which substantially affects the subsequent steps for an effective and reliable analysis of histopathological images. On the other hand, cell segmentation is a challenging task in histopathological images where there are cells with different pixel intensities and morphological characteristics. The approaches that integrate both pixel intensity and morphological characteristics of cells are likely to achieve successful segmentation results. This thesis proposes a deep learning based approach for a reliable segmentation of cells in the images of histopathological tissue samples stained with the routinely used hematoxylin and eosin technique. This approach introduces two stage convolutional neural networks that employ pixel intensities in the first stage and morphological cell features in the second stage. The proposed TwoStageCNN method is based on extracting cell features, related to cell morphology, from the class labels and posteriors generated in the first stage and uses the morphological cell features in the second stage for the final segmentation. We evaluate the proposed approach on 3428 cells and the experimental results show that our approach yields better segmentation results compared to different segmentation techniques."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezin ilk bölümünde, çağrı modellerinde ortaya çıkan çizge problemlerine odaklanılmaktadır. Bu tür modeller, tekli çağrı, çoklu çağrı ve karşılıklı çoklu çağrı bağlantılarını içeren bazı çağrı tiplerinin kombinatoryel özelliklerini incelemek için kullanılır. Burada, karşılıklı çoklu çağrılara odaklanıyoruz ve arayanların sayısı veya alıcıların sayısı 2 veya 3'e sabitlendiğinde etiketsiz karşılıklı çoklu çağrıların sayısı için kapalı form ifadeleri sağlıyoruz. Bu durumda, çizge teorisinde açık bir problemi çözerek, yani etiketsiz iki parçalı çizgeleri sayarak bu tür çağrıların sayısıyla ilgili alt ve üst sınırlar elde ediyoruz. Daha sonra, bu sonuçlar, sol(sağ) tarafı küme olarak etiketli ve iki tarafı da küme olarak etiketli iki parçalı çizgelere genişletilmektedir. Tezin ikinci bölümünde, tek taraflı, ikili ağaç anahtarlama ağları için bağlama ve yönlendirme problemlerine odaklanıyoruz. özellikle, tek taraflı, ikili ağaç anahtarlama ağları için yönlendirme algoritmasının O(n) hesaplama zamanını O (lg n)'e düşürüyoruz. Tek taraflı, ikili ağaç anahtarlama ağları için yeni bir bağlama algoritması da sunuyoruz. Son olarak, bağlama tasarımı verilen tek taraflı, ikili ağaç anahtarlama ağının terminallerinin eşleştirildiği kümenin yerini belirlemek için bir algoritma sunulmuştur. Bu algoritmanın zaman karmaşıklığının O(lg n) olduğu gösterilmiştir.","In the first part of this dissertation, we focus on graph problems that arise in call models. Such models are used to study the combinatorial properties of certain types of calls that include unicast, multicast, and bicast interconnections. Here we focus on bicast calls, and provide closed-form expressions for the number of unlabeled bicast calls when either the number of callers or number of receivers is fixed to 2 or 3. We then obtain lower and upper bounds on the number of such calls by solving an open problem in graph theory, namely counting the number of unlabeled bipartite graphs. Next, these results are extended to left (right) set labeled and set labeled bipartite graphs. In the second part of the dissertation, we focus on wiring and routing problems for one-sided, binary tree switching networks. Specifically, we reduce the O(n) time complexity of the routing algorithm for the one-sided, binary tree switching network to O(lg n). We also present a new wiring algorithm for one-sided, binary tree switching networks. Finally, an algorithm is presented to locate the cluster in which the terminals of the corresponding one-sided binary tree switching network are paired. The time complexity of this algorithm is shown to be O(lg n)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derin öğrenme alanındaki son gelişmelerle birlikte, bilgisayarla görüde öğrenme makineleri ve ana yaklaşımlar, sınıfandırıcılarla birlikte kodlanmış sabit özelliklerden uçtan uca eğitilmiş, bilgisayarla görü araştırma alanlarının çoğunda başarılı sonuçlar veren derin konvülsiyonel sinir ağlarına dönüşmektedir. Tek görüntü süper çözünürlüğü, derin öğrenme gelişmelerinden önemli derecede etkilenen alanlardan biridir. Süper çözünürlük problemindeki mevcut en başarılı yöntemlerin çoğu, ağ mimarilerinde ardışık konvolusyonel katmanları kullanarak uzamsal alanda düşük çözünürlüklü görüntülerden yüksek çözünürlüklü görüntülere doğrusal olmayan bir eşleştirme öğrenirler. Bununla birlikte, bu sonuçlar, her bir farklı ölçek faktörü için ayrı bir sinir ağı mimarisi eğitimi ile elde edilir. Her ölçek faktörü için ayrı bir sinir ağının eğitilmesinin gerekliliğini ortadan kaldırmak için, spektral tanım kümesinde sınırlı sayıda öğrenme parametresine sahip yeni bir tek görüntü süper özünürlük sistemi öneriyoruz. Görüntüleri uzamsal tanım kümesinden frekans tanım kümesine dönüştüren bir spektral dönüşüm fonksiyonu olarak, ayrık Fourier dönüşümünün bir varyantı olan ayrık kosinüs dönüşümü kullanılır. Buna ek olarak, işlem sonrasında, spektral dönüşümlerden dolayı meydana gelen yapay salınımların kaldırılması için bir salınım azaltma modülü eklenmiştir. Süper-çözünürlük sistemimizin PSNR ölçümü mevcut başarılı yöntemlerden daha düşük olsa bile, spektral tanım kümesi, herhangi bir ölçek faktörü için tek bir veri kümesi ile tek bir model geliştirmemizi ve nispeten daha iyi SSIM sonuçları elde etmemizi sağlar.","With recent advances in deep learning area, learning machinery and mainstream approaches in computer vision research have changed dramatically from hardcoded features combined with classifiers to end-to-end trained deep convolutional neural networks (CNN) which give the state-of-the-art results in most of the computer vision research areas. Single-image super-resolution is one of these areas which are considerably influenced by deep learning advancements. Most of the current state-of-the-art methods on super-resolution problem learn a nonlinear mapping from low-resolution images to high-resolution images in the spatial domain using consecutive convolutional layers in their network architectures. However, these state-of-the-art results are obtained by training a separate neural network architecture for each different scale factor. We propose a novel singleimage super-resolution system with the limited number of learning parameters in spectral domain in order to eliminate the necessity to train a separate neural network for each scale factor. As a spectral transform function which converts images from the spatial domain to the frequency domain, discrete cosine transform (DCT) which is a variant of discrete Fourier transform (DFT) is used. In addition, in the post-processing step, an artifact reduction module is added for removing ringing artifacts occurred due to spectral transformations. Even if the PSNR measurement of our super-resolution system is lower than current stateof-the-art methods, the spectral domain allows us to develop a single model with a single dataset for any scale factor and relatively obtain better SSIM results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda yüksek verimli sıralama (HTS) platformları geliştirilmiştir. Araştırmacılar bu teknolojileri kullanarak bireylerin genomlarının tamamı veya hedeflenmiş kısımlarını elde edebilir. Elde edilen bu veriler araştırmacıların çeşitli biyolojik sorulara cevap vermelerini sağlar. Ancak yüksek verimli sıralama teknolojilerindeki gelişmeler çok büyük miktarda veri üretilmesine neden olur. Dosyaların asıl boyutları çok büyük olduğu için en iyi sıkıştırma algoritmalarının uygulanması bile veri miktarını çok fazla azaltamaz. Genom projelerinde yer alan katılımcılar genellikle farklı ülkelerde yer almaktadır. Bu yüzden genom projelerindeki büyük dosyaların aktarımı sorun haline gelmektedir. Araştırmacıların şu anda kullandıkları yöntemler şunlardır; dosya aktarım iletişim kuralı (FTP), Tsunami iletişim kuralı veya Aspera yazılımı ile veri aktarımı, dosyaları herkese açık veritabanlarında ya da bulutta depolamak, dosyaları ortak sunucuda tutup onun üzerinde çalışmak ya da verileri harici disklere kaydedip katılımcılar arasında dolaştırmak. Ancak tüm bu yöntemlerin maliyet, hız ve gizlilik gibi dezavantajları vardır. Bu sorunları ortadan kaldırmak için BioPeer adında bir uygulama geliştirdik. Bu uygulama, veri aktarımı için Barchart şirketi tarafından yazılmış açık kaynaklı UDP tabanlı veri aktarım protokolü UDT kullanıyor. BioPeer'e eşler arası dosya paylaşım (P2P) mimarisi uygulanmıştır. Bu mimari, BitTorrent'te olduğu gibi büyük dosyaları küçük parçalara ayırarak aynı projede yer alan eşler arasında senkronize eder. Tüm kullanıcıların birbirine bağlanabildiğinden emin olmak için NAT'ı Aşma (NAT Traversal) yöntemleri arasından UDP Delik Açma (UDP Hole Punching) yöntemi kullanılmıştır. Bu sayede NAT cihazları arkasında kalan kullanıcılar da diğer katılımcılarla dosya alışverişi yapabilir. Uygulamada güvenli veri aktarımı sağlamak için dosyalar Gelişmiş şifreleme Standardı (AES) kullanılarak şifrelenir. Simetrik şifreleme anahtarları RSA (Rivest-Shamir-Adleman) algoritması kullanılarak kullanıcılar arasında aktarılır. Ayrıca, uygulamadan daha yüksek verim elde etmek için içerik dağıtım ağı (CDN) altyapısı uygulanmıştır.","High throughput sequencing (HTS) platforms have been developed in recent years. These technologies enable researchers to answer a wide range of biological questions by obtaining whole or targeted segments of genomes of individuals. However, HTS technologies generate very large amounts of data. Even after using the best compression algorithms, data size is still huge due to large original file size. As most of the genome projects' contributors are located in different countries, transfer of the data becomes an important problem in genomics. Currently used methods for genome data sharing is transferring the files via File Transfer Protocol (FTP), Tsunami protocol or Aspera Software, storing them on public databases or clouds, working on the files stored on central servers and circulating external hard disks. However, all of these methods have some drawbacks like cost, speed, or privacy. In this thesis, to address this problem, we introduce an application called BioPeer. BioPeer uses an open source UDP-based UDT protocol written by Barchart, Inc for data transfer. We implement peer-to-peer file sharing architecture to BioPeer. This architecture is similar to BitTorrent, where large files are transferred in chunks, and synchronized between peers within the same project. To ensure every client is able to connect other clients, we employ NAT traversal via UDP hole punching method. So, users who are behind NAT devices are able to send and receive data from other peers. To provide secure file transfer, BioPeer encrypts files using Advanced Encryption Standard (AES) cipher. Symmetric encryption keys are exchanged via RSA (Rivest-Shamir-Adleman) algorithm. Additionally, content distribution network (CDN) infrastructure is implemented in order to achieve high throughput with BioPeer."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birden fazla alt sistemden oluşan ve Markov özelliğine sahip bir sistemi çok boyutlu bir Markov zinciri olarak göstermek mümkündür. Bu Markov zincirinin ulaşılabilir durum uzayı, genellikle çarpım durum uzayının bir öz alt kümesidir. Böyle bir Markov zincirine karşı gelen matrisin sıkıştırılmış olarak saklanması ve Kronecker işlemlerin etkili bir şekilde gerçekleştirilebilmesi, ulaşılabilir durum uzayının alt sistem durum uzaylarının alt kümelerinin Kartezyen çarpımlarının birleşimi olarak yazılmasını gerektirir. Bu çalışmada ilk olarak, üç veya daha fazla boyutlu bir sistemin ulaşılabilir durum uzayının en az sayıda alt sistem durum uzaylarının alt kümelerinin Kartezyen çarpımlarına bölümleme probleminin, NP-tam bir problem olduğunu gösteriyoruz. Bu problemi çözmek için, eniyi çözümü bulması kesin olmayan, biri birleştirme ve diğeri geliştirme temelli olmak üzere iki algoritma öneriyoruz. Literatürde yer alan ve rasgele olarak oluşturulmuş örneklerle yaptığımız deneyler, geliştirme temelli algoritmanın daha çok zaman ve bellek gerektirdiğini, ancak neredeyse her zaman daha az sayıda bölümleme bulabildiğini gösteriyor. Markov zincirine karşılık gelen matris, Kronecker çarpımları kullanılarak sıkıştırılmış olarak gösterildiğinde, çözümleme yöntemleri vektör-Kronecker terim çarpımının üzerine bina edilir. Kronecker terimlerdeki çarpan matrisler yoğun olduğunda, karma algoritması vektör-Kronecker terim çarpımını etkili bir şekilde hesaplayabilir. çarpan matrisler seyrek olduğunda ise, Markov zincirine karşılık gelen matrisin sıfırdan farklı elemanlarını çarpım esnasında oluşturup vektörde karşılık gelen elemanlarla çarpmak daha etkili olabilmektedir. Karma algoritmasını, Kronecker terimin çarpan matrislerindeki tamamı sıfır olan satır ve sütünları gözardı edecek şekilde değiştirmeyi öneriyoruz. Bu değişiklik, sonucu sıfır olan kayan noktalı sayı işlemlerinin gerçekleştirilmemesini sağlıyor. çok sayıda modelde, değiştirilmiş karma algoritması pek çok kayan noktalı sayı işleminden kaçınılmasını sağlıyor. Yine bazı modellerde, değiştirilmiş karma algoritmasının sıfırdan farklı elemenları çarpım esnasında oluşturan algoritmaya kıyasla daha az kayan noktalı sayı işlemi ve bellek gerektiriyor. Markov zincirine karşılık gelen matris, Kronecker çarpımları kullanılarak sıkıştırılmış olarak gösterilse bile, işlemler için gereken bellek miktarı ulaşılabilir durum uzayının büyüklüğüyle doğru orantılı olarak değişmektedir. özellikle boyut sayısı arttığında, bu daha büyük bir sorun oluşturmaktadır. Hiyerarşik Tucker ayrıştırması kullanarak, çözüm vektörlerinin görece sıkıştırılmış olarak tutulup, temel vektör-Kronecker terim çarpma işlemlerinin görece etkili olarak gerçekleştirilebildiğini gösteriyoruz. Sürekli zamanlı bir Markov zinciri olarak modellenmiş bir rassal kimyasal sistemin zamana bağlı değişimi kimyasal ana denklemi olarak da bilinen bir adi diferansiyel denklem sistemi olarak tanımlanabilir. Kimyasal ana denklemi, zamanı ayrıklaştırarak ve sayılabilir sonsuz durum uzayınının kesilerek elde edilmiş doğrusal sistemin çözülmesiyle çözümlenebilir. Durum uzayının ihmal edilebilir az sayıda durum içererek, kesilmiş durum uzayı dışında az miktarda olasılık kitlesi kalacak şekilde kesilmesi ise apaçık değildir. Hiyerarşik Tucker ayrıştırması kullanarak adi diferansiyel denklem çözücüsünün ihtiyaç duyduğu bellek miktarını azaltılabileceğini gösteriyoruz. Ayrıca, sonsuz durum uzayını kesmek için tahmin vektörlerini kullanan yenilikçi bir yöntem öneriyoruz. Sayısal deneyler uyarlamalı kesim stratejilerinin zaman ve bellek ihtiyacını, sabit kesim stratejilerine kıyasla ciddi olarak azalttığını gösteriyor. Son olarak, birden fazla sınıfa ait müşteri kabul eden, müşterilerin sınıfa bağımlı Markov varış sürecine göre geldikleri ve çok sayıda sunucusu olan bir yeniden denemeli kuyruk sistemini ele alıyoruz. Ele aldığımız sistemde, servis zamanlarınının sınıfa bağımlı evre-tipli, yeniden deneme zamanlarının ise çevrimsiz sınıfa bağımlı evre-tipli dağılım gösterdiklerini farz ediyoruz. Bu sistemin ölçümkallığının gerek ve yeter koşulunu ise sürüklenme fonksiyonlarına dayanan ölçütler kullanarak elde ediyoruz. Sonsuz durum uzayını uygun bir şekilde seçilmiş Lyapunov fonksiyonu kullanarak kesiyoruz. Elde ettiğimiz kesilmiş modeli ise çok boyutlu bir Markov zincir olarak tanımlayıp bu zincire karşılık gelen matrisin Kronecker temelli sayısal çözümlemesini gerçekleştiriyoruz.","A system with multiple interacting subsystems that exhibits the Markov property can be represented as a multi-dimensional Markov chain (MC). Usually the reachable state space of this MC is a proper subset of its product state space, that is, Cartesian product of its subsystem state spaces. Compact storage of the infinitesimal generator matrix underlying such a MC and efficient implementation of analysis methods using Kronecker operations require the set of reachable states to be represented as a union of Cartesian products of subsets of subsystem state spaces. We first show that the problem of partitioning the reachable state space of a three or higher dimensional system with a minimum number of partitions into Cartesian products of subsets of subsystem state spaces is NP-complete. Two algorithms, one merge based the other refinement based, that yield possibly non-optimal partitionings are presented. Results of experiments on a set of problems from the literature and those that are randomly generated indicate that, although it may be more time and memory consuming, the refinement based algorithm almost always computes partitionings with a smaller number of partitions than the merge based algorithm. When the infinitesimal generator matrix underlying the MC is represented compactly using Kronecker products, analysis methods based on vector-Kronecker product multiplication need to be employed. When the factors in the Kronecker product terms are relatively dense, vector-Kronecker product multiplication can be performed efficiently by the shuffle algorithm. When the factors are relatively sparse, it may be more efficient to obtain nonzero elements of the generator matrix in Kronecker form on-the-fly and multiply them with corresponding elements of the vector. We propose a modification to the shuffle algorithm that multiplies relevant elements of the vector with submatrices of factors in which zero rows and columns are omitted. This approach avoids unnecessary floating-point operations that evaluate to zero during the course of the multiplication. Numerical experiments on a large number of models indicate that, in many cases the modified shuffle algorithm performs a smaller number of floating-point operations than the shuffle algorithm and the algorithm that generates nonzeros on-the-fly, sometimes with minimum number of floating-point operations and amount of memory possible. Although the generator matrix is stored compactly using Kronecker products, solution vectors used in the analysis still require memory proportional to the size of the reachable state space. This becomes a bigger problem as the number of dimensions increases. We show that it is possible to use the hierarchical Tucker decomposition (HTD) to store the solution vectors during Kronecker-based Markovian analysis relatively compactly and still carry out the basic operation of vector-matrix multiplication in Kronecker form relatively efficiently. The time evolution of a stochastic chemical system modelled as a continuous-time MC (CTMC) can be described as a system of ordinary differential equations (ODEs) known as the chemical master equation (CME). The CME can be analyzed by discretizing time and solving a linear system obtained by truncating the countably infinite state space at each time step. However, it is not trivial to choose a truncated state space that includes few states with negligible probabilities and leaves out only a small probability mass. We show that it is possible to decrease the memory requirement of the ODE solver using HTD with adaptive truncation strategies and we propose a novel approach to truncate the countably infinite state space using prediction vectors. Numerical experiments indicate that adaptive truncation strategies improve time and memory efficiency significantly when fixed truncation strategies are inefficient. Finally, we consider a multi-class multi-server retrial queueing system in which customers arrive according to a class-dependent Markovian arrival process (MAP). Service and retrial times follow class-dependent phase-type (PH) distributions with the further assumption that PH distributions of retrial times are acyclic. Here, we obtain a necessary and sufficient condition for ergodicity from criteria based on drifts. The countably infinite state space of the model is truncated with an appropriately chosen Lyapunov function. The truncated model is described as a multi-dimensional MC and a Kronecker representation of its infinitesimal generator matrix is numerically analyzed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğrusal sistemleri çözmek, bir çok bilimsel ve endüstriyel uygulamalarda çekirdek bir işlemdir. Bu uygulamalar genellikle büyük ve seyrek katsayı matrisi içeren doğrusal sistemler meydana getirmektedirler. Bu büyük ve seyrek matrisleri makul bir zamanda çözme ihtiyacı, etkin ve verimli paralel çözüm metodlarını gerekli kılmaktadır. Bu tezde, doğrusal sistemlerin paralel çözüm zamanlarını hızlandıran üç yeni ve orjinal yaklaşım sunulmaktadır. İlk olarak, katsayı matrisi sütun kesişen blok köşegen biçiminde olan eksik-tanımlı (underdetermined) doğrusal sistemlerin en küçük 2-norm çözümünü bulan yeni bir paralel algoritma olan ParBaMiN sunulmaktadır. Paylaşılan bellek (shared memory) ve dağıtık bellek (distributed memory) mimarilerinde yapılan deneyler ParBaMiN'in ölçeklenebilirliğini göstermektedir. İkinci olarak, blok Cimmino algoritmasının iterasyon sayısını düşürmek için yeni bir çizge-kuramsal bölümleme metodu önerilmektedir. Deney sonuçları, gerekli iterasyon sayısının azalımı bakımından önerilen metodun etkinliğini doğrulamaktadır. Son olarak, yoğun (dense) sütunlari olan matrisler için blok Cimmino algoritmasının iterasyon sayısını daha da azaltan yeni bir paralel hibrit metot olan BCDcols'u sunmaktayız. BCDcols, sistemin çözümü için blok Cimmino iteratif algoritması ile bir yoğun direkt çözüm metodunu birleştirmektedir. Deneysel sonuçlar, BCDcols'un blok Cimmino algoritmasının yakınsama hızını önemli ölçüde iyileştirdiğini göstermektedir ve böylelikle çözüme ulaşmak için gereken paralel zamanı azaltmaktadır.","Solving system of linear equations is a kernel operation in many scientific and industrial applications. These applications usually give rise to linear systems in which the coefficient matrix is very large and sparse. The need for solving these large and sparse systems within a reasonable time necessitates efficient and effective parallel solution methods. In this thesis, three novel approaches are proposed for reducing the parallel solution time of linear systems. First, a new parallel algorithm, ParBaMiN, is proposed in order to find the minimum 2-norm solution of underdetermined linear systems, where the coefficient matrix is in the form of column overlapping block diagonal. The conducted experiments demonstrate the scalability of ParBaMiN on both shared and distributed memory architectures. Secondly, a new graph theoretical partitioning method is introduced in order to reduce the number of iterations in block Cimmino algorithm. Experimental results validate the effectiveness of the proposed partitioning method in terms of reducing the required number of iterations. Finally, we propose a new parallel hybrid method, BCDcols, which further reduces the number of iterations of block Cimmino algorithm for matrices with dense columns. BCDcols combines the block Cimmino iterative algorithm and a dense direct method for solving the system. Experimental results show that BCDcols significantly improves the convergence rate of block Cimmino method and hence reduces the parallel solution time."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mobil ağ operatörleri çok fazla alarm olayları üreten Operasyon Destek Sistem- leriyle (OSS) çalışırlar. Bu alarm olayları değişik önem seviyelerinde ve değişik etki alanlarında olurlar ve birbirlerini tetikleyebilirler. Ağ işletmenleri bu sis- tem sorunlarının önemini ve kök sorunlarını gerçek zamanlı olarak tespit et- mekte ve müşteri memnuniyetini makul bir maliyetle garanti ederken çare olacak eylem sayısını optimal bir seviyede tutmakta zorlanırlar. Ölçeklenebilir alarm yönetim sistemine yardım amacıyla; alarmları ilişkilendiren, kurallar üreten ve kök sorunlarını bulan bir çözüm sunuldu. Bu çozüm; önde gelen mobil telekom sağlayıcısının ağ operasyon merkezinde kullanılan Alarm Collector and Ana- lyzer (ALACA) platformuna uygulanmıştır. Alarmların; akan veri yöntemi ile ilişkilendirilmesi, ve işlenerek kök sorunlarını bulunması için kullanılmaktadır. Geliştirilen sistem, aktif alarmları eşleştirmek için dinamik bir indeks, aday alarm kuralları üretmek için bir algoritma, sistem kaynaklarını daha az kullanmak için kayan pencere tabanlı bir yaklaşım ve kök sorunları belirlemek için grafik tabanlı bir çözüm içerir. ALACA, şebeke operatörlerine, veri ve olay akışlarının sürekli ve birleştirilmiş analizine izin vererek, şebeke davranışını ve kök sorunlarının analizi ile olası arızaları tahmin ederek alarm yönetimi sistemlerinin tasarımını geliştirmelerine yardımcı olur. Ayrıca, bu gerçek zamanlı alarm veri analiz sis- teminin performansı hakkında bilgi veren deney sonuçlarını da sunulmuştur.","Mobile network operators run Operations Support Systems (OSS) that produce vast amounts of alarm events. These events can have different significance levels, domains, and also can trigger other ones. Network Operators face the challenge to identify the significance and root causes of these system problems in real-time and to keep the number of remedial actions at an optimal level, so that customer satisfaction rates can be guaranteed at a reasonable cost. A solution containing alarm correlation, rule mining and root cause analysis is described to help scal- able streaming alarm management systems. This solution is applied to Alarm Collector and Analyzer (ALACA), which is operated in the network operation center of a major mobile telecom provider. It is used for alarm event analyses, where the alarms are correlated and processed to find root-causes in a stream- ing fashion. The developed system includes a dynamic index for matching active alarms, an algorithm for generating candidate alarm rules, a sliding-window based approach to save system resources, and a graph based solution to identify root causes. ALACA helps operators to enhance the design of their alarm management systems by allowing continuous analysis of data and event streams and predict network behavior with respect to potential failures by using the results of root cause analysis. The experimental results that provide insights on performance of real-time alarm data analytics systems are presented."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir küme ilgili belgelerin bir araya gelmesiyle oluşur. Küme etiketleme, kümelere tanımlayıcı etiketler atama işlemidir. Bu çalışma, çeşitli küme etiketleme yaklaşımlarını incelemekte ve yeni yöntemler sunmaktadır. İlki kümelerin kendilerini kullanır ve farklı istatistiksel özellik seçim yöntemleriyle kümeleri birbirinden ayıran önemli terimleri çıkarır. Daha sonra onların sonuçlarını birleştirmek için farklı veri birleştirme yöntemleri uygular. Sonuçlarımız, bazı durumlarda istatistiksel olarak daha iyi sonuçlar vermesine rağmen bu yöntemin istikrarlı ve güvenilir bir etiketleme yöntemi olmadığını göstermektedir. Bu durum iyi bir etiketin kümede bulunmayabileceği gerçeğiyle açıklanabilir. ̇İkinci yöntem Wikipedia'yı harici bir kaynak olarak kullanır ve etiket havuzunu zenginleştirmek için bağlantı metinleri ve kategorilerinden faydalanmaktadır. Bağlantı metinleri kullanılarak önerilen etiketler ikincil temalara odaklanmaya meyilli olduğundan bu yöntem başarısız olmuştur. Her ne kadar ikincil temalar birbirleriyle ve ana temayla ilgili olsalar da tam olarak ana temayı tanımlamıyorlar. Bu gözlem sonrasında, etiket havuzumuzu iyileştirmek için Wikipedia sayfalarının kategorilerini iki şekilde kullanıyoruz. Birincisi, önemli terimleri ve Wikipedia kategorilerini sıra esaslı birleştirme yöntemleriyle birleştirir. İkincisi Wikipedia sayfalarının kümelere olan ilişkinliğine bakar ve yalnızca ilişkili sayfaların kategorilerini kullanır. Deneysel sonuçlar, her iki yöntemin de bu çalışmada incelediğimiz diğer küme etiketleme yaklaşımlarına göre istatistiksel olarak daha iyi sonuçlar verdiğini göstermektedir.","A cluster is a set of related documents. Cluster labeling is the process of assigning descriptive labels to clusters. This study investigates several cluster labeling approaches and presents novel methods. The first uses clusters themselves and extracts important terms, which distinguish clusters from each other, with different statistical feature selection methods. Then it applies different data fusion methods for combining their outcomes. Our results show that although it provides statistically significantly better results for some cases, it is not a stable and reliable labeling method. This can be explained by the fact that a good label may not occur in the cluster at all. The second exploits Wikipedia as an external resource and uses its anchor texts and categories to enrich the label pool. Labeling with Wikipedia anchor text fails because the suggested labels tend to focus on minor topics. Although the minor topics are related to the main topic, they do not exactly describe it. After this observation, we use categories of Wikipedia pages to improve our label pool in two ways. The first fuses important terms and Wikipedia categories with rank based fusion methods. The second looks relatedness of Wikipedia pages to the clusters and use only categories of related pages. The experimental results show that both methods provide statistically significantly better results than the other cluster labeling approaches that we examine in this study."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada kişisel bilgilerin yetkisiz kişilerce paylaşılmasından kaynaklanabilecek sorumluluk (liability) sorunlarını ele alacağız. Bir kişinin sıralı verilerini (genomik veri veya konum verisi gibi) birkaç servis sağlayıcısı (SP) ile paylaştığı bir senaryoyu düşünüyoruz. Böyle bir senaryoda veriler üçüncü şahıslarla rızası olmadan paylaşılıyorsa, veri sahibi bu yetkisiz paylaşımdan sorumlu servis sağlayıcısını belirlemek ister. Bu işlevselliği sağlamak için sıralı verileri paylaşırken yeni bir optimizasyona dayalı filigran şemasının (watermarking scheme) kullanılmasını öneriyoruz. Böylece, önerilen şema hassas verilerin yetkisiz olarak paylaşılması durumunda sızdırılan verilerdeki filigranı kontrol ederek sızıntının kaynağını bulabilir. Önerilen şema özellikle şunları garanti eder: (i) verileri alan SP, filigranlı veri noktalarını anlayamamaktadır, (ii) birden fazla SP aynı veriye sahipken hala filigranlı veri noktalarını belirleyememektedir, (iii) ilgili SP orijinal verinin yalnızca bir bölümünü paylaşsa bile sızıntıdan sorumlu tutulabilir ve (iv) eklenen filigran ilgili verilerin niteliğine uygundur. Yani, verilerde doğal korelasyonlar varsa, eklenen filigran bu korelasyonları hala korur. Damgalama (watermarking), tipik olarak verilerin belirli bölümlerini değiştirme anlamına gelir ve bu nedenle veri yararını (data utility) olumsuz yönde etkileyebilir. Önerilen şema yukarıda sözü edilen güvenlik teminatlarını sağlarken, bu tür kullanım kaybını en aza indirmektedir. Son olarak, genomik veri üzerinde önerilen veri şemasına ilişkin bir vaka çalışması yürütüyor ve önerilen şemanın güvenlik ve yarar garantilerini gösteriyoruz.","In this work, we address the liability issues that may arise due to unauthorized sharing of personal data. We consider a scenario in which an individual shares his sequential data (such as genomic data or location patterns) with several service providers (SPs). In such a scenario, if his data is shared with other third parties without his consent, the individual wants to determine the service provider that is responsible for this unauthorized sharing. To provide this functionality, we propose a novel optimization-based watermarking scheme for sharing of sequential data. Thus, in the case of an unauthorized sharing of sensitive data, the proposed scheme can find the source of the leakage by checking the watermark inside the leaked data. In particular, the proposed schemes guarantees with a high probability that (i) the SP that receives the data cannot understand the watermarked data points, (ii) when more than one SPs aggregate their data, they still cannot determine the watermarked data points, (iii) even if the unauthorized sharing involves only a portion of the original data, the corresponding SP can be kept responsible for the leakage, and (iv) the added watermark is compliant with the nature of the corresponding data. That is, if there are inherent correlations in the data, the added watermark still preserves such correlations. Watermarking typically means changing certain parts of the data, and hence it may have negative effects on data utility. The proposed scheme also minimizes such utility loss while it provides the aforementioned security guarantees. Furthermore, we conduct a case study of the proposed scheme on genomic data and show the security and utility guarantees of the proposed scheme."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genom veri setleri genellikle hassas fenotipler ile ilişkilidirler. Bu nedenle bir kişinin veri setinde olduğunun anlaşılması büyük bir mahremiyet riskidir. Beacon sistemleri veri paylaşımı için güvenli, kolay kurulabilir ve standardize bir arayüz sunmayı amaçlar. Bu sistemler sadece kendilerine sorulan, belli alellerin veri setinde olup olmadığına dair evet/hayır sorularını cevaplarlar. Bu kısıtlayıcı prosedür nedeniyle kimlik tespiti ataklarına karşı güvenilir oldukları düşünülen beacon sistemlerinin, risk taşıdığı gösterilmiştir. Yakın zamandaki çalışmalar, bir kişinin veri setinde olup olmadığını anlamanın, beacon sistemlerini bu kişinin nokta mutasyonları ile defalarca sorgulayarak mümkün olabilecegini göstermiştir. Bu tezde özgün bir kimlik tespiti saldırısı tanımlanmakta ve riskin önceden düşünüldüğünden daha büyük olduğu gösterilmektedir. Bu saldırı ile, saldırıya uğrayan kişinin tanımlayıcı mutasyonları sistematik olarak gizlenmiş olsa bile, bu aleller çıkarım yolu ile bulunabilir ve aynı zamanda beacon sisteminin verdiği cevaplar yüksek güven ile tahmin edilebilir. Algoritma, farklı pozisyonlardaki alellerin bağımsız olmamasını temel alarak çalışır ve linkaj dengesizliği ile yüksek seviye Markov zinciri kullanmaktadır. 65 Avrupalı (CEU) bireyi içeren beacon sistemi simülasyonunda, sadece 5 sorgu ile bir kişinin veri setinde olup olmadığını %95 güvenilirlik ile belirleyebilieceğimiz gösterilmiştir (minör alel frekansı 0.05'ten küçük olan olan mutasyonlar sistematik olarak gizlendiğinde bile). Bu rakam, diğer metotların gerek duyduğu sorgu sayısının %0.5'ına denk gelmektedir. Son olarak, literatürde önerilmiş olan, genom verisinin bazı bölgelerinin saklanması ya da kişi başına bir sorgu bütçesi atanması gibi savunma metotlarının da bizim modelimizde katılımcıların mahremiyetini korumakta yetersiz kaldığı gösterilmiştir.","Genomic datasets are often associated with sensitive phenotypes. Therefore, the leak of membership information is a major privacy risk. Genomic beacons aim to provide a secure, easy to implement, and standardized interface for data sharing by only allowing yes/no queries on the presence of speci c alleles in the dataset. Previously deemed secure against re-identi cation attacks, beacons were shown to be vulnerable despite their stringent policy. Recent studies have demonstrated that it is possible to determine whether the victim is in the dataset, by repeatedly querying the beacon for his/her single nucleotide polymorphisms (SNPs). In this thesis, we propose a novel re-identi cation attack and show that the privacy risk is more serious than previously thought. Using the proposed attack, even if the victim systematically hides informative SNPs (i.e., SNPs with very low minor allele frequency -MAF-), it is possible to infer the alleles at positions of interest as well as the beacon query results with very high con dence. Our method is based on the fact that alleles at di erent loci are not necessarily independent. We use the linkage disequilibrium and a high-order Markov chain-based algorithm for the inference. We show that in a simulated beacon with 65 individuals from the CEU population, we can infer membership of individuals with 95% con dence with only 5 queries, even when SNPs with MAF less than 0.05 are hidden. This means, we need less than 0.5% of the number of queries that existing works require, to determine beacon membership under the same conditions. We further show that countermeasures such as hiding certain parts of the genome or setting a query budget for the user would fail to protect the privacy of the participants under our adversary model."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Haber akışlarında olayların geçmiş, şimdiki ve gelecek zamanı ile ilgili birçok araştırma imkanı bulunmaktadır. Geçmiş zaman olayların ve aktörlerin ilişkileri barındırmakta; şimdiki zaman haber okuyucularının ihtiyaçlarını yansıtmakta; gelecek zaman ise tahmin edilmeyi beklemektedir. Bu tez, bahsedilen üç zaman dilimiyle ilgili şu bölümlerden oluşmaktadır: Geçmişte zikzaklı arama yaparak haber zincirlerini keşfetmekte, günümüz haberlerinden genel amaçlı anasayfa oluşturmakta ve mikroblog yazılarını toplumsal olay tahmini için haberlere göre filtrelemekteyiz. İlk bölümde, verilen bir haber yazısına göre bir koleksiyon içerisinden haber zincirlerini keşfeden bir çerçeve geliştirmekteyiz. Haber zinciri, farklı haber yazılarının bir araya gelmesiyle oluşmakta ve farklı olayların nasıl bir araya geldiğini ortaya çıkarmaktadır. Geliştirdiğimiz çerçeve yöntem birbirini tamamlayan şu üç bölümden oluşmaktadır. i) Koleksiyonun taranması, ii) zincir ile zincire eklenecek aday arasındaki benzerliğin hesaplanması ve iii) haber yazıları arasındaki benzerliğin hesaplanmasıdır. Tarama işlemi için, güncellenen zincire göre önceki dokümanları tekrar inceleyen zikzaklı arama yapan yeni bir metin madenciliği yöntemi uygulamaktayız. Haber yazıları arasındaki ilişkilerin ortaya çıkarılması için ise haber aktörlerinin sosyal ağından faydalanmaktayız. Etkinliğin dört farklı yöntem—ilgi, kapsam, ahenk ve ilişkilerin keşfi—açısından değerlendirildiği iki kullanıcı araştırması yapmaktayız. İlk kullanıcı araştırması çerçeve yöntemin farklı versiyonlarını kıyaslayarak kullanıcılara bir rehber oluşturmaktadır. İkincisi ise çerçeve yöntemi üç altçizgi yöntem ile kıyaslamaktadır. Sonuçlara göre yöntemimiz ikili kıyaslamaların %61'inde, orta ya da büyük etki boyutunda istatistiksel anlamda farklı olacak şekilde, etkinliğin iyileşmesini sağlamaktadır. Öteki kıyaslamalarda herhangi bir altçizgi yöntemi bizim yöntemimizi istatistiksel olarak geçememektedir. İkinci bölümde, tıklama sayıları gibi meta-özellikler kullanmadan, sadece düz metin kullanarak haberler için toplumsal anasayfa seçmekteyiz. Anasayfa haber seçimi, haber toplayıcılarında önemli haberlerin bulunmasıdır. Anasayfaların uzunluğu ve seçilen haberlerin önemi ve çeşitliliği beraber düşünülerek yeni bir algoritma geliştirilmektedir. Haberlerin önemini, çeşitliliği de sağlamak amacıyla, konu başlığı modelleme yöntemiyle tahmin etmekteyiz. Önemli dokümanları daha sonra önemli konu başlıklarından, anasayfa uzunluğunu dolduracak şekilde öncelik-tabanlı bir method ile seçmekteyiz. Etkinliğin ve çeşitliliği bir kullanıcı araştırmasıyla ölçmekteyiz. Sonuçlara göre haber yazılarının 10 tanesinin en çok yedi tanesi önemli bulunmakta, dokuz tanesi ise farklı konu başlıklarından gözükmektedir. İleride yapılacak araştırmalara yol göstermesi için genel amaçlı anasayfa seçimindeki zorluklardan da bahsetmekteyiz. Üçüncü bölümde ise haber olaylarına karşı ileride gerçekleşecek toplumsal tepkiyi tahmin etmekte kullanılabilecek filtreleme işlemini gerçekleştirmekteyiz. Twitter gibi mikroblog ortamları, toplumun görüşlerini ortaya çıkarmasıyla gün geçtikçe daha fazla önem kazanmaktadır. Terör olayları gibi 2015 ve 2017 yılları arasında gerçekleşmiş olayı ve bu olaylar sırasında atılan tweet'leri içeren BilPredict-2017 adında yeni bir toplumsal tepki veri setini geliştirmiş durumdayız. Önemli kelimelere göre tweet'leri filtrelemekte ve bunları çeşitli özelliklere göre analiz etmekteyiz. Sonuçlar, frekans, duygusallık, yer ve zaman özelliklerinin haber olaylarının doğasını yansıttıklarından dolayı gelecek tahmininde yararlanılabileceklerini göstermektedir.","News streams have several research opportunities for the past, present, and future of events. The past hides relations among events and actors; the present reflects needs of news readers; and the future waits to be predicted. The thesis has three studies regarding these time periods: We discover news chains using zigzagged search in the past, select front-page of current news for the public, and filter microblogs for predicting future public reactions to events. In the first part, given an input document, we develop a framework for discovering story chains in a text collection. A story chain is a set of related news articles that reveal how different events are connected. The framework has three complementary parts that i) scan the collection, ii) measure the similarity between chain-member candidates and the chain, and iii) measure similarity among news articles. For scanning, we apply a novel text-mining method that uses a zigzagged search that reinvestigates past documents based on the updated chain. We also utilize social networks of news actors to reveal connections among news articles. We conduct two user studies in terms of four effectiveness measures: relevance, coverage, coherence, and ability to disclose relations. The first user study compares several versions of the framework, by varying parameters, to set a guideline for use. The second compares the framework with 3 baselines. The results show that our method provides statistically significant improvement in effectiveness in 61% of pairwise comparisons, with medium or large effect size; in the remainder, none of the baselines significantly outperforms our method. In the second part, we select news articles for public front pages using raw text, without any meta-attributes such as click counts. Front-page news selection is the task of finding important news articles in news aggregators. A novel algorithm is introduced by jointly considering the importance and diversity of selected news articles and the length of front pages. We estimate the importance of news, based on topic modelling, to provide the required diversity. Then, we select important documents from important topics using a priority-based method that helps in fitting news content into the length of the front page. A user study is conducted to measure effectiveness and diversity. Annotation results show that up to 7 of 10 news articles are important, and up to 9 of them are from different topics. Challenges in selecting public front-page news are addressed with an emphasis on future research. In the third part, we filter microblog texts, specifically tweets, to news events for predicting future public reactions. Microblog environments like Twitter are increasingly becoming more important to leverage people's opinion on news events. We create a new collection, called BilPredict-2017 that includes events including terrorist attacks in Turkey from 2015 to 2017, and also Turkish tweets that are published during these events. We filter tweets by using important keywords, analyze them in terms of several features. Results show that there is a high correlation between time and frequency of tweets. Sentiment and spatial features also reflect the nature of events, thus all of these features can be utilized in predicting the future."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özellik çıkarımı, bilgi getirimi ve doğal dil işleme alanlarındaki uygulamalar için önemli bir bileşendir. Bu bileşen, dökümanlar için ayırt edici kelimeler seçmek için kullanılır ve bu kelimeler kümeleme, çoklu döküman özetleme ve sınıflandırma için kullanılabilir. Seçilen özellikler dökümaları için ilgisiz kelimeler olabileceğinden seçildikleri bu dökümanları her zaman en iyi biçimde temsil edemeyebilirler. Bu problemi ele aldığımızda biz iki yönlü bir katkı sağlıyoruz. Birinci olarak, özellik gruplarının kalitesini arttırmak amacıyla kelimelerin, kümelerinin konularıyla arasındaki bağlamsal ilişkiyi kullanarak ilgisiz kelimelerin özellik listelerinden silen yeni bir yaklaşım sunuyoruz. İkinci olarak, kelimelerin, kümelerinin konularıyla arasında bir ilişki olup olmadığına karar vermek amacıyla yeni bir yöntem öne sürüyoruz. Yöntemimiz, söz konusu bir kelimenin, bir döküman kümesi için ayırt edici olarak seçilmiş olan kelimeler ile dış bir kaynakta beraber bulunma sayısına göre bağlamsal olarak ilişkili olup olmadığına karar veriyor. Bu çalışmamız için dış kaynak olarak Wikipedia'yı kullandık. Özellik setlerinden ilgisiz olan kelimelerin silinmesi daha iyi ve ilgili özellik listelerinin ortaya çıkmasını sağlıyor. Yaklaşımlarımızı, özellik setlerinin direk olarak etiket adayı olarak kullanılabildiği kümeleme etiketleme problemi üzerinde değerlendiriyoruz. Bu problem için birçok kez kullanılmış olan 20NG ve ODP veri setleri üzerinde çalışıyoruz. Bulgularımıza göre, bağlamsal ilişki değerlendirme yöntemimiz başarılı bir şekilde kelimelerin konularla olan bağlamsal ilişki durumunu tespit ediyor ve bu ilişki bilgisini kullanarak ilgisiz kelimelerin etiket adayları arasından silinmesi kümeleme etiketleme kalitesini kayda değer biçimde geliştiriyor.","Feature selection is an important component of information retrieval and natural language processing applications. It is used to extract distinguishing terms for a group of documents; such terms, for example, can be used for clustering, multi-document summarization and classification. The selected features are not always the best representatives of the documents due to some noisy terms. Addressing this issue, our contribution is twofold. First, we present a novel approach of filtering out the noisy, unrelated terms from the feature lists with the usage of contextual relatedness information of terms to their topics in order to enhance the feature set quality. Second, we propose a new method to assess the contextual relatedness of terms to the topic of their documents. Our approach automatically decides the contextual relatedness of a term to the topic of a set of documents using co-occurrences with the distinguishing terms of the document set inside an external knowledge source, Wikipedia for our work. Deletion of unrelated terms from the feature lists gives a better, more related set of features. We evaluate our approach for cluster labeling problem where feature sets for clusters can be used as label candidates. We work on commonly used 20NG and ODP datasets for the cluster labeling problem, finding that it successfully detects relevancy information of terms to topics, and filtering out irrelevant label candidates results in significantly improved cluster labeling quality."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu calsma, kalabalk simulasyonlarnn gercek videolarla dogrulanmasn ve benzetim parametrelerinin sonuclarn videolardaki sonuclara benzemesini saglayacak sekilde ayarlanmasn saglayan bir iyilestirme yontemi sunmaktadr.  Oncelikle, dogrulama ve iyilestirmede kullanlabilecek olaylarn ve videolarn ozellikleri aktar lr. Daha sonra bu surecte izlenecek video isleme, veri ckarma, hata fonksiyonu tespiti, sanal sahne olusturulmas, admlar okuyucuya aktarlr. Daha sonra, sunulan yontemin calsmas; bir canl bomba saldrs, bir metro vagonundaki panik hali ve Kara Cuma arbedesinden olusan uc farkl gercek olay ve bu olaylarn videolar uzerinden gosterilir. Son olarak, calsmalarn sonunda elde edilen bulgular, modelin basarm sunulur.","We propose a data-driven approach for tuning, validating and optimizing crowd simulations by learning parameters from real-life videos. We discuss the common traits of incidents and their video footages suitable for the learning step. We then demonstrate the learning process in three real-life incidents: a bombing attack, a panic in subway and a Black Friday rush. We reanimate the incidents using an existing emotion contagion and crowd simulation framework and optimize the parameters that characterize agent behavior with respect to the data extracted from the video footages of the incidents."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hücresel prosesler ve yolaklar hakkında bilgi Systems Biology Graphical Notation (SBGN) gibi hesaplanabilir standart formatlarda artarak daha ulaşılabilir bir hale geliyor. Bu bilginin etkin olarak görselleştirilmesi biyolojik bilgi analizi için temel bir gereklilik. Biyolojik bilgi analizi hızla web tabanlı platformlara taşınmakta; bu yüzden bu platformları ve diğer kullanım senaryolarını destekleyen çok yönlü web tabanlı yolak görselleyici ve düzenleyicilere büyük bir ihtiyaç var. Bu ihtiyacı karşılamak için modüler bir yazılım mimarisi geliştirmeyi öneriyoruz. Önerilen mimari yeniden kullanılabilir web tabanlı kütüphaneler ve bu kütüphaneleri kullanarak geliştirilen kolaylıkla uyarlanabilir ve gömülebilir araçlar içeriyor. Kütüphanelerimiz SBGNViz.js, SBGN diyagramlarıyla gösterimlenen yolak modellerini görselleştirmek için bir işleyici ve uygulama programlama arayüzü sağlayan Cytoscape.js tabanlı bir kütüphane, ve ChiSE.js, SBGN diyagramlaryla gösterimlenen yolak modellerini görselleştirmek ve inşa etmek için SBGNViz.js tabanlı bir kütüphane, ve çok yönlü Cytoscape.js eklentilerinden oluşmakta. Bu kütüphaneleri kullanarak geliştirilen araçlarımız sırasıyla SBGNViz.js ve ChiSE.js için birer örnek uygulama olan SBGNViz Viewer ve Newt'i içermekte. Newt, moleküler kompleksler ve kompartımanlar gibi bileşik yapıları, örgü ve hizalama yönergeleri gibi ileri şemalaştırma olanaklarını, durgun ve artımlı yerleştirme algoritmalarını, ve büyük haritalarda karmaşıklık yönetimini destekleyen ilk web tabanlı araç olarak geliştirilmekte.","Information about cellular processes and pathways is becoming increasingly available in detailed, computable standard formats including Systems Biology Graphical Notation (SBGN). Effective visualization of this information is a key recurring requirement for biological data analysis, especially for -omic data. Biological data analysis is rapidly migrating to web based platforms thus there is a substantial need for sophisticated web based pathway viewing and editing tools that support these platforms and other use cases. We propose to develop a modular software architecture to meet this need. This proposed architecture includes reusable web based libraries and easily customizable and embeddable tools developed using these libraries. Our libraries include SBGNViz.js, a Cytoscape.js based library providing a renderer and an API to develop tools visualizing pathway models represented by SBGN Diagrams, and ChiSE.js, an SBGNViz.js based library to visualize and construct pathway models represented in SBGN Diagrams, and miscellaneous Cytoscape.js extensions. Our tools are built using these libraries and include SBGNViz Viewer and Newt, which are sample applications for SBGNViz.js and ChiSE.js, respectively. Newt is being developed to become a first web based, open source SBGN editor with full support for compound structures such as molecular complexes and compartment, advanced diagramming facilities including grid and alignment guidelines, static and incremental layout, and complexity management of large maps."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sosyal bilimleri, özellikle Uluslararası İlişkiler ve Siyaset Bilimi alanlarında, doğru ve güvenilir niceliksel modeller doğrultusunda ilerletmek için, Bilgisayar Bilimleri ve İstatistik'ten alınan yeni metodolojiler kullanılmalıdır. Uluslararası İlişkiler'de, karar alıcıların kamusal söhylemlerinde, dış politika konuları arasındaki ilişkileri anlamak için niceliksel analiz yapılabilir. İç siyasette, Seçim Tahmini, nicelenmiş oy sonuçlarının varlığı ve iç politika ve dış politika karar alma sürecindeki öneminden dolayı uygun bir alandır. Bu çalışma, seçim sonuçlarını ve dış politika yönelimlerini, istatistikten ve bilgisayar bilimlerinden (Makine Öğrenimi ve Doğal Dil İşleme) en yeni metodolojilerle analiz edilen ve modellenen, sosyal medya (Twitter) verileri ve metinlerin anlamı üzerine kurulu sayısal bir modelle tahmin etmektedir. Modeli doğrulamak için, 2015 Türkiye Genel Seçimi, 2016 ABD Başkanlık Seçimi ve Donald Trump'in seçim kampanyası dönemi analiz edildi. Bu çalışma, siyasi yönelimlerin yüksek doğrulukla (%92 Türkçe, %96 İngilizce) yakalanabildiğini ve anket sonuçlarındaki dalgalanmaların tahmin edilebileceğini gösteriyor. Ayrıca, bir adayın dış politika yönelimlerinin, kendisinin ve ekibinin kampanya döneminde attığı tweetlerle yakalanabildiği gösterilmiştir. Anahtar Kelimeler: Dış Politika Analizi, Doğal Dil Işleme, Duygu Analizi, Makine Öğrenmesi, Seçim Tahmini","To advance social science in the direction of accurate and reliable quantitative models, especially in the fields of International Relations and Political Science, new novel methodologies borrowed from the Computer Science and Statistics should be employed. In International Relations, quantitative analysis can be carried out to understand foreign policy topic relations in public discourse of decision makers. In domestic politics, Election Forecasting is a suitable area, because of its offering of already quantified vote results and its importance in the decision-making process in domestic politics and foreign policy. This work embarks upon a computational- statistical model built on social media (Twitter) data and texts' meaning extracted, analyzed and modeled with the state-of-the-art methodologies from Computer Science (Machine Learning and Natural Language Processing) and statistics to forecast election results and foreign policy orientation. To verify the model, Turkish General Election 2015, US Presidential Election 2016 and campaign period of Donald Trump are analyzed. This work shows that, sentiment of political tweets can be captured with high predictive accuracy (92% in Turkish, 96% in English) and using opinion poll results for a given period of time, vote percentage fluctuations can be predicted. Furthermore, it is possible to capture the foreign policy orientation of a candidate by his and his team's tweets in the campaign period. Keywords: Election Forecasting, Foreign Policy Analysis, Machine Learning,Natural Language Processing, Sentiment Analysis"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüz tıbbında, dokuların histopatolojik incelenmesi, kanserin teşhisinde önemli rol oynamaktadır. Fakat bu işlem, hem gözlemci değişkenliğine açık, hem de patologlar için zaman alıcıdır. Dolayısıyla, otomatik nesnel araçların geliştirilmesi önemlidir. Bu araçların ilk basamakları genellikle görüntünün bölütlenmesidir. Bu ihtiyaca yönelik olarak, bu tez çalışmasında, histopatolojik doku görüntülerinin bölütlenmesi için, deepSeg adını verdiğimiz iki aşamalı metodla, yeni bir yaklaşım sunuyoruz. İlk aşama, AlexNet adlı ve medikal olmayan ImageNet alanında eğitilmiş evrişimli yapay sinir ağında saklanan bilgiyi, medikal alanda bulunan histopatolojik doku görüntüsü karakterizasyonu için aktarmaktadır. İkinci aşama ise bu karakterizasyonu, çekirdek kontröllü bir bölge büyütme algoritmasında kullanarak, heterojen doku görüntülerini homojen bölgelerine öğreticisiz olarak bölütlemektedir. Bu bölütlemenin doğruluğunu test etmek için, mikroskobik kolon dokusu görüntülerinde testler yapılmıştır. Elde ettiğimiz sayısal sonuçlar, önerdiğimiz metodun, aynı veri kümesi üzerinde çalışan diğer metodların performanslarını arttırdığını göstermiştir. Bu çalışma, hem derin öğrenme tekniklerini doku görüntü bölütlemesi için kullanan ilk başarılı örneklerden biri olarak yerini almış; hem de histopatolojik görüntü analizinde derin öğrenme temelli özniteliklerin, elle çıkarılanlara üstün geldiğini göstermiştir.","In the current practice of medicine, histopathological examination of tissues is essential for cancer diagnosis. However, this task is both subject to observer variability and time consuming for pathologists. Thus, it is important to develop automated objective tools, the first step of which usually comprises image segmentation. According to this need, in this thesis, we propose a novel approach for the segmentation of histopathological tissue images. Our proposed method, called deepSeg, is a two-tier method. The first tier transfers the knowledge from AlexNet, which is a convolutional neural network (CNN) trained for the non-medical domain of ImageNet, to the medical domain of histopathological tissue image characterization. The second tier uses this characterization in a seed-controlled region growing algorithm, for the unsupervised segmentation of heterogeneous tissue images into their homogeneous regions. To test the effectiveness of the segmentation, we conduct experiments on microscopic colon tissue images. Quantitative results reveal that the proposed method improves the performance of the previous methods that work on the same dataset. This study both illustrates one of the first successful demonstrations of using deep learning for tissue image segmentation, and shows the power of using deep learning features instead of handcrafted ones in the domain of histopathological image analysis."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Konum verilerinden anlamlı çıkarımlar yapmak işletmelerin daha iyi kararlar vermelerine yardımcı olmaktadır. Konum analizi yapabilmek için işletmelerin müşterilerinin konumlarını bilmeleri gerekmesine rağmen çoğunlukla işletmeler bu değerli veriye sahip değillerdir. Konum verileri genellikle mobil telekomünikasyon operatörleri ve konum tabanlı servis sağlayıcılar tarafından toplanmaktadır. Bu tezde, konum verisinin ortaklaşa analiz edilebilmesi için ölçeklenebilir ve gizliliği koruyan çözümler geliştirilmiştir. İşletmelerin müşterilerinin konum bilgilerine sahip olmadıklarında kullanabilecekleri iki farklı yaklaşım türü önerilmektedir. Önerilen yaklaşımlar şirketlerin yeni şubeleri için en iyi yeri bulması problemi bağlamında açıklanmaktadır. İlk yaklaşım türü, gizliliği koruyan sorgular aracılığıyla konum verisi sahibinden müşteri konumları hakkında toplu bilgiler elde etmektir. Bu amaçla en iyi yer seçiminde kullanılabilecek toplu sorgular tanımlanmış ve bu sorguları cevaplayabilmek için güvenli iki taraflı protokoller geliştirilmiştir. Önerilen protokoller kısmi homomorfik şifreleme kullanılarak geliştirilmiştir ve ayrımsal gizliliği sağlamaktadır. İkinci yaklaşım ise bireylerin gizliliğini ihlal etmeden analiz yapmak için sentetik konum verisi yaratılmasıdır. İşletmelerin müşterilerinin konumları hakkında kısmi bilgiye sahip olduklarında en iyi yeri tahmin etmek için kullanabilecekleri komşuluk tabanlı veri üretimi yöntemi önerilmiştir. Ayrıca, konum verisi sahiplerinin, gizliliği koruyan sentetik konum verisi paylaşımında kullanabilecekleri karelere bölme ve kümeleme tabanlı veri üretim yöntemleri de önerilmiştir. Önerilen yaklaşımlar işletmelerin müşterilerinin konumlarını bilmeden en iyi yer seçimi yapmalarına yardımcı olacaktır.","Deriving meaningful insights from location data helps businesses make better decisions. While businesses must know the locations of their customers to perform location analytics, most businesses do not have this valuable data. Location data is typically collected by other services such as mobile telecommunication operators and location-based service providers. We develop scalable privacy-preserving solutions for collaborative analytics of location data. We propose two classes of approaches for location analytics when businesses do not have the location data of the customers. We illustrate both of our approaches in the context of optimal location selection for the new branches of businesses. The first type of approach is retrieving the aggregate information about the customer locations from location data owners via privacy-preserving queries. We define aggregate queries that can be used in optimal location selection and we propose secure two-party protocols for processing these queries. The proposed protocols utilize partially homomorphic encryption as a building block and satisfy differential privacy. Our second approach is to generate synthetic location data in order to perform analytics without violating privacy of individuals. We propose a neighborhood-based data generation method which can be used by businesses for predicting the optimal location when they have partial information about customer locations. We also propose grid-based and clustering-based data generation methods which can be used by location data owners for publishing privacy-preserving synthetic location data. Proposed approaches facilitate running optimal location queries by businesses without knowing their customers' locations."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğrusal mantık, tek kullanımlık varsayımları kullanmaya zorlayan tekdüze ol- mayan bir mantık olduğu için, dinamik durumlu alanları etkili olarak göstermeye olanak sağlıyor. İçinde, bir robotun durumunda fiziksel ve bilgisel bileşenlerin bir- likte tekdüze olmayan özellikler sergilendiği robotik görev planlaması, bu tür alan- lar için önemli bir örnektir. STRIPS planlama problemleri için, doğrusal mantıkta ispatları otomatik ortaya çıkaracak iki adet yeni ve etkili teorem ispatlayıcı or- taya koyuyoruz. Ortaya koyduğumuz ilk planlayıcı olan Doğrusal Planlama Mantığı, Prolog ve Lolli gibi programlama dillerinde sıkça kullanılan geriye zincir- leme prensibiyle çalışmaktadır ve atomik olmayan sonuçları da ele alacak şekilde genişletilmiştir. Bu yeni planlayıcının deneysel bir uygulaması olan RHex robotu için görsel yönlendirmeyle otomatik gezinme, robotik görev planlayıcı kapsamında gösterilmiştir. Ortaya koyduğumuz ikinci planlayıcı olan Doğrusal Mantık Grafik Planlayıcısı, doğrusal mantık için grafik tabanlı teorem ispaylayıcı olarak formüle edilmis, rastgele olmayan ve eş zamanlı alanlar icin otomatik bir planlayıcıdır. Bu yeni grafik tabanlı teori ispatlayıcı, çoklu sayıdaki özdeş nesnelerin olduğu zamanlarda (sürü icindeki robotlar, büyük fabrikadaki parçalar), özellikler plan- lama problemleriyle alakası olmayan ispat permütasyonlarını azaltarak planlama performansını arttırıyor. İkinci planlayıcının, eş zamanlı üretim alanında eylem planlaması icin uygulamasını örnek üzerinde gösteriyoruz ve literatürde farklı problem tiplerinde ve alanlarında performanslarıyla bilinen, dört farklı otomatik planlayıcı olan BlackBox, Symba-2, Metis ve Temporal Fast Downward (TFD) ile karsılaştırmasını sağlıyoruz. Yeni planlayıcımızın herhangi bir buluşsala bağlı olmamasına rağmen, diğer sistemleri çoklu özdeş nesnelerin varlığında eş zamanlı alanlarda yendiğini gösteriyoruz. Simetri azaltma ve sayısal akışkanlar ile ilgili mevcut metodlar kullanılsa bile, yukarıdaki kazanımlar sürüyor ve yeni plan- layıcımız binlerce nesneli problemleri çözebiliyor. Bu çıkarımlara ek olarak, bu yeni planlayıcı ile plan oluşturmanın, çoklu küme yeniden yazım sistemlerine eşit olduğunu gösteriyoruz.","Linear Logic is a non-monotonic logic, with semantics that enforce single-use assumptions thereby allowing native and efficient encoding of domains with dy- namic state. Robotic task planning is an important example for such domains, wherein both physical and informational components of a robot's state exhibit non-monotonic properties. We introduce two novel and efficient theorem provers for automated construction of proofs for an exponential multiplicative fragment of linear logic to encode deterministic STRIPS planning problems in general. The first planner we introduce is Linear Planning Logic (LPL), which is based on the backchaining principle commonly used for constructing logic programming languages such as Prolog and Lolli, with a novel extension for LPL to handle program formulae with non-atomic conclusions. We demonstrate an experimen- tal application of LPL in the context of a robotic task planner, implementing visually guided autonomous navigation for the RHex hexapod robot. The sec- ond planner we introduce is the Linear Logic Graph Planner (LinGraph), an automated planner for deterministic, concurrent domains, formulated as a graph- based theorem prover for a propositional fragment of intuitionistic linear logic. The new graph-based theorem prover we introduce in this context substantially improves planning performance by reducing proof permutations that are irrele- vant to planning problems particularly in the presence of large numbers of objects and agents with identical properties (e.g. robots within a swarm, or parts in a large factory). We illustrate LinGraph's application for planning the actions of robots within a concurrent manufacturing domain and provide comparisons with four existing automated planners, BlackBox, Symba-2, Metis and the Tempo- ral Fast Downward (TFD), covering a wide range of state-of-the-art automated planning techniques and implementations that are well-known in the literature for their performance on various of problem types and domains. We show that even though LinGraph does not rely on any heuristics, it still outperforms these sys- tems for concurrent domains with large numbers of identical objects and agents, finding feasible plans that they cannot identify. These gains persist even when existing methods on symmetry reduction and numerical fluents are used, with LinGraph capable of handling problems with thousands of objects. Following these results, we also formally show that plan construction with LinGraph is equivalent to multiset rewriting systems, establishing a formal relation between LinGraph and intuitionistic linear logic."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uzaktan algılama verisinde nesne tespiti, oldukça popüler bir problem olmuştur ve tarım, navigasyon, çevre düzenlemesi, kentsel görüntüleme ve haritacılık gibi çok geniş bir yelpazedeki uygulama alanlarında yaygın olarak kullanılmaktadır. Fakat bu problemi çözmek için sadece tek bir veri kaynağını kullanmak yeterli olmayabilir. Havadan çekilmiş optik ve LiDAR verilerinin füzyonu, birbirini tamamlayıcı bilgiler taşıdığı için uzaktan algılama verisinde nesne tespitinde umut verici bir yaklaşım olmuştur. Biz verinin çoklu düzey bölünmesini gerçekleştiren ve bölünmüş veriyi minimum düzeyde öğrenme kullanarak nesneleri tespit eden sistemler öneriyoruz. Yöntemimiz ön işleme aşamasında veriyi yükseklik değerine göre eşikleme ve daha etkili bir biçimde işlemek için onu küçük bileşenlere bölme adımları içeriyor. Sınıflandırma görevi için ise, LiDAR verisinden yükseklik, havadan çekilmiş veriden spektral ve komşuluk haritasından konumsal bilgileri kullanarak, her bir bileşende nesneleri tespit eden iki farklı çizge kesme tabanlı yöntem öneriyoruz. İlk yöntem ikili sınıflandırma sağlarken, ikincisi ise çoklu sınıflandırma gerçekleştiriyor. İlk yöntemi kullanarak yüksek piksellerde binaları ağaçlardan, alçak piksellerde ise yolları çimenlik alanlardan ayırt ediyoruz. İkinci yöntem ise her bir bileşendeki bütün nesneleri tek seferde tespit etmek için kullanılıyor. Bizim önerdiğimiz yöntemin ihtiyaç duyduğu tek öğrenme çizge kesme yöntemindeki çizgenin kenarlarının ağırlıklarını belirlemek için kullanılan örneklerdir. Bir kıyas veri seti üzerinde gerçekleştirilen deneyler gösteriyor ki önerdiğimiz az miktarda öğrenme kullanan yöntem literatürdeki sonuçlarla uyumludur.","Object detection in remotely sensed data has been a popular problem and is commonly used in a wide range of applications in domains such as agriculture, navigation, environmental management, urban monitoring and mapping. However, using only one type of data source may not be sufficient to solve this problem. Fusion of aerial optical and LiDAR data has been a promising approach in remote sensing as they carry complementary information for object detection. We propose frameworks that partition the data in multiple levels and detect objects with minimal supervision in the partitioned data. Our methodology involves thresholding the data according to height, and dividing the data into smaller components to process it efficiently in the preprocessing step. For the classification task, we propose two graph cut based procedures that detect objects in each component using height information from LiDAR, spectral information from aerial data, and spatial information from adjacency maps. The first procedure provides a binary classification, whereas the second one performs a multi-class classification. We use the first framework to separate buildings from trees in the high pixels, and roads from grass areas in the low pixels. The second procedure is used to detect all of the classes in each component at once. The only supervision our proposed methodology requires consists of samples that are used to estimate the weights of the edges in the graph for the graph-cut procedures. Experiments using a benchmark data set show that the performance of the proposed methodology that uses small amount of supervision is compatible with the ones in the literature."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yeni nesil dizi analizi teknolojisi ile hasta genomik değişimlerin nitelendirilmesi kanser alt tiplerinin belirlenmesinde yeni olanaklar ortaya çıkarıyor. Farklı omik verileri, tümörlerin moleküler biyolojilerine farklı bakış açıları sağlar; bununla birlikte, tümör hücreleri yüksek seviyede heterojenlik sergiler, ve farklı hastalar farklı kombinasyonlarda moleküler değişikliklere sahiptir. Öte yandan, farklı değişiklikler aynı biyolojik yolakları bozabilir. Bu çalışmada, yeni bir çizge çekirdeği aracılığıyla hastaların alterasyon profillerinden yolaklar üzerindeki benzerliklerini nicelleştiren yeni bir kümeleme prosedürü öneriyoruz. Her bir yol ve hasta çifti için, hastanın moleküler değişiklikleri ve yolak etkileşimlerine dayanarak düğümleri etiketlenmiş yönsüz bir çizge oluşturulur. Önerilen dağıtılmış en kısa yol çizge çekirdeği (smSPK), bir yolağa göre hasta çiftlerinin düğümleri etiketli çizgelerini karşılaştırarak benzerliklerini değerlendirir. Gruplama prosedürümüz iki adımdan oluşur. İlk adımda, her yol ve veri tipi için smSPK çekirdek matrisleri, hasta çiftleri için birden çok çekirdek matrisi oluşturmak üzere hesaplanır ve sonraki adımda, bu çekirdek matrisleri, hastaları katmanlaştırmak için çok bakışlı çekirdek gruplandırma yaklaşımına girdi olarak verilir. Metodolojimizi 361 renal hücreli karsinoma hastasında somatik mutasyonlar, gen ve protein ifadeleri verileri kullanarak uyguluyoruz. Bu yaklaşım, hayatta kalma sürelerinde önemli farklılık gösteren hasta alt gruplarını ortaya çıkarıyor (p-değeri < 1.5 x 10^{-8}). Önerilen yöntem, diğer omik verilerin entegrasyonuna izin verir ve her hasta alt grubundaki bozuk yolaklarla ilgili fikir verir.","Characterizing patient genomic alterations through next-generation sequencing technologies opens up new opportunities for refining cancer subtypes. Different omics data provide different views into the molecular biology of the tumors. However, tumor cells exhibit high levels of heterogeneity, and different patients harbor different combinations of molecular alterations. On the other hand, different alterations may perturb the same biological pathways. In this work, we propose a novel clustering procedure that quantifies the similarities of patients from their alteration profiles on pathways via a novel graph kernel. For each pathway and patient pair, a vertex labeled undirected graph is constructed based on the patient molecular alterations and the pathway interactions. The proposed smoothed shortest path graph kernel (smSPK) assesses similarities of pair of patients with respect to a pathway by comparing their vertex labeled graphs. Our clustering procedure involves two steps. In the first step, the smSPK kernel matrices for each pathway and data type are computed for patient pairs to construct multiple kernel matrices and in the ensuing step, these kernel matrices are input to a multi-view kernel clustering algorithm to stratify patients. We apply our methodology to 361 renal cell carcinoma patients, using somatic mutations, gene and protein expressions data. This approach yields subgroup of patients that differ significantly in their survival times (p-value < 1.5 x 10^{-8}). The proposed methodology allows integrating other type of omics data and provides insight into disrupted pathways in each patient subgroup."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çok çekirdekli işlemcilerdeki (CMP) çekirdek sayısı arttıkça önbellek tutarlılığını verimli bir şekilde sağlamak zorlaşmaktadır. Dinleme temelli protokoller küçük çaplı sistemler için uygun çözümler olsa da bant genişliğine bindirdikleri ek yükten dolayı daha geniş sistemler için verimli olmamaktadırlar. Bu sebepten geniş çaplı CMP'ler dizin temelli çözümlere ihtiyaç duyarlar. Dizinin görevi bütün bellek bloklarının hangi çekirdeğin önbelleğinde kopyasının bulunduğunu tutmaktır. Dizin sadece ilgili blokları tutan önbelleklere mesaj yollar ve önbelleğe yapılan eş zamanlı erişim isteklerini düzenler. Dizin temelli protokoller çok sayıda çekirdeğe ölçeklenirken, başarım, ağ trafiği ve bant genişliği önemli problemler olmaktadır. Bu tezde, paylaşımlı bellek kullanan CMP'lerde dizin temelli protokollerin verimini arttıracak yazılım temelli bir çözüm sunmaktayız. Çok iş parçacıklı uygulamalarda, veri erişimlerinin bazıları önbellek tutarlılığını bozmaz ancak yine de çekirdekler arasında tutarlılık mesajları üretir. Örneğin, salt okunur (özel) veriler bu kategoride değerlendirilebilir. Öte yandan, verilere en az iki çekirdeğin eriştiği ve bunların en az birinin bir yazma işlemi olduğu takdirde ilgili erişilen veriye paylaşılan veriler denir. Önerdiğimiz sistemde, özel veriler ve paylaşılan veriler derleme zamanı belirlenir ve önbellek tutarlılık protokolü yalnızca paylaşılan veriler için uygulanır. Bu çalışmada, yaklaşımımızı iki aşamalı olarak uyguluyoruz. İlk olarak, bir programı analiz etmek ve özel komutlarını, yani derleme zamanında özel veriyi yükleyen veya depolayan talimatları işaretlemek için Andersen'in işaretçi analizini kullanıyoruz. İkinci olarak, Sniper Multi-Core Simulator kullanarak önerilen donanım ayarlarında testleri çalıştırıyoruz. Yaklaşımımızı test etmek için SPLASH-2 ve PARSEC-2.1 paralel uygulamalarını kullandık. Simülasyon sonuçları, yaklaşımımızın döngü sayısını, dinamik rastgele erişimli bellek (DRAM) erişimlerini ve tutarlılık trafiğini azalttığını göstermektedir.","With increasing number of cores in chip multiprocessors (CMPs), it gets more challenging to provide cache coherency efficiently. Although snooping based protocols are appropriate solutions to small scale systems, they are inefficient for large systems because of the limited bandwidth. Therefore, large scale CMPs require directory based solutions where a hardware structure called directory holds the information. This directory keeps track of all memory blocks and which core's cache stores a copy of these blocks. The directory sends messages only to caches that store relevant blocks and also coordinates simultaneous accesses to a cache block. As directory based protocols scaled to many cores, performance, network-on-chip (NoC) traffic, and bandwidth become major problems. In this thesis, we present software mechanisms to improve effectiveness of directory based cache coherency on CMPs with shared memory. In multithreaded applications, some of the data accesses do not disrupt cache coherency, but they still produce coherency messages among cores. For example, read-only (private) data can be considered in this category. On the other hand, if data is accessed by at least two cores and at least one of them is a write operation, it is called shared data. In our proposed system, private data and shared data are determined at compile time, and cache coherency protocol only applies to shared data. We implement our approach in two stages. First, we use Andersen's pointer analysis to analyze a program and mark its private instructions, i.e instructions that load or store private data, at compile time. Second, we run the program in Sniper Multi-Core Simulator\cite{Carlson} with the proposed hardware configuration. We used SPLASH-2 and PARSEC-2.1 parallel benchmarks to test our approach. Simulation results show that our approach reduces cycle count, dynamic random access memory (DRAM) accesses, and coherency traffic."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bitişik tekrarlar, genomda kısa nükleotid sekanslarının düzenli olarak tekrarlanmasıdır. Eğer tekrar eden kısım 2-6 bp uzunluğunda ise mikrosatellitler ya da kısa bitişik tekrarlar olarak adlandırılır. Mikrosatellitlerin kopya sayısının artmasıyla ilişkilendirilmiş birçok hastalık bulunmaktadır, bunlara örnek olarak Huntington hastalığı ve Frajil X sendromu gösterilebilir. Bu yüzden insan genomunun yüzde üçünü oluşturan bitişik tekrarların tespit edilmesi önemli bir araştırma alanıdır. Mikrosatellit lokus yer alan varyantlar tekrarlı yapıları, dizileme sırasında meydana gelen hatalar, kısa DNA okumaları ve son olarak sıklıkla meydana gelen PCR hataları yüzünden genom birleştirme ve dizi hizalama için her zaman problem teşkil etmiştir. Büyük öneme sahip olmalarına rağmen mikrosatellitlerin bulunması hiçbir zaman DNA dizileme süreçlerinin kalıcı bir parçası olarak görülmemiştir. İnsan genomları arasındaki farklılıkların kataloglanmasını ama ̧clayan 1000 Genom Projesi'nin başlatılmasından sonra ilgili konsorsiyum sadece iki farklı mikrosatellit bulma methodunun sonucunu yayınladı (lobSTR ve RepeatSeq). Diğer büyük projelerin de mikrosatellit konusunu aydınlığa kavuşturmak için harcadığı çabalar başarısızlıkla sonuçlandı. Bu çalışmanın ana amacı genom birleştirme yöntemlerini, referans genom dizilimden bildiğimiz mikrosatellit pozisyonları üzerinde kullanarak incelenen genomun referanstan farklılıklarını bulmaktır. Bunun için DNA okumalarını girdi olarak alıp çıktı olarak mikrosatellit kopya sayısını veren bir süreç geliştirilmiştir. Yapılan araştırmaya göre hali hazırda genom birleştirme problemini farklı algoritmalar ve veri yapıları kullanarak çözmeye çalışan otuzdan fazla method bulunmaktadır. Bu tez kapsamında uğraştığımız problem bütün genomdan ziyade lokal birleştirme olarak görülebilir, çünkü sadece mikrosatellit bölgesine karşılık gelen okumaları birleştirmeye çalışıyoruz. Bu çalışmada genom birleştirme için sıklıkla kullanılan iki farklı çizge yapısından yararlanıyoruz: de Bruijn and OLC. Genom birleştirme bir çok çalışmanın bulunduğu bir alan olmasına rağmen, şu ana kadar mikrosatellit için kullanan bir çalışma bulunmamaktadır ve diğer mikrosatellit methodlarından daha iyi bir performans gösterdiği kanıtlanmaktadır. Üç farklı genom birleştirme methodunu, yukarıda bahsedilen iki çizge yapısını kullanan, üç farklı deney modelinde inceledik. Deneyler genotip olarak farklı durumlarda, değişen teminatlarla ya da birleştirilen bölgeye komşu bölgelerin dahil edilmesi durumundaki farklılıkları incelemek için düzenlendi. Her birinin sonucu OLC çizge yapısını kullanan methodun mikrosatellit bulunmasındaki üstünlüğünü kanıtlamaktadır.","Tandem repeats are pieces of DNA where a pattern has multiple consecutive copies adjacent to itself. If the repeat unit (pattern) consists of 2 to 6 nucleotides, it can be referred to as a short tandem repeat or a microsatellite. There are many genetic diseases (such as Huntington disease and Fragile-X syndrome) linked with STR expansions and because tandem repeats make up 3% of the sequenced human genome STR detection research is significant. STR variations have always been a challenge for genome assembly and sequence alignment due to their repetitive nature, sequencing errors, short read lengths, and the high incidence of polymerase slippage at STR regions. Despite the information they carry being very valuable, STR variations have not gained enough attention to be a permanent step in genome sequence analysis pipelines. After The 1000 Genomes Project, which aimed to establish the most detailed genetic variation catalogue for humans, the consortium released only two STR prediction sets which are identified by two STR caller tools, lobSTR and RepeatSeq. Many other large research efforts have failed to shed light on STR variations. The main aim of this study is to use sequence assembly methods for regions where we know that there is an STR, based on reference genome, and release a complete pipeline from sample's reads to STR genotype. According to our literature survey, there are approximately 30 sequence assembly tools which use different algorithms and data structures to optimize their resource consumption. The assembly problem we are dealing with in the scope of this thesis can be considered as local assembly, which is the assembly procedure of reads that maps to a small part of the genome. We will be focusing on two general assembly approaches that make use of graph data structures: de Bruijn graph (DBG) based methods that rely on a variant of k-mer graph, overlap-layout-consensus (OLC) methods that are based on an overlap graph. Even though, sequence assembly is a well studied problem, there is not any work that uses assembly algorithms to characterize STRs. We demonstrate that using sequence assembly on STR regions increases the true positive rate of callers compared to state-of-art tools. We evaluated the performance of three different local assembly methods on three different experimental settings: focusing on (i) genotype based performance, (ii) coverage impact, and (iii) evaluating pre-processing and including flanking regions. All these experiments supported our belief on using assembly. Besides, we show that OLC based assembly methods bring much higher sensitivity to STR variant calling when compared to DBG based approaches. This concludes that assembly with OLC is a better way for genotyping STRs according to our experiments."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Davranışsal olarak inandırıcı sanal kalabalıkların modellenmesi ve canlandırılması, kalabalık benzetimi araştırmalarının önemli problemleridir. Tez çalışmamızda, sanal bireyleri, otonom olarak birbirleriyle iletişim kurma yeteneği ile donatmak için bir iletişim modeli önerilmiştir. Böyle bir iletişim modelinin, benzetimi yapılan kalabalıkların inandırıcılığını artırıp artırmayacağı araştırılmıştır. Başlangıçta çabalarımız, mümkün olduğunca insana benzer bir modele ve bu modeli psikolojik nitelikler de içeren bir sanal birey mimarisiyle birleştirmeye yönelikti. Erken deneysel sonuçlar, kalabalığa baktığımızda, farklı sanal birey kişiliklerinin farklı iletişimsel davranışlara neden olması gibi etkilerin fazla görünür olmadığını göostermiştir. Ayrıca, bu etkilerin başarılması karmaşıklığı artırmaktadır. Bu yüzden, insan benzeri bir iletişim modeli yerine genel ve kullanımı kolay bir iletişim modeli hedeflenmiştir ve psikolojik sanal birey niteliklerinin modellenmesinden vazgeçilmiştir. Önerilen iletişim modeli ve çeşitli senaryolarda uygulanmasının sonuçları bu tezde sunulmaktadır. İkinci bir katkı olarak, uygulama senaryolarının birinde ihtiyaç duyulması sonucunda, bir sanal bireyin bilmediği bir ortamda planlama yapmasını sağlayan bir algoritma geliştirilmiştir. Benzetim sonuçları hem görsel olarak hem de çeşitli ölçümler ve metrikler kullanılarak analiz edilmiştir. Vardığımız sonuç, gözlenen davranışsal çeşitliliğin iyileştirilmesine ek olarak, iletişim modeli kullanıldığında etkilerin sayısal sonuçlarda da gözlemlendiği ve bu etkilerin gerçekleştirilen senaryolarda beklentilerimizle uyumlu olduğu yönündedir.","Modeling and animation of behaviorally plausible virtual crowds are important problems of crowd simulation research. We propose a communication model in order to equip virtual agents with the ability to autonomously communicate with each other. We investigate whether such a communication model would improve the plausibility of the simulated crowds. Initially, our efforts were towards a model that is as human-like as possible and towards combining this model with an agent architecture that contains psychological attributes. Early experimental results showed that when we look at a crowd, the influences such as different agent personalities causing different communicative behavior are hardly visible. Besides, achieving these effects introduces complexity. Thus, a generic and easy-to-use communication model instead of a human-like one became the target and psychological agent attributes were dropped. The proposed communication model and its application in several scenarios are presented in this dissertation. As a second contribution, one of the application scenarios led us to develop a planning algorithm for an agent in an unknown environment. Simulation results are analyzed both visually and by using various measurements and metrics. Our conclusion is that in addition to improving observed behavioral variety, the effects of employing the communication model are clear in the quantitative results and these effects are in line with our expectations in each scenario."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genomik verilerin hızlı ve düşük maliyetli dizilimi, katılımcılara ait genomik bilgilerin saklandığı veri tabanlarını kullanan genetik araştırmalar ve kişisel servis uygulamalarını yaygınlaştırmaktadır. Bu veri tabanlarında kişilerin kimlikleri anonimleştirilse de, genomik veriler gizlilik korunmadan paylaşıldığında, kişiler hakkında hassas bilgiler edinilebilir. Akrabalık ilişkilerinin uygun şekilde saklanması güvenlik ihlallerinin engellenebilmesi icin önemli noktalardan biridir. Bu çalışmada, yalnızca tek nükleotid polimorfizm (SNP) verilerinin kamuya açık kayıtlarını kullanıldığı bir durumda bile akrabalık ilişkilerinin tespit edilebilir olduğunu gösteriyoruz. Kişilerin genomik benzerlikleri ve aile üyelerinin arasındaki aykırı alel çift sayılarının varlığının, akrabalık ilişkilerini risk altına koyduğunu gözlemliyoruz. Çalışma kapsamında, riskleri en aza indirgemek için, akrabalık gizliliğinden ödün vermeden verilerin, maksimum fayda ile paylaşımını mümkün kılan hesaplama modelleri sunuyoruz. Bu modellerde, aile üyelerinin veri tabanına sırayla geldiklerini varsayıyoruz. Modeller, yeni aile üyeleri veri tabanına eklendikçe, sistematik olarak genomik veride saklanacak asgari bölümleri tespit ediyor. Hangi pozisyonların ne ölçüde saklanması gerektiğini, saklanan pozisyonlarının sayısını en aza indirildiği ve akrabalık bilgilerinin sızdırılmamasını engelleyen mahremiyet kısıtlamalarına tabi tutulduğu bir optimizasyon problemi ile buluyoruz. Beş bireyden oluşan iki farklı ailenin, aile bireylerinin veri tabanına geldiği farklı sıralarda, modelleri uyguladık. Aldığımız sonuçlara göre, bir ebeveyn ve bir çocuğun genomik verilerinin eşzamanlı paylaşımı, akrabalık ilişkisini yüksek risklerle açığa çıkartırken, daha uzak akrabalarda, güvenli veri paylaşımının mümkün olduğunu görüyoruz. Öte yandan, aynı aile üyeleri veri tabanına farklı sıralarla geldiklerinde, farklı derecede gizlilik riskleri ve veri paylaşım fayda değerleri ile sonuçlanabildiğini gösteriyoruz. Önerilen yöntemin benimsenmesinin, gelecek araştırmalarda ve kamu genom hizmetleri alanlarında, akrabalık gizliliği koruyarak güvenli genom veri paylaşımına izin vereceğini umuyoruz.","Rapid and low cost sequencing of genomic data enables widespread use of genomic information in research studies and personalized customer applications, where people share their genomic data in public databases. Although the identities of the participants are anonymized in these databases, sensitive information about individuals can still be inferred if the stored data is not shared in a privacy-preserving manner. Proper handling of kinship information is one such caveat that needs to be addressed to avoid exposure of privacy-sensitive information. In this work, we show that by using only the publicly available single nucleotide polymorphism (SNP) data of anonymized individuals, kinship relationships can be inferred. We present two scenarios that result in privacy leakage; one based on genomic similarity of the individuals; the other, through the outlier allele pair counts of the family members. In the proposed models, we assume that the family members join to the database sequentially and we systematically identify minimal portions of data to withhold as the new participants are added to the database. Choosing the proper positions to hide is cast as an optimization problem. Therein, the number of positions to mask is minimized subject to several privacy constraints that ensure the kinship information among any pair of the family members is not leaked. We evaluate the proposed technique on real genomic data of two different families of size five by considering different sequential arrival orders for the family members. Results indicate that concurrent sharing of data pertaining to a parent and an offspring results in high risks of privacy leakages, whereas the sharing data from further relatives together is often safer. We also show that different arrival orders of the members can lead to different levels of privacy risks and the utility of shared data can vary. Adoption of the proposed method shall allow safe sharing of genomic data in terms of kinship privacy in future research studies and public genomic services."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris hesaplamaları lineer cebirin en önemli yapıtaşlarından olup bilim ve mühendislik alanlarında birçok problemde ortaya çıkmaktadırlar. Bu hesaplamalar, problem türüne bağlı olarak seyrek matris yoğun matris çarpımı (SyMM), seyrek matris vektör çarpımı (SyMV), veya seyrek simetrik bir matrisin faktorizasyonu şeklinde olabilirler. Dağıtık bellekli mimarilerde gerçekleştirilen SyMM ve SyMV işlemlerinde, özellikle düzensiz seyreklik örüntüsü olan matrisler için, işlemciler arasındaki veri ve görev bölümlemesi paralel performansı fazlaca etkilemektedir. Paralel SyMM işlemciler arasındaki yüksek hacimli iletişimler ile nitelendirilirken, paralel SyMV için hem iletişim hacmi hem de mesaj sayısı önemli olmaktadır. Zarf yöntemlerinde gerçekleştirilen faktörizasyonda, matris zarfının büyüklüğü yani matris profili faktörizasyon performansını belirleyen önemli bir etmendir. Bahsi geçen hesaplamaların performanslarını iyileştirmek amacıyla bu hesaplamaların herbiri için özyinelemeli ikiye bölümleme (ÖİB) paradigmasını kullanan çizge/hiperçizge bölümleme modelleri önermekteyiz. Önerilmekte olan modeller, ÖİB tarafından sunulan avantajları ilgili hesaplamanın performans iyileşmesi yönünde spesifik ihtiyaçlarını karşılama amacıyla kullanmaktadırlar. ÖİB işlemi bölümleme objektifinin, SyMM için önerilen modellerde birden fazla hacim tabanlı iletişim maliyeti ölçütlerini, SyMV için ise hem hacim hem mesaj sayısı ölçütlerini hedefleyecek şekilde kullanılmasını sağlamaktadır. Zarf yöntemlerindeki faktörizasyon için önerilen modelde ise, ÖİB işlemi, matris profilini küçültme ile yakından ilişkili olan iki yeni kalite ölçütünün tanımlanıp hedeflenmesine izin vererek girdi matrisin satır ve sütunlarını bu hedefle yeniden sıralanmasını sağlamaktadır. Deneysel sonuçlar ÖİB yaklaşımının bahsi geçen herbir hesaplama için alanında var olan en iyi yöntemlerden daha başarılı olduğunu göstermektedir.","Sparse matrix computations are among the most important building blocks of linear algebra and arise in many scientific and engineering problems. Depending on the problem type, these computations may be in the form of sparse matrix dense matrix multiplication (SpMM), sparse matrix vector multiplication (SpMV), or factorization of a sparse symmetric matrix. For both SpMM and SpMV performed on distributed-memory architectures, the associated data and task partitions among processors affect the parallel performance in a great extent, especially for the sparse matrices with an irregular sparsity pattern. Parallel SpMM is characterized by high volumes of data communicated among processors, whereas both the volume and number of messages are important for parallel SpMV. For the factorization performed in envelope methods, the envelope size (i.e., profile) is an important factor which determines the performance. For improving the performance in each of these sparse matrix computations, we propose graph/hypergraph partitioning models that exploit the advantages provided by the recursive bipartitioning (RB) paradigm in order to meet the specific needs of the respective computation. In the models proposed for SpMM and SpMV, we utilize the RB process to enable targeting multiple volume-based communication cost metrics and the combination of volume- and number-based communication cost metrics in their partitioning objectives, respectively. In the model proposed for the factorization in envelope methods, the input matrix is reordered by utilizing the RB process in which two new quality metrics relating to profile minimization are defined and maintained. The experimantal results show that the proposed RB-based approach outperforms the state-of-the-art for each mentioned computation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Iki uçlu sıralama problemi, bir veri kümesindeki pozitif örnekleri negatif olanlardan daha yüksek konumlara yerlestiren bir fonksiyon bulma problemi olarak tanımlanır. Finansal ve tıbbi alanlar, sıralama algoritmalarının ortak uygulama alanlarından bazılarıdır. Bununla birlikte, bu tür alanlar için ortak bir endise, veri kümesindeki kisilerin mahremiyetidir. Yani, böyle bir alandan elde edilen bir veri kümesindeki bilgiyi kesfetmek isteyen bir arastırmacı bir sıralama algoritması çalıstırmak için veri kümesindeki bireylerin tüm bilgilerine erismek zorundadır. Gizlilik endisesi, bu tür analizler için hassas kisisel verilerin kullanımına iliskin sınırlamalar getirmektedir. Arastırmacının, verilerden bir sıralama modeli ögrenmek için örneklerin ham verilerine ihtiyaç duymadıgı, gizliligi koruyan iki uçlu sıralama problemi için verimli bir çözüm önermekteyiz. RIMARC (ROC Egrisi Altındaki Alanı Maksimize Ederek Örnekleri Sıralama) algoritması, örnekleri sıralamak için bir model ögrenerek iki uçlu sıralama problemini çözer. Modelin bir parçası olarak, alıcının çalısma karakteristigi (ROC) egrisi altındaki alanı analiz ederek her bir özellik için bir agırlık ögrenir. RIMARC algoritmasının benzer sıralama algoritmalarından daha basarılı ve hızlı oldugu gösterilmistir. Dolayısıyla, RIMARC algoritmasını bir yapı tası olarak alıp, homomorfik sifreleme ve güvenli çok partili hesaplama kullanarak bu algoritmanın gizliligi koruyan bir versiyonunu gelistirdik. RIMARC algoritmasının büyük veri kümelerinde zaman verimliligini artırmak için, Resilient Distributed Datasets adlı programlama paradigması ile popüler, bir paralellestirme çerçevesi olan Apache Spark'da gizliligi koruyan versiyonunu gelistirdik. Önerilen algoritmamız, bir veri sahibinin, sifreli veri kümesinin depolanmasını ve islenmesini, yarı güvenilir bir bulut ortamında dıs kaynak olarak saglar. Bir arastırmacı, bir sıralama fonksiyonu ögrenmek için bulut ile etkilesim kurarak veri kümesindeki sorgularının sonuçlarını alabilir. Bu süreçte ne arastırmacı ne de bulut, islenmemis veri kümesiyle ilgili herhangi bir bilgiye erisemez. Önerilen algoritmanın güvenligi kanıtlanmakta ve gerçek veriler üzerindeki deneyler ile verimliligi gösterilmektedir.","The bipartite ranking problem is defined as finding a function that ranks positive instances in a dataset higher than the negative ones. Financial and medical domains are some of the common application areas of the ranking algorithms. However, a common concern for such domains is the privacy of individuals or companies in the dataset. That is, a researcher who wants to discover knowledge from a dataset extracted from such a domain, needs to access the records of all individuals in the dataset in order to run a ranking algorithm. This privacy concern puts limitations on the use of sensitive personal data for such analysis. We propose an efficient solution for the privacy-preserving bipartite ranking problem, where the researcher does not need the raw data of the instances in order to learn a ranking model from the data. The RIMARC (Ranking Instances by Maximizing Area under the ROC Curve) algorithm solves the bipartite ranking problem by learning a model to rank instances. As part of the model, it learns a weight for each feature by analyzing the area under receiver operating characteristic (ROC) curve. RIMARC algorithm is shown to be more accurate and efficient than its counterparts. Thus, we use this algorithm as a building-block and provide a privacy-preserving version of the RIMARC algorithm using homomorphic encryption and secure multi-party computation. In order to increase the time efficiency for big datasets, we have implemented privacy-preserving RIMARC algorithm on Apache Spark, which is a popular parallelization framework with its revolutionary programming paradigm called Resilient Distributed Datasets. Our proposed algorithm lets a data owner outsource the storage and processing of its encrypted dataset to a semi-trusted cloud. Then, a researcher can get the results of his/her queries (to learn the ranking function) on the dataset by interacting with the cloud. During this process, neither the researcher nor the cloud can access any information about the raw dataset. We prove the security of the proposed algorithm and show its efficiency via experiments on real data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Elektronik sa˘glık kayıtlarına olan taleplerin artması nedeniyle, ki¸sisel sa˘glık kaydı gibi hassas bilgilerin yetkisiz kullanıcılardan korunmasına ¸cok ¨onemli bir ihtiya¸c vardır. Kriptografi sistemleri ¨onemli ¨ol¸c¨ude geli¸stirilmi¸s olsa da, siber saldırılar son iki yılda b¨uy¨uk ¨ol¸c¨ude artmı¸stır. S¸ifreleme y¨ontemlerinde y¨uksek entropi parolalar kullanmak d¨u¸sman saldırısının ba¸sarısını azaltabilirse de, kullanıcılar arasında b¨oyle ¸sifreleri se¸cmek pop¨uler de˘gildir. Bununla birlikte, zayıf bir ¸sifre kullanmak, sistemi kaba kuvvet saldırılarına a¸cık hale getirir. Bu ama¸cla, ¸sifre entropisine bakılmaksızın ki¸sisel sa˘glık kaydı verilerinin g¨uvenli bir ¸sekilde depolanabilmesi i¸cin yeni bir sistem sunuyoruz. Sistemimiz, kaba kuvvet sınırının ¨otesinde bir g¨uvenlik sa˘glayan ve bu nedenle parola tabanlı ¸sifreleme'ye ¨ust¨unl¨uk sa˘glayan ve yeni bir yakla¸sım olan Honey Encryption (HE) ¸semasının bir uygulamasıdır. HE'nin temel unsuru olarak kesin bir kodlayıcı/kod¸c¨oz¨uc¨u modeli olu¸sturmak i¸cin ¸ce¸sitli veri setlerinden yakla¸sık 10000 hasta bilgisi kullanıyoruz. Onerilen modeli sa˘glayarak, ge¸cersiz anahtarlarla yapılan ¸sifrelemenin d¨u¸smana, hastanın ge¸cerli g¨or¨un¨uml¨u ancak yanlı¸s sa˘glık bilgilerini vermesini sa˘glıyoruz. HE'nin daha ¨onceki uygulamaları genellikle zaman i¸cinde de˘gi¸smeyen statik veri k¨umeleriyle ilgilidir. Ancak biz ki¸sisel sa˘glık kayıtları i¸ceren olduk¸ca dinamik bir veri k¨umesinde HE tabanlı bir model tasarladık. Edindi˘gimiz bilgiler do˘grultusunda, parola entropisine bakılmaksızın sa˘glık kayıtlarının kaba kuvvet saldırılarına kar¸sı gelebildi˘gi parola tabanlı ilk sistemi ¨onerdik. ¨ Onerilen kodlama y¨onteminin, parola tabanlı ¸sifreleme ¸semasının do˘grudan uygulanmasıyla kar¸sıla¸stırılmasının sonu¸cları, bir d¨u¸smanın herhangi bir yanlı¸s ¸sifreyi elemesinin hemen imkansız oldu˘gunu g¨ostermektedir. Aynı zamanda, bir hastanın sa˘glıkla ilgili ¨ozelliklerine dayanan yan bilgiler i¸ceren farklı saldırılar i¸cin ger¸cek hayat senaryolarını ele alıyoruz. Ki¸sisel sa˘glık kaydı verilerini depolamak ve i¸slemek i¸cin sa˘glam bir sistem uyguluyoruz. Sistemimiz ki¸sisel sa˘glık kaydı verilerini korumak i¸cin yeni ve pratik bir ¸c¨oz¨umd¨ur.","There is a crucial need for protecting patient's sensitive information, such as personal health record (PHR), from unauthorized users due to the increase in demands of electronic health records. Even though cryptography systems have been significantly developed, cyber attack is dramatically increased during the last couple of years. Although using high entropy passwords in the encryption methods can decrease the success of an adversarial attack, it is not popular among the users to choose such passwords. However, using a weak password makes the system vulnerable to brute-force attacks. Towards this end, we present a new framework as a solution for a secure storage of PHR data regardless of the password entropy. Our system is an application of Honey Encryption (HE) scheme which is a new approach that provides a security beyond the brute-force bound and therefore dominates the Password Based Encryption (PBE). We utilize almost 10K patients' information from various datasets in order to construct a precise encoder/decoder model as a core element of HE. By providing the proposed model, we ensure that the encryption with invalid keys yields a valid-looking but incorrect health information of a patient to an adversary. The previous applications of HE are mainly on the static datasets that are not changing over the time. However, we were able to design an HE based model on a highly dynamic dataset of PHR. To the best of our knowledge, we are the first to provide a robust password based framework against brute-force attacks of health records regardless of the password entropy. The results of the comparing our proposed encoding method with the direct application of the PBE scheme show that it is almost impossible for an adversary to eliminate any wrong password. We also consider real-life scenarios for different attacks with side information about a patient's health related attributes. We implement a robust and concrete framework for storing and processing the PHRs that is also a novel, practical solution for protecting PHR data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde amacımız, gizliliği koruyan kurumlararası veri paylaşımını gerçekleştirmektir. Bu amaçla iki farklı sistem önerilmiştir: (i) kötü niyetli modelde gizliliği koruyan benzerlik testi yapmayı sağlayacak doğrulanabilir hesaplamaya dayanan bir sistem ve (ii) yarı güvenilir modelde bağlantı tahmini yapan bir sistem. Önerilen bu sistemler, servis kalitesini arttrmak için gerekli görevleri yaparken, sistemde yer alan tarafların gizliliğini korumayı amaçlar. Doğrulanabilir hesaplamaya dayanan sistemimizde, bir istemci ve birden fazla sunucunun olduğu merkezi bir sistem öneriyoruz. Bu çalışmada, farklı hastanelerde bulunan hastaların benzerliklerinin hesaplandığı durumu ele alıyoruz. Hasta verisinin sahibi olan istemci farklı hastanelerde bulunan birden fazla sunucuya istek gönderir. İstemcinin amacı, bu hastanelerdeki benzer hastaları bulmak ve onlara uygulanan tedavi yöntemlerini öğrenmektir. Bağlantı tahminine dayanan sistemimizde ise ortak kullanıcılara sahip olan iki sosyal ağ bulunmaktadır. Aralarında bağlantı tahmini yapmak için iki kullanıcı seçilir. Gizliliği koruyan bir şekilde bağlantı tahmini gerçekleştirilir. Böylece sosyal ağlardan hiçbiri karşı tarafın sosyal ağ yapısını öğrenmez. Farklı metrikler hesaplanarak kullanıcıların benzerliği belirlenirken gizliliği koruyan sayı karşılaştırması kullanılmıştır.","In this thesis, we aim to enable privacy-preserving data sharing between entities and propose two systems for this purpose: (i) a verifiable computation scheme that enables privacy-preserving similarity computation in the malicious setting and (ii) a privacy-preserving link prediction scheme in the semi-honest setting. Both of these schemes preserve the privacy of the involving parties, while performing some tasks to improve the service quality. In verifiable computation, we propose a centralized system, which involves a client and multiple servers. We specifically focus on the case, in which we want to compute the similarity of a patient's data across several hospitals. Client, who is the hospital that owns the patient data, sends the query to multiple servers, which are different hospitals. Client wants to find similar patients in these hospitals in order to learn about the treatment techniques applied to those patients. In our link prediction scheme, we have two social networks with common users in both of them. We choose two nodes to perform link prediction between them. We perform link prediction in a privacy-preserving way so that neither of the networks learn the structure of the other network. We apply different metrics to define the similarity of the nodes. While doing this, we utilize privacy-preserving integer comparison."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yeni Nesil Dizileme teknolojileri birçok değişkende birbirleri arasında farklılık göstermektedir. Kısa parçalı dizileme ya da uzun parçalı dizileme teknolojileri arasında yapılacak bir seçim ise parçaların doğruluk oranı ya da uzunlukları arasında bir tercih gerektirir. Bu tezde ilk olarak, kısa parçaların kullamıyla yapılan analizlerin yeniden üretilebilirliği konusundaki problemleri belirtiyorum. Yeni nesil dizileme teknolojileri kullanılarak genomik farklılıkların karakteristikleri üzerinde yaptığımız geniş çalışma göstermektedir ki tekrarlayan dizileme parçaları, dizilerin muğlak bir şekilde haritalanmasına sebep olabilir. Kısa parçalar tekrarlamaya daha yatkın olduklarından dolayı bu parçaların kullanıldığı deneylerin yeniden üretilebilmesinde problemler yaşanması mümkündür. Bu tezde ikinci olarak, özgün bir algoritma olan Hercules'i sunuyorum. Hercules, uzun parçalardaki hataların düzeltilmesi için makine öğrenimi tekniğini kullanan ilk algoritmadır. De novo yöntemiyle haritalama, yapısal farklılıkların araştırılması gibi birçok araştırma uzun ve hatasız parçaların kullanımını gerektirmektedir. Bu durumlarda, araştırmacılar genellikle uzun parçalardaki hataların düzeltilmesini kısa parçalar ile yapmaktadırlar. çizge yapısı ve hizalama temelli güncel düzeltme yöntemleri, dizileme teknolojisinin hata profilini göz ardı etmektedirler. Hata profilini el alan, hafıza ve zaman konusunda elverişli makine öğrenimi teknikleri, hataları daha iyi düzeltme ve daha her iki teknolojiyi daha iyi birleştirme konusunda potansiyele sahiptirler. Sunduğumuz algoritma, her bir uzun parçayı, kullanılan teknolojinin hata profiline uygun bir profile Hidden Markov Model'i şeklinde tasarlamaktadır. Algoritmamız, geçiş ve emisyon olasılıklarını bütün uzun parçalar için öğrenip, değiştirerek uzun parçalardaki hataların düzeltilmesini sağlamaktadır. DNA diziliminden iki adet veri dizisi (CH17-157L1 ve CH17-227A2) ve RNA diziliminden bir adet veri dizisi (human brain cerebellum polyA) kullanarak, Hercules tarafından hataların giderildiği parçaların, diğer algoritmalar kullanılarak hataların düzeltilmesine kıyasla en yüksek haritalama oranına ve uzun parçaların büyük bölümü kısa parçalarla kaplandığı durumlarda en yüksek hatasızlık oranına sahip olduğunu gösteriyoruz.","Next Generation Sequencing technologies differ by several parameters where the choice to use whether short or long read sequencing platforms often leads to trade-offs between accuracy and read length. In this thesis, I first demonstrate the problems in reproducibility in analyses using short reads. Our comprehensive analysis on the reproducibility of computational characterization of genomic variants using high throughput sequencing data shows that repeats might be prone to ambiguous mapping. Short reads are more vulnerable to repeats and, thus, may cause reproducibility problems. Next, I introduce a novel algorithm Hercules, the first machine learning-based long read error correction algorithm. Several studies require long and accurate reads including de novo assembly, fusion and structural variation detection. In such cases researchers often combine both technologies and the more erroneous long reads are corrected using the short reads. Current approaches rely on various graph based alignment techniques and do not take the error profile of the underlying technology into account. Memory- and time- efficient machine learning algorithms that address these shortcomings have the potential to achieve better and more accurate integration of these two technologies. Our algorithm models every long read as a profile Hidden Markov Model with respect to the underlying platform's error profile. The algorithm learns a posterior transition/emission probability distribution for each long read and uses this to correct errors in these reads. Using datasets from two DNA-seq BAC clones (CH17-157L1 and CH17-227A2), and human brain cerebellum polyA RNA-seq, we show that Hercules-corrected reads have the highest mapping rate among all competing algorithms and highest accuracy when most of the basepairs of a long read are covered with short reads."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris-matris çarpımları (SyGEMM) bir çok alanda en sık kullanılan operasyonlardan biridir. Bu işlemler genel olarak karmaşık ve uzun çalışma sürelerine sahiptir. Dağıtık bellek sistemlerinde bu işlemleri parallelleştirmek için bir çok yöntem mevcuttur. Bunlar: dış çarpım, iç çarpım, satır-satır çarpım ve sütun-sütun çarpımıdıır. Bu tezde, düşük önhazırlık, iyi performans ve sembolik çarpma gerektirmemesi gibi bir çok getirisinden dolayı satır-satır çarpımına yoğunlaşılmıştır. Satır-satır çarpımının paralelleştirilmesinde iki-kümeli çizgeler ve hiper çizgeler kullanılabilmektedir. Daha verimli bir paralleştirme için, toplam hacim ve en yüksek hacim gibi bir çok hacim odaklı ölcüt dikkate alınabilir. Satır-satır çarpımlar için var olan yöntemler, bir çok hacim odaklı olcutu aynı anda gerçeklestirmekte başarsz olmaktadırlar. Bu tezde, bir çok hacim odaklı ölçütü aynı anda düşürmek için iki aşamalı bir yöntem önerdik. İlk aşamada, toplam hacim iki kümeli çizge kullanılarak düşürülmüştür. İkinci aşamada ise toplam hacimdeki artışı en azda tutmaya çalışarak en yüksek hacimi düşürdük. Deneylerimizde görülebilmektedir ki, önerdiğimiz yöntem çeşitli SyGEMM işlemleri için bir çok hacim odaklı ölçeği aynı anda düşürmüştür.","Sparse matrix-matrix multiplication of the form of C = A x B, C = A x A and C = A x AT is a key operation in various domains and is characterized with high complexity and runtime overhead. There exist models for parallelizing this operation in distributed memory architectures such as outer-product (OP), inner-product (IP), row-by-row-product (RRP) and column-by-column-product (CCP). We focus on row-by-row-product due to its convincing performance, row preprocessing overhead and no symbolic multiplication requirement. The parallelization via row-by-row-product model can be achieved using bipartite graphs or hypergraphs. For an efficient parallelization, we can consider multiple volume-based metrics to be reduced such as total volume, maximum volume, etc. Existing approaches for RRP model do not encapsulate multiple volume-based metrics. In this thesis, we propose a two-phase approach to reduce multiple volume-based cost metrics. In the rst phase, total volume is reduced with a bipartite graph model. In the second phase, we reduce maximum volume while trying to keep the increase in total volume as small as possible. Our experiments show that the proposed approach is effective at reducing multiple volume-based metrics for different forms of SpGEMM operations."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek çekirdeklerin ve işlemlerin büyük-ölçekli dağıtık bellekli sistemlerde paralelizasyonu modern yüksek performanslı hesaplama sistemlerinin sürekli artan ölçekleri ve paralel performansı etkileyen birbiriyle çakışan birçok etmenin varlığı nedenleriyle büyük bir zorluk olarak kalmaktadır. Seyrek işlemlerin düşük hesaplama yoğunlukları ve yüksek bellek izleri daha fazla vurgulanan iletişim darboğazlarını işaret ederek bu zorluklara yenilerini eklemekte ve ölçeklenebilir performans için hızlı ve verimli paralelizasyon model ve metotlarını zorunlu kılmaktadır. Seyrek işlemler genelde seyrek matrislerle ilgili veri yapıları üzerinde gerçekleştirilmekte ve matrisler koşma öncesinde işlemcilere dağıtılmak için bölümlenmektedir. Literatür bu alanda çok zengin olmasına karşın, literatürde iletişim performansını belirleyen birçok etmeni bu etmenlere hakedilen önemi atfederek tam anlamıyla aynı anda işleyebilecek yöntemlerin eksikliği bulunmaktadır. Bu tezde iletişim performansının daha doğru bir yakınsamasını elde etmek amacıyla seyrek matrislerin akıllı bölümlenmesini sağlayan model ve metotları incelemekteyiz. Paralel seyrek işlemlerin iletişim performansını arttırmak için bütün iletişim maliyetlerinde başlıca bir bileşen olarak kendisini gösteren gecikim darboğazlarının azaltılmasına odaklanmaktayız. Bunun yanı sıra, önerilen yaklaşımlar literatürde halihazırda kabul görmüş iletişim maliyet ölçütlerini de hesaba katarak olabildiğince fazla ölçütü aynı anda işlemeye çalışmaktadır. Bir-boyutlu (1D) ve iki boyutlu (2D) seyrek matrislerin bölümlenmesinde gecikim maliyetlerini azaltmak için sırasıyla bir-fazlı ve iki-fazlı bölümleme modelleri önermekteyiz. 1D bölümleme için önerilen model sıkça kullanılan özyinelemeli bölümleme metoduna dayanmakta olup gecikim gerektiren ilişkileri gösterebilmek için yeni yapılar kullanmaktadır. 2D bölümleme için önerilen bölümleme modelleri asimetrik lineer sistemler için kullanılan çözücülerin performanslarını, bu çözücülerde kullanılan vektörler üzerinde farklı bölümler aracılığıyla gecikim maliyetlerini azaltarak geliştirmeyi hedeflemektedir. Tezde elde ettiğimiz bulgular dağıtık bellekli sistemlerde ölçeklenebilir performans elde edilebilmesi için gecikim maliyetlerinin kesinlikle düşünülmesi gerektiğini göstermektedir.","Parallelization of sparse kernels and operations on large-scale distributed memory systems remains as a major challenge due to ever-increasing scale of modern high performance computing systems and multiple conflicting factors that affect the parallel performance. The low computational density and high memory footprint of sparse operations add to these challenges by implying more stressed communication bottlenecks and make fast and efficient parallelization models and methods imperative for scalable performance. Sparse operations are usually performed with structures related to sparse matrices and matrices are partitioned prior to the execution for distributing computations among processors. Although the literature is rich in this aspect, it still lacks the techniques that embrace multiple factors affecting communication performance in a complete and just manner. In this thesis, we investigate models and methods for intelligent partitioning of sparse matrices that strive for achieving a more correct approximation of the communication performance. To improve the communication performance of parallel sparse operations, we mainly focus on reducing the latency bottlenecks, which stand as a major component in the overall communication cost. Besides these, our approaches consider already adopted communication cost metrics in the literature as well and aim to address as many cost metrics as possible. We propose one-phase and two-phase partitioning models to reduce the latency cost in one-dimensional (1D) and two-dimensional (2D) sparse matrix partitioning, respectively. The model for 1D partitioning relies on the commonly adopted recursive bipartitioning framework and it uses novel structures to capture the relations that incur latency. The models for 2D partitioning aim to improve the performance of solvers for nonsymmetric linear systems by using different partitions for the vectors in the solver and uses that flexibility to exploit the latency cost. Our findings indicate that the latency costs should definitely be considered in order to achieve scalable performance on distributed memory systems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Öğrenciler, okulla ilgili soruları için ilk tercih olarak arama motorlarını kullanırlar. Arama motorları, her ne kadar genel popülasyon için oldukça kullanışlı olsa da eğitim kurgusunun dışında yanıtlar getirebilir. Bir başka eğilim, sosyal ağ soru-cevap web siteleri ise emsallerinden cevaplar almak isteyen öğrenciler için ikinci bir seçenek olarak karşımıza çıkmaktadır. Çalışmamızda, bu iki bilgi kaynağının birbirlerinden faydalanılarak geliştirilmesi üzerinde durulmuştur. Çalışmamızın ilk kısmı soru-cevap web siteleri ile ilgilidir. Eğitsel soru-cevap web siteleri üzerinde bağlamsal ve davranışsal anlayışa sahip olmak için bir soru-cevap web sitesinin içeriği toplanmıştır. Bu içerik, kullanıcı davranışları ve eğitsel soru-cevap sitelerinin genel soru-cevap sitelerinden ne derece farklı olduğunu anlamak açısından analiz edilmiştir. İkinci kısımda, eğitsel sorular için bir sınıflandırıcı geliştirilmiştir. Bu sınıflandırıcı makine öğrenmesi tabanlı bir kaç algoritma ile dış kaynaklar üzerinde oluşturulmuş bir kaç arama tabanlı sınıflandırıcıdan oluşan bir ""ensemble"" sınıflandırıcıdır. Ayrıca, sınıflandırmayı güçlendirmek için bir sorgu genişletme yöntemi geliştirilip kullanılmıştır. Oluşan bu sınıflandırıcı, son olarak arama motoru sonuç sayfaları da kullanılarak daha da geliştirilmiştir. Üçüncü kısımda, eğitsel arama motoru sıralamasının sınıflandırma modeli kullanılarak geliştirilebilirliğini test etmek için, bir arama motorundan alınan sonuç sayfaları toplanıp etiketlenmiştir. Sorgu-doküman sınıf ilişkisinin ilgi düzeyi ile alakalı olduğu varsayımından yola çıkarak, arama motoru sıralamasını geliştirmek üzere beş yöntem kullanılmıştır. Bu yöntemler, çeşitli sorgu setleri üzerinde uygulanıp eğitsel sıralama bağlamında kayda değer gelişme olduğu görülmüştür. Son olarak, eğitsel yazım denetimi üzerinde durulmuştur. Eğitsel arama sistemlerinde, kullanıcıların yazım hataları yapması sık rastlanan bir durumdur. İlk olarak iki ticari arama motorunun sorgu kayıtları, eğitsel amaçlı tasarlanmamış ve eğitsel kelimeleri içermeyen fakat genel olarak iyi bilinen beş sorgu denetimi ve düzeltmesi yazılımı kullanılarak denetlenmiştir. Bu yazılımlardan bir tanesinin sözlüğünün, küçük boyutlu bir eğitsel kelime listesi ile bile desteklendiğinde ""precision"", ""recall"" ve F1 değerlerinin gelişme gösterdiği görülmüştür.","Students use general web search engines (GSEs) as their primary source of research while trying to find answers to school related questions. Although GSEs are highly relevant for the general population, they may return results that are out of education context. Another rising trend; social community question answering websites (CQ&A) are the secondary choice for students who try to get answers from other peers online. We focus on discovering possible improvements on educational search by leveraging both of the two information sources. The first part of our work involves Q&A websites. In order to gain contextual and behavioral insights, we extract the content of a commonly used educational Q&A website with a scraper we implement. We analyze the content in terms of user behavior and try to understand to what extent the educational Q&A differs from the general purpose Q&A. In the second part, we implement a classifier for educational questions. This classifier is built by an ensemble method that employs several regular learning algorithms and retrieval based ones that utilize external resources. We also build a query expander to facilitate classification. We further improve the classification using search engine results. In the third part, in order to find out whether search engine ranking can be improved in the education domain using the classification model, we collect and label a set of query results retrieved from a GSE. We propose five ad-hoc methods to improve search ranking based on the idea that the query-document category relation is an indicator of relevance. We evaluate these methods on various query sets and show that some of the methods significantly improve the rankings in the education domain. In the last part, we focus on educational spell checking. In educational search systems, it is common for users to make spelling mistakes. Actual query logs of two commercial search engines in the education domain are analyzed in terms of spelling mistakes using 5 well-known spell correction software that are not education specific and lack the terms that are used in the education field. It is shown that by extending the spell-check dictionary of one of them, even with a small-sized education oriented word-list, one can improve the precision, recall and F1 values of a spell-checker."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çevrimdışı hesaplanan hava durumu efektlerinin oldukça detaylı olarak araştırılmasına rağmen, günümüz donanımında gerçek zamanlı çalıştırmak mümkün değil. Bu yüzden daha az hesaplamalı yöntemler geliştirildi. Buna rağmen, gerçek zamanlı programlarda hava koşulları, her mekan için sanatçılar tarafından tek tek hazırlanıyor, çünkü yapımcı şirketler, simülasyon için ekstra performans kaybetmek istemiyorlar. Bunun asıl sebebi ise günümüzdeki algoritmaların, küçük sahneler için bile hesaplama gücünün büyük kısmını kullanıyor olmalarıdır. Sonuç olarak, hesaplamalar gereğinden fazla kaynak kullandığı için tercih edilmiyorlar. Bu tezde kullanılan yöntemde hava koşulları tahmine dayalı hesaplanıyor, bu yüzden toplam hesaplama süresi 0.5 milisaniyeden daha az bir seviyede oluyor. Hava durumundan oluşan birikmelerin tahmini; sahnedeki objelerin oryantasyonu, yeryüzü, yükseklik, pozisyon, güneş ve rüzgarların yönüne göre değişiyor. Hava koşullarının etkisi, objelerin orijinal kaplamalarından oluşuyor ve daha sonra gerçek hayattan taranan veri, İki Yönlü Yansıma Yayılım Fonksiyonu (İYYF) doğrultusunda karşılaştırılıp düzeltiliyor. Tüm hesaplamalar Grafik İşlem Birimi'nde (GİB) Fiziksel Bazlı Görsel Hesaplama (FBGH) yöntemi ile kullanılmak üzere işleme sokuluyor. Hava koşulundaki tüm değişimlerin tahmine dayalı olmasına ve objelerin orijinal kaplamalarından oluşturulmasına rağmen, sonuçlar gerçek hava koşullarına yaklaşık bir görüntü oluşturuyor.","Although weather effects have been developed thoroughly in offline renderings, their computational cost exceed current hardware to run them in real-time. Therefore, less expensive methods are developed. Still, in many real-time applications weather conditions are prepared by artists for each environment independently because developers do not want to compromise performance over simulated weather. The main reason of this is current developed algorithms use most of the graphical processing power just for weather effects in a relatively small scale scene. At the end, they become unusable in real scenario, because weather conditions are only one part of the overall system. Therefore, in current approach, all weather conditions are executed based on prediction, where its computational cost is negligible under 0.5 ms on current generation hardware. Prediction is done based on many factors such as orientation, height and position on terrain and global wind and sun direction. Appearance of weather conditions are generated from original textures of the objects then compared and re-adjusted to achieve similar values of real life scanned Bidirectional Reflectance Distribution Function (BRDF) data. All computation is done on Graphics Processing Unit (GPU) to be used with Physically Based Rendering (PBR) approach. Even though all changes are made based on prediction and textures are generated, at the end, results are comparable to weather conditions in real life."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"En ölümcül kanser tiplerinden olan meme kanseri, deri tipli olmayan kanserler arasında en sık görünen ikinci kanser tipidir. Meme kanserinde erken ve doğru teşhis tam tedavi için oldukça kritiktir. Son zamanlardaki tıbbi görüntü işleme araştırmaları bu konuda umut vadeden sonuçlar elde etmişlerdir. Bu sonuçların biyopsi görüntülerinin analizi sırasında daha doğru anlaşılması ve olası sağlıksız dokuların tesbitinde fayda sağlayabileceği düşünülmektedir. Ancak ne yazık ki, tamamen otomatik bir bilgisayar destekli teşhis için tüm slayt histopatoloji görüntülerinin işlenmesi gerekirken, bu araştırmalar genellikle özel olarak kesilmiş ve etiketlenmiş görüntüler üzerine olmaktadır. Bununla beraber tüm slayt görüntülerinin boyutlarının oldukça büyük olmasından dolayı kabul edilebilir bir işlem gücü ve zaman içerisinde işlenmesi güçleşmektedir ve farklı bölgelerde sağlıklı veya tümör dokularını farklı aşamalarda bulunması tüm slayt görüntülerde tümör tesbiti ve sınıflandırılmasını zorlaştırmaktadır. Biz, ham tüm slayt görüntüsünden son teşhise kadar hızlı bir biçimde tümör tesbiti ve sınıflandırması yapabilen ve bunu yaparken güncel derin öğrenme tekniklerini etkili bir biçimde kullanan bir bilgisayar destekli teşhis sistemi tasarladık. Bu sistem uzman patologların sistematik çalışma akışı ve aşamalı detaya yakınlaşma tarzından esinlenerek geliştirilmiş olup temelde iki aşamadan oluşmaktadır: (1) teşhis için ilgili alanların tesbiti, (2) tesbit edilen alanların beş kanser tipine sınıflandırılması. Özgün ilgili alan tesbit yaklaşımımız uzmanların birden fazla çözünürlük seviyesinde verimli arama örüntüsünü taklit etmektedir. Bunun için dört adet derin ağ patologların tüm slayt inceleme kayıtlarından çıkartılan örneklerle eğitilmiştir. Daha sonra sadece tesbit edilen ilgili alanlar üzerinde daha derin bir ağ ve ardıl-işleme kullanılarak her bir tüm slayt görüntüsü tek bir kanser tipine sınıflandırılmaktadır. 240 tüm slayt görüntüsü üzerinde yapmış olduğumuz deneylerimizde, tasarladığımız ilgili alan tesbit yaklaşımımız bu sorunun çözümünde en gelişkin diğer bir yöntemden daha verimli ve etkin çalıştığı gözlemlenmiştir. Bütün sistemin nihai sınıflandırması ise 45 patoloğun ortalama başarısının hemen altındadır. Ayrıca derin öğrenme öznitelikleri, farklı görselleştirme teknikleri kullanılarak incelenmiş ve öğrenilen bilgiler görüntülenmiştir.","The most frequent non-skin cancer type is breast cancer which is also named one of the most deadliest diseases where early and accurate diagnosis is critical for recovery. Recent medical image processing researches have demonstrated promising results that may contribute to the analysis of biopsy images by enhancing the understanding or by revealing possible unhealthy tissues during diagnosis. However, these studies focused on well-annotated and -cropped patches, whereas a fully automated computer-aided diagnosis (CAD) system requires whole slide histopathology image (WSI) processing which is, in fact, enormous in size and, therefore, difficult to process with a reasonable computational power and time. Moreover, those whole slide biopsies consist of healthy, benign and cancerous tissues at various stages and thus, simultaneous detection and classification of diagnostically relevant regions are challenging. We propose a complete CAD system for efficient localization and classification of regions of interest (ROI) in WSI by employing state-of-the-art deep learning techniques. The system is developed to resemble organized workflow of expert pathologists by means of progressive zooming into details, and it consists of two separate sequential steps: (1) detection of ROIs in WSI, (2) classification of the detected ROIs into five diagnostic classes. The novel saliency detection approach intends to mimic efficient search patterns of experts at multiple resolutions by training four separate deep networks with the samples extracted from the tracking records of pathologists' viewing of WSIs. The detected relevant regions are fed to the classification step that includes a deeper network that produces probability maps for classes, followed by a post-processing step for final diagnosis. In the experiments with 240 WSI, the proposed saliency detection approach outperforms a state-of-the-art method by means of both efficiency and effectiveness, and the final classification of our complete system obtains slightly lower accuracy than the mean of 45 pathologists' performance. According to the McNemar's statistical tests, we cannot reject that the accuracies of 32 out of 45 pathologists are not different from the proposed system. At the end, we also provide visualizations of our deep model with several advanced techniques for better understanding of the learned features and the overall information captured by the network."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümuüzde çok sayıda orta katman uygulaması veri katarlarını işlemede kullanılmaktadır. Bu uygulamaların işlediği verinin boyutu düşünüldüğünde, bu uygulamaların veriyi paralel olarak işleyen düğümlere sahip olması kaçınılmazdır. Bu dağıtık veri katarı işleme uygulamaları veriyi anlık olarak çalışma sırasında işlemektedirler. Diğer bir deyişle bu uygulamalar büyük veri için gerçek zamanlı ve dağıtık işleme imkanı sağlarlar. Bu dağıtık düğümlerin durum yönetimi ile ilgili işlemler, bir düğümün bozukluğu, sağlama noktaları oluşturulması, verinin zenginleştirilmesi ve verinin tekrar bölümlere ayrılması gibi kullanım senaryolarından dolayı ele alınması gereken önemli bir görevdir. Bu tezde, dağıtık veri katarı uygulamaları için saydam, veri yerelliği farkındalığında bir durum yönetimi mekanizması sunulmaktadır. Önerilen mekanizma dağıtık veri katarı uygulamaları için veri yerelliği farkındalığı olan, saydam bir veri bölümlendirme ve durum yönetimi sistemi sağlamaktadır. Mekanizma veriyi, veri yerelliğini koruyarak bölümlendirmekte ve bölümlendirme şemasında gerçekleşebilecek olası değişimlerde, veriyi işleyen düğümler arasında durum verisinin saydam olarak aktarımını sağlamaktadır. Buna ek olarak, mekanizma veri ile ilgili saklama ünitesinde sağlama noktaları oluşturmak uzere yüksek performanslı bir durum yönetimi özelliği de sunmaktadır. Bu fikir, açık kaynaklı, dağıtık veri katarı motoru Apache Storm için bir tak-çıkar kütüphane olarak gerçeklenmiştir.","Today, there are many applications that deal with high-volume data streams. These distributed stream processing applications process data on-the-fly and provide real-time distributed computing for big data. Due to the volume of data they process, some of these applications make use of data parallel nodes. The state management for distributed nodes in these applications is an important task to handle, because of different use cases such as: dealing with node failures, checkpointing, data enrichment, and re-partitioning. Therefore, distributed stream processing applications need a state management mechanism. In this thesis, we present a locality-aware state management mechanism for distributed stream processing applications. The proposed mechanism provides a transparent locality-aware data partitioning and state management system for distributed stream processing applications. The mechanism partitions data while preserving locality and handles state transfer among nodes transparently, in order to adapt to potential changes in the partitioning. In addition to this, it provides operators with a high-performance state management facility that can tackle check-pointing scenarios. The idea is implemented as a pluggable library for the open-source, distributed stream-processing engine, Apache Storm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Artık insanların, yazılım sistemlerinin ve fiziksel dünyanın daha önce hiç olmadığı kadar büyük bir hızda, hacimde ve çeşitlilikte veri ürettikleri, herşeyin bağlı ve daha erişilebilir olduğu bir dünyada yaşıyoruz. Telekomünikasyon, sosyal medya ve canlı veri kaydı gibi bir çok uygulama alanında insanlar, sistemler ve bulundukları ortamlar arasında kolayca bir ilişki bulunabilir. Bu ilişkiler çoğunlukla, varlıkları düğümler, aralarındaki ilişkilerde kenarları gösterecek şekilde zamanla gelisen bir çizge olarak karşımıza çıkabilmektedir. Bu yüzden, bir çizge yapısı içinde dinamik değişebilen ilişkileri yönetmek, modern veri işleme sistemlerinin ortak gereksinimlerinden biri olmuştur. Veri işleme sistemlerinin yaygınlaşması da çizge veritabanlarının önem kazanmasına sebep olmuştur. Bu çalışmada, artımlı güncellemeleri destekleyen, disk tabanlı bir çizge veritabanı geliştirdik. Veritabanına çizge güncelleştirmeleri (ekleme/çıkarma) bir akış şeklinde gelir, sistem gelen veriye göre bir depolama düzeni oluşturur ve en optimize olacak şekilde bu düzeni yönetir. Böylece, optimize depolama düzeni üzerinde düşürülen disk girdi-çıktısı, gezinme tipi algoritmaların daha verimli olarak koşulabilmesini sağlar. Geliştirdiğimiz depolama düzeni optimizasyonları, artımlı biçimde çizge güncellemeleri gelirken oluşan çizge kenarlarının boyutsal lokalitelerini hesaba katarak, çizge üzerinde gezinme sırasında oluşan disk girdi-çıktı sayısını düşürmeyi amaçlamaktadır.","The world has become ever more connected, where the data generated by people, software systems, and the physical world is more accessible than before and is much larger in volume, variety, and velocity. In many application domains, such as telecommunications and social media, live data recording the relationships between people, systems, and the environment is available. This data often takes the form of a temporally evolving graph, where entities are the vertices and the relationships between them are the edges. For this reason, managing dynamic relationships represented by a graph structure has been a common requirement for modern data processing applications. Graph databases gained importance with the proliferation of such data processing applications. In this work, we developed a disk-based graph database system, which is able to manage incremental updates on the graph structure. The updates arrive in a streaming manner and the system creates and maintains an optimized storage layout for the graph in an incremental way. This optimized storage layout enables the database to support traversal based graph algorithms more efficiently, by minimizing the disk I/O required to execute them. The storage layout optimizations we develop aim at taking advantage of spatial locality of edges to minimize the traversal I/O cost, but achieves this in an incremental way as new edges/vertices get added and removed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris-vektör çarpımı (SyMV), bilimsel uygulamalarda yaygın olarak kullanılan önemli bir çekirdek işlemdir. Düzensiz seyrek matrislerde SyMV işlemi, girdi vektör elemanlarına düzensiz erişim gerçekleştirdiği için önbellek başarımının düşük olmasına neden olmaktadır. Bu çalışmada, SyMv işleminde yüksek başarım elde etmek amacıyla, girdi-vektör elemanlarının erişiminde uzaysal ve zamansal yerellikten yararlanmak için, çizge ve hiper çizge bölümleme yöntemlerine dayanan matris satır ve sütun sıralama yöntemleri önerilmektedir. Bu yon temler, seyreklik örüntüsü benzer olan matris satır ve sütunlarını gruplayarak birbirlerine yakın sıralamaktadırlar. Önerilen çizge ve hiper çizge bölümleme tabanlı uzaysal ve zamansal yerellik sağlama yöntemleri, tek a ̧samada ayrı ayrı kullanılabileceği gibi, tek veya iki aşamada birlikte de kullanılabilmektedirler. Önerilen yöntemlerin başarımı, 60 ̧cekirdekli Xeon-Phi işlemcide, çeşitli uygulamalarda ortaya çıkan geniş bir matris kümesi kullanılarak denenmiştir. Deney sonuçları, önerilen yöntemlerin geçerlilik ve etkinliğini doğrulamaktadırlar.","Sparse Matrix-Vector multiplication (SpMV) is a very important kernel opera- tion for many scientific applications. For irregular sparse matrices, the SpMV operation suffers from poor cache performance due to the irregular accesses of the input vector entries. In this work, we propose row and column reordering methods based on Graph partitioning (GP) and Hypergraph partitioning (HP) in order to exploit spatial and temporal localities in accessing input vector entries by clustering rows/columns with a similar sparsity pattern close to each other. The proposed methods exploit spatial and temporal localities separately (using either rows or columns of the matrix in a GP or HP method), simultaneously (using both rows and column) and in a two-phased manner(using either rows or columns in each phase). We evaluate the validity of the proposed models on a 60- core Xeon Phi co-processor for a large set of sparse matrices arising from different applications. The performance results confirm the validity and the effectiveness of the proposed methods and models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek çözünürlüklü uzaktan algılama görüntülerinin analizinde önemli bir problem kendi içinde heterojen yapıların sezilmesidir. Bina, yol ve ağaç gibi \textit{temel nesnelerin} uzamsal yerleşimlerinden oluşan farklı türlerdeki yerleşim alanları, tarım alanları, ticari ve endüstriyel alanlar \textit{bileşik yapılar} olarak da adlandırılan bu yapılara örnek olarak verilebilir. Farklı temel nesne katmanlarından gelen bilinmeyen sayıda temel nesnelerin yerlesimlerini içeren bileşik yapıların modellenmesi ve büyük görüntülerde otomatik sezimi için genel bir yöntem sunmaktayız. Verilen örnek bileşik yapılardaki bölgeler olasılıksal değişkenler olarak temsil edilmekte, bir Markov rasgele alanı ile bu bölgelerin uzamsal yerleşim modeli oluşturulmakta ve bu modele ait parametre kümesi ilgili maksimum entropi olasılık dağılımından örneklenerek öğrenilmektedir. Benzer bölge gruplarının sezimi, farklı temel nesne katmanlarını temsil eden çoklu bölge sıradüzenlerinden gelen çoklu aday nesneler arasından sorgun modelini enyükselten altkümelerin seçilmesine indirgenmektedir. Kombinatoryal seçme problemi, bölge gruplarının ilgi duyulan yapı örneklerinden öğrenilen model altında bireysel görünüşleri ve bağıl uzamsal yerleşimleri enyükseltilerek ortaklaşa örneklenmesi ile çözülmektedir. Bundan başka, birbiriyle örtüşen fazlalık bölgelerin beraberce seçilmesini engellemek ve seçilen bölgeleri belirli bir uzamsal yerleşimin sağlanmasına zorlamak amacıyla aday bölgeler üzerine içbükey eşitlik ve eşitsizlik kısıtları koymaktayız. Kısıtlı seçme problemi, lineer kısıtlı karesel program olarak formule edilmekte ve dışbükey olmayan bu program iki dışbükey programın farkı olarak yeniden yazılarak ilkin-eşlek (primal-dual) algoritmasının bir çeşidi olan Dışbükeylerin Farkı algoritması ile çözülmektedir. çok yüksek çözünürlükteki görüntüler kullanılarak yapılan kapsamlı deneyler, görüntülerdeki sadece spektral ve şekilsel öznitelikler kullanılarak elde edilemeyen bilinmeyen sayıdaki farklı bileşik yapıların yerlerinin önerilen yöntemle doğru bir şekilde saptandığını göstermektedir.","A challenging problem in remote sensing image interpretation is the detection of heterogeneous compound structures such as different types of residential, industrial, and agricultural areas that are comprised of spatial arrangements of simple primitive objects such as buildings and trees. We describe a generic method for the modeling and detection of compound structures that involve arrangements of unknown number of primitives appearing in different primitive object layers in large scenes. The modeling process starts with example structures, considers the primitive objects as random variables, builds a contextual model of their arrangements using a Markov random field, and learns the parameters of this model via sampling from the corresponding maximum entropy distribution. The detection task is reduced to the selection of multiple subsets of candidate regions from multiple hierarchical segmentations corresponding to different primitive object layers where each set of selected regions constitutes an instance of the example compound structures. The combinatorial selection problem is solved by joint sampling of groups of regions by maximizing the likelihood of their individual appearances and relative spatial arrangements under the model learned from the example structures of interest. Moreover, we incorporate linear equality and inequality constraints on the candidate regions to prevent the co-selection of redundant overlapping regions and to enforce a particular spatial layout that must be respected by the selected regions. The constrained selection problem is formulated as a linearly constrained quadratic program that is solved via a variant of the primal-dual algorithm called the Difference of Convex algorithm by rewriting the non-convex program as the difference of two convex programs. Extensive experiments using very high spatial resolution images show that the proposed method can provide good localization of unknown number of instances of different compound structures that cannot be detected by using spectral and shape features alone."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"DNA dizi hizalama problemi, kısaca, bir ya da birden fazla örnekten alınan DNA dizilerinin aynı veya benzer türe ait referans genomunu içeren veri tabanı ile karakter seviyesinde karşılaştırılması olarak tanımlanabilir. Yüksek kapasiteli dizileme (YKD) teknolojileri ilk olarak 2006 yılında kullanılmaya başlanmıştır. Bugün, YKD teknolojileri insan genomunun sadece 3 gün içerisinde yaklaşık 1.000$ maliyetle okunmasına imkan sağlamaktadır. Hızlı bir şekilde gelişmeye devam etmekte olan bu teknoloji ile birlikte çok büyük miktarda okuma ile karşılaşmak mümkün olmaktadır. YKD verilerinin analizi bir milyardan fazla kısa parçanın (100 karakter veya baz çifti) oldukça uzun olan (yaklaşık 3 milyar baz çifti) referans genomu ile karşılaştırılmasını gerektirdiğinden, yüksek mikltarda hesaplamaya dayalı bir problem olarak sunulabilir. DNA molekülü çift sarmal bir yapıda olduğundan, gereken kıyaslama sayısı iki katına çıkmaktadır. Bu nedenle yürütme zamanı ve bu büyüklükteki kısa parçaların referans ile karşılaştırılması zor ve önemli bir problemdir. O(n2) zamanda milyarlarca kısa parçanın uzun parçalara lokal hizalanma için geliştirilmiş algoritmaları kullanmak yerine, süreci hızlandıran keşifsel yaklaşımlar uygulanmaktadır. İlk olarak Burrows Wheeler Transform (BWT) ile sıkıştırılmış referansı ardından Ferragina-Manzini yöntemi ile endeksleyerek ya da daha basit komut tablosu kullanarak kısmi dizi eşleşmeleri hızlıca bulunabilir. Ardından, bulunan aday lokasyanlar Levenshtein uzaklığını hesaplayan ve karesel zaman gerektiren dinamik programlama algoritması ile doğrulanır. Bu yaklaşımlar lokal hizalama algoritmalarının doğrudan uygulanmasından oldukça daha hızlı olmasına rağmen, insan genomunun tekrarlı yapısından dolayı, her bir okuma için ağır hesaplama yüküne yol açan yüzlerce doğrulama gerektirmektedir. Ancak, bu milyarlarca hizalamanın her biri birbirinden bağımsız olduğundan okuma yerleştirme problemi paralelleştirilebilir bir yapıya sahiptir. Bu tez çalışmasında, sadece merkezi işlem birimi kullanan algoritma kapasitesinin üzerinde güce sahip, dizi hizalama prosedürünü hızlandırmak için optimize edilmiş, birden fazla sayıda grafik işlem birimleri kullanabilen yeni bir algoritma sunuyoruz. Okuma yerleştirme iş gücünü çoklu grafik işlem birimlerinin paralel mimarisi parçalarına dağıtarak, milyonlarca hizalamayı aynı anda gerçekleştiriyoruz. Yöntemimiz hem çok çekirdekli merkezi işlem birimlerini hem de bir ya da birden fazla çoklu grafik işlem birimleri kullanabilmektedir. Bu çalışmada amacımız büyük ölçekli hesaplama altyapılarına veya bulut platformlarına duyulan ihtiyacı azaltma hedefi doğrultusunda gelişmiş GPGPU cihazları ile tek bir sunucunun yapabileceği şekle dönüştürmektir","DNA sequence alignment problem can be broadly de ned as the character-level comparison of DNA sequences obtained from one or more samples against a database of reference (i.e., consensus) genome sequence of the same or a similar species. High throughput sequencing (HTS) technologies were introduced in 2006, and the latest iterations of HTS technologies are able to read the genome of a human individual in just three days for a cost of  $1,000. With HTS technologies we may encounter massive amount of reads available in di erent size and they also present a computational problem since the analysis of the HTS data requires the comparison of >1 billion short (100 characters, or base pairs) \reads"" against a very long (3 billion base pairs) reference genome. Since DNA molecules are composed of two opposing strands (i.e. two complementary strings), the number of required comparisons are doubled. It is therefore present a dicult and important challenge of mapping in terms of execution time and scalability with this volume of di erent-size short reads. Instead of calculating billions of local alignment of short vs long sequences using a quadratic-time algorithm, heuristics are applied to speed up the process. First, partial sequence matches, called \seeds"", are quickly found using either Burrows Wheeler Transform (BWT) followed with Ferragina-Manzini Index (FM), or a simple hash table. Next, the candidate locations are veri ed using a dynamic programming alignment algorithm that calculates Levenshtein edit distance (mismatches, insertions, deletions di erent from reference), which runs in quadratic time. Although these heuristics are substantially faster than local alignment, because of the repetitive nature of the human genome, they often require hundreds of veri cation runs per read, imposing a heavy computational burden. However, all of these billions of alignments are independent from each other, thus the read mapping problem presents itself as embarrassingly parallel. In this thesis we propose novel algorithms that are optimized for multiple graphic processing units (GPGPUs) to accelerate the read mapping procedure beyond the capabilities of algorithmic improvements that only use CPUs. We distribute the read mapping workload into the massively parallel architecture of GPGPUs to performing millions of alignments simultaneously, using single or many GPGPUs, together with multi-core CPUs. Our aim is to reduce the need for large scale clusters or cloud platforms to a single server with advanced parallel processing units"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir hastanın sağkalımını tahminleme, hastaya uygun tedavi planını belirleyebilmek için kritik öneme sahiptir ve geleneksel olarak hastanın yaşı, tümör evresi gibi klinik ve patolojik değişkenlere dayandırılarak yapılmaktadır. Bu tezde, yeni nesil dizilime yöntemleri ile elde edilen moleküler verileri kullanarak başarılı ve yorumlanabilir sağkalım tahminleme modellerine ulaştıran iki yöntem sunuyoruz. Mevcut sağkalım modellerinin çoğu, moleküllerin tekil ifadeleme değerlerinin tekil temsili üzerine kurulmaktadır. Fakat kanser, moleküler mekanizmaların çok farklı yollarla bozulması ile meydana çıkan karmaşık bir hastalıktır. Bu çalışmada sistemsel biyoloji yaklaşım ile, moleküler değerlerin birbirlerine göre sıralamasını göz önünde bulunduran bir öznitelik temsili öneriyoruz. Önerdiğimiz model moleküler özniteliklerin doğrudan bağlantılı oldukları biyolojik mekanizmaları açığa çıkmarmayı mümkün kılmanın yanında, genel kabul görmüş, L1 regülasyonlu çok değişkenli Cox nisbi risk ve Rastgele Sağkalım Ormanı modelleri ile over kanserinde sağ kalımı tahmin için kullanıldığında, tekil değerler ile kurulan modellerden daha iyi sonuçlar vermektedir. Önerdiğimiz öznitelik temsilini over kanseri hasta gruplarını bulmakta kullandığımızda da grupların sağkalım dağılımları arasında istatiksel olarak anlamlı ölçüde bir ayrım sağladığını görmekteyiz. İkinci olarak ise, sıralama tahminini karar destek vektörleri ile öğrenen yeni bir model (RsurVM) sunuyoruz. RsurVm, sağkalım modellerinin başarımlarını ölçmek için en yaygınca kullanılan metrik olan konkordans indisini optimize etmeye odaklanmakta ve sansürlü örneklemleri de hiç bir ön kabul yapmadan kullanabilmektedir. Over kanseri hastalarının moleküler verileri üzerinde yaptığımız geniş kapsamlı testler göstermektedir ki RsurVM, kullanılan moleküler veriden bağımsız olarak (mRNA,protein, miRNA, kopya sayısı farklılıkları ve metilasyon), net bir şekilde, Cox nisbi risk ve Rastgele Sağkalım Ormanı modellerini geri bırakarak en iyi sağkalım tahminlemesini yapmaktadır.","Predicting the survival of a cancer patient is critical for choosing patient specific treatment strategies and is traditionally based on clinical or pathological factors such as patient age and tumor stage. In this thesis, we present two methodologies to build effective and interpretable survival models that utilize high-dimensional molecular profiles made available through next-gen sequencing technologies. Firstly, we present a method that focuses on partial ordering in the feature space. Existing models rely on the individual molecular quantities recorded in tumors; however, cancer is a complex disease where molecular mechanisms are dysregulated in various ways. This study, based on a system level perspective, incorporates the partial ordering of molecules (POF) in lieu of individual quantities. This strategy not only unveils predictive features with direct relevance to the biological mechanism and but also yields better performance in survival prediction compared to multivariate l1 penalized Cox proportional hazard and Random Survival Forest models. Testing the partial order representation of features in the subgroup identification task, we find that these features yield groups of patients, which are more quantifiably distinct in terms of survival distributions. Secondly, we develop a survival prediction method based on ranking and support vector machines – Ranking Survival Vector Machines (RsurVM). RsurVM obtains a pairwise ranking of the patient survival times by learning to rank. It focuses on optimizing the most commonly used metric concordance index and can handle the censored data without making any assumptions. Our extensive tests on the ovarian adenocarcinoma patient molecular data demonstrate that RsurVM achieves better survival predictions regardless of the input molecular data (mRNA, protein, miRNA, Copy number variation and DNA methylation) than the two most commonly used methods: Cox-proportional hazards model and Random Survival Forest."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kanser heterojen bir hastalıktır; her bir kanser tipi moleküler, histopatolojik ve klinik olarak farklılıklar gösteren bir çok alt tipi barındırır. Bir kanser tipine ait alt gruplarının belirlenmesi, kişiye özel ve hedefe yönelik tedavi yöntemleri geliştirilebilmesini ve alt tiplerin moleküler karakteristiklerinin anlaşılmasıyla hastalığın mekanizmalarına dair bilgileri açığa çıkarabilmesini mümkün kıldığı için önemlidir. Geleneksel olarak kanser alt gruplarını keşfetmek için genomik veriler üzerinde gözetimsiz kümeleme teknikleri uygulanır ve bu yolla belirlenen gruplar, ancak hasta sağ kalımı gibi kritik bir parametre açısından ilişkililer ise anlamlı olarak değerlendirilirler. Biz bu gözetimsiz öğrenme çerçevesi yerine, WS-RFClust adını verdiğimiz, grupların ayrışmasına klinik parametrenin yön verdiği zayıf gözetimli bir kümeleme tekniği öneriyoruz. Bu yöntemde, rastgele orman sınıflandırıcısı kurulup, ormandaki ağaçların ara dallarında hastaların aynı gruplara düşüp düşmediği bilgisine dayalı olarak bir hasta benzerlik matrisi oluşturulmaktadır. Bu matris daha sonra bir kümeleme algoritmasına girdi olarak verilmekte ve hasta grupları bulunmaktadır. WS-RFClust, yapısı gereği sınıflandırma adımında oluşturulan, özniteliklerin doğrusal olmayan kombinasyonlarından oluşan öznitelik alt uzayını kullanmaktadır. WS-RFClust yöntemini el yazısı rakamlarında kullandığımızda, rakamların yapısal özelliklerini yakaladığını görmekteyiz. WS-RFClust'ın mRNA, protein ve microRNA ifadeleme veri setlerini kullanarak meme kanserini alt tiplerini bulmak için uyguladığımızda genel geçer kullanılan gözetimsiz kümeleme teknikleri ile oluşan kümelemelerden daha iyi çalıştığını göstermekteyiz.","Each cancer type is a heteregonous disease consisting of subtypes, which may be distinguished at the molecular, histopathological, and clinical level. Identifying the patient subtypes of a cancer type is critically important as the unique molecular characteristics of a particular patient subgroup reveal distinct disease states and opens up possibilities for targeted therapeutic regimens. Traditionally, unsupervised clustering techniques are applied on the genomic data of the tumor samples and the patient clusters are found to be of interest if they can be associated with a clinical outcome variable such as the survival of patients. In lieu of this unsupervised framework, we propose a weakly supervised clustering framework, WS-RFClust, in which the clustering partitions are guided with the clinical outcome of interest. In WS-RFClust a random forest is trained to classify the patients based on a categorical clinical variable of interest. We use the partitions of patients on the tree ensemble to construct a patient similarity matrix, which is then used as input to a clustering algorithm. WS-RFClust inherently uses the nonlinear subspace of the original features that is learned in the classification step for clustering. In this study, we demonstrate the effectiveness of WS-RFClust on hand-written digit datasets, which captures salient structural similarities of digit pairs. Finally, we employ WS-RFClust to find breast cancer subtypes using mRNA, protein and microRNA expressions as features. Our results on breast cancer subtype identification problem show that WS-RFClust could identify patients more effectively in comparison to the commonly used unsupervised clustering methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bulut bilişim ve ilgili hizmetlere yönelik artan talepten dolayı, bulut sağlayıcıları, veri merkezlerinin ve bulut sistemlerinin performansını, elverişliliğini ve güvenirliliğini artıracak yöntemler oluşturmak zorundadır. Bir fiziksel makinenin kaynaklarının birden çok sanal makine tarafından paylaşılmasını sağlayan sunucu sanallaştırması, bunun gerçekleştirilmesi için ana bileşendir. Sanallaştırmanın eniyileştirilmesi, bulut bilişim sisteminin genel performansı üzerinde önemli bir etkiye sahiptir ve bu da sanal makinelerin, fiziksel makinelere etkili ve verimli bir şekilde yerleştirilmesini gerektirir. Bu durum birden fazla kısıtlama içeren bir eniyileştirme problemi olduğundan dolayı, sanal makineleri yerleştirmek için genetik algoritma temelli bir yöntem önermekteyiz. Makinelerin kullanım oranlarını ve düğümler arası uzaklıkları dikkate alan yöntemimiz; kaynak israfını, ağ yükünü ve enerji tüketimini aynı anda düşürmeyi hedeflemektedir. Yöntemimiz diğer birkaç yöntem ile; gerçekleştirilen kullanım oranı, tüketilen ağ band genişliği ve sebep olunan enerji masrafı kıstasları üzerinden kullanıma açık CloudSim simulasyon platformu kullanılarak karşılaştırılmıştır. Sonuçlar, yaklaşımımızın, karşılaştırılan diğer benzer yaklaşımlara göre daha gelişmiş bir performans sağladığını göstermiştir.","Due to increasing demand for cloud computing and related services, cloud providers need to come up with methods and mechanisms that increase performance, availability and reliability of datacenters and cloud computing systems. Server virtualization is a key component to achieve this, which enables sharing of resources of a physical machine among multiple virtual machines in a totally isolated manner. Optimizing virtualization has a very significant effect on the overall performance of cloud computing systems. This requires efficient and effective placement of virtual machines into physical machines. Since this is an optimization problem that involves multiple constraints and objectives, we propose a method based on genetic algorithms to place virtual machines. By considering utilization of machines and node distances, our method aims at reducing resource waste, network load, and energy consumption at the same time. We compared our method with several other methods in terms of utilization achieved, networking bandwidth consumed, and energy costs incurred, using the publicly available CloudSim simulation platform. The results show that our approach provides improved performance compared to other similar approaches."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çevrimiçi kullanılabilir verideki artışla birlikte veri analizi bugünün veri merkezlerinde çözülmesi gereken önemli bir problem haline gelmiştir. Ayrıca çizge çözümleme uygulamaları, içinde bulunduğumuz büyük veri çağında önemli uygulamalardan biridir. Bununla birlikte, merkezi işlem birimi (CPU) ve grafik işlemcileri (GPU) gibi geleneksel mimariler çizge uygulamalarının ihtiyaçlarını karşılamakta yetersiz kalmaktadırlar. Çizge uygulamalarının düzensiz bellek erişimi, dengesiz yük dağıtımı ve düzensiz hesaplama gibi özellikler taşıması üretilen iş odaklı ya da yerellik bazlı bellek sistemleri üzerine kurulu olan mevcut hesaplama sistemlerini zorlamaktadır. Öte yandan gelişmekte olan donanım özelleştirme teknikleri yukarıda bahsi geçen problemlerin çözülmesinde yardımcı olmakta ve bu çözümlerin enerji tasarruflu olması beklenmektedir. Bu tezde; düzensiz, düğüm merkezli (vertex centric) ve eşzamansız (asynchronous) çizge uygulamalarının üstesinden gelebilecek bir donanım hızlandırıcı taslağı önerilmektedir. Gelişmiş yüksek seviyeli SystemC modellerinin programcıya soyut bir arayüz vermesiyle programcının arka plandaki mimari hakkında ayrıntılı bilgiye sahip olmadan donanımı gerçekleştirmesi hedeflenmektedir. Verilen taslak sayesinde programcı tek bir uygulamaya bağımlı olmaktan kurtulur ve bu soyut taslağa uyduğu sürece herhangi bir çizge uygulamasını geliştirebilir. Bunun yanında, verilen şablon kullanılarak farklı uygulamalar geliştirmeye olanak sağlanması, bu uygulamaları geliştirmek ve denemek için harcanan süreyi kısaltır. Buna ek olarak, kapsamlı deneysel çalışmalar sonucunda, önerilen taslağın son teknolojiye sahip 24 çekirdekli CPU sistemlerinden 3 kata kadar daha hızlı ve 65 kata kadar daha güç tasarruflu olduğu gözlenmiştir.","With the increase in data available online, data analysis became a significant problem in today's datacenters. Moreover, graph analytics is one of the significant application domains in big data era. However, traditional architectures such as CPUs and Graphics Processing Units (GPUs) fail to serve the needs of graph applications. Unconventional properties of graph applications such as irregular memory accesses, load balancing, and irregular computation challenge current computing systems which are either throughput oriented or built on top of traditional locality based memory subsystems. On the other hand, an emerging technique hardware customization, can help us to overcome these problems since they are expected to be energy efficient. Considering the power wall, hardware customization becomes more desirable. In this dissertation, we propose a hardware accelerator framework that is capable of handling irregular, vertex centric, and asynchronous graph applications. Developed high level SystemC models gives an abstraction to the programmer allowing to implement the hardware without extensive knowledge about the underlying architecture. With the given template, programmers are not limited to a single application since they can develop any graph application as long as it fits to the given template abstract. Besides the ability to develop different applications, the given template also decreases the time spent on developing and testing different accelerators. Additionally, an extensive experimental study shows that the proposed template can outperform a high-end 24 core CPU system up to 3x with up to 65x power efficiency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar grafi kleri teknolojisindeki hızlı ilerlemelere rağmen, görsel kaliteyi geliştirmek ve görüntüleme maliyetini düşürmek halen araştırmacıların temel hedefi dir; çünkü bilgisayar gücü yükseldikçe kullanıcıların beklentileri de yükselmektedir. Özellikle interaktif 3B oyun ve sinema endüstrisinde gerçekçi içerikler gerçek zamanlı olarak talep edilmektedir. Bunun yanında, sosyal ağların ve veri paylaşımının popülerleşmesine bağlı olarak, büyük miktarda verinin verimli bir şekilde görselleştirilmesine gereksinim vardır. Dikkatli bir şekilde kullanıldığında, bilgi görselleştirmesi uygulamaları için üçüncü boyut yeni bir veri kanalı sağlamaktadır. Tüm bu nedenlerden dolayı, 3B sahnelerin görsel kalitesinin artırılması bilgisayar gra fikleri ve görselleştirme topluluğunun ilgisi dahilindedir. Son yıllarda, görsel algı alanındaki bulguların bilgisayar grafiklerinde kullanılması yaygınlaşmaya başlamıştır; çünkü aslında görsel kalite insan algısı tarafından ölçülmektedir ve algılanamayacak detayların fiziksel gerçekliğe uygun olması için fazladan maliyete gerek yoktur. Halen algısal prensiplerin bilgisayar grafiklerinde uygulanması alanında araştırmaya ihtiyaç vardır. Bu çalışma ile algıya dayalı bilgisayar grafikleri alanına iki temel katkı sağlamaktayız: İlk olarak, statik ve dinamik 3B modellerin görsel kalitesinin değerlendirilmesi amacıyla birkaç tane algısal hata ölçütü önermekteyiz. İkinci olarak, 3B görselleştirme uygulamalarında algılanan derinlik kalitesini artırmayı amaçlayan bir sistem geliştiriyoruz. 3B model üzerinde uygulanan sayısal damgalama veya sıkıştırma gibi bir operasyonun algılanan kaliteyi nasıl etkilediğini değerlendirmek için bir ölçüte ihtiyaç vardır. Bu alandaki çalışmalar 2B üzerindeki çalışmalara kıyasla çok limitlidir. Bu çalışmada, dinamik 3B modeller üzerinde bölgesel distorsiyonların görünürlüğünü ölçen genel amaçlı bir kalite ölçütü geliştirmek için insan görme sisteminin uzamsal ve zamansal bileşenlerini içeren aşağıdan yukarıya bir yaklaşım önerilmektedir. Buna ek olarak, yine 3B modeller için yeni bir veri tabanlı kalite ölçütü geliştirmek amacıyla makine öğrenmesi ve kitle kaynaklı çalışma kullanımı örneklendirilmektedir. 3B içeriğin görselleştirilmesi sırasında, derinlik ipuçlarının tasarım amaçlarını destekleyecek şekilde seçilmesi ve kullanıcının sahnedeki nesneler arasındaki uzamsal ilişkiyi algılamasının kolaylaştırılması önemli bir meseledir. Bu doğrultuda, verilen bir sahne ve görev için uygun derinlik ipuçlarını ve bu ipuçlarını sağlayan görüntüleme yöntemlerini belirleyen bir çerçeve sistem öne sürülmüştür. Bu sistem, derinlik ipuçlarının öneminin belirlenmesinde bulanık mantıktan, görüntüleme yöntemlerinin maliyetleri ve derinlik algısına katkıları arasındaki maliyet-fayda analizinin modellenmesinde sırt çantası metodundan yararlanmaktadır. Bu tezde önerilen bütün yöntemler kullanıcı deneyleri ile doğrulanmış ve yeni araştırmalar için teşvik edici sonuçlar elde edilmiştir. Ayrıca çalışmada kullanılan veriler ve elde edilen sonuçlar bilgisayar grafikleri topluluğunun kullanımına sunulmuştur. Sonuç olarak, bu çalışmada önerilen yöntemler ile görsel algı ve bilgisayar grafi kleri disiplinleri arasındaki açıklık biraz daha daraltılmaya çalışılmıştır.","Despite the rapid advances in computer graphics technology, enhancing the visual quality and lowering the rendering cost is still the essential goal for computer graphics researchers; since improvements in computational power raise the users' expectations too. Especially in interactive 3D games and cinema industry, very realistic graphical contents are desired in real-time. In the meantime, due to the increasing popularity of social networking systems and data sharing, there is a huge amount of data to be visualized e ffectively. When used carefully, 3D introduces a new data channel for information visualization applications. For that reason, improving the visual quality of 3D computer-generated scenes is still of great interest in the computer graphics and visualization community. In the last decade, utilization of visual perception fi ndings in computer graphics has started to get popular since visual quality is actually judged by the human perception and there is no need to spend additional cost for the physical realism of the details that cannot be perceived by the observer. There is still room for employing the perceptual principles in computer graphics. We contribute to the perceptual computer graphics research in two main aspects: First we propose several perceptual error metrics for evaluating the visual quality of static or animated 3D meshes. Second, we develop a system for ameliorating the perceived depth quality and comprehensibility in 3D visualization applications. A measure for assessing the quality of a 3D mesh is necessary in order to determine whether an operation on the mesh, such as watermarking or compression, aff ects the perceived quality. The studies on this field are limited when compared to the studies for 2D. A bottom-up approach incorporating both the spatial and temporal components of the low-level human visual system processes is suggested to develop a general-purpose quality metric designed to measure the local distortion visibility on dynamic triangle meshes. In addition, application of crowdsourcing and machine learning methods to implement a novel data-driven error metric for 3D models is also demonstrated. During the visualization of 3D content, using the depth cues selectively to support the design goals and enabling a user to perceive the spatial relationships between the objects are important concerns. In this regard, a framework for selecting proper depth cues and rendering methods providing these cues for the given scene and visualization task is put forward. This framework benefi ts from fuzzy logic for determining the importance of depth cues and knapsack method for modeling the cost-pro fit tradeoff between the rendering costs of the methods and their contribution to depth perception. All the proposed methods in this study are validated through formal user experiments and we obtain encouraging results for further research. These results are made publicly available for the benefi t of graphics community. In conclusion, we try to make the gap between visual perception and computer graphics fi elds narrower with the suggested methods in this work."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sanal makine çizelgeleme problemi bulut sağlayıcıları ve özel bulutlar için açık uçlu bir problemdir. Dağıtmada, kullanıcının ağ iletişimi, kaynak ve toplam bütçe gibi gereksinimleri bilinerek optimizasyon sağlanabilir. Fakat herkese açık bulut sistemlerinde böyle bir seçenek bulunmamakla birlikte özel bulutlarda da gelişmiş dağıtma seçenekleri bulunmamaktadır. Kullanıcı programlarının çalışmasını planlayacağı zamanda, sağlayıcılardan kaynak isteklerini bilinçli kararlar sonucunda istemekte ve dağıtmayı da kendi yapmaktadır. Bu tezde, geçmiş araştırmaları genişleterek, elimizdeki kaynakları daha verimli bir şekilde kullanan yeni bir çizelgeleme yöntemi sunmaktayız. Tezimizde yapılacak işlerin kaynak kulanımlarının önceden analizi ile farklı karakteristiklere sahip işlerin aynı sanal makinelerde planlanmasıyla, işlerin birbirini etkilemeden kaynaklardan yararlanımının arttırılabildiğini gösteriyoruz. Buna ek olarak, kaynakları kapasitelerinin üzerinde kullandığımız zaman da toplam süreden feragat ederek maliyetin düşebildiğini gösteriyoruz. Bu planlama yöntemini tezin 2.ci parçası olan Programlama Ödevleri Notlandırma Sisteminde'de (PAGS) kullanıyoruz. Programlama ödevleri, programlama derslerinin önemli bir parçası olmakla beraber, notlandırmada yapılan, dosyaların indirilmesi, güvenliğin sağlanması ve çalıştırılması gibi tekrarlayan işler bu süreci zorlaştırmaktadır. PAGS, Bilkent üniversitesi İşletim Sistemleri dersinde öğrenciler ve asistanlar üzerinde denenmiş olup, bu tezde öğrencilerin davranışsal sonuçları incelenmiştir. Ayrıca, öğrenci kodlarının çalıştırılma isteklerinin çizelgelenmesi de ilk kısımda anlatılan methodlarla yapılmıştır.","Virtualization and use of virtual machines (VMs) is important for both public and private cloud systems and also for users. The allocation and use of virtual machines can be optimized by using knowledge about expectations of users, such as resource demands, network communication patterns, and total budget. However, both public and private cloud providers do not expose advanced configuration options to make use of custom needs of users. Adding upon to previous research, we propose a new approach for allocating and scheduling user jobs to virtual machines by use of container technologies like Docker, so that VM utilization can be increased and costs for users can be decreased. In our approach, by predicting resource demands, we can schedule different kinds of jobs on a single virtual machine without jobs affecting each other and without degrading performance to unacceptable levels. We also allow cost-performance tradeoff for users. We verified our approach in a real test-bed and evaluated it with extensive simulation experiments. We also adapted our approach into a real web-based application we developed, called PAGS (Programming Assignment Grading System), which enables efficient and convenient testing, submission and evaluation of programming assignments of a large number students in an interactive or batch manner in identical and isolated system environments. Our approach effectively schedules requests from teachers and students so that the system can horizontally scale in a cost efficient manner."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gerçekçi kalabalık insan grubu davranışı yaratmak kalabalık simülasyonlarının en temel amaçlarından birisi olarak yer alır. Sanal karakterlerin bakış davranışını simüle etmek ve ilgi çekici noktaları tahmin etmek inandırıcı sahneler oluşturmada önemli rol oynamaktadır. Fakat bu yaklaşım bu zamana kadar bilgisayar grafiği alanında yeteri kadar ilgi çekmemiştir. Bu çalışma sanal karakterlerin bakış davranışı üretmesini sağlayan bir dikkat çekicilik modeli sunmaktadır. Model algı üzerine yapılmış güncel çalışmaları ele alarak dikkat çekici unsurlar belirler ve bu unsuların etkilerini ölçer. Bir karakterin ilgisini çeken noktayı tahmin ederken sahnede karakterin görüş alanınında bulunan diğer karakterler ve nesnelerin dikkat çekme skorları bir ağırlıklı toplam fonksiyonu ile hesaplanır. Ardından bu skorlar karşılaştırılarak izleyici karaktere göre sahnedeki en dikkat çekici varlık bulunur. Bu işlem sahnedeki her bir karakter için hesaplanır ve böylece tüm karakterler çevreleri hakkında görsel bir algıya sahip olurlar. Bunun yanında, modelimiz kalabalık algısına yeni bakış açıları da getirmektedir. Örnek olarak, kalabalıkların gruplar halinde ele alınması, bakış kopyalama, hızın dikkate etkisi verilebilir. Önerilen modelin değerlendirmesi için elde ettiğimiz dikkat çekme modelini videolardan elde ettiğimiz gerçek dünya kalabalık davranış verileri ile karşılaştırdık. Hazırlanan senaryolarda, gerçek kalabalık davranışlarının simülasyonunu yaptık. Elde edilen sonuçlar modelin gerçekçi bakış davranışı sunan ve farklı senaryolara kolaylıkla uygulanabilir olduğunu göstermektedir.","Creating realistic crowd behavior is one of the major goals in crowd simulations. Simulating gaze behavior and predicting interest points of virtual characters play a significant role in creating believable scenes, however this aspect has not received much attention in the field. This study proposes a saliency model, which enables virtual agents to produce gaze behavior. The model measures the effects of distinct pre-defined saliency features that are implemented by examining the state-of-the-art perception studies. When predicting an agent's interest point, we compute the saliency scores by using a weighted sum function for other agents and environment objects in the field of view of the agent for each frame. Then we determine the most salient entity in the virtual scene according to the viewer agent by comparing the scores. We execute this process for each agent in the scene, thus agents gain a visual understanding about their environment. Besides, our model introduces new aspects to crowd perception, such as perceiving characters as groups of people, gaze copy phenomena and effects of agent velocity on attention. For evaluation, we compare the resulting saliency gaze model with real world crowd behavior in captured videos. In the experiments, we simulate the gaze behavior in real crowds. The results show that the proposed approach generates plausible gaze behaviors and is easily adaptable to varying scenarios for virtual crowds."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Videolardan hareketlerin tanımlanması yaygın olarak çalışılan bir problemdir ve yıllar boyunca önerilen birçok çözüm olmuştur. Sınıflandırma için gerekli olan eğitim verisini etiketleme, bu yöntemlerin ölçeklenebilirliği için önemli bir sorun olmuştur. Diğer taraftan, videoların gürültülü içeriği nedeniyle çok sayıdaki zayıf etiketli web verilerinin kullanımı bir sorun olmaya devam etmektedir. Bu çalışmada, koleksiyonu budama yoluyla ilgisiz videoların elenmesi ve en temsilci elemanların keşfedilmesi problemlerini ele almaktayız. Görüntü sınıflandırma için ayrıştırıcı parçaları keşfeden yöntemlerin başarısından esinlenerek, seçilen ayırt edici örnekleri temel alan yeni bir yöntem öneriyoruz. Her hareket sınıfından o sınıf için tanımlayıcı olacak şekilde ayrı ayrı seçilen bu ayrıştırıcı örneklere ""prototipler"" demekteyiz. Sonrasında, bu prototipleri tüm veri kümesini betimlemede kullanmaktayız. Geleneksel sınıflandırma yöntemlerini izleyerek ve mevcut olan en güncel alt ve derin düzeydeki öznitelikleri kullanarak, en basit seçim ve tanımlama yöntemleriyle bile, prototip kullanımının hareket tanıma performansını arttırabileceğini gösteriyoruz. Ayrıca, eğitme verilerinin sadece seçilmiş prototiplere azaltılmasıyla, daha az sayıdaki dikkatle seçilmiş örneklerin, daha büyük sayıdaki eğitme verilerinin performansına ulaşabileceğini gösteriyoruz. Prototiplere ek olarak, hareket tanımlamada ilgisiz veri elemesinin etkisini araştırıyoruz ve en güncel çalışmalardan daha iyi veya kıyaslanabilir olan deneysel sonuçları, kriter olarak görülen UCF-101 ve ActivityNet video veri kümeleri üzerinden veriyoruz.","Recognition of actions from videos is a widely studied problem and there have been many solutions introduced over the years. Labeling of the training data that is required for classification has been an important bottleneck for scalability of these methods. On the other hand, utilization of large number of weakly-labeled web data continues to be a challenge due to the noisy content of the videos. In this study, we tackle the problem of eliminating irrelevant videos through pruning the collection and discovering the most representative elements. Motivated by the success of methods that discover the discriminative parts for image classification, we propose a novel video representation method that is based on selected distinctive exemplars. We call these discriminative exemplars as ""prototypes"" which are chosen from each action class separately to be representative for the class of interest. Then, we use these prototypes to describe the entire dataset. Following the traditional supervised classification methods and utilizing the available state-of-the-art low and deep-level features, we show that even with simple selection and representation methods, use of prototypes can increase the recognition performance. Moreover, by reducing the training data to the selected prototypes only, we show that less number of carefully selected examples could achieve the performance of a larger training data. In addition to prototypes, we explore the effect of irrelevant data elimination in action recognition and give the experimental results which are comparable to or better than the state-of-the-art studies on benchmark video datasets UCF-101 and ActivityNet."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet üzerinden mobil mesajlaşma en sık kullanılan mesajlaşma yöntemlerinden biridir. Birçok farklı konuda sıklıkla kullanıldığı için, güvenliği de önemli bir hale gelmiştir. Ancak mobil mesajlaşma için kabul görmüş bir güvenlik standardı yoktur. Her uygulama güvenliğini kendi belirledikleri veya başka kaynaklardan uyarladıkları protokollerle sağlamaktadırlar. Biz de internet üzerinden güvenli mobil mesajlaşma için birtakım güvenlik gereksinimleri belirledik. En çok kullanılan uygulamalardan bazılarını (Cryptocat, Telegram, Threema ve Signal) bu gereksinimlere göre analiz ettik. Ayrıca bu gereksinimleri karşılayan kendi çözümümüzü tasarladık. Kullanılabilirliğine zarar vermemek koşuluyla güvenliğini mümkün olduğunca üst seviyede tuttuk. Çözümümüz, mükemmel iletme gizliliği (PFS) desteğiyle uçtan uca şifreli mesajlaşma, sertifika iğneleme ve kullanıcı girdisiyle geliştirilmiş rassal sayı üretimini desteklemektedir ve güçlü bir anahtar üretme fonksiyonu kullanmaktadır. Güvenlik gereksinimlerini sağlayan tasarımsal kararlarımız sunulmuş ve detaylı olarak açıklanmıştır.","Mobile messaging over the Internet is one of the most actively used communication methods. As it is heavily used for almost all kind of topics, the security of it becomes a major concern. However, there is no widely accepted security protocol standard for it. Each implementation either defines its own security protocol or adopts an existing one. We have defined a set of security requirements for secure messaging applications. Some of the most popular secure messaging applications (Cryptocat, Telegram, Threema and Signal) are analyzed according to these requirements. We have also designed our solution to match the requirements and improved its security as much as possible without harming the usability. Our solution provides E2E encrypted messaging with PFS support, local disk encryption, certificate pinning, improved random number generation with user input and uses a strong KDF. Our design rationales for the requirements are presented and discussed in detail."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yapay öğrenme metodlarının gerçek hayattaki birçok uygulamasında bol miktarda etiketlenmemiş veri bulunmasına karşılık etiketlenmiş veriler pahalı ve/veya sınırlı sayıdadır. Bir etkin öğrenici, etiketleme için yararlı örnekler seçerek mümkün olduğunca az etiketli örnek kullanımı ile yüksek doğrulukta bir model elde etmeyi amaçlamaktadır. Bu tezde havuz tabanlı etkin öğrenme kurgusu için iki yeni metot önerilmektedir: Her adımda bir tane etiketlenmemiş örneği seçerek etiketini sorgulayan (tek-seçimli) ALEVS metodu ve her adımda bir grup etiketlenmemiş örneği seçerek etiketlerini sorgulayan (grup-seçimli) DBALEVS metodu. ALEVS ve DBALEVS metodları örneklerin istatistiksel kaldıraç değerlerini kullanarak en etkili örneği/örnekleri seçer. n x n boyutlu bir K çekirdek matrisinin i-inci satırına ait k-kerte istatistiksel kaldıraç değerleri, kolonları K matrisinin üst-k özdeğer vektörlerinden oluşan U matrisinin i-inci satır düzgesinin karesidir. İstatistiksel kaldıraç değerlerinin etkili satırları seçerek düşük-kerte matris yaklaşıklama algoritmalarında yararlı oldukları gösterilmiştir. ALEVS ve DBALEVS metodları örneklerin önemini ölçmek için havuzdaki örnekler kullanılarak hesaplanmış çekirdek matrisinin istatistiksel kaldıraç değerlerini kullanır. Bunlara ek olarak, DBALEVS her adımda bir altmodüler küme fonksiyonunu maksimize ederek etkili, ama etiketlenmiş örneklere ve birbirlerine benzemeyen örnekleri seçmeye çalışır. Farklı verisetleri üzerinde yapılan deneylerle, ALEVS ve DBALEVS metodlarının karşılaştırılan diğer tek-seçimli ve grup-seçimli metodlara kıyasla data etkili yöntemler olduğu gösterilmiştir.","In many real-world machine learning applications, unlabeled data are abundant whereas the class labels are expensive and/or scarce. An active learner aims to obtain a model with high accuracy with as few labeled instances as possible by effectively selecting useful examples for labeling. We propose two novel active learning approaches for pool-based active learning setting: ALEVS for querying single example at each iteration and DBALEVS for querying a batch of examples. ALEVS and DBALEVS select the most influential instance(s) based on statistical leverages scores of examples. The rank-k statistical leverage score of i-th row of an n x n kernel matrix K is the squared norm of the i-th row of the matrix U whose columns are the top-k eigenvectors of K. Statistical leverage scores are shown to be useful in matrix approximation algorithms in finding influential rows of a matrix. ALEVS and DBALEVS assess the influence of the examples by the statistical leverage scores of kernel matrix computed on the examples of the pool. Additionally, through maximizing a submodular set function at each iteration DBALEVS selects a diverse a set of examples that are highly influential but are dissimilar to selected labeled set. Extensive experiments on diverse datasets show that the proposed methods, ALEVS and DBALEVS offer more effective strategies in comparison to other single and batch mode active learning approaches, respectively."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsanlar arasındaki katmanlara ayrışma daha önceden çalışılmış bir konudur ve bu ayrışma sosyal ağlardaki ilişkileri de etkilemektedir. Online sosyal ağlardaki ilişkilerin insanlar arasındaki bağlantılar olduğundan yola çıkarsak, benzer sosyal katmanlar ve ayrışma sosyal ağlarda da bulunmaktadır. Bu tezde, sosyal ağlardaki gizli hiyerarşiyi bulmaya çalışıyoruz. Bu sorunu çözmek için motivasyonumuz, reklam veya ilan vermek için kullanıcıların seviyelere ayrılması gerekmesidir. Problemimizi kullanıcıları 3 ana metriğimizi düşük seviyede tutacak şekilde seviyelere bölmek şeklinde tanımlayabiliriz. Bu metrikler: ağdaki tersine bağlantılardan dolayı oluşan agony, yüksek seviyedeki kullanıcıları az etkisi olmasından oluşan support disorder ve aynı seviyedeki kullanıcıların farklı miktarda etkileri olmasından kaynaklanan support imbalance. Birkaç sezgisel algoritma kullanarak bu problemi çözmeye çalıştık. Algoritmalarımızın kalitesini ve çalışma sürelerini gerçekte varolan ağlar ve sentetik olarak üretilmiş ağlar üzerinde test ettik.","Stratification among humans is a well studied concept that significantly impacts how social connections are shaped. Given that on-line social networks capture social connections among people, similar structure exist in these networks with respect to the presence of social hierarchies. In this thesis we study the problem of finding hidden hierarchies in social networks, in the form of social levels. The problem is motivated by the need for stratification for social advertising. We formulate the problem into dividing the users of a social network into levels, such that three main metrics are minimized: agony induced by the reverse links in the hierarchy, support disorder resulting from users in higher levels having less impact, and support imbalance resulting from users in the same level having diverse impact. We developed several heuristic algorithms to solve the problem at real-world scales. We present an evaluation that showcases the result quality and running time performance of our algorithms on real-world as well as synthetically generated graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu oyuncu takibi, gerçek zamanlı spor video analizi için çok önemlidir. Ancak, ortam ışığındaki değişkenlik, arka plan karışıklığı, benzer görünümlü oyuncuların düşük çözünürlükte sıkça birbirlerini engellemeleri, hedeflerin hızlı ve doğrusal olmayan hareketleri sporda oyuncu takibini zorlaştırmaktadır. Hedefleri görünüm kapanması ve hızlı hareket altında da takip edebilme yeteneklerinden dolayı parçacık filtresini temel alan yöntemlerden sıkça faydalanılmaktadır. Bu çalışmada, parçacıkları hedefler üzerinden seçen yaygın kullanımdan farklı olarak, parçacıkları bir saha modeli üzerindeki sabit noktalardan yoğun olarak örnekleme kavramı sunulmaktadır. Hedeflerin saha parçacıkları üzerinde olma olasılıkları, birleşik görünüm ve hareket modeli ile hesaplanmaktadır. Parçacıklar hedefler arasında dağıtıldıktan sonra, tüm parçacıklara oyuncu algılama skoru kullanan bir görünüm modeli ile ağırlıklar atanmakta ve bu ağırlıklar kullanılarak hedeflerin yeri güncellenmektedir. Böylece, oyuncular arasındaki etkileşim yöntem içinde kapsanmakta ve oyuncular zorlu koşullar altında takip edilebilmektedir. Ayrıca, sunulan toplu hareket modeli ve pozisyon tabanlı görünüm öğrenimi ile kaybolan oyuncular geri kazanılmakta ve hedefler arasındaki kimlik değişimleri algılanmaktadır. Sunulan yöntem gerçek bir futbolcu takip sisteminin içine gömülmüştür. Bu sistemin tüm adımları anlatılmakta ve yöntem büyük ölçekli görüntü verisi üzerinde değerlendirilmektedir. Deneysel sonuçlar, sunulan yöntemin, standart parçacık filtresi ve tek nesne takibi yöntemlerine göre daha az hedef kaybettiğini ve daha fazla hedef kimliği koruduğunu göstermiştir. Dahası, sunulan yöntem herkese açık bir veri kümesi üzerinde, önceki çalışmalardan daha başarılı sonuçlar almıştır.","Tracking multiple players is crucial to analyzing sports videos in real time. Yet, illumination variations, background clutter, frequent occlusions among players who look similar in low-resolution, and non-linear motion patterns of the targets make sports player tracking difficult. Particle-filtering based approaches have been utilized for their ability in tracking under occlusion and rapid motions. Unlike the common practice of choosing particles on targets, we introduce the notion of shared particles densely sampled at fixed positions on the model field. Likelihoods of being on different particles are calculated for the targets using the proposed combined appearance and motion model. After globally distributing particles among the tracks, particles are weighted using an appearance model with a player detection score, and the track locations are updated by the weighted combination of the particles. This enables encapsulating the interactions among the targets in the state-space model and tracking players through challenging occlusions. We further introduce collective motion model and positional appearance learning to recover lost players and detect identity switches among the tracks. The proposed algorithm is embedded into a real player tracking system. Complete steps of the system are described and the proposed approach is evaluated on large-scale video. Experimental results show that the proposed tracker performs better than standard particle filtering and the state-of-the-art single-object trackers by losing less number of tracks and preserving more identities. Moreover, the proposed approach achieves a higher tracking accuracy with lower error rates on a publicly available soccer tracking dataset when compared to the previous methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Değişmekte olan veri akışlarının büyüklüğü ve dinamik yapısı bu ortamlar için hedeflenen uyabilen sınıflandırıcıların tasarımını zorlaştırmaktadır. Veri akışının tasnifinde çevrim içi bireysel sınıflandırıcıların bir topluluk içinde çoklu bir yaklaşımla kullanılması bilinen yöntemlerden biridir. Çoklu sınıflandırıcının bileşenlerinden bir kısmının, zamanla değişen bir biçimde, diğerlerinden daha iyi olması olasıdır. Bileşen sınıflandırıcılara optimum ağırlık atanması tam olarak incelenmiş bir problem değildir. Bu çalışmada, en son veri örneklerini içeren kayan bir pencere kullanarak bileşen sınıflandırıcılara geometrik açıdan optimum ağırlık atayan çevrim içi bir çoklu sınıflandırıcı (GOOWE) yaklaşımı önerilmektedir. Bu amaçla, bileşen sınıflandırıcıların verdikleri oylar ve gerçek sınıf etiketleri uzaydaki noktalarla eşleştirilmektedir. Önerdiğimiz yeni yöntem, en küçük kareler (EKK) çözüm yaklaşımında, bileşenlerin oy puanları ve ideal noktalar arasındaki öklid mesafesini kullanarak, bileşenlere optimum ağırlık atamaktadır. EKK yaklaşımı yığınlar için tasarlanmış olan çoklu sınıflandırıcılar için daha önceden kullanılmıştır. Çalışmada, bu yaklaşım ilk kez çevrim içi çoklu sınıflandırıcılar için uzaysal bir model yaklaşımıyla kullanılmaktadır. Algoritmanın sağlamlığını göstermek için MOA kütüphanelerinin yanı sıra gerçek ve sentetik veri derlemlerini de kullanan, literatürde önde gelen 8 çoklu sınıflandırıcının sonuçlarını içeren, kapsamlı bir karşılaştırma sunulmaktadır. Deneyler, farklı kavram değişimi gözlenen bilgi akışı ortamlarında, GOOWE ile elde edilen başarının istatistiksel anlamda daha iyi olduğunu göstermektedir.","Designing adaptive classifiers for an evolving data stream is a challenging task due to its size and dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is one of the well-known solutions. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel dynamic and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the MOA libraries. We compare our results with 8 state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Information visualization is concerned with effective visual presentation of abstract information, which reinforces human cognition. Graphs are structures that are well suited to represent relational information. Graph visualization is vital since the underlying relational information of the graph provides fine analysis and comprehension opportunities. Biological pathway visualization is one of the most popular areas, where graph visualization is highly favored. Interactive analysis and visualization of cancer related pathways in the context of genomic data, such as those available through the TCGA project, might reveal valuable information for scientists about disease conditions and potential causes. As the size and complexity of such cancer pathways and associated genomic data increase, exchangeable in-silico representations and their effective, enhanced visualizations, and complexity management become crucial for effective analysis of such data to potentially discover cause-effect relations. In this thesis, we designed and implemented software solutions to visualize cancer genomics data in the context of networks from simple gene interaction networks to process description diagrams within the cBioPortal for Cancer Genomics (cBioPortal). cBioPortal is a popular web portal, getting about 60.000 visits globally per month, providing visualization, analysis and download of large-scale cancer genomics data sets. The network view in cBioPortal presents neighborhood of genes of interest. The alteration data is overlaid on the network with numerous ways to filter and manage complexity of the network (e.g. by alteration percentage or by type or source of the interactions). Upon demand, the user can obtain a more detailed, mechanistic view of the interactions among gene pairs, from Pathway Commons database with a live query using the SBGN process description notation. Finally, we also developed a new pathway visualization component, specifically for cancer pathways, using a uniform notation found in TCGA cancer publications. This tool also facilitates curation of pathways from scratch with support for collaborative editing.","Veri Görselleme soyut olarak temsil edilmiş bilginin, insan algısını destekleyecek, etkili bir biçimde görsel olarak temsil edilmesini hedefler. Çizgeler ilişkisel bilginin temsil edilmesi için tercih edilen uygun yapılardır. Bir çizgenin görsellenmesi çizgenin yapısındaki ilişkisel bilginin daha iyi anlaşılması ve çözümlenmesi bakımından fayda sağladığı için önemlidir. Biyolojik yolak görselleme¸ Çizge görsellenmesinin yaygın olarak kullanıldığı bir alandır. Kanser ile ilişkili yolakların ilgili kanser genomik bilgisi dahilinde interaktif bir biçimde çözümleme ve görsellenmesi, örn: TCGA projesi dahilinde erişilebilen yolaklar, araştırmacılar için hastalıkların olası nedenlerini anlamak, neden sonuç ilişkilerini gözlemlemek adına değerli bilgiler sunar. Bu yolakların boyutu ve karmaşıklığı ve de ilgili genomik datanın boyutu arttıkça, daha etkili karmaşıklık yönetim teknikleri, etkili ve gelişmiş çizge görsellemeleri, bilgisayar ortamında kolay olarak paylaşıma olanak sağlayan bilgi temsilleri araştırmacıların neden sonuç ilişkilerini daha iyi çözümlemesine imkan sağlar. Bu tez çalışması kapsamında genomik datanın yolaklar dahilinde görsellenmesini sağlayan yazılım bileşenleri tasarlanmış ve gerçekleştirilmiştir. Bu yazılım bileşenleri özel olarak cBioPortal for Cancer Genomics (cBioPortal) isimli, kanser araştırmalarında yaygın olarak kullanılan web tabanlı bir yazılım dahilinde genlerin birbirleriyle ilişkilerini gösteren düzenleyici yolakları ve moleküler seviyede prosesleri görselleyebilmek için hayata geçirilmiştir. cBioPortal dünya çapında aylık olarak yaklaşık 60.000 kişi tarafından ziyaret edilen, büyük çapta kanser genomik bilgisinin görsellenmesi¸ çözümlenmesi ve indirilmesi hizmetleri veren web tabanlı bir yazılımdır. cBioPortal dahilinde geliştirilen çizge görselleme bileşeni, verilen bir gen listesinin , bir kanser tipi dahilinde etkileştiği diğer genleri içeren komşuluk yolaklarını kanser genomik bilgisiyle beraber etkili bir biçimde görselleyebilmektedir. Buna ek olarak çeşitli filtreleme ve karmaşıklık yönetimi olanakları da ( örn: genomik alterasyona göre çizge köşelerini filtreleme, gen etkileşimlerini alındığı veri tabanına göre filtreleme) bu yazılım bileşeni dahilinde hayata geçirilmiştir. Istenildiği takdirde gen etkileşimleri daha detaylı bir şekilde, moleküler seviyede prosesler dahilinde Systems Biology Graphical Notation (SBGN) diye ad- landırılan bir notasyonla görsellenebilmektedir. Bu detaylı proses seviyesinde olan yolak bilgisi Pathway Commons isimli yolak veritabanından bir sorguyla, SBGN proses dili diye adlandırılan bir dilde alınmaktadır. Son olarak, özellikle TCGA yayınlarında sıkça rastlanan ve yaygın olarak kullanılan bir notasyonda yolakların görsellenmesi ve oluşturulması amacıyla yeni bir yolak görselleme aracı tasarlanmış ve geliştirilmiştir. Bu yazılım aracı standard bir çizge görselleme ve oluşturma aracından farklı olarak, ortak olarak bir TCGA yolağının araştırmacılarca oluşturulmasına olanak sağlamaktadır."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"C = AB şeklindeki genel seyrek matris-matris çarpımı (SyGEMM), moleküler dinamik benzetimi, cizge işlemleri, doğrusal programlama gibi pek çok uygulamada çekirdek işlem olarak kullanılmaktadır. SyGEMM işlemi için farklı paralelleştirme yöntemleri bulunmaktadır. Bu yöntemler için paralel SyGEMM algoritmaları önermekteyiz. Önerilen algoritmalar iki evreden oluşmaktadır. Evrelerden birisi yerel çarpma işlemleri içermekte olup, çarpma evresi olarak isimlendirilmektedir. Diğer evre ise, çarpma evresi için gerekli matris elemanlarının taşınması veya çarpma evresinde üretilen kısmi sonuçların aktarılarak toplanmasından oluşmakta olup, iletişim evresi olarak isimlendirilmektedir. Bu paralel algoritmalar için, girdi ve çıktı matrislerini aynı anda veri yinelemesiz olarak bölümleyebilen üç tane hiperçizge modeli önermekteyiz. Bu üç model, girdi A ve B matrislerini tek boyutlu (1D) olarak bölümlemekle beraber, ilk model çıktı C matrisini sıfır-dışı tabanlı olarak iki boyutlu (2D) ve geri kalan modeller ise çıktı C matrisini 1D olarak bölümlemektedir. Bu modellerde, köşe ağırlıkları üzerinde tanımlı olan bölümleme kısıtı, işlemcilerin işlemsel yüklerini dengelemeye karşılık gelmektedir. Keside kalan hiperkenarlar üzerinde tanımlanan kesi boyutunun azaltılması olan bölümleme amacı ise, iletişim evresinde yapılan toplam iletişim hacmini azaltmaya karşılık gelmektedir. Ayrıca, toplam mesaj sayısını azaltmakla beraber her bir işlemcinin yönettigi iletişimin hacmini dengelemeyi hedefleyen hiperçizge modelleri de önermekteyiz. Önerilen hiperçizge modellerinin geçerliliğini deneysel olarak da doğrulamak amacıyla, MPI (Message Passing Interface) tabanlı SyGEMM paket programı geliştirilmiştir. Çok çeşitli seyrek matrisler üzerinde bu program kullanılarak JUQUEEN isimli bir IBM Blue-Gene/Q sisteminde büyük ölçekli deneyler gerçekleştirilmiştir. Yapılan deneylerin sonucunda, önerilen hiperçizge modellerinin hesaplamarı önemli miktarda hızlandırdığı gözlemlenmiştir.","Multiplication of two sparse matrices (i.e., sparse matrix-matrix multiplication, which is abbreviated as SpGEMM) is a widely used kernel in many applications such as molecular dynamics simulations, graph operations, and linear programming. We identify parallel formulations of SpGEMM operation in the form of C = AB for distributed-memory architectures. Using these formulations, we propose parallel SpGEMM algorithms that have the multiplication and communication phases: The multiplication phase consists of local SpGEMM computations without any communication and the communication phase consists of transferring required input/output matrices. For these algorithms, three hypergraph models are proposed. These models are used to partition input and output matrices simultaneously. The input matrices A and B are partitioned in one dimension in all of these hypergraph models. The output matrix C is partitioned in two dimensions, which is nonzero-based in the first hypergraph model, and it is partitioned in one dimension in the second and third models. In partitioning of these hypergraph models, the constraint on vertex weights corresponds to computational load balancing among processors for the multiplication phase of the proposed SpGEMM algorithms, and the objective, which is minimizing cutsize defined in terms of costs of the cut hyperedges, corresponds to minimizing the communication volume due to transferring required matrix entries in the communication phase of the SpGEMM algorithms. We also propose models for reducing the total number of messages while maintaining balance on communication volumes handled by processors during the communication phase of the SpGEMM algorithms. An SpGEMM library for distributed memory architectures is developed in order to verify the empirical validity of our models. The library uses MPI (Message Passing Interface) for performing communication in the parallel setting. The developed SpGEMM library is run on SpGEMM instances from various realistic applications and the experiments are carried out on a large parallel IBM BlueGene/Q system, named JUQUEEN. In the experimentation of the proposed hypergraph models, high speedup values are observed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağ, bir ortamı algılayabilen ve toplanan veriyi alıcı düğüme ileten çok sayıda küçük algılayıcı düğümden oluşur. Çoğu senaryoda algılayıcı düğümler değistirilemez pillerle çalıstırılırlar ve bu durum alıcı düğüme komşu algılayıcı düğümlerin aşırı yüklenmesine bağlı olarak ağ yaşam süresini önemli ölçüde sınırlar. Alıcı düğümü hareket ettirme ve bu yolla algılayıcı düğümler arasındaki iletme yükünü eşit olarak dağıtmak, algılayıcı ağların yaşam süresini iyileştirmek için önemli tekniklerden biridir. Alıcı düğümleri, önceden tanımlanmış bir dizi alıcı yerine doğru etkili biçimde hareket ettirmek üzere tek ve çoklu alıcı hareketlilik problemleri için farklı hareketlilik algoritmaları öneriyoruz. Öncelikle, PLMA ve ELMA adı verilen düğüm yükü parametrelerinin bir tabloda birleştirildiği ve bu tablonun her bir tur için hangi alıcı yerine gidileceğini belirlemek için kullanıldığı, paket ve enerji yükü bazlı alıcı hareketlilik algoritmaları öneriyoruz. Ayrıca en iyi sonuçları elde etmek ve karşılaştırmalı değerlendirme yapmak amacıyla bir tamsayı doğrusal programlama modeli de veriyoruz. Yönlendirme topolojisi alıcı hareketlilik planlarının önemli bir bileşeni olduğundan, ağ yaşam süresini daha da artıran merkezi ve dağıtık yönlendirme topoloji oluşturma algoritmaları da öneriyoruz. İlaveten, ağ topolojisini öğrenmek için başlangıçta öğrenme aşamasına gerek duymayan A-ELMA adlı enerji-yük bazlı alıcı hareketlilik algoritması öneriyoruz. Bu algoritma bir yeri her ziyaret ettiğinde enerji yük tablosunu aşamalı olarak oluşturup güncellemektedir. Son olarak, tek alıcı hareketlilik problemi için önerilen algoritmalar dışında çoklu alıcı hareketlilik problemi için de iki farklı algoritma öneriyoruz. MSMA adlı çoklu alıcı hareket algoritması ağ yasam süresine önemli ölçüde zarar vermeyecek şekilde alıcı hareketlerinin zamanını programlarken hesaplama ve iletişim ek yükünü azaltmak için alıcı yer kombinasyonlarını etkili biçimde sınırlayan merkezi bir algoritmadır. PMA algoritması ise topoloji bilgisinin toplanmasını gerektirmeyen tamamen dağıtık bir algoritmadır. Bu algoritma, yerleri, kalan enerji değeri ve uzaklık metriklerine göre seçmektedir. Algoritmalarımızı değerlendirmek ve literatürdeki bazı temel yaklaşımlarla karşılaştırmak için kapsamlı benzetim deneyleri gerçekleştirdik. Benzetim sonuçları algoritmalarımızın ağ yaşam süresi, gecikme ve katedilen mesafe açısından bazı diğer seçeneklerden daha iyi sonuç verdiğini göstermektedir. Ayrıca algoritmalarımızın bu metriklerin her biri için hangi koşullarda daha iyi sonuç verdiğini belirliyoruz. Algoritmalarımızın kablosuz algılayıcı ağlardaki tek ve çoklu alıcı hareketlilik problemi için kullanımı kolay, etkin ve verimli çözümler sağladığını gözlemledik.","A wireless sensor network consists of a large number of tiny sensor nodes which are capable of sensing an environment and sending the collected data to a sink node. For most scenarios, sensor nodes are powered with irreplaceable batteries and this dramatically limits the lifetime of the network, especially due to overloading of the sensor nodes neighboring sink node. Such nodes need to forward more traffic than other nodes in the network. Moving sink node and in this way distributing forwarding-load evenly among sensor nodes is one of the important techniques for improving lifetime of sensor networks. We propose different mobility algorithms for single-sink and multiple-sink mobility problem to efficiently move sink nodes through a predefined set of sink sites. We first provide packet-load and energy-load based sink mobility algorithms, called PLMA and ELMA, in which node-load parameters are incorporated into a table and this table is used to determine which sink site to visit in each round. We also give an integer programming model to get optimal results and do benchmarking. Since routing topology is an important component of sink mobility schemes, we also propose centralized and distributed routing topology construction algorithms to further increase network lifetime. Additionally, we propose an adaptive energy-load based sink movement algorithm, called A-ELMA, which does not require an initial training phase to learn about network topology. It incrementally constructs and updates energy-load table each time it visits a site location. Finally, besides proposing algorithms for single-sink mobility problem, we also propose two different algorithms for multiple-sink mobility problem. Our Multiple Sink Movement Algorithm (MSMA) is a centralized algorithm and effectively limits the sink site combinations to reduce computation and communication overhead in scheduling sink movements without harming network lifetime significantly. Our Prevent and Move Away (PMA) algorithm is a fully distributed algorithm and does not require topology information to be collected. It selects sites based on remaining energy values and distance metrics. We evaluated our algorithms and compared them to some basic approaches in the literature by conducting extensive simulation experiments. Our simulation results show that our algorithms can perform better than some other alternatives in terms of network lifetime, latency and travel distance. We also identify under which conditions our algorithms perform better for each of these metrics. We observed that our algorithms provide simple-to-use, efficient, and effective solutions for single- and multiple-sink mobility problems in wireless sensor networks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda sosyal ağ kullanımının hızlı bir şekilde yaygınlaşmasına tanıklık etmekteyiz. Bu yaygınlaşmanın neticesindeyse sosyal ağlar tarafından oluşturulan veri büyüklüğü devasa boyutlara geldi ve mevcut bu verinin işlenip anlamlandırılması gerek akademi gerekse sanayii için mühim bir konu haline dönüştü. Bu verinin büyük bir kısmıysa çizgeler halinde saklanmaktadır. Bu nedenledir ki son birkaç yılda büyük ölçekli çizgeleri işleyebilmek amacıyla pek çok sistem geliştirilmiştir. Bu sistemlerin öncelikli hedefleri ise mevcut çizge algoritmalarını büyük ölçekli çizgelerde etkin bir şekilde uygulanmasını saglamaktır. Fakat ilişkisel verilerin aksine çizgeler yarı yapısal temeldedir. Bu nedenle ikincil depolama alanları üzerinden çizgelere ulaşmak ve işlemek çizge içerisindeki bezerlikleri göz önüne alan farklı çözümlere ihtiyaç duymaktadır. Bu yüzden bu çalışmada disk üzerinde rastgele gerçekleştirilen okuma yazmaları indirgemek amacıyla çizgelerin disk üzerindeki yerleşimlerini ölçeklenebilir bir şekilde gerçekleyen bir metot önermekteyiz. Bu amaçla, ICBP adını verdiğimiz, çizgeleri dağıtık ve ölçeklenebilir bir şekilde öbeklere bölebilen bir metodu Hadoop yapısını baz alarak hayata geçirdik. önerdiğimiz bu metot öbek oluşturmanın yanında oluşturulan bu öbeklerin disk üzerinde yerleşimini de sağlamaktadır. Bu çalışmada bu metodun detaylı açıklamasının beraberinde metodun etkinliğini, kalitesini ve ölçeklenebilirliğini deneysel olarak sunacağız.","We are witnessing an enormous growth in social networks as well as in the volume of data generated by them. As a consequence, processing this massive amount of data has become a major problem. An important portion of this data is in the form of graphs. In recent years, several graph processing and management systems emerged to handle large-scale graphs. The primary goal of these systems is to run graph algorithms in an efficient and scalable manner. Unlike relational data, graphs are semi-structured in nature. Thus, storing and accessing graph data using secondary storage requires new solutions that can provide locality of access for graph processing workloads. In this work, we propose a novel scalable disk layout technique for graphs, which aims at reducing the I/O cost of disk- based graph processing algorithms. To achieve this goal, we designed a scalable Map/Reduce-style method called ICBP, which can divide the graph into a series of disk blocks that contain sub-graphs with high locality. Furthermore, ICBP can order the resulting blocks on the disk to further reduce non-local accesses. We experimentally evaluated ICBP to showcase its scalability, layout quality, as well as the effectiveness of automatic parameter tuning for ICBP. We also deployed the graph layouts generated by ICBP to the Neo4j graph database management system. Our experimental results show that the default layout results in 1.5 to 2.5 times higher running times compared to ICBP."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mobil cihazların internet eri ̧siminin yıldan yıla yaygınla ̧smasıyla, kullanıcılar hareket halindeyden mobil interneti ve arama servislerini kullanabilir oldular. Bu cihazların sundu ̆gu konum bilgisi sayesinde mobil kullanıcılar yerel arama uygulamaları ile etraflarındaki mekanları ve etkinlikleri ke ̧sfedebilme imkanı bul- dular. Mobil yerel arama her ne kadar bir arama aktivitesi olsa da, genel web aramasından belli farklılıklar i ̧cermektedir. Genel web araması web say- falarıyla ilgilenirken, mobil yerel arama ise yerel i ̧sletmelerle ve ilgi alanlarıyla ilgilenir. Ayrıca yerel aramalar zaman, hava durumu, konum gibi kullanıcının durumunu etkileyen fakto ̈rlerden etkilenirler. O ̈nceki ̧calı ̧smalar yerel arama sonu ̧c sıralamalarındaki sinyalleri ve kullanıcı durumunu etkileyen fakto ̈rleri ku ̈ ̧cu ̈k bir ̈ozellik ku ̈mesinde incelemi ̧stir. Biz ise konum tabanlı sosyal ag ̆lardaki veriyi kullanarak yerel aramaları geni ̧slettik. Gezinio ismini verdig ̆imiz bir mobil yerel arama uygulaması geli ̧stirerek yerel arama sorgularını i ̧ceren bir veri ku ̈mesi topladık. Kullanıcılar Gezinio'yu kullanarak yerel aramalar yapıp etraflarında bulunan mekanlar hakkında sosyal nitelikli bilgilere eri ̧stiler. Daha sonra topladıg ̆ımız veriyi inceleyerek, sosyal o ̈zelliklerin kullanıcıların arama sonucu deg ̆erlendirmelerini etkiledi ̆gini ve bu o ̈zellikleri kullanan sonuc ̧ sıralama modellerinin yerel arama sonuc ̧larını iyile ̧stirdi ̆gini go ̈rdu ̈k. Buna ba ̆glı olarak, farklı kategorilerdeki sonu ̧c sıralamalarının, farklı sosyal o ̈zelliklerden faydala- nabileceg ̆ini go ̈sterdik.","As availability of internet access on mobile devices develops year after year, users have been able to make use of mobile internet and search services while on the go. Location information on these devices has enabled mobile users to utilize local search applications for discovering places and activities around them. Although mobile local search is a kind of search activity, it is inherently different than general web search. Mobile local search focuses on local businesses and points of interest, instead of web pages as in general web search. Moreover, users' context has a significant effect on their decision process. In previous studies, ranking signals and user context have been investigated on a small set of features. We extend ranking signals and user context in mobile local search with using data of location-based social networks. We developed a mobile local search application, Gezinio, and collected a data set of local search queries. Gezinio helps users to issue local queries and see various kinds of social information about local businesses around them. We built ranking models and investigated how social features affect decision process of users. We show that social features influence users' click decisions and they can be utilized by ranking models to improve the local search experience. Additionally, we propose different social features for different query categories."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet üzerinden arama yapmak, İnternet kullanıcılarının tercih ettiği en popüler aktivitelerden birisidir. Arama motorlarının oldukça fazla kullanılması sayesinde, kullanıcıların yapmış oldukları aramaların yer aldığı bilgilerin kullanılabilmesi oldukça kolaylaşmıştır. Araştırmacılar bu bilgileri kullanarak kullanıcıların sahip oldukları arama davranışları ile ilgili faydalı sonuçlar elde etmişlerdir. Çalışmamızda, ticari olarak kullanılan eğitim amaçlı arama motoruna ait sorgu bilgilerini detaylı bir şekilde analiz ettik. Analiz sonrası aldığımız sonuçları, İnternet kullanıcılarının sahip olduğu arama davranışlarıyla karşılaştırarak aralarındaki farkları belirledik. Bu farkları da göz önünde bulundurarak eğitim alanına daha iyi uyum sağlayacak, sıralama amaçlı öğrenme algoritmalarını kullanarak bir model ortaya çıkardık. Sahip olduğumuz sorgu kümesi öğrencilere ait olduğu için, modeli oluştururken eğitim alanına has özgün özellikler kullandık. Modelin performansını karşılaştırmak adına, sahip olduğumuz sorgu kütüklerindeki orjinal sıralamayı ve bu alanda sıklıkla kullanılan modern yöntemlerle oluşturulan modeli referans olarak aldık. Sonuçlarımıza göre her iki referans modeline kıyasla sırasıyla %14 ve %11'lik bir gelişme sağladık. Bunun yanında, yapılan sorgunun türlerine göre ayrılmak kaydı ile, her bir sorgu grubu için farklı öğrenme modelleri tanımladık. Elde ettiğimiz verilere göre, sorgunun ders bilgisinin ve sorguyu soran öğrencinin sınıf bilgisinin farklı model oluştururken oldukça fayda sağladığını öğrendik. Sorgu kütük kümesinde yer alan bazı sorgular (yalnızca bir defaya mahsus sorulan sorgular) için elimizde yeteri kadar bilgi olmadığından, modelin genel performansını artırmak adına bu sorgular için özgün bir algoritma geliştirdik. Yaptığımız deneylere göre, bu özel sorgular için geliştirdiğimiz algoritma, oluşturduğumuz genel modelin performansını da artırmaktadır.","Web search is one of the most popular internet activities among users. Due to high usage of search engines, there are huge data available about history of user search issues. Using query logs as a source of implicit feedback, researchers can learn useful patterns about general search behaviors. We employ a detailed query log analysis provided by a commercial educational vertical search engine. We compare the results of our query log analysis with the general web search characteristics. Due to difference in terms of search behavior between web users and students, we propose an educational ranking model using learning to rank algorithms to better reflect the search habits of the students in the educational domain to further enhance the search engine performance. We introduce novel features best suited to the educational domain. We show that our model including educational features outperforms two baseline models which are the original ranking of the commercial educational vertical search engine and the model constructed using the state of the art ranking functions, up to 14\% and 11\%, respectively. We also employ different learning to rank models for different clusters of queries and the results indicate that having models for each cluster of queries further enhances the performance of our proposed model. Specifically, the course of the query and the grade of the user issuing the query are good sources of feedback to have a better model in the educational domain. We propose a novel \textit{Propagation Algorithm} to be used for queries having lower frequencies where information derived from query logs is not enough to exploit. We report that our model constructed using the features generated by our proposed algorithm performs better for singleton queries compared to both the educational learning to rank model we introduce and models learned with common features introduced in the literature."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bir bulut yazılımının ölçeklenebilir olması için, herhangi bir müşterisinden herhangi bir zamanda gelen isteklere vereceği cevabı servis sözleşmesinde belirtilen kriterler içerisinde yapabilmesi gerekir. İzole olmamış bir servis yazılımında farklı müşteriler yazılım aracılığıyla sunucuya istek yollayarak, sunucunun donanımsal gücünü meşgul edebilirler. Donanımsal gücün aşırı istek gönderen kullanıcılar tarafından meşgul edilmesi sistem performansının düşmesine sebep olur ve düşen bu performanstan diğer kullanıcılar da olumsuz olarak etkilenirler. Müşterilerden gelebilecek yoğunluklara karşı sistem performansını iyi durumda tutabilmek için genellikle bulut yazılımlarında esneklik kavramı kullanılır. Ancak esneklik kavramı sistemin daha fazla donanımla birleşmesini sağlayarak performansın arttırılması veya daha az donanımın kullanılarak performansın azaltılması olarak kullanılabilir. Gereken donanım miktarını müşteriyle yapılan servis anlaşması belirleyebilir, ancak bu durum yine de performansın yalıtımını sağlayan bir çözüm değildir. Performans yalıtımı yapılmış olan sistemlerde amaç, müşterilerle yapılan servis anlaşmalarındaki minimum gereksinimlerin müşteriye mevcut donanımlarla sağlanabilmesidir. Bir sisteme performans yalıtımı uygulayabilmek için öncelikle bu sistemdeki kullanıcıların adil kullanım ilkelerine uymasını sağlamak gerekir. Bu ilkelere bağlı olarak farklı performans yalıtımı uygulayan yöntemler geliştirilmiştir. Bu yöntemler sanal bekletme, sırası gelene cevaplama, kara listeye alma ve işlem havuzu yöntemleridir. Ancak bu yöntemler hizmet yazılımlarında bulunan müşterilerin zayıf donanıma sahip olduğu varsayımıyla geliştirilmişlerdir. Bu tez Tork Yazılım Geliştirme Platformu'nu tanıtmaktadır. Bu platform ile ölçeklenebilir ve performans yalıtımı sağlayan hizmet yazılımları geliştirebilmek mümkün olmaktadır. Bu platform yeni bir yöntem ile en başından bir bulut tabanlı yazılım geliştirebilmeyi veya mevcutta olan bir yazılımı bulut tabanlı ölçeklenebilir bir yazılma dönüştürebilmeyi sağlar. Platform aynı zamanda veri tabanı, uygulama havuzu ve istemci tarafları için yazılım geliştirme kütüphanesi sağlamaktadır. Bu kütüphane sayesinde düzenlenen yazılımın güvenlik, işlem geçmişi tutma, veri tabanları için çoklu-kiracı altyapısı kullanabilme gibi özellikleri otomatik olarak sağlanabilmektedir. Bu özellikler nesne tabanlı bir mimaride geliştirilmiş ve istemci tarafının da farklı platformlarda da çalışabilecek bir yapıda çalışabilmesi sağlanmıştır. Aynı zamanda platformda bulunan bütünleşmiş yazılım geliştirme ortamı sayesinde yazılımın ayarlamaları ve yazılım mantığında değişecek yerler için kod yazabilmek mümkündür. Dolayısıyla yazılımı geliştirmek için sağlanan çevrimiçi, grafiksel kullanıcı ara yüzü, geliştiricileri başka bir yazılım geliştirme aracına ihtiyaç duymadan yazılımları geliştirebilmelerini sağlar. Bu platform yazılım tasarımını ve performans yalıtımı gerçekleştirebilen; aynı zamanda güçlü donanımsal istemcilerle çalışırken gereken işgücünü bu donanımlara yükleyebilmeye olanak sağlayan bir araçtır. Bu platform anlatmak ve kullanılan yöntemleri de onaylamak amacıyla örnek bir bulut yazılımının test edilmesi olayı ele alınmıştır. Bu yazılımın kullandığı farklı yalıtım teknikleri bu örnek olay üzerinde zayıf ve güçlü istemci senaryolarıyla test edilip değerlendirilmiştir.","Scalability of a cloud application is the ability of an application that handles the service level agreement (SLA) requirements for all customers. In a non-isolated SaaS system, the different clients of a SaaS can freely use the resources of the SaaS. Hereby, disruptive tenants who exceed their limits can easily cause degradation of performance of the provided services for other tenants. To ensure performance demands of the multiple tenants usually elasticity of the SaaS is needed, which supports changing the hardware resources. In case the users' demand change, the system can scale itself up or down according to the minimum necessary computational power to handle all the requests within the minimum requirements of SLA. The main goal of elasticity, however, is not directly to support performance isolation but scalability of the SaaS in general. As such, relying only on elasticity does not explicitly solve the problem of performance isolation. Performance isolated systems aim to ensure the minimum requirements of the SLA to all of the customers. Ensuring the fair behavior with respect to customers is a need for building a performance isolated system. Various performance isolation approaches have been introduced including artificial delay, round robin, blacklist, and thread pool. However, these approaches assume a SaaS system with thin clients whereby all the tasks are handled by the SaaS. This thesis introduces the ""Tork Framework"", for developing scalable and performance isolated enterprise SaaS applications. Tork Framework provides a new way for developing a cloud application from scratch or converts existing software applications to scalable cloud applications. Tork Framework introduces built-in features applicable for the application layer, database layer and client layer. These features include techniques for security controls, logging, and multi-tenancy on relational database management systems (RDMS). Built-in server components of Tork Framework provide service oriented architecture for all business objects in the RDMS, and client components make the application work on different platforms. Also, this framework provides a cloud based IDE for configuring parameters and writing custom codes without the need of any other individual tool via online graphical user interface (GUI). The framework as such provides an approach for supporting the design and realization of performance isolated cloud system by also considering the usage of resources of thick clients. To illustrate the framework and validate our approach we have adopted a real SaaS environment in which a case study is implemented using the Tork framework. Different Implemented performance isolation techniques are tested in this case study using both thin and thick client types."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Akan veri işleme, canlı veriyi havada işleme üzerine olan bir programlama paradigmasıdır. Bu paradigma içinde barındırdığı ardışık düzen, veri ve görev paralelleştirme methodlarını kullanarak, uygulamaların birim zamanda üretilen iş miktarını arttırmasına yada birim iş parçası başına duşen ortalama işlem süresinin azalmasına olanak saglar. Bu tez çalışmasında, elastik akan veri işleme motoru olan C-Stream dizayn ve uygulamaları geliştirilmiştir. İlk olarak C-Stream, literatürdeki çalışmaların çoğunluğunun benimsediği olaya-dayalı işleç geliştirme methodunun aksine eş-program tabanlı işleç geliştirme modelini sunmaktadır. Bu modelde her işleç, kendi kontrol döngüsüne sahip olmakta, veri erişilebilirlik uygulama programi arabirimi (UPA) ile veri işleme zamanını kontrol edebilmektedir. Bu model çok-portlu işleç geliştirme sürecini basitleştirmektedir. Ikinci olarak, C-Stream işleçlerin çalışmasını kontrol eden, çok izlekli dinamik zamanlayıcı barındırmaktadır. Bu zamanlayıcı, eklentiler ile de özelleştirilebilmektedir. Eklentilerden bagımsız olarak, zamanlayıcı geri-baski problemini çozer, işleçlerin veri erişilebilirlik UPA'sına ulaşımını saglar, işleçlerin çalısma esnasında durdurulması ve sonlanmasını kontrol eder. Son olarak, C-Stream elastik paralelleştirme ozelliğine sahiptir. Dinamik olarak aktif çalısan izlek sayısını kontrol etmekle beraber, uygulamada tıkanmaya sebep olan işleçleri tespit ederek, onların kopya sayısını arttırır ve tıkanmayı ortadan kaldırır. Yaptığımız deneyler gostermektedir ki, C-Stream ölçeklenebilir, özelleştirilebilir ve esnek paralelleştirme ozelliğine sahip bir akan veri işleme uygulaması geliştirme motorudur.","Stream processing is a computational paradigm for on-the-fly processing of live data. This paradigm lends itself to implementations that can provide high throughput and low latency, by taking advantage of various forms of parallelism that is naturally captured by the stream processing model of computation, such as pipeline, task, and data parallelism. In this thesis, we describe the design and implementation of C-Stream, which is an elastic stream processing engine. C-Stream encompasses three unique properties. First, in contrast to the widely adopted event-based interface for developing stream processing operators, C-Stream provides an interface wherein each operator has its own control loop and rely on data availability APIs to decide when to perform its computations. The self-control based model signifcantly simplifes development of operators that require multi-port synchronization. Second, C-Stream contains a multi-threaded dynamic scheduler that manages the execution of the operators. The scheduler, which is customizable via plug-ins, enables the execution of the operators as co-routines, using any number of threads. The base scheduler implements back-pressure, provides data availability APIs, and manages preemption and termination handling. Last, C-Stream provides elastic parallelization. It can dynamically adjust the number of threads used to execute an application, and can also adjust the number of replicas of data-parallel operators to resolve bottlenecks. We provide an experimental evaluation of C-Stream. The results show that C-Stream is scalable, highly customizable, and can resolve bottlenecks by dynamically adjusting the level of data parallelism used."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çok çekirdekli işlemciler'de (CMP) yüksek performans sağlamak için etkili önbellek tutarlılık protokolleri ve yanı sıra hızlı sanal-fiziksel adres çeviri mekanizmaları gerekir. Dizin (Directory) temelli önbellek tutarlılık protokolleri çok çekirdekli işlemcilerede, veri bloklarının son seviye özel önbelleklerde tutarlı bir Şekilde bulunmasını sağlamak amacıyla yaygın bir şekilde kullanılan bir yaklaşımdır. Ancak, dizin yapılarının büyük fiziksel alan işgal etmeleri ve önbellek ilişkilendirmesinin yüksek olması sebebiyle, çekirdek sayısı çoğaldıkça ölçeklenebilirlik derecesi düşebilir. Daha önceki çalışmalarda gösterildiği gibi, veri bloklarının önemli bir yüzdesi sadece tek bir çekirdek tarafından erişilir. Bu nedenle, dizin yapısında bu veri bloklarınını takip etmek gerekli değildir. Bu tez, iki büyük katkıyı sunmaktadır: ilk olarak, daha önceki çalışmalarda önerilmiş olan sayfa düzeyini göze alarak sınıflandırmaya göre, alt sayfa düzeyinde veri bloklarını sınıflandırma önemli bir ölçüde daha çok özel veri bloklarının tespit edilmesine yardımcı olabileceğini gösterdik. Sonuç olarak, bu yaklaşım, benzer sayfa düzeyinde sınıflandırma yaklaşımlarına göre, dizinde takip edilmesi gereken blokların yüzdesini önemli bir ölçüde düşürür. Bu da, olabildiğince çok çekirdekli işlemcilerde performansa zarar vermeden daha küçük ve daha az ilişkilerdirmeli olan önbellek dizinlerinin kullanılmasını mümkün kılıp, böylece dizin yapısının çekirdek sayısının çoğalmasıyla beraber ölçeklenmesine yardım eder. Ancak alt sayfa düzeyinde bellek bloğu sınıflandırma, işletim sisteminin sayfa tablosu girdilerinde saklanan alt sayfalara ait bakım bitlerinin güncelleme yükselmesine neden olabilir. Bu yüzden alt sayfa düzeyinde veri sınıflandırma performans avantajlarının bir kısmı boşa çıkarılabilir. Bunun üstesinden gelmek için, ikinci bir katkı olarak, bu tezde dağıtımlı yonga üstü sayfa tablosu kavramı önerilmektedir. Önerilen yonga üstü sayfa tablosu sistemde en son erişilmiş olan sayfaları saklamaktadır. Simülasyon sonuçlarımıza göre, önerdiğimiz yöntem ortalama olarak dizin belleklerin tahliye sayısı oranını 58% azaltmaktadır. Ayrıca, yonga üstü sayfa tablosu işletim sistemi sayfa tablosu erişimini 84% azaltıp ve o yüzden sistem performansında ilerlemeye yardımcı olmaktadır.","Chip multiprocessors (CMPs) require effective cache coherence protocols as well as fast virtual-to-physical address translation mechanisms for high performance. Directory-based cache coherence protocols are the state-of-the-art approaches in many-core CMPs to keep the data blocks coherent at the last level private caches. However, the area overhead and high associativity requirement of the directory structures may not scale well with increasingly higher number of cores. As shown in some prior studies, a significant percentage of data blocks are accessed by only one core, therefore, it is not necessary to keep track of these in the directory structure. In this thesis, we have two major contributions. First, we showed that compared to the classification of cache blocks at page granularity as done in some previous studies, data block classification at subpage level helps to detect considerably more private data blocks. Consequently, it reduces the percentage of blocks required to be tracked in the directory significantly compared to similar page level classification approaches. This, in turn, enables smaller directory caches with lower associativity to be used in CMPs without hurting performance, thereby helping the directory structure to scale gracefully with the increasing number of cores. Memory block classification at subpage level, however, may increase the frequency of the operating system's involvement in updating the maintenance bits belonging to subpages stored in page table entries, nullifying some portion of performance benefits of subpage level data classification. To overcome this, we propose as a second contribution, the distributed on-chip page table. The proposed on-chip page table stores recently accessed pages in the system. Our simulation results show that, our approach reduces the number of evictions in directory caches by 58\%, on the average. Moreover, system performance is improved further by avoiding 84\% of the references to OS page table through the on-chip page table."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım test etme bir sistemin amacını yerine getirip getirmediğini ve istenilen özellikleri karşılayıp karşılamadığını denetleme sürecidir. Bir sistemi test etmek için olası program hatalarını keşfedebilen test durumlarını çalıştırmak gerekir. Genelde birçok gerçek program için olası girdi ve operasyon dizilerinin çok sayıda olması sebebiyle ayrıntılı test etmek mümkün ve pratik değildir. Büyük test setlerinden sadece seçilen olası test durumları kısıtlı bir zamanda koşturulabilir. Görüldüğü gibi, yazılım test etmede ki esas sorun sistemdeki hataları ortaya çıkarabilen test durumlarını seçebilmektir. Modele dayalı test etme (MDT) sistemin gereksinimlerinin ve davranımlarının modellerine dayanarak test durumlarının oluşturulmasını ve koşturulmasını otomatikleştirir. Modele dayalı test etme sistemin farklı temsillerini kullanarak yazılım sistemin farklı yönleri için test prosedürleri oluşturur. Örnek modeller içerisinde sonlu makineler, Petri Netler, otomatalar ve Markov zincirleri bulunur. Modele dayalı test etmede yazılım mimarisi kullanılarak sistemik özelliklerde bulunan hataları ortaya çıkarmak yeni bir eğilim olarak görülmektedir. Bu sistemik özellikler tipik olarak mimari bakışlarında tanımlanmıştır. Bir yazılım sisteminin belirtilen mimari özelliklere göre doğruluğunu ölçmek mimari tabanlı yazılım testi (MBYT) olarak adlandırılır. Birçok çalışma farklı modeller kullanarak mimari tabanlı test yöntemleri üzerinde yoğunlaştı. Ancak bu çalışmaların hiç birisinde yazılım mimarisi bakış açıları test durumlarını üretirken kullanılması benimsenmedi. Bu tezde biz, öncellikle var olan model güdümlü mimari tabanlı test etme yöntemlerinin sistematik incelemesini sunuyoruz. Literatürde var olan yöntemleri tanımlıyor ve yöntemlerin sınırlarını tartışıyoruz. Sistematik incelememiz ve analizlerimiz sonucunda yeni bir mimari bakış açılarını kullanan model güdümlü mimari tabanlı test yöntemi geliştiriyoruz. Yöntemimizde mimari bakış açılarında belirtilen tanımlardan sapan kodlar üzerinde yoğunlaşıyoruz. Bunun için mimari bakış açısı modellerini kullanan dönüşüm modellerini koşturarak sistem üzerinde koşturulacak test durumlarını üretiyoruz. Yöntemimiz Türkiye Bilimsel ve Teknolojik Araştırma Kurumu - Yazılım Teknolojileri Enstitüsü'ndeki (TÜBİTAK-YTE) bir proje üzerinde değerlendirildi. Bu değerlendirmenin sonucu olarak model güdümlü mimari tabanlı test yöntemimiz test durumları oluşturmak ve koşturmak ve sistemin güvenilirliğini arttırmak için etkili bir yöntem olduğunu gördük.","Software testing is the process of checking whether a system meets the specifications and fulfills its intended purpose. Testing a system requires executing the test cases that can detect the potential defects in the program. In general, exhaustive testing is not possible or practical for most real programs due to the large number of possible inputs and sequences of operations. Because of the large set of possible tests only a selected set of tests can be executed within feasible time limits. As such, the key challenge of testing is how to select the tests that are most likely to expose failures in the system. Model-based testing (MBT) relies on models of system requirements and behavior to automate the generation of the test cases and their execution. Model based testing can use different representations of the system to generate testing procedures for different aspects of the software systems. Example models include finite state machines (FSMs), Petri Nets, I/O automata, and Markov Chains. A recent particular trend in MBT is to adopt architecture models to identify the defects related to systemic properties. These systemic properties are typically defined in architecture views which represent the gross level structure of the system from particular concern perspective. Assessing software system correctness with respect to architectural specifications is called architecture based testing (ABT). Many studies have focused on architecture based testing in which different models have been applied. However none of these have so far explicitly focused on adopting architecture views for deriving the test cases. In this thesis, we first provide a systematic review on existing model-driven architecture based testing. We define all the existing processes in the literature and discuss the current limitations. Based on the result of the systematic review and our own analysis we provide a novel model-driven architecture based testing approach using architecture views. With the approach we focus on detecting the deviations in the code from the architectural views. For this we use models of architecture views together with executable transformation model to generate the test cases which are then executed on the real code. Our approach has been evaluated within a real industrial context of The Scientific and Technological Research Council of Turkey Software Technologies Institute (STRCT-STI). The results of the industrial case study showed that model-driven architecture based testing can be effective for reducing the time to generate and execute the test cases, and enhancing the reliability of the system."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Anomali tespiti, hiperspektral görüntü analizindeki ana uygulamalardan biridir. Anomali tespitindeki problem, görüntüde seyrek olarak bulunan, çevresine göre farklılık gösteren küçük nesnelerin tespitidir. Yaygın yaklaşımlardan biri görüntü arka planının modellenmesi ve sınanan pikselin bu modele olan farklılığına göre sınıflandırılmasıdır. Ancak, karmaşık arka planların modellenmesi kolay değildir. Bu kapsamda, Gauss karışım modeli tabanlı bir anomali tespiti yöntemi önerilmektedir. Öncelikle görüntü belirli sayıda spektral gruplara ayrılmaktadır. Ardından seyrek spektral ayrıştırma bütün spektral grouplarda uygulanmaktadır. Sonrasında, elde edilen son eleman bolluk değerleri kullanılarak, eşikleme tabanlı, sıra düzenli tabanlı, ve ̧cift yönlü öbekleme tabanlı olmak üzere üç farklı yöntem ile baskın arka plan grupları tespit edilmektedir. Ardından, belirlenen arka plan gruplarını temsil eden pikseller başlangıç değerlerinin hesaplanmasında kullanılmak üzere, her bir spektral grup için bir Gauss karışım modeli öğrenilmektedir. Önerilen yöntem karışım modeli için bileşen sayısının otomatik belirlenmesine ve kestirim sürecinin etkili başlatılmasına olanak sağlamaktadir. Son olarak, ̈öğrenilen Gauss karışım modelleri istatistiksel olarak birleştirilerek sonuç anomali haritası elde edilmektedir. Deneysel veriler göstermektedir ki önerilen yöntemler, özellikle düşük yanlış alarm değerleri için, diğer yöntemlere göre daha yüksek performans sağlamaktadır.","One of the main applications of hyperspectral image analysis is anomaly detection where the problem of interest is the detection of small rare objects that stand out from their surroundings. A common approach to anomaly detection is to first model the background scene and then to use a detector that quantifies the difference of a particular pixel from this background. However, identifying the dominant background components and modeling them is a challenging task. We propose an anomaly detection framework that uses Gaussian mixture models for characterizing the scene background in hyperspectral images. First, the full spectrum is divided into several contiguous band groups for dimensionality reduction as well as for exploiting the peculiarities of different parts of the spectrum. Then, sparse spectral unmixing is performed for each band group for identifying significant endmembers in the scene. Three methods for identifying the dominant background groups such as thresholding, hierarchical clustering and biclustering are used in the endmember abundance space to retrieve the sets of pixel groups that represent dominant background components. Next, these pixel groups are used for initializing individual Gaussian mixture models that are estimated sep- arately for each spectral band group. The proposed method enables automatic identification of the number of mixture components and effective initialization of the estimation procedure for the mixture model. Finally, the Gaussian mixture models for all groups are statistically fused for obtaining the final anomaly map for the scene. Comparative experiments showed that the proposed methods performed better than two other density-based anomaly detectors, especially for small false positive rates, on an airborne hyperspectral data set."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Artırılmış Gerçeklik (AG) ortamlarındaki sanal nesnelerin gerçekçi bir şekilde aydınlatılması görsel uyumluluğun sağlanması açısından önemlidir. Bu tez hızlandırılmıs çekim videolarda aydınlatma tahminini kolaylaştıran özgün bir yaklaşım önermekte ve AG teknolojisiyle hızlandırılmıs çekim videoların görsel olarak uyumlu bir şekilde birleşmesine olanak sağlamaktadır. Önerilen yaklaşım ana ışık kaynağının Güneş olduğu iç ve dış ortamlarda çalışmaktadır. Bu yaklaşımda, ilk olarak ortamın aralıklı ışınım haritasını çıkarmaya çalışan mevcut bir aydınlatma tahmin yöontemi kullanılarak Güneş'in ilk pozisyonunu tahmin edilir. Daha sonra hızlandırılmış çekim videodaki sert yer gölgeleri enerjiye dayalı piksel bazlı bir yöntem kullanılarak takip edilir. Bu yöntem gölgeleri oluşturan piksellerin enerji değerlerini gölgeleri takip etmek için kullanmayı amaçlar. Önerilen yöntem iç ve dış ortamlarda çekilmiş çeşitli hızlandırılmış çekim videolarda denenmiş ve başarılı sonuçlar elde edilmiştir.",Realistic illumination of virtual objects on Augmented Reality (AR) environments is important in terms of achieving visual coherence. This thesis proposes a novel approach that facilitates the illumination estimation on time-lapse videos and gives the opportunity to combine AR technology with time-lapse videos in a visually consistent way. The proposed approach works for both outdoor and indoor environments where the main light source is the Sun. We first modify an existing illumination estimation method that aims to obtain sparse radiance map of the environment in order to estimate the initial Sun position. We then track the hard ground shadows on the time-lapse video by using an energy-based pixel-wise method. The proposed method aims to track the shadows by utilizing the energy values of the pixels that forms them. We tested the method on various time-lapse videos recorded in outdoor and indoor environments and obtained successful results.
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Geçtiğimiz yıllar içerisinde, algılanan derinlik alanını üretim/çekim sonrası kanallarda kontrol edebilme üzerine önemli gelişme sağlanmıştır. Öte yandan, çevrimdışı üretimden farklı olarak, hareketli kameranın bulunduğu bir sanal ortamda algılanan derinliği izleyicinin hedeflenen konfor aralığında tutmak için bir ihtiyaç mevcuttur. Örneğin, stereo görüntü çıktısının kullanıcı girdisine bağlı olarak dinamik biçimde değiştiği bir oyun ortamında, optimize stereo kamera parametrelerini bulmak hayli zorlu bir uğraştır. Konforlu bir seyir sağlamanın bu tür zorluklarını ele alarak, bu eserde sanal ortamlarda daha iyi üç boyutlu stereo görüntü tecrübesi sunmaya yönelik metotlar ortaya konmuştur. İlk kısımda, üç boyutlu etkileşimli ortamlarda binoküler derinlik algısının ve sahne içeriğinin karşılıklı etkilerini göz önünde bulundurarak stereo kameraların iki ana parametresini oluşturan kamera-aksları arası uzaklığı ve kameraların yakınsama uzaklığını kontrol etmek üzere bir yaklaşım ortaya konulmuştur. Bu önerilen İlgi-Bilinçli Dinamik Disparite Kontrolü metodu stereo parametrelerinin bütünleşik optimizasyonunu sağlamak suretiyle zengin derinlikli stereo görsel gerçekleme üreterek izleyicinin seyir konforunu iyileştirmektedir. Optimizasyon modeli geliştirilirken sahne elemanlarının tek tek önemleri ve aynı zamanda her birinin kameraya ve sahnedeki ilgi odağına uzaklığı dikkate alınır. Önerilen metot kullanıcının bireysel stereo disparite alanını ve konforlu seyir tecrübesini göz önünde bulundurarak akomodasyon-yakınsama ikiliğini kontrol altında tutarak sahnenin derinlik efektini de optimize etmektedir. Yapılan kurallı kullanıcı deneyleri, metodun işlerliğini ortaya koymuş ve aynı zamanda yüksek nitelik ve pratikte uygunluğunu ortaya çıkarmıştır. İkinci kısımda, daha önceki iki stereo parametresinin ve bunlara ilaveten lens odak uzunluğunun içerik oluşturucusu veya editörü tarafından sahnedeki belli başlı elemanlara belirli bir çevre ve görüş açısı aralığı için halihazırda belirlenmiş olduğu bir senaryoda kameranın stereo parametreleri otomatik olarak ayarlamaya yönelik bir metot sunulmuştur. Metot, kısaca, kamera sahne içerisinde hareket ederken sahne elemanlarının sayısındaki ve dağılımındaki değişimler ve elemanların atanmış parametreleri için sürekli sahneyi taramak vasıtasıyla stereo kamera parametrelerini gerçek zamanlı olarak hesaplamaktadır. Bu değişkenleri hesaba katarak, ana olarak radyal taban fonksiyonları ile interpolasyon temelli bir biçimde sahnenin girekalanı için uygun stereo kamera parametrelerini üretmektedir. Ortaya konan sistem, gerçek zamanlı çalıştığı için, sezgisel tasarım çerçevesinde gerçekleştirilen kullanıcı ara yüzü vasıtasıyla kullanıcı her arzu ettiğinde her bir sahne elemanı için atanmış parametrelerin değiştirilebilmesini sağlayarak kullanıcının sahnenin genel derinlik hissini kişisel olarak şekillendirebilmesine olanak verir.","In recent years, significant progress has been made on controlling the perceived depth range in post-production pipeline. On the other hand, different from offline production, in a virtual environment with a mobile camera, there remains a need to keep the perceived depth in the comfortable target range for the viewer. For instance, in a game environment where the stereoscopic output changes dynamically based on the user input, finding optimized stereoscopic camera parameters brings about a great challenge. Addressing such challenges of presenting a comfortable viewing setting, this work demonstrates several methods that are developed towards the goal of providing better stereo 3D experience in virtual environments. The first part presents an approach for controlling the two stereo camera parameters, camera convergence distance and interaxial separation, in interactive 3D environments in a way that specifically addresses the interplay of binocular depth perception and saliency of scene contents. The proposed Dynamic Attention-Aware Disparity Control (DADC) method produces depth-rich stereo rendering that improves viewer comfort through joint optimization of stereo parameters. While constructing the optimization model, the importance of scene elements is considered, as well as their distance to the camera and the locus of attention on the display. The method also optimizes the depth effect of a given scene by considering the individual user's stereoscopic disparity range and comfortable viewing experience by controlling accommodation/convergence conflict. The method is validated in a formal user study that also reveals the advantages, such as superior quality and practical relevance, of considering the presented method. In the second part, a novel method is introduced for automatically adjusting the stereo camera parameters, now also including focal length of the virtual camera lens in addition to the previous two, in a given 3D virtual scene for the scenario where there are scene elements that already have their camera parameters set for a certain perimeter and viewing angle range by the content developer and/or editor. The method, in a nutshell, computes the stereo camera parameters online by continuously scanning the scene as the virtual camera moves about it for changes in the number and the relative distribution of scene elements and the preset parameters, as well. Taking these variables into account, the method produces the camera parameters for the rest of the entire scene mainly by the employment of a radial basis function interpolation-based approach. As it works online, the framework allows for adjustment of camera parameters per scene element on-demand with an intuitively-designed interface so that the user can fine-tune the overall depth feeling of the scene."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım mimarisi, bir sistemin ana yapısını gösterdiği ve her bir paydaşın ihtiyacını dikkate aldığı için yazılım geliştirme sürecinin temel taşlarından biridir. Yazılım geliştirme sürecinde mimariden yararlanabilmek için, hazırlanan mimarinin ve öngörülen tasarım kararlarının kodla uyumlu olması gerekmektedir. Ancak, projelerde koda dair değişen gereksinimler ve/veya uyarlamalar mimari ve kod arasında istenmeyen uyumsuzlukların oluşmasına yol açabilmektedir. Bu mimari ayrışma problemi, mimarinin tanımı ve ortaya çıkan kod arasındaki tutarsızlığı belirtmektedir. Projelerin başında tanımlanan tasarım amaçlarına ulaşmayı sağlayan ve kodla yazılım mimarisi arasındaki uyumsuzlukları bulmaya yarayan, literatürde bir takım çalışmalar öne sürülmüştür. Pratikte yazılım mimarisi, paydaşların ihtiyacını öne sürdüğü mimari bakış açılarıyla belgelenmektedir. Mimari bakış açıları ve kod arasındaki uyum nasıl olmalıysa, aynı şekilde her bir bakış açısı kendi içinde ve diğer bakış açılarıyla uyumlu olmalıdır. Ancak, varolan mimari uygunluğu yöntemleri öncelik olarak kod ile mimari arasındaki uyuma odaklanmış ve bakış açılarının kendi aralarındaki uyumsuzlukları dikkate almamıştır. Bu tezde, mimari bakış açılarının kendi aralarındaki uyumsuzluğu ele alan sistematik bir yöntem sunmaktayız. Bu yöntem ile mimari bakış açılarının kendi içinde ve diğer bakış açılarıyla olan uyumsuzlukları sistematik bir şekilde tespit edilebilmektedir. Bu amaç doğrultusunda, meta modelleri tanımlanan mimari bakış açılarını uygulamaya sokan bir araç geliştirdik. Bakış açıları arasındaki uyumsuzlukları bulan yöntemimizi Görünümler ve Ötesi yaklaşımı ile örneklendirdik. Sunulan yöntemi değerlendirmek için hata enjekte metodunu kullandık. Değerlendirmemizin sonuçları, bakış açılarının kendi içinde ve diğer bakış açılarıyla olan uyumsuzluk bulma durumunu, sunulan yöntem ile etkili bir şekilde tespit edilebildiğini göstermektedir.","Software architecture is one of the key artefacts in the software development process since it provides the gross-level structure of the system and supports the stakeholder concerns. To benefit from the architecture it is important that the code is consistent with the architecture and the corresponding design decisions. Unfortunately, changing requirements and/or the adaptations to the code can lead to undesired inconsistencies among the architecture and the code. This so-called architectural drift problem is the discrepancy between the architecture description and the resulting implementation. Several approaches have been proposed to detect the inconsistencies between the software architecture and the code to ensure that the original design goals are maintained. In practice, software architecture is documented using a coherent set of architecture views, each of view addresses particular stakeholder concerns. Similar to the consistency with the code it is important that an architecture view is consistent within itself and with other related architecture views. Unfortunately, the existing architecture conformance analysis approaches have primarily focused on checking the inconsistencies between the architecture and code, and did not explicitly consider the consistency among views. In this thesis, we provide a systematic architecture conformance analysis approach that explicitly focuses on conformance analysis among architecture views. The approach is used for detecting the inconsistencies within and across architectural views. To this end, we define the meta-models of architecture viewpoints, present the conformance analysis approach, and provide the tool ArchViewChecker. We illustrate our approach for detecting inconsistencies using the Views and Beyond approach. We adopt a fault injection approach to evaluate the effectiveness of the approach. The results show that the approach is effective in detecting inconsistencies within views and across views."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım ürün hattı mühendisliği (YÜHM), çekirdek varlık temelinden uygulama geliştirmek için geniş çapta (ortak alan ve ürün seviyesinde) ileriye yönelik, planlı yeniden kullanım sağlanmasını hedefler. Yeniden kullanılabilir varlıkların hazırlanmasına yapılan ön yatırım ile ürünlerin daha düşük maliyet ile geliştirilmesi, onların pazara daha hızlı sunulması ve üretimlerinde kalitenin artması beklenir. Bu hedefler doğrultusunda farklı YÜHM süreçleri önerilmiştir. Önerilen bu süreçler, genel olarak YÜHM sürecini, temel mühendisliği ve uygulama mühendisliği şeklindeki iki üretim döngüsünü kullanacak şekilde tanımlar. Temel mühendisliğinde yeniden kullanılabilir bir platform ve ürün hattı mimarisi geliştirilir. Uygulama mühendisliğinde ürün üyelerinin geliştirilmesi için temel mühendisliği sürecinin sonuçları kullanılır. Yazılım mimarisi, YÜHM'ndeki en önemli çekirdek varlıklarından biridir. Bundan dolayı ürün hattı mimarisi ile uygulama mimarisi ayrımını yapabiliriz. Ürün hattı mimarisi, temel mühendisliği sürecinde geliştirilir ve ayni familyaya ait ürünler için referans mimarisini temsil eder. Uygulama mimarisi, tek bir ürün için mimariyi temsil eder ve ürün hattı mimarisinin yeniden kullanılması ile geliştirilir. Genel tutarlılığın sağlandığından emin olmak için, uygulama mimarilerinin ürün hattı mimarisi ile uyumlu kalması önemlidir. Ancak ürün hattı mimarisinin ve/veya uygulama mimarisinin gelişiminden dolayı, bir mimari sapmaya neden olacak uyumsuzluklar ortaya çıkabilir. Literatürde birkaç mimari uyum analiz yöntemi önerilmiş olsa da, bunlar öncelikli olarak mimari ve kod arasındaki uyumsuzlukların denetlenmesine odaklanmıştır. YÜHM kapsamında, mimari uyum analizi pek fazla ilgi görmemiştir. Bu tezde, ilk olarak, yazılım ürün hattı test sürecini inceleyen sistematik taramaların değerlendirildiği üçüncül sistematik literatür taramamızı sunuyoruz. Daha sonra, ürün hattı mimarisi ile uygulama mimarisi arasındaki uyumsuzlukları ortaya çıkaracak sistematik bir mimari uyum analiz yöntemi öneriyoruz. Yöntemi desteklemek için, kapsamında, ürün hattı mimarisinin mimari görünümleri ile uygulama mimarisinin mimari görünümlerinin karşılaştırıldığı, yansıma modellemesi kavramını benimsiyoruz. Yöntemimizi daha iyi açıklamak için, Görümler ve Ötesi yöntemini çalışan bir örnek-olay incelemesi ile birlikte kullanıyoruz. Üstelik, verilen yöntem için sağlanan araç desteğini de sunuyoruz. Değerlendirmemiz, yöntem ve ilgili aracın, ürün hattı mimarileri ile uygulama mimarileri arasındaki uyumsuzlukların belirlenmesinde etkili olduklarını gösteriyor.","Software product line engineering (SPLE) aims to provide pro-active, pre-planned reuse at a large granularity (domain and product level) to develop applications from a core asset base. By investing upfront in preparing the reusable assets, it is expected to develop products with lower cost, get them to the market faster and produce with higher quality. In alignment with these goals different SPLE processes have been proposed that usually define the SPLE process using the two lifecycles of domain engineering and application engineering. In domain engineering a reusable platform and product line architecture is developed. In application engineering the results of the domain engineering process are used to develop the product members. One of the most important core assets in SPLE is the software architecture. Hereby we can distinguish between the product line architecture and application architecture. The product line architecture is developed in the domain engineering process and represents the reference architecture for the family of products. The application architecture represents the architecture for a single product and is developed by reusing the product line architecture. It is important that the application architectures remain consistent with the product line architecture to ensure global consistency. However, due to evolution of the product line architecture and/or the application architecture inconsistencies might arise leading to an architecture drift. In the literature several architecture conformance analysis approaches have been proposed but these have primarily focused on checking the inconsistencies between the architecture and code. Architecture conformance analysis within the scope of SPLE has not got much attention. In this thesis we first present the results of our tertiary systematic literature review to systematic reviews on software product line testing. Subsequently, we propose a systematic architecture conformance analysis approach for detecting inconsistencies between product line architecture and application architecture. For supporting the approach we adopt the notion of reflexion modeling in which architecture views of product line architecture are compared to the architecture views of the application architecture. For illustrating our approach we use the Views and Beyond approach together with a running case study. Furthermore, we present the provided tool support for the presented approach. Our evaluation shows that the approach and the corresponding tool are effective in identifying the inconsistencies between product line architectures and application architectures."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"RC4, 1987 yılında Ronald Rivest tarafından tasarlanan, en yaygın olarak kullanılan akan şifre algoritmalarından biridir. RC4 basit tasarımı nedeniyle bilim camiasının çok dikkatini çekti. Son yirmi yılda, kripto analiz uzmanları tarafından RC4 ile ilgili pek çok analiz çalışması yayınlandı. Bu analizlerde, istatiksel önyargılar ve uygulamaları RC4'ün en önemli zayıflığı olarak dikkat çekmektedir. Bu zayıflıklara olan direnci arttırmak için, RC4'un çok farklı türleri tasarlandı. RC4A ve VMPC bunlardan ikisidir ve her ikisi de FSE 2004 konferansında önerilmiştir. Burada öncelikle, RC4'e yönelik istatiksel önyargılara dayanan iki atak ki bunlar; doğrusal korelasyon ve açık metin ele geçirme atağı, tekrar uygulayacağız. Sonra, bu atakları düzenleyerek, RC4A ve VMPC üzerinde uygulayacağız. Bu iki algoritma için, daha önce keşfedilmemiş bazı doğrusal korelasyonlar ve istatiksel önyargılar gözlemledik. Ayrıca, deneysel sonuçları değerlendirerek bu iki algoritmanın güçlü ve zayıf yönlerini tespit etmeye çalıştık. Tespit edilen bu özelliklere göre RC4, RC4A ve VMPC için değişiklikler önerdik ve bu algoritmaların tasarımındaki küçük değişikliklerin, istatiksel önyargı ataklarına karşı dirençlerini önemli ölçüde arttırabileceğini ya da azaltabileceğini gösterdik.","RC4 is one of the most widely used stream cipher, designed by Ronald Rivest in 1987. RC4 has attracted a lot of attention of the community due to its simple design. In the last twenty years, lots of analyses about RC4 have been published by cryptanalysts. In these analyses, statictical biases and their applications stand out as the main weaknesses of RC4. To resist against this kind of weaknesses, many different varients of RC4 were designed. RC4A and VMPC are two of them, both proposed in FSE 2004. Here, we first reproduce two attacks against RC4 that depend on statistical biases; the linear correlation attack (Sepehrdad et al., 2010), and the plaintext recovery attacks (Alfardan et al., 2013). Then, we modify and apply them against RC4A and VMPC. We observe some previously undiscovered linear correlations and statistical biases for these two ciphers. Then, we try to identify the strong and weak aspects of these ciphers by evaluating the experimental results. We propose modifications for RC4, RC4A and VMPC according to these aspects and show that small changes in the design of these ciphers can increase or decrease their resistance against statistical bias attacks significantly."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İşletme içinde sağlanan ve kontrol edilen altyapı ve hizmetlere dayanan geleneksel kurumsal uygulamalardan farklı olarak, bulut bilişim sağlayıcıları Internet üzerinden barındırılan hizmetleri temel alır. Bu vesileyle, hizmetler tamamen sağlayıcı tarafından yönetilirken, tüketiciler ise gerekli miktardaki hizmetleri talebi üzerine elde edebilir, yükleme olmadan uygulamaları kullanabilir ve internet erişimi olan herhangi bir bilgisayar üzerinden kişisel dosyalarına erişebilir. Son zamanlarda hem sanallaştırma ve dağıtılmış bilgi işlemdeki önemli gelişmeler, hem de yüksek hızlı İnternete gelişmiş erişim sağlanması ve kaynakların ekonomik olarak en uygun şekle sokma ihtiyacı sayesinde bulut bilişim üzerinde artan bir ilgi gözlenebilmektedir. Yazılım uygulamalarının bulut üzerinden sağlandığı hizmet olarak sunulan yazılım alanı bulut bilişimin önemli bir kategorisidir. Hizmet olarak sunulan yazılımı anlatırken genellikle, belirli bir uygulama mimarisi belirtilmez, ancak bunun yerine genel bileşenler ve yapı tanımlanır. Sağlanan referans hizmet olarak sunulan yazılım mimarisine dayanarak farklı hizmet olarak sunulan yazılım mimarileri elde edilebilir. Bu mimarilerin her biri genel anlamda farklı kalite faktörlerini uygulayacaktır. Ölçülebilirlik, hizmet olarak sunulan yazılım mimarileri tasarımı konusunda önemli bir kalite faktörüdür. Ölçülebilirlik, sistemin artan iş yükü miktarıyla yetenekli bir şekilde başa çıkabilme veya bu artışa uyum sağlayabilmek için genişleyebilmesidir. Bu tezde ölçeklenebilir hizmet olarak sunulan yazılım mimarilerinin tasarımı için sistematik modelleme ve bir tasarım yaklaşımı sunuyoruz. Hizmet olarak sunulan yazılım tabanlı sistemlerin ölçülebilirliğini etkileyen yönleri tespit etmek için ilgili birincil çalışmaları tespit ettiğimiz ve incelediğimiz sistematik bir kaynak taraması yaptık. Çalışmamız ölçeklenebilir sistemlerin tasarımında dikkate alınması gereken yönleri açığa vurmuştur. Araştırmamız, sonraki iki yönde devam etti. İlk olarak, ölçeklenebilir hizmet olarak sunulan yazılım mimarilerinin modellemesini desteklemek için bir UML profili tanımladık. Bu profil, profiller tanımlayan ve belgeleyen mevcut uygulamalara uygun olarak tanımlanmıştır. İkinci olarak, ölçeklenebilir hizmet olarak sunulan yazılım sistemlerini tasarlamak için mimari perspektifi sunduk. Mimari perspektifler, varolan bir dizi görünümleri değiştirmek, kalite özelliklerini belgelemek ve analiz etmek için kullanılan faaliyetler koleksiyonundan, taktiklerden ve talimatlardan oluşmaktadır. Mimari perspektifler temelde birden çok görünüm üzerinde birlikte çalışan talimatlardır. Şimdiye kadar mimari perspektifler performans, yeniden kullanım ve güvenlik gibi çeşitli kalite faktörleri için belirlenmiştir. Ancak, ölçeklenebilir hizmet olarak sunulan yazılım sistemlerini tasarlamaya özel bir mimari perspektif açıkça tanımlanmış değildir. Bizim tanımladığımız mimari perspektif, hem sistematik kaynak taramasından elde edilen ölçeklenebilirlik yönlerini hem de önemli olduğu kanıtlanmış tasarım kurallarını ve uygulamalarını temsil eden mimari tasarım taktiklerini göz önünde bulundurur. Ayrıca, mimari perspektif ölçeklenebilirlik için bizim tanımladığımız UML profili benimser. Ölçeklenebilir perspektif, gerçek bir endüstriyel vaka çalışmasının hizmet olarak sunulan yazılım mimari tasarımı üzerinde gösterilmiştir.","Different from traditional enterprise applications that rely on the infrastructure and services provided and controlled within an enterprise, cloud computing is based on services that are hosted on providers over the Internet. Hereby, services are fully managed by the provider, whereas consumers can acquire the required amount of services on demand, use applications without installation and access their personal files through any computer with internet access. Recently, a growing interest in cloud computing can be observed thanks to the significant developments in virtualization and distributed computing, as well as improved access to high-speed Internet and the need for economical optimization of resources. An important category of cloud computing is the software as a service domain in which software applications are provided over the cloud. In general when describing SaaS, no specific application architecture is prescribed but rather the general components and structure is defined. Based on the provided reference SaaS architecture different application SaaS architectures can be derived each of which will typically perform differently with respect to different quality factors. An important quality factor in designing SaaS architectures is scalability. Scalability is the ability of a system to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth. In this thesis we provide a systematic modeling and design approach for designing scalable SaaS architectures. To identify the aspects that impact the scalability of SaaS based systems we have conducted a systematic literature review in which we have identified and analyzed the relevant primary studies that discuss scalability of SaaS systems. Our study has yielded the aspects that need to be considered when designing scalable systems. Our research has continued in two subsequent directions. Firstly, we have defined a UML profile for supporting the modeling of scalable SaaS architectures. The profile has been defined in accordance with the existing practices on defining and documenting profiles. Secondly, we provide the so-called architecture design perspective for designing scalable SaaS systems. Architectural Perspectives are a collection of activities, tactics and guidelines to modify a set of existing views, to document and analyze quality properties. Architectural perspectives as such are basically guidelines that work on multiple views together. So far architecture perspectives have been defined for several quality factors such as for performance, reuse and security. However, an architecture perspective dedicated for designing scalable SaaS systems has not been defined explicitly. The architecture perspective that we have defined considers the scalability aspects derived from the systematic literature review as well as the architectural design tactics that represent important proved design rules and practices. Further, the architecture perspective adopts the UML profile for scalability that we have defined. The scalability perspective is illustrated for the design of a SaaS architecture for a real industrial case study."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hareket veritabanları üzerinden benzer hareket erişimi bilgisayar animasyonu araştırmalarının önemli bir konusu haline gelmiştir. İkili değerli geometrik özellik ve ters indis kullanımı bu problemin verimli çözümlerinden birisidir. Bu yaklaşım bulanık aramalar için çeşitlilik ve hatalılık algoritmaları ile birlikte kullanılabilmektedir. Fakat yakın benzerlik aramaları bu yaklaşım ile mümkün değildir. Buna ek olarak bu yaklaşım ikili değerli geometrik özellik seçiminde insan girişi gerektirdiğinden otomatik değildir. Başka bir verimli yaklaşımda, orta boyutta rakamsal özellik kümeleri, k-d ağacı ve yönlü çizge üzerinde en kısa yol arama kullanılmıştır. Fakat bu yaklaşım bulanık aramalar için bir araç sunmamaktadır. Bu tezde, rakamsal özellik kümeleri, k-d ağacı tabanlı dizinleme yapısı ve ters indisler kullanan bir melez yaklaşım öneriyoruz. Bizim melez yaklaşımımız kullanıcı girişine ihtiyaç duymamaktadır ve çeşitlilik algoritmaları ile birlikte kullanılabilmektedir. Sonuçlarımız melez yaklaşımımızın hareket veritabanı üzerinde benzerlik aramaları için kullanılabilir ve verimli olduğunu göstermektedir.","Retrieving similar motions from motion databases has become an essential topic of computer animation research. The use of binary geometric features and inverted indexes is one of the efficient solutions to this problem. This approach can be used with variation and inexactness algorithms for fuzzy searches. However, close similarity searches are not possible. In addition, the process is not automatic since it needs user input for selecting binary features to use. In another efficient approach, k-d tree with medium sized numerical based feature sets and shortest path search on a directed graph are used. However, it does not propose any tool for fuzzy searches. In this thesis, we propose a hybrid approach that utilizes numerical based feature sets, k-d tree based indexing structure and inverted index based motion matching technique. Our hybrid approach does not need user input, can be used in environments requiring close similarity of motions and can be used with variation and inexactness algorithms. Our results show that our hybrid approach is useful and efficient for similarity searches on motion databases."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalpte oluşan fiziksel bozuklukların kan akışını etkilemesi sonucu oluşan seslere üfürüm denir. Üfürümler birçok kalp hastalığının birincil habercisidir. Kalp seslerini stetoskop aracılığıyla dinlemek (oskültasyon) suretiyle üfürümler üzerinden teşhis yapılabilse de, bu tür teşhisler tıbbi uzmanlık gerektirmektedir ve hatalara oldukça açıktır. Literatürde otomatik sınıflandırma algoritmaları önerilmiş olsa da; ses dalgabiçimlerinden ayıklanacak olan özniteliklerin kalp atımı içerisindeki konumları teşhis açısından önem taşıdığı için, öncelikle birinci ve ikinci kalp seslerinin konumlarını belirlemek ve buna göre öznitelik seçmek gereklidir. Kalp atımlarının saptanması ve ayrıştırılması işlemine bölütleme (segmentasyon) adı verilmektedir. Kalp sinyalinin organik yapısı dolayısıyla öngörülebilir bir frekans-zaman profilinin olmaması, sadece ses kaydını kullanarak bölütleme işini zorlaştırmaktadır. Birçok ticari sistemde harici olarak bir EKG sinyali da kaydedilse de, EKG aygıtlarının stetoskop kadar yaygın olmaması bu sistemlerin erişilebilirliğini düşürmektedir. Yalnızca kalp sesi kullanarak bölütleme yapan algoritmalar olsa da, bu algoritmaların hepsinin üzerinde sonuç sunduğu ortak bir veritabanı yoktur. Bu çalışmada, var olan algoritmalara çeşitli uyarlamaların yanı sıra, bu çeşitlemeleri karşılaştırmak için kullanılabilecek bir değerlendirme ölçütü öneriyoruz. Örnekleri için elle yapılmış bölütlemelerin mevcut olduğu Pascal veritabanından alınmış 66 kayıt üzerinde bu yaklaşımların tüm kombinasyonlarını karşılaştırıyoruz. Bunun yanı sıra birkaç yöntemi karıştırarak sonuç kalitesini artırmayı amaçlayan bir birleşim de öneriyoruz. Karşılaştırdığımız tekil algoritmalar % 62'ye varan başarılar gösterirken önerdiğimiz birleşim ile % 75'lik bir başarı oranına ulaştık. Son olarak, Pascal gibi denetleme amaçlı veritabanlarının kolayca ve hatasız şekilde oluşturulabilmesi için bir araç ürettik.","Heart murmurs are pathological heart sounds that originate from blood flowing with abnormal turbulence due to physiological defects of the heart, and are the prime indicator of many heart-related diseases. Murmurs can be diagnosed via auscultation; that is, by listening with a stethoscope. However, manual detection and classification of murmur requires clinical expertise and is highly prone to error. Although automated classification algorithms exist for this purpose; they heavily depend on feature extraction from 'segmented' heart sound waveforms. Segmentation in this context refers to detecting and splitting cardiac cycles. However, heart sound signal is not a stationary signal; and typically has a low signal-to-noise ratio, which makes it very difficult to segment using no external information but the signal itself. Most of the commercial systems require an external ECG signal to determine S1 and S2 peaks, but ECG is not as widely available as stethoscopes. Although algorithms that provide segmentation using sound alone exist, a proper comparison between these algorithms on a common data set is missing. We propose several modifications to many of these algorithms, as well as an evaluation method that allows a unified comparison of all these approaches. We have tested each combination of algorithms on a real data set, which also provides manual annotations as ground truth. We also propose an ensemble of several methods, and a heuristic for which algorithm's output to use. Whereas tested algorithms report up to 62 % accuracy, our ensemble method reports a 75 % success rate. Finally, we created a tool named UpBeat to enable manual segmentation of heart sounds, and construction of a ground truth data set. UpBeat is a starting medium for auscultation segmentation, time-domain based feature extraction and evaluation; which has automatic segmentation capabilities, as well as a minimalistic drag-and-drop interface which allows manual annotation of S1 and S2 peaks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hedefli saldırılar devlet ve ticari kurumlar için büyük bir tehdit oluşturmaktadır. Her yıl giderek artan sayıda hedefli saldırı, özellikle Gelişmiş Sürekli Tehditler, çeşitli siber güvenlik firmaları tarafından tespit edilerek ortaya çıkarılmaktadır. Bu saldırıların temel özellikleri, iyi finanse edilen ve yetenekli aktörlerin devamlı olarak belirli kurumları hedef alması, karmaşık araç ve tekniklerin kullanımı, tespit edilene kadar sızılan ortamlarda uzun süreli kalınması ve gizli faaliyet yürütülmesidir. Zararlı yazılımlar hedefli saldırılarda sistemlerin ele geçirilmesi, kalıcılığın sağlanması, aktörlerle haberleşme, komutların yerine getirilmesi gibi çeşitli görevlerde çok hayati bir rol oynar. Gizli çalışma doğası gereği hedefli saldırılarda kullanılan zararlı yazılımların dinamik olarak kontrollü sanal bir ortamda incelendiğinde geleneksel zararlı yazılımlara göre farklı davranması beklenir. Bu tezde hedefli saldırılarda kullanılan zararlı yazılımlara odaklandık ve hedefli zararlı yazılımları davranışsal ve hafıza öznitelikleri kullanarak makine öğrenimi yoluyla tespit eden ve sınıflandıran bir yöntem sunduk. Çalışmanın hedefli zararlı yazılımların sınıflandırıldığı ve davranışsal özniteliklere hafıza özniteliklerinin eklendiği literatürdeki ilk çalışma olduğunu belirtmek gerekir. Sunulan yöntem, geleneksel ve hedefli zararlı yazılımların dinamik analiz sisteminde hafıza analizi aracıyla birlikte çalıştırılması, analiz raporlarında bulunan davranışsal ve hafıza izlerinden ayırt edici özniteliklerin çıkartılması ve çıkartılan öznitelikler üzerinde makine öğrenmesi uygulanması adımlarını içermektedir. Hedefli zararlı yazılımları daha etkili sınıflandırmak üzere yeni davranışsal ve hafıza öznitelikleri tanımlandı. Yöntem, hedefli ve geleneksel zararlı yazılımlardan oluşan bir veri kümesi üzerinde farklı gözetimli öğrenme algoritmaları ile test edilerek değerlendirildi. Elde edilen sonuçlar, dinamik analiz sonuçlarından davranışsal ve hafıza öznitelikleri çıkartılıp makine öğrenimi kullanılarak hedefli zararlı yazılımların başarılı bir şekilde tespit edilip sınıflandırılabileceğini gösterdi.","Targeted attacks pose a great threat to governments and commercial entities. Increasing number of targeted attacks, especially Advanced Persistent Threats, are being discovered and exposed in each year by various cyber security organizations. Key characteristics of these attacks are well-funded and skilled actors persistently targeting specific entities, sophisticated tools and tactics, long-time presence in breached environments before detection and stealth operation. Malware plays a crucial role in a targeted attack for various tasks such as compromising systems, maintaining presence, communicating with the operators, carrying out commands, etc. Because of its stealthy nature, malware used in targeted attacks is expected to act different than the traditional malware when it is dynamically analyzed in a sandbox environment. In this thesis we focused on the malware used in targeted attacks and present a method to automatically detect and classify targeted malware through machine learning using behavioral and memory features. It's worth noting that it is a first work published in the literature that classifies targeted malware and incorporates memory features into the dynamic features. The method comprises the steps of running both traditional and targeted malware in a dynamic analysis system along with a memory analysis tool, extracting features from behavioral and memory artifacts found in analysis results and employing machine learning on the extracted features. New behavioral and memory features were defined in order to classify targeted malware more effectively. Method is then evaluated over a dataset comprised of targeted and traditional malware with different supervised learning algorithms. The results show that machine learning can be employed successfully to automatically detect and classify targeted malware from dynamic analysis results using behavioral and memory features."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sosyal ağlar sürekli değişirken ve gelişirken, dinamik yapıdaki bu ağların analizi için gerekli metotların önemi de sosyal eğilimleri anlamak açısından artmaktadır. Fakat sosyal ağ servis sağlayıcıları tarafından uygulanan kısıtlamalar nedeniyle, bir sosyal ağın topolojik durumu ve paylaşımlarıyla birlikte tüm içeriğini toplamak için mevcut kaynaklar yetersiz kalmaktadır. Sonuç olarak, değişken sosyal ağ verisinin analizi; verinin yaklaşık bir kopyasını yerel olarak muhafaza etmeyi ve zaman içinde yenilemeyi gerektirir. Biz, sosyal ağ geliştikçe, ağ üzerindeki etkili kişileri belirlemek ve zaman içerisinde takip etmek adına; hem ağ hem de metin verisinin sınırlı kaynaklar altında toplanması problemi üzerinde çalışıyoruz. Sınırlı sayıda kullanıcının her zaman aralığı için ilişkilerini (genel etki hesaplaması için gereklidir) ve metin paylaşımlarını (konu tabanlı etki hesaplaması için gereklidir) toplamak için kullanıcıların etki eğilimlerini ve eylemlerini göz önünde bulunduran bir algoritma öneriyoruz. Yeni toplanmış kullanıcı verisini ve lokal olarak sürdürülen ağın en son versiyonunu temel alarak; güncel ağ yapısını çıkarsıyoruz. Buna ek olarak, ağ çıkarsama metodumuzun doğruluğunu daha da artırmak adına, bağlantı önerme algoritmaları kullanıyoruz. ""PageRank puanı"" nı genel etki hesaplaması için ölçü olarak belirledik. Önerdiğimiz çözümlerin, genel etki için ""PageRank skorları"" nı ve konu tabanlı etki için paylaşım metinlerinin anlamsal analizleri ile paylaşım istatistikleri harmanlanarak ve belirlenen ağırlıklar kullanılarak oluşturulan konu temelli ağlar üzerinden hesaplanan ağırlıklı ""PageRank skorları"" nı, nasıl yüksek bir doğrulukla yenileyerek sürdürdüğünü gösterdik. Algoritmalarımızın etkinliğini ölçmek adına sonuçlarımızı, bir mikroblog servisi olan ""Twitter"" üzerinden toplanan veriler ile oluşturan ağın, tam ve en güncel hali üzerinden hesaplanan gerçek etki skorları ile karşılaştırdık. Elde edilen sonuçlara göre; önerdiğimiz teknikler temel referans tekniklerini önemli bir ölçüde geri bırakırken (ağ çekimi için %80,metin çekimi için %77 daha doğrudur), literatürdeki en gelişmiş tekniklerden de daha iyi bir performans göstermiştir (%21 daha doğrudur).","As social networks are constantly changing and evolving, methods to analyze dynamic social networks are becoming more important in understanding social trends. However, due to the restrictions imposed by the social network service providers, the resources available to fetch the entire contents of a social network are typically very limited. As a result, analysis of dynamic social network data requires maintaining an approximate copy of the social network for each time period, locally. We study the problem of dynamic network and text fetching with limited probing capacities, for identifying and maintaining influential users as the social network evolves. We propose an algorithm to probe the relationships (required for global influence computation) as well as posts (required for topic-based influence computation) of a limited number of users during each probing period, based on the influence trends and activities of the users. We infer the current network based on the newly probed user data and the recent version of the network maintained locally. Additionally, we propose to use link prediction methods to further increase accuracy of our network inference. We employ PageRank as the metric for influence computation. We illustrate how the proposed solution maintains accurate PageRank scores for computing global influence, and topic-sensitive weighted PageRank scores for topic-based influence. The latter relies on a topic-based network constructed via weights determined by semantic analysis of posts and their sharing statistics. We evaluate the effectiveness of our algorithms by comparing them with the true influence scores of the full and up-to-date version of the network, using data from the micro-blogging service Twitter. Results show that our techniques significantly outperform baseline methods (80% higher accuracy for network fetching and 77% for text fetching) and are superior to state-of-the-art techniques from the literature (21% higher accuracy)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Görüntü arama sistemleri kullanıcıların yönelimleri doğrultusunda yıllar içinde gelişim göstermiştir. Tek sorgu resmi kullanan resim arama sistemleri, sorgu nesnesinin sadece bir yönü hakkında bilgi sahibi olacağından, nesnenin tamamını temsil etme konusunda yetersiz kalmaktadır. Bu yüzden çoklu sorgu yöntemi önem kazanmaya başlamaktadır. Sunduğumuz sistem, ilgi noktaları ve görsel kelime histogramlarını kullanan mobil bir çoklu görüntü arama sistemidir. Sorgu nesnesini daha iyi tanımlayabilmek için, her biri sorgu resminin farklı bir yönünü tanımlayan, çoklu ilgi noktası çıkarma yöntemi kullanılmış ve sonuçlar birleştirilerek kullanılmıştır. Çalışmamızda ayrıca farklı açılardan çekilmiş resimler birleştirilerek resim arama sonuçları iyileştirilmektedir. Farklı açılardan çekilen resimler sorgu resmini daha kapsamlı tanımlamaya olanak sağlamaktadır. Çalışmamızda ayrıca her nesnenin farklı açılardan ve uzaklıklardan resimlerinin bulunduğu bir veritabanı sunulmuştur. Veritabanının bu özelliği, sorgu resminin herhangi bir veritabanı resmi ile eşleşme ihtimalini artırmakta, sonuç olarak da resim arama sisteminin performansı artmaktadır. Resim arama sistemimiz literatürdeki önemli çalışmalardan birisi ile ortalama duyarlık açısından karşılaştırılmıştır. Bu deneyler sırasında hem tekli, hem de çoklu sorgu resimleri kullanılmıştır. Deneyler önerdiğimiz resim arama sisteminin daha iyi sonuçlar verdiğini göstermiştir. Ayrıca, yeni oluşturduğumuz veritabanı ile yapılan deneyler, farklı açılardan resimlere sahip nesnelerin veritabanında kullanılmasının resim arama sisteminin performansını iyileştirdiğini göstermiştir.","Visual search has evolved over the years, according to the demand of users. Single image query search systems are inadequate to represent a query object, because they are limited to a single view of the object. Therefore, multi image query search systems have gained importance to increase search performance. We propose a mobile multi-image search system that makes use of local features and bag-of-visual-words (BoVW ) approach. In order to represent the query object better, we combine multiple local features each describing a di_erent aspect of the query image. Employing di_erent features in search improves the performance of the image search system. We also increase the retrieval performance using multi-view query approach together with fusion methods. Using multi-view images provides more comprehensive representation of the query image. We also develop a new multi-view object image database (MVOD), with the aim of evaluating the performance impact of using multi-view database images. Multi-view database images from di_erent views and distances increase the possibility to match the query images to database images. As a result, using multi-view database images increases the precision of our search system. We compare our image search system with a state-of-the-art work in terms of average precision. In our experiments, we use single and multi image queries together with single viewed database. The results show that our image search system performs better with both single and multi image queries. We also performed experiments using MVOD database and show that using a multi-view database increases the precision."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Genel olarak kopya sayısı varyasyonu (KSV) ve dengeli yeniden düzenlemeler olarak sınıflandırılabilen çok çeşitli genomik yapısal varyasyon tipleri bulunmaktadır. Her ne kadar literatürde KSV'lerin karakterizasyonu için çok sayıda algoritma varsa da, inversiyon ve translokasyon gibi dengeli yeniden düzenlemelerin keşfi henüz açık bir problemdir. Bunun başlıca sebebi, bu tür varyasyonların kırılma noktalarının parçasal duplikasyonlar ve yaygın tekrarlara denk gelmesi, ve bu durumun kısa okumaların güvenilir şekilde hizalandırılmasını zorlaştırmasıdır. 1000 Genom Projesi inversiyonlarin bulunması için bazı metotların geliştirilmesine önayak olduysa da, geliştirilen algoritmalar göreceli olarak kısa inversiyonlarin keşfiyle sınırlıdır, ve büyük inversiyonlarin yeni nesil dizileme (YND) kullanılarak keşfi için halihazırda bir algoritma bulunmamaktadır. Bu çalışmada, daha önce haplotip haritalama için geliştirilmiş olan bir dizileme metotunu (Kitzman vd., 2011) kullanarak büyük inversiyonlarin karakterizasyonunu öneriyoruz. Toplanmış klon dizileme adı verilen bu yöntem, klon tabanlı dizilemenin sağladığı avantajları YND teknolojilerinin hız ve masraf etkinliği ile birleştirmektedir. Bu yöntem ile elde edilmiş verileri kullanarak, dipSeq adında, büyük inversiyonları (>500 Kbp) keşfedebilen bir algoritma geliştirdik. dipSeq algoritmasının gücünü önce simüle edilmiş verilerle ispatlayıp, daha sonra da NA12878 kodlu insan DNA'sından elde edilmiş gerçek veriye uyguladık. Bu genomda daha önceden keşfedilmiş ve deneysel olarak ispatlanmış bütün büyük inversiyonlari bulabildik. Ayrıca önceden bilinmeyen yeni bir inversiyon polimorfizmini de bulup florasan in situ hibridizasyon yöntemi ile tahminimizi doğruladık.","An inversion is a chromosomal rearrangement in which an internal segment of a chromosome has been broken twice, flipped 180 degrees, and rejoined. Most known examples of large inversions were found indirectly from studies on human disease where inversions have no detectable effect in parents, but increase the risk of a disease-associated rearrangement in the offspring. The development of a map of inversion polymorphisms will provide valuable information regarding their distribution and frequency in the human genome and will help unravel how inversions and the segmental duplications architecture associated with inverted haplotypes contribute to genomic susceptibility to disease rearrangements. The 1000 Genomes Project spearheaded the development of several methods to identify inversions, however, they are limited to relatively short inversions, and there are currently no available algorithms to discover large inversions using high throughput sequencing technologies (HTS). This is mainly because the breakpoints of such events typically lie within segmental duplications and common repeats, reducing the mappability of short reads. We propose using pooled clone sequencing (PCS), a method originally developed to improve haplotype phasing, to characterize large genomic inversions. PCS merges the advantages of clone based sequencing approaches with the speed and cost efficiency of HTS technologies. Using this sequencing data, we developed a novel algorithm, dipSeq for discovering large inversions (>500 Kbp) following the observation that clones that span the inversion breakpoint will be split into two sections, split clones, when mapped to the reference genome. We evaluate the performance of dipSeq on 3 sets of simulated data, demonstrating its correctness and robustness to structural duplications and other types of structural variations. We further applied dipSeq to the genome of a HapMap individual (NA12878). dipSeq was able to accurately discover all previously known and experimentally validated large inversions. We also identified a new inversion and confirmed using fluorescent in situ hybridization. Although dipSeq displays a relatively high false positive rate using real data, it performed better with simulated data, suggesting that the performance with the NA12878 genome may be improved with higher depth of coverage."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Emniyet-kritik sistemlerdeki bir aksama ya da işlev bozukluğu ölümlere, insanlar üzerinde ciddi yaralanmalara ya da çevresel hasarlara neden olabilir. Bu riskleri ortadan kaldırmak ya da azaltmak için emniyet-kritik sistemler dikkatli bir şekilde tasarlanmalı ve analiz edilmelidir. Tasarım aşamasında karar alınırken farklı paydaşlar için mimari görünümlerin ve perspektiflerin modellenmesi, yazılım mimari tasarımında kullanılan yaygın pratiklerden birisidir. Literatürde var olan yaklaşımlar genel amaçlı olarak kullanılmış ve özel olarak emniyet ilgisi ele alınmamıştır. Emniyet ilgisini mimari düzeyde adresleyebilmek ve emniyet-kritik sistemlerin tasarım sürecini desteklemek amacıyla literatürde var olmayan emniyet perspektifi ve yazılım emniyeti için mimari çerçeve yaklaşımlarını sunuyoruz. Emniyet-kritik sistemler tasarlandıktan sonra gerçekleştirim, kurulum ve işletim süreçlerinden önce sistemlerin analiz aşaması gerçekleştirilmelidir. Yapılan analizle birlikte olası hataların belirlendiği ve belirlenen hataları tolere etmek ya da ortadan kaldırmak için uygun maliyetli çözümlerin uygulandığından emin olunmalıdır. Emniyet-kritik sistemler karmaşık sistemler olduğu için, bu sistemlerin testinin gerçekleştirilmesi ve uygun test durumlarının yazılması oldukça zorludur. Literatürde yazılım mimarisi kalitesini değerlendirmek açısından birçok senaryo-tabanlı yazılım mimari analizi yaklaşımları sunulmuştur. Fakat bu yaklaşımlar genel çözümler sunmakta ve emniyet ilgisini doğrudan göz önünde bulundurmamaktadır. Bu kapsamda, emniyet-kritik sistemler için oluşturulan test durumlarının uygulanan emniyet taktikleri ve hata bilgileri kullanılarak etkinliğini değerlendirebilmek için hata-tabanlı test yaklaşımı sunulmaktadır.","A safety-critical system is defined as a system in which the malfunctioning of software could result in death, injury or damage to environment. To mitigate these serious risks the architecture of safety-critical systems need to be carefully designed and analyzed. A common practice for modeling software architecture is the adoption of architectural perspectives and software architecture viewpoint approaches. Existing approaches tend to be general purpose and do not explicitly focus on safety concern in particular. To provide a complementary and dedicated support for designing safety-critical systems we propose safety perspective and an architecture framework approach for software safety. Once the safety-critical systems are designed it is important to analyze these for fitness before implementation, installation and operation. Hereby, it is important to ensure that the potential faults can be identified and cost-effective solutions are provided to avoid or recover from the failures. In this context, one of the most important issues is to investigate the effectiveness of the applied safety tactics to safety-critical systems. Since the safety-critical systems are complex systems, testing of these systems is challenging and very hard to define proper test suites for these systems. Several fault-based software testing approaches exist that aim to analyze the quality of the test suites. Unfortunately, these approaches do not directly consider safety concern and tend to be general purpose and they doesn't consider the applied the safety tactics. We propose a fault-based testing approach for analyzing the test suites using the safety tactic and fault knowledge."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Metin dokümanlarında konusal yapının anlaşılması, etkili erişim ve arama, otomatik özetleme ve dokümanları konuları hakkında tanımlamak, biraraya getirmek ve takip etmek gibi görevler için önemlidir. Dokümanlar genellikle içerdiği bölümleri birbirinden ayıran başlıklar ve yapısal ayıraçlar içeriyor olsalar da, bazı dokümanlar bu özelliklere sahip değildir ve bu durum konu bakımından metin bölümleme sistemlerine olan ihtiyacı ortaya çıkarmaktadır. Konuşma verisinden elde edilen transkript metinler ve gazete, blog yazıları gibi konusal bakımdan yapısı belirsiz olan metinler, bu tür dokümanlara örnek olarak gösterilebilir. Metinlerde konu bölümlendirme için, yani metni kendi içerisinde tutarlı konusal bölümlere ayırmada, yerel içerik tabanlı ve kelimeler arasındaki ilişkilerden yararlanan yeni bir yöntem sunulmaktadır. Kelimeler arasındaki anlam bütünlüğünü ifade etmede, önerilen yöntem HAL anlamsal uzayından yararlanmaktadır. Bu uzay, metin içerisinde birlikte gözüken kelimelerin incelenip sabit uzunluktaki bir pencerenin metin boyunca kaydırılmasıyla oluşturulur. Önerilen algoritma olan BTS, konusal değişiklikleri döngüsel olarak tespit etmektedir. Her döngüde, cümlelerden oluşan bir blok ele alınarak, birbiriyle en ilişkili cümle ikilileri bulunur ve bu çiftlerin incelenmesiyle yeni bir bölüm oluşturulur. Önerilen yöntem, hata içermeyen haber bülteni transkriptlerinde ve yapay olarak farklı bölümlerin biraraya getirildiği dokümanlar üzerinde değerlendirilmektedir. Türkçe dili için, otomatik olarak haber metinlerinin kullanılmasıyla yapay bir veri seti oluşturulmuştur. Performans karşılaştırması için, TextTiling ve C99 yöntemleri kullanılmaktadır ve sonuçlar, önerilen yöntemin bu yöntemlerle karşılaştırılabilir olduğunu göstermektedir. Sonuçlar ayrıca, ANOVA ve Tukey testleri ile istatistiksel olarak doğrulanmaktadır. Anahtar sozcukler: Metin Bölümlendirme, Konu Bölümlendirme, Doğal Dil İşleme, Kelime bütünlüğü, Anlamsal ilişki","Understanding the topical structure of text documents is important for effective retrieval and browsing, automatic summarization, and tasks related to identifying, clustering and tracking documents about their topics. Despite documents often display structural organization and contain explicit section markers, some lack of such properties thereby revealing the need for topical text segmentation systems. Examples of such documents are speech transcripts and inherently unstructured texts like newspaper columns and blog entries discussing several subjects in a discourse. A novel local-context based approach depending on lexical cohesion is presented for linear text segmentation, which is the task of dividing text into a linear sequence of coherent segments. As the lexical cohesion indicator, the proposed technique exploits relationships among terms induced from semantic space called HAL (Hyperspace Analogue to Language), which is built upon by examining the co-occurrence of terms through passing a fixed-sized window over text. The proposed algorithm (BTS) iteratively discovers topical shifts by examining the most relevant sentence pairs in a block of sentences considered at each iteration. The technique is evaluated on both error-free speech transcripts of news broadcasts and documents formed by concatenating different topical regions of text. A new corpus for Turkish is automatically built where each document is formed by concatenating different news articles. For performance comparison, two state-of-the-art methods, TextTiling and C99, are leveraged and the results show that the proposed approach has comparable performance with these two techniques. The results are also statistically validated by applying the ANOVA and Tukey post-hoc test. Keywords: Text Segmentation, Topic Segmentation, Natural Language Processing, Lexical Cohesion, Semantic Relatedness."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"DNA dizileme teknolojileri genomların insan hayatını nasıl etkilediği, organizma evrimini ve aynı türler arasındaki genetik ilişki bilgilerini oluşturarak bilim adamlarını yönlendirecek büyük umutlar vaat ediyor. Günümüzde genom dizileme verilerini elde etmek eski teknolojilere göre daha hızlı ve ucuz olmasına rağmen henüz dizileme hataları aşılamamıştır. Bu verilerin birleştirilerek yüksek kalitede bitmiş genom dizilimi elde etmek zorlu bir süreçtir. Günümüzde bir ̧cok genom birleştirme algoritmaları mevcuttur fakat performans ve cıktıları açısından farklıdırlar. Daha da önemlisi, birleştirilmiş genom dizilimlerinin kalitesini de ̆gerlendirmek büyük ölçüde belirsizdir. Bu tezde, aynı insan genomundan oluşturulmuş iki farklı tipteki verileri kullanarak bir takım genom iskeleme algoritmalarının hassasiyetini inceledik: (i) genom saçma dizileme (WGS), (ii) havuzlanmış klon dizileme (PCS). Eğer PCS verisi kullanılırsa WGS verisine göre toplam birleştirme uzunluğu daha fazla ve daha az iskele sayısı elde etmenin mümkün olduğunu gözlemledik. Fakat ̧su anki iskeleme algoritmaları sadece WGS için geliştirilmiştir ve PCS ile iskeleme algoritmaları hala çözülmemiş bir problemdir.","The DNA sequencing technologies hold great promise in generating information that will guide scientists to learn more about how the genome affects human health, organismal evolution, and genetic relationships between individuals of the same species. The process of generating raw genome sequence data becomes cheaper, faster, but more error prone. Assembly of such data into high-quality, finished genome sequences remains challenging. Many genome assembly tools are available, but they differ in terms of their performance, and in their final output. More importantly, it remains largely unclear how to best assess the quality of assembled genome sequences. In this thesis, we evaluated the accuracies of several genome scaffolding algo- rithms using two different types of data generated from the genome of the same human individual: i) whole genome shotgun sequencing (WGS), and ii) pooled clone sequencing (PCS). We observed that, it is possible to obtain less number of scaffolds with longer total assemble length if PCS data is used, compared to using only WGS data. However, the current scaffolding algorithms are developed only for WGS, and PCS-aware scaffolding algorithms remain an open problem."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Metin tipindeki veri kümesi sayısı her geçen gün akıl almaz bir şekilde artmaktadır. Bu durum, bu büyük metin veri kümelerinden el değmeden bilgisayarlar yardımıyla ve hızla kısa özetler çıkarmayı kaçınılmaz hale getirmektedir. Büyük metin veri kümelerinden, onların bilinmeyen, saklı konularını belirlemeye çalışan birçok çalışma olsa da, bunların hepsi sözcük torbası modelini kullanmışlardır. Bu çalışma, metin veri kümelerindeki bilinmeyen, saklı konuları ve bu konulara ait olasılık dağılımlarını ortaya çıkaran yeni bir gözetimsiz öğrenme metodu sunmaktadır. Bu çalışmaya göre veri kümesinde bulunan metinler, her tümcenin tek bir konudan türetildiği ve ardışık tümcelerin konularının bir gizli Markov zinciri oluşturduğu türetici bir çizgisel model tarafından açıklanmaktadır. Sözcük torbası modelinin tersine, önerdiğimiz model tümceyi metnin en küçük yapıtaşı olarak ele alır ve aynı tümce içerisindeki sözcüklerin birbirine anlamca sıkı sıkıya bağlı olduğunu, birbirini takip eden tümcelerin konularının ise yavasça değiştiğini kabul eder. Önerilen modelin uygulama sonuçları hem konu dağılımlarının en olası kelimelerini ve tümcelere atanan konuları inceleyerek nitel, hem de modelin genelleştirme başarımını ölçerek nicel bir şekilde değerlendirilmektedir. Anahtar sozcukler: olaslksal cizgisel model, konu modeli, gizli Markov modeli, Markov zincirleri Monte Carlo","Fast augmentation of large text collections in digital world makes inevitable to automatically extract short descriptions of those texts. Even if a lot of studies have been done on detecting hidden topics in text corpora, almost all models follow the bag-of-words assumption. This study presents a new unsupervised learning method that reveals topics in a text corpora and the topic distribution of each text in the corpora. The texts in the corpora are described by a generative graphical model, in which each sentence is generated by a single topic and the topics of consecutive sentences follow a hidden Markov chain. In contrast to bag-of-words paradigm, the model assumes each sentence as a unit block and builds on a memory of topics slowly changing in a meaningful way as the text flows. The results are evaluated both qualitatively by examining topic keywords from particular text collections and quantitatively by means of perplexity, a measure of generalization of the model. Keywords: probabilistic graphical model, topic model, hidden Markov model, Markov chain Monte Carlo."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"The common approach for implementing social networks has been using centralized infrastructures, which inherently include problems of privacy, censorship, scalability, and fault-tolerance. Although decentralized systems offer a natural solution, significant research is needed to build an end-to-end peer-to-peer social network where data is stored among trusted users. The centralized algorithms need to be revisited for a P2P setting, where the nodes have connectivity to only neighbors, have no information of global topology, and may go offline and churn resulting in changes of the graph structure. The social graph algorithms should be designed as robust to node failures and network changes. We model P2P social networks as uncertain graphs where each node can go offline, and we introduce link recommendation algorithms that support the development of decentralized social networks. We propose methods to recommend top-k links to improve the underlying topology and efficiency of the overlay network, while preserving the locality of the social structure. Our approach aims to optimize the probabilistic reachability, improve the robustness of the local network and avoid loss from failures of the peers. We model the problem through discrete optimization and assign a score to each node to capture both the topological connectivity and the social centrality of the corresponding node. We evaluate the proposed methods with respect to performance and quality measures developed for P2P social networks.","Sosyal ağları hayata geçirmek için kullanılan merkezi altyapılar beraberinde gizlilik, sansür, ölçeklenebilirlik ve hataya dayanıklılık sorunlarını getirmektedir. Dağıtılmış sistemler sosyal ağlar için doğal bir çözüm sunsa da, bir uçtan uca sosyal bir ağ oluşturmak için ciddi bir araştırma gereklidir. Merkezi algoritmalar P2P altyapısı kullanıldığında yeniden ele alınmalıdır çünkü P2P altyapıda kişiler sadece komşularını bilmekte, tümçizgeye ait bilgiden yoksun ve zaman zaman çevrimdışı olabilmektedirler. Sosyal ağ algoritmaları kullanıcıların çevrimdışı kaldığı ve ağın değiştiği durumlara karşı sağlam birşekilde tasarlanmış olmalıdır. Biz sosyal ağı, kişilerin zaman zamançevrim dışı olabildiği, belirsiz çizgeler olarak tanımlıyoruz ve bu ağların gelişmesini sağlamak için bağlantı öneri algoritmalarını sunuyoruz. Varolan sosyal ağı geliştirmek için en iyi k tane bağlantı önerisi yaparken sosyal ağın ve yerel yapıların korunması için çalışıyoruz. Hedefimiz olasılığa bağlı ulaşılabilirliği eniyileyerek yerel ağ sağlamlığını artırmak ve kayıplardan doğan hataları en aza indirmektir. Bu problemi her kişiye topolojik bağlılık ve sosyal ağdaki durumuna göre puanlama olarak modelliyoruz. Sunduğumuz yöntemleri geliştirdiğimiz performans ve nitelik ölçüleri ile değerlendiriyoruz."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Güvenlik açığı içeren kötü tasarımlı bir tarayıcı eklentisi bütün bir sistemi saldırgana açık hale getirebilir. Bu sebeple ""iyi niyetli fakat hatalı"" ve kötü niyetle yazılmış eklentiler bunları çalıştıran sistemlere ciddi tehditler oluşturur. Yapılan çalışmalar birçok Firefox eklentisinin gereğinden fazla yetkiye sahip olduğunu gösteriyor ve bu onları saldırganlar için cazip yapıyor. Malesef kullanıcıların kendilerini bu tür zararlı eklentilerden korumak için pek seçenekleri yok. Halihazırda bir kere yüklenip çalıştırıldığında, eklentiye güvenilmesi gerekiyor. Bu tezde varolan JavaScript Firefox eklentilerine kullanıcının hassas bir şekilde yetki kontrolü ve dayatması yapabilmesini sağlayan SENTINEL'i sunuyoruz. Kullanıcı bu şekilde poliçe tanımlayarak yahut önceden tanımlanmış policeleri kullanarak çeşitli yaygın saldırıları önleyebiliyor. Veri kaçırma, uzaktan kod çalıştırma, kaydedilmiş şifre çalma ve tercihlerin değiştirilmesi bu yaygın saldırıların örnekleri. Değerlendirmemiz gösteriyor ki sunduğumuz SENTINEL prototipi başarılı bir şekilde gerçek senaryolarda olan Firefox eklenti saldırılarından kullanıcıları engelleyici olmaksızın koruyor.","A poorly designed web browser extension with a security vulnerability may expose the whole system to an attacker. Therefore, attacks directed at ``benign-but-buggy'' extensions, as well as extensions that have been written with malicious intents pose significant security threats to a system running such components. Recent studies have indeed shown that many Firefox extensions are over-privileged, making them attractive attack targets. Unfortunately, users currently do not have many options when it comes to protecting themselves from extensions that may potentially be malicious. Once installed and executed, the extension needs to be trusted. This thesis introduces SENTINEL, a policy enforcer for the Firefox browser that gives fine-grained control to the user over the actions of existing JavaScript Firefox extensions. The user is able to define policies (or use predefined ones) and block common attacks such as data exfiltration, remote code execution, saved password theft, and preference modification. Our evaluation of SENTINEL shows that our prototype implementation can effectively prevent concrete, real-world Firefox extension attacks without a detrimental impact on users' browsing experience."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Makinalar arası iletişim (M2M), akıllı ev eşyaları dahil çeşitli türdeki cihazların oluşturduğu geniş ölçekli ağlara olanak sağlar. Ev M2M ağları için kritik mesele, ev eşyalarının IP tabanlı bir kablosuz M2M ağına enerji verimli ve şu anki cihazlarda minimum değişikle nasıl entegre edileceğidir. Ev M2M ağlarının hızlı entegrasyonu ve yaygınlaşması için Wi-Fi ideal bir çözüm olabilir çünkü günümüzde Wi-Fi ev ağlarında popüler ve yaygın olarak kullanılmaktadır. Fakat, Wi-Fi'ın şu anki alt yapısının M2M ağlarına uygun olması için enerji tasarruf mekanizmalarına ihtiyacı vardır. Bu tez, Wi-Fi teknolojisinin enerji verimli bir şekilde akıllı ev sistemlerinde kullanılacak gömülü cihazlara nasıl entegre edileceğini gösterir. Tezde, ev M2M ağlarında enerji tasarrufu için IEEE 802.11 güç tasarruf modu (PSM) ve uyku zamanlama algoritması tabanlı yeni bir yaklaşım önerilmektedir. Bu yaklaşım, IEEE 802.11 MAC katmanında değişiklik yapılmadan gerçeklenir. Yaklaşımın gerçeklenmesinde cihazların erişilebilirliği etkilenmeksizin mDNS protokol mesajlarından da faydalanılır. Tezdeki önerilen yaklaşım, bir ev M2M ağına prototip olarak uygulanmıştır. Bu prototiplerle ve hazırlanan simulasyonlarla, önerilen yaklaşımın performans değerlendirmesi yapılmıştır. Prototip üzerinde yapılan testlerin ve simulasyonların sonuçlarna göre, herhangi bir enerji tasarruf mekanizması kullanmayan standart altyapıya kıyasla yaklaşık %70, IEEE 802.11 güç tasarruf moduna kıyasla %20 enerji tasarrufu sağlandığı görülmüştür.","Machine-to-machine communications (M2M) technology enables large-scale communication and networking of devices of various kinds including home devices and appliances. A critical issue for home M2M networks is how to efficiently integrate the already existing home consumer devices and appliances into an IP based wireless M2M network with least modifications to existing components. Due to its popularity and widespread usage in closed spaces, Wi-Fi is a good alternative as a wireless technology to enable M2M networking for home devices. This thesis addresses the energy-efficient integration of home appliances to a Wi-Fi and IP based home M2M network. Towards this goal, we first propose an integration architecture that requires least modifications in existing components. Then, we propose a novel long-term sleep scheduling algorithm to be applied together with the existing 802.11 power save mode (PSM). The proposed scheme utilizes the multicast DNS (mDNS) protocol to maintain device and service availability when devices go into deep sleep mode. We implemented our proposed architecture and algorithm as a prototype to build an M2M network of home appliances as a test-bed. We performed various experiments on this test-bed to evaluate the proper operation and energy savings of our proposal. We also did extensive simulation experiments for larger-scale scenarios. As a result of our test-bed and simulation experiments, we observed energy savings up to 70% compared to the existing infrastructure which applies no sleep mechanism, and up to 20% compared to standard 802.11 PSM scheme, while ensuring device and service availability at the same time."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Artırılmış gerçeklik (AG), gerçek ortamların sanal nesnelerle zenginleştirilmesidir. AG kullanıcı deneyimi ve etkileşimini geliştirmek için çeşitli şekillerde kullanılmaktadır. Eğitim uygulamaları, mimari görselleştirmeler, askeri eğitim senaryoları ve eğlence amaçlı uygulamalar çoğu zaman daha gerçekçi bir tecrübe yaşatmak amacıyla artırılmış gerçeklik yöntemleriyle desteklenir. Taşınabilir cihazların gelişmesi ve ucuzlamasıyla birlikte bu tarz uygulamalar oldukça yaygın hale gelmiştir. Daha doğal AG tecrübesi yaşatmak adına dışsal kamera parametrelerinin hatasız, güvenilir ve hızlı bir biçimde hesaplanması gerekir. Böylece, sanal nesneler gerçek ortam görüntülerinin üzerine isabetli bir şekilde yerleştirilebilir. Dışsal kamera parametrelerinin gerçek zamanlı tespiti zordur. Birçok kamera takip çatısında, kamera parametrelerinin hesaplanması için temel yöntem olarak görsel takip algoritmaları kullanılmaktadır. Görsel takip sistemlerinde anahtar nokta ve kenar özellikleri poz hesabı için sıkça kullanılır. Zengin kaplamalı ortamlarda anahtar nokta tabanlı metotlar iyi sonuçlar vermektedir ve yaygınca kullanılmaktadır. Buna karşılık, kenar tabanlı takip yöntemleri geometri bakımından zengin fakat kaplama açısından zengin olmayan ortamlarda daha çok tercih edilir. Kenar tabalı takip yöntemiyle yapılan poz hesabı genel olarak model kenarlarında belirlenen kontrol noktalarını kullanır. Hatasız takip için bu noktaların görünürlüğü doğru bir şekilde hesaplanmalıdır. Kontrol noktalarının görünürlüğünün belirlenme süreci hesaplama açısından masraflıdır. Kontrol noktalarının görünürlüğünü önceden hesaplayarak kenar tabanlı takip yönteminin hesaplama masrafını düşürmek için bir yöntem öneriyoruz. Bu amaçla, ön işleme aşamasında dünya koordinat düzleminde belirlenmiş kalıcı kontrol noktaları kullanmaktayız. Ek olarak, kalıcı kontrol noktalarının ekranda daha düzenli dağılımını sağlayabilmek adına daha isabetli olan uyarlanabilir bir izdüşüm yöntemi kullanıyoruz. Önerdiğimiz algoritmanın performans ve kalitesini ölçmek için kamera takipçimizi çeşitli ortamlarda test ettik. Önceden işlenmiş görünürlük değerleri, takip kalitesinden ödün vermeden sabit zamanlı görünürlük hesabını mümkün kılmıştır. Geliştirdiğimiz AG çatısı kullanıcı etkileşimi bulunan bir uygulamayla sunulmuştur. Bahsi geçen uygulama çatısı, yaygın olarak kullanılan ticari bir oyun motoru için geliştirilmiştir.","Augmented reality (AR) is the enhancement of real scenes with virtual entities. It is used to enhance user experience and interaction in various ways. Educational applications, architectural visualizations, military training scenarios and pure entertainment-based applications are often enhanced by augmented reality to provide more immersive and interactive experience for the users. With hand-held devices getting more powerful and cheap, such applications are becoming very popular. To provide natural AR experiences, extrinsic camera parameters (position and rotation) must be calculated in an accurate, robust and efficient way so that virtual entities can be overlaid onto the real environments correctly. Estimating extrinsic camera parameters in real-time is a challenging task. In most camera tracking frameworks, visual tracking serve as the main method for estimating the camera pose. In visual tracking systems, keypoint and edge features are often used for pose estimation. For rich-textured environments, keypoint-based methods work quite well and heavily used. Edge-based tracking, on the other hand, is more preferable when the environment is rich in geometry but has little or no visible texture. Pose estimation for edge based tracking systems generally depends on the control points that are assigned on the model edges. For accurate tracking, visibility of these control points must be determined in a correct manner. Control point visibility determination is computationally expensive process. We propose a method to reduce computational cost of the edge-based tracking by preprocessing the visibility information of the control points. For that purpose, we use persistent control points which are generated in the world space during preprocessing step. Additionally, we use more accurate adaptive projection algorithm for persistent control points to provide more uniform control point distribution in the screen space. We test our camera tracker in different environments to show the effectiveness and performance of the proposed algorithm. The preprocessed visibility information enables constant time calculations of control point visibility while preserving the accuracy of the tracker. We demonstrate a sample AR application with user interaction to present our AR framework, which is developed for a commercially available and widely used game engine."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sayısal patolojide, etkin görüntü temsili tasarımı, gürbüz otomatik tanı sistemlerinin geliştirilmesi için çok önemlidir. Histopatolojik görüntüler tipik olarak piksel seviyesinde kayda değer gürültü içerebilmektedir. Bundan dolayı, birçok çalışma, pikselleri doğrudan kullanmak yerine, nesne tabanlı gösterimlerin geliştirilmesini önermiştir. Bu önceki çalışmalar, çoğunlukla, görüntüdeki histolojik doku bileşenlerini yaklaşık temsil edecek şekilde renk bilgisini temel alan nesneler tanımlar ve tanımlanan bu nesnelerin uzamsal dağılımını görüntü temsili ve sınıflandırması için kullanır. Bu nedenle, nesne tanımlamasının, görüntü temsilinde doğrudan etkisi vardır, dolayısıyla da sınıflandırma doğruluğunu etkilemektedir. Bu tezde, histopatolojik görüntülerin etkin temsili ve sınıflandırması için yeni bir model sunmaktayız. Bu modelin iki tane katkısı bulunmaktadır. İlk olarak; görüntüler üzerinde çok tipli nesnelerin tanımlanması için, iki katmanlı yeni bir doku ayrıştırma yöntemi sunmaktadır. Önceki çalışmalardan farklı olarak, nesneler; desen, şekil ve boyut bilgileri birleştirilerek tanımlanır ve elde edilen bu nesneler, tek bir histolojik bileşene ya da farklı özellikteki doku alt bölgelerie karşılık gelebilir. İkinci katkı olarak, sunduğumuz bu yeni model, verilen nesnenin şekil ve boyutunu tek bir skalar değer olarak karakterize etmek için, ""baskın blob ölçeği"" adını verdiğimiz yeni bir metrik tanımlar. Kolon doku görüntüleri üzerinde yaptığımız deneyler, bu yeni nesne tanımlaması ve karakterizasyonunun, normal ve kanserli histopatolojik görüntülerinin ayırt edici bir temsiline olanak verdiğini ortaya koymaktadır. Bu ise, önceki çalışmalara göre, daha yüksek doğrulukta sınıflandırma sonuçlarının elde edilmesinde etkindir.","In digital pathology, devising effective image representations is crucial to design robust automated diagnosis systems. To this end, many studies have proposed to develop object-based representations, instead of directly using image pixels, since a histopathological image may contain a considerable amount of noise typically at the pixel-level. These previous studies mostly define their objects, based on the color information, as to approximately represent histological tissue components in an image and then use the spatial distribution of these objects for image representation and classification. Thus, object definition has a direct effect on the way of representing the image, which in turn affects classification accuracies. In this thesis, we present a new model for effective representation and classification of histopathological images. The contributions of this model are twofold. First, it introduces a new two-tier tissue decomposition method for defining a set of multi-typed objects in an image. Different than the previous studies, these objects are defined combining the texture, shape, and size information and they may correspond to individual histological components as well as tissue sub-regions of different characteristics. As its second contribution, it defines a new metric, which we call ""dominant blob scale"", to characterize the shape and size of an object with a single scalar value. Our experiments on colon tissue images reveal that this new object definition and characterization provides distinguishing representation of normal and cancerous histopathological images, which is effective to obtain more accurate classification results compared to its counterparts."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mu ̈zik Enstru ̈manlarında kaynak ayırma i ̧slemi, bir ses karı ̧sımı i ̧cindeki ses- leri ayırmayı hedefler. Bu ̧calı ̧smada, aynı anda, aynı sabit notayı ̧calan, iki enstru ̈mandan olu ̧san karı ̧sımlar u ̈zerinde ̧calı ̧sılmı ̧stır. Mu ̈zik Enstru ̈manları aynı notayı ̧caldıg ̆ında olu ̧san sesler, birbirleri i ̧cinde karı ̧sıp tek bir sesmi ̧s gibi algılanırlar. Bu c ̧alı ̧smada, bu sesleri ayırmak i ̧cin istatistiksel kaynak ayırma algoritmaları kullanılmı ̧stır. Bu algoritmalar sesler arasındaki istatistik- sel bag ̆ımlılıg ̆ı maksimum yapmaya c ̧alı ̧sarak ses ayırma i ̧slemini ger ̧cekle ̧stirirler. Bir karı ̧sım birden c ̧ok mikrofon kullanılarak kayıt edilebilir; bu durum kayıtları birbiriyle kar ̧sıla ̧stırarak enstru ̈manlarla ilgili konum bilgisinin ̧cıkarılmasını sag ̆lar. Burada tek mikrofon ile alınmı ̧s kayıtlar kullanılmı ̧stır. Bazı mu ̈zik en- stru ̈manlarında genlik modu ̈lasyonu vardır ve bu modu ̈lasyon bir karı ̧sım i ̧cinde de go ̈zlemlenebilir. Bu ̧calı ̧smada, genlik modu ̈lasyonu tespit edilerek kaynak ayırma kalitesi arttırılmaya ̧calı ̧sılmı ̧stır. Kaynak ayırma i ̧slemi i ̧cin NTF (Non- negative Tensor Factorization) algoritması kullanılmı ̧stır. NTF, karı ̧sımı birden ̧cok bile ̧sene ayırır. Kaynak sesleri tekrar elde etmek i ̧cin bu bile ̧senlerin uygun bir ̧sekilde birle ̧stirilmesi gerekir. Bu birle ̧stirme i ̧slemi i ̧cin ise k-means demetleme al- gorithması ve bunun yanında el ile birle ̧stirme yapılmı ̧stır. El ile birle ̧stirme i ̧slemi i ̧cin bile ̧senlerin SDR (Signal to Distortion Ratio) deg ̆erleri orjinal kaynaklar ile kar ̧sıla ̧stırılmı ̧stır.","Musical Instrument Source Separation aims to separate the individual instru- ments from a mixture. We work on mixtures where there are two instruments, playing the same constant pitch at the same time. When musical instruments play the same note, they overlap with each other and act as a single source. We use statistical source separation algorithms which perform separation by maximizing the statistical independence between the sources. A mixture can be recorded with more than one microphone; this enables us to extract spatial information of the instruments by comparing the recordings. However we work with the monaural case where there is one microphone. Some musical instruments have amplitude modulation and this modulation can be seen in the mixtures. We also aim to detect amplitude modulations to support the source separation success. We use NTF (Non-negative tensor factorization) to perform the separation. NTF sepa- rates the mixture into many components. These components should be clustered in order to synthesize the individual sources. We use k-means as well as man- ual clustering by comparing the SDR (Signal to Distortion Ratio) values of the components with the original sources."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Serviks kanseri dünya üzerinde kadınlarda en sık görülen ve kanser ölümlerine sebep olan ikinci kanser çeşididir. Serviks kanseri prekanseröz aşamalarda erken teşhis ve tedavi ile önlenebilmektedir. Pap smear testi, serviks hücrelerinde meydana gelen displastik değişiklikleri belirlemek üzere kullanılan yaygın, etkili ve kullanımı kolay manuel bir tarama yöntemidir. Ancak Pap smear testlerinde bulunan binlerce hücrenin sitologlar tarafından manuel olarak analiz edilmesi zorlu, zaman alan ve gözlemci öznelliği içeren bir süreçtir. Çalışmamızda, bu sorunların üstesinden gelmek için tarama işlemini otomatikleştirmeyi ve sitologlara yardımcı olacak hücrelerin sıralanmış listesini sağlamayı amaçladık. Tarama sürecini otomatikleştirme, karmaşık hücre yapılarından dolayı uzun süreli ve zorlu bir görev olarak durmaktadır. Literatürdeki mevcut yöntemler çoğunlukla problemi tekli ve ayrılmış hücre bölütlemesi olarak ele almakta ve Pap smear test görüntülerinin, zayıf kontrast, tutarsız boyama ve bilinmeyen hücre sayısı gibi gerçek sorunlarına değinmemektedirler. Bu tezde, Pap smear görüntülerindeki hücrelerin doğru bir biçimde bölütlenmesi ve anormallik derecelerine göre sıralanması için öğreticisiz bir yöntem önerilmektedir. Önerilen yöntem ilk olarak çekirdeklerin doğru bir şekilde elde edilmesi için çoklu-ölçekli hiyerarşik bölütleme algoritması kullanmaktadır. Yüksek büyütme değeri ile çekilen Pap smear görüntüleri daha detaylı doku bilgisine ancak daha kötü kontrast değerine sahiptirler. Kontrast bölütleme aşaması için önemli bir özellik iken, detaylı doku bilgisi öznitelik çıkarma aşaması için önemli bir özelliktir. Bu nedenle, çalışmamızda bölütleme problemine bir çözüm olarak, iki aşamada ilerledik. İlk olarak, Pap smear görüntüleri düşük büyütme (20x) seviyesinde bölütlendi ve çıkarılan çeşitli özniteliklere dayanarak çekirdek olmayan bölütlenmiş alanlar elendi. Daha sonra, yüksek seviyede (40x) çekilen Pap smear görüntülerine geçilerek kalan çekirdeklerin daha detaylı bölütlenmesi gerçekleştirildi. Bölütleme aşamasının ardından, elde edilen her çekirdek için öznitelikler çıkarıldı. Literatürdeki sınıflandırma için öğrenme aşaması gerektiren ilgili çalışmalardan farklı olarak, yöntemimiz 40x büyütme oranındaki görüntülerden çıkarılan özniteliklere dayanarak çekirdeklerin öğreticisiz olarak sıralamasını gerçekleştirmektedir. Farklı sıralama algoritmaları, elde edilen çekirdeklerin anormallik derecelerine göre sıralanması üzerinden karşılaştırıldı. Bölütleme ve sıralama yöntemlerimizi iki veri kümesi kullanarak değerlendirdik. Sonuçlarımız önerilen yöntemlerin hücrelerin hem bölütlenmesi hem de sıralanması aşamasında gelecek vaat eden sonuçlar verdiğini gösterdi.","Cervical cancer is the second most common cause of cancer death among women worldwide, and it can be prevented if it is detected and treated in the pre-cancerous stages. Pap smear test is a common, efficient and easy manual screening examination technique which is used to detect dysplastic changes in cervical cells. However, manual analyses of thousands of cells in Pap smear test slides by cyto-technicians is difficult, time consuming and subjective. To overcome these problems, we aim to automate the screening process and provide an ordered nuclei list to help the cyto-experts. Automating the screening procedure has been a longstanding challenge because of complex cell structures where current methods in the literature mostly consider the problem as the segmentation of single isolated cells and leave real challenges of Pap smear images such as poor contrast, inconsistent staining, and unknown number of cells unaddressed. We propose an unsupervised method to accurately segment the nuclei and order them according to their abnormality degree in Pap smear images. The method first uses a multi-scale hierarchical segmentation algorithm for accurate identification of the nuclei. The Pap smear images captured at high level magnification have more detailed texture but worse contrast. Contrast is an important property for segmentation and detailed texture is an important property for feature extraction. Therefore, as a solution to the segmentation problem, we proceed in two steps. First, we segment the Pap smear images at low (20x) magnification and eliminate non-nucleus regions based on several features. Then, we switch to high (40x) magnification and obtain a more detailed segmentation of the remaining nuclei. Following segmentation, we extract features for each resulting nucleus. Unlike related works that require a learning phase for classification, our method performs an unsupervised ordering of the nuclei based on features extracted at 40x magnification. We compare different ordering algorithms for ranking the nucleus regions according to their abnormality degrees. We evaluate our segmentation and ordering methods using two data sets. Our results show that the proposed method provides promising results for both segmentation and ordering steps."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda zengin içerikli çok büyük ağlardaki artış kompleks/sosyal ağ analizine dönük ilgiyi yeniden artırmıştır. Söz konusu analizler bir taraftan büyük çapta sosyal etkileşimleri anlamayı mümkün kılarken diğer taraftan O(n) üzeri kompleksitiye sahip algoritmalara dayalı önceki çalışmalarda sorun oluşturmaktadır. Bu tez önemli parametrelerini ve özelliklerini etkin ve verimli bir şekilde bulmak amacıyla büyük veri platformu kullanarak çok büyük ölçekli sosyal ağları analiz eder. Mobil telefon kullanımının popülerleşmesi ile birlikte telekomünikasyon ağları sosyal bağlayıcı ortamlara dönüşmüştür ve araştırmacıların sosyal etkileşimleri çok büyük ölçekte analiz etmesine olanak sağlamıştır. Derece dağılımları sosyal ağların en önemli karakteristikleri arasında yer alır ve büyük ölçekli sosyal ağlarda derece karakteristiği ile yapısal özellikleri araştırmak için biz bu tezde öncelikle tera-ölçekli bir telekomünikasyon arama detay kaydı veriseti derledik. Biz bu veriyi kullanarak bazı istatistik modelleri ülke çağrı çizgesi derece dağılımına karşı deneysel olarak değerlendirdik ve literatürdeki \enquote{power-law en iyi modeldir} iddalarına karşın, Pareto log-normal dağılımının en iyi uyumu sağladığına karar verdik. Ayrıca, sosyal ağlarda derece dağılımını yöneten parametreleri anlamak amacıyla, ağ operatörünün, büyüklüğünün, yoğunluğunun ve lokasyonunun derece dağılımını nasıl etkilediğini sorgulayıp ve cevap elde ettik. Yapısal özellik analizi dışında, bir sosyal ağda farklı konularda çok bağlantılı alt ağları bulmak için yapılan topluluk tespiti çalışmaları pratikte büyük ilgi çekmektedir. Çizge teorisinde, $k$-core çizgenin `yoğun' alanları olarakta bilinen çok bağlantılı alt çizgelerin tespiti için kullanılan anahtar bir ölçüttür. Sosyal ağ çizgeleri gibi gerçek dünya çizgeleri boyut yönünden büyüyüp, içerik yönünden zenginleşip ve topolojiler dinamik olarak değiştikçe, yalnız k-core altçizgesini bir defalığına hesaplama problemi ile değil ayrıca bunu dinamik değişikliklere göre güncel tutma problemi ile karşılaştık. Bu zorluklar bize yatay ölçeklenebilir saklama ve hesaplama platformu üzerinde $k$-core görüntü hesaplama ve sürdürme amaçlı bir takım algoritmalar önerme konusunda esin vermiştir. önerdiğimiz algoritmaların deneysel değerlendirme sonuçları bütün yeniden hesaplama yaklaşımına göre aşamalı ve yığın olarak $k$-core sürdürme avantajı ile birlikte birkaç basamak hızlandırma göstermiştir. Bununla birlikte, topluluğa katılımın yoğunluğu birçok seviyede seçilebilir ki bu da zamanla sürdürülmesi gerekli çok-çözünürlüklü topluluk gösterimini sonuç doğurur. Bu nedenle biz ayrıca çoklu-$k$-core çizgesi hesaplayıp sürdürecek Apache HBase ölçeklenebilir büyük-veri platformunda uygulanmış dağıtık algoritmalar önerdik. Deneysel değerlendirme sonuçları aşamalı çoklu-$k$-core sürdürmenin bütün yeniden hesaplamaya göre birkaç basamak hızlandırma sağladığını göstermiştir. Diğer taraftan, dağıtık çizge işleme amaçlı tasarlanmış bir çizge-bilinçli önbellek sistemi önerdik. Deney sonuçları geleneksel LRU bazlı sistemlerle karşılaştırıldığında 15 kata kadar hızlanma göstermiştir.","In recent years, the rise of very large, rich content networks re-ignited interest to complex/social network analysis at the big data scale, which makes it possible to understand social interactions at large scale while it poses computation challenges to early works with algorithm complexity greater than O($n$). This thesis analyzes social networks at very large-scales to derive important parameters and characteristics in an efficient and effective way using big-data platforms. With the popularization of mobile phone usage, telecommunication networks have turned into a socially binding medium and enables researches to analyze social interactions at very large scales. Degree distribution is one of the most important characteristics of social networks and to study degree characteristics and structural properties in large-scale social networks, in this thesis we first gathered a tera-scale dataset of telecommunication call detail records. Using this data we empirically evaluate some statistical models against the degree distribution of the country's call graph and determine that a Pareto log-normal distribution provides the best fit, despite claims in the literature that power-law distribution is the best model. We also question and derive answers for how network operator, size, density and location affect degree distribution to understand the parameters governing it in social networks. Besides structural property analysis, community identification is of great interest in practice to learn high cohesive subnetworks about different subjects in a social network. In graph theory, $k$-core is a key metric used to identify subgraphs of high cohesion, also known as the `dense' regions of a graph. As the real world graphs such as social network graphs grow in size, the contents get richer and the topologies change dynamically, we are challenged not only to materialize $k$-core subgraphs for one time but also to maintain them in order to keep up with continuous updates. These challenges inspired us to propose a new set of distributed algorithms for k-core view construction and maintenance on a horizontally scaling storage and computing platform. Experimental evaluation results demonstrated orders of magnitude speedup and advantages of maintaining $k$-core incrementally and in batch windows over complete reconstruction approaches. Moreover, the intensity of community engagement can be distinguished at multiple levels, resulting in a multiresolution community representation that has to be maintained over time. We also propose distributed algorithms to construct and maintain a multi-$k$-core graphs, implemented on the scalable big-data platform Apache HBase. Our experimental evaluation results demonstrate orders of magnitude speedup by maintaining multi-$k$-core incrementally over complete reconstruction. Furthermore, we propose a graph aware cache system designed for distributed graph processing. Experimental results demonstrate up to 15x speedup compared to traditional LRU based cache systems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde kullanımı büyük oranda artan video verileri araştırmacıları bu verilerden elde edilebilecek ipuçlarını kullanmaya yöneltmiştir. Çünkü yazısal ipuçlarının günümüzde görsel ipuçları kadar başarılı sonuçlar veremediği gözlemlenmiştir. Bu soruna büyük ölçekli resim ve video verilerini çıkarımız için kullanarak, videolardaki önemli karakteristik bilgileri bularak yaklaşıyoruz. Bu tezde üç farklı konuya odaklanılmaktadır. Bunlari olağan dışı olaylardaki ortak motifleri bulmak, geniş ölçekli multimedya olay tespit edilmesi ve videoların anlamsal dizinlenmesi olarak isimlendirebiliriz. İstenmeyen olayların gerçekleşmesinin bildiricisi olduğu için, olağan dışı olaylarin erken tespit edilmesi gerekli görülmektedir. Bu konuya genellikle sıradan olayların motiflerinin bulunması ile yaklaşılmaktadır. Elimizdeki bu zorlu problemi çözümlemek için videolardaki hızlı hareketleri yakalayabilen orijinal bir tanımlayıcı, piksel yörüngelerinden yoğun aralıklar ile çıkartılarak sunulmaktadır. Sunulan tanımlayıcı, yörünge parça seleleri, olağan dışı videoları sıradan videolardan ayırt etmek için kullanılmaktadır. Daha sonra olağan dışı olarak belirlenen videoların fotoğraf kareleri ile gösterimi için diğer bir yöntem kullanılmaktadır. Daha sonra TRECVID video erişim değerlendirmesinin bir parçası olan Multimedya Olay Tespiti olarak adlandırılan problemi ele almaktayız. Bu probleme videoları prototipler ile temsil ederek yaklaşmaktayız. Prototipler olayların farklı görsel karakteristik özelliklerini temsil eden modellerdir. Son olarak, TRECVID'in bir diğer parçası olan Anlamsal Dizinleme problemine, İnternet'den topladığımız resimleri kavramları modellemek için kullanarak yaklaşmaktayız.","The large amount of video data shared on the web resulted in increased interest on retrieving videos using usual cues, since textual cues alone are not sufficient for satisfactory results. We address the problem of leveraging large scale image and video data for capturing important characteristics in videos. We focus on three different problems, namely finding common patterns in unusual videos, large scale multimedia event detection, and semantic indexing of videos. Unusual events are important as being possible indicators of undesired consequences. Discovery of unusual events in videos is generally attacked as a problem of finding usual patterns. With this challenging problem at hand, we propose a novel descriptor to encode the rapid motions in videos utilizing densely extracted trajectories. The proposed descriptor, trajectory snippet histograms, is used to distinguish unusual videos from usual videos, and further exploited to discover snapshots in which unusualness happen. Next, we attack the Multimedia Event Detection (MED) task. We approach this problem as representing the videos in the form of prototypes, that correspond to models each describing a different visual characteristic of a video shot. Finally, we approach the Semantic Indexing (SIN) problem, and collect web images to train models for each concept."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizgeler, ilişkisel bilgiyi modellemek amacıyla yazılım mühendisliğinde, ilişkisel veritabanlarında, sosyal ve biyolojik ağların gösteriminde yaygın olarak kullanılırlar. Bilginin etkin kullanımı için otomatik yerleştirme, interaktif ortamda değişiikliklerin yapılabilmesi ve kalabalık çizgelerin karmaşıklıklarının yönetilmesi, çizge gösterimi için gerekli yöntemlerdir. Çizge gösteriminde interaktif ortamda değişiklik yapma ve diyagramlama teknikleri, grafiksel kullanıcı arabirimlerindeki gelişmelerle birlikte önemli ve etkin hale geldi. Gösterilmek istenen bilginin artışı ile birlikte, bu ağların analizi ve elde edilmek istenen ilişkisel verinin kullanımı oldukça zorlaşmaktadır. Bu problemi çözmek için, karmaşık ve alana özgü karmaşıklık yönetimi tekniklerinin geliştirilmesi ve sağlanması gerekmektedir. Systems Biology Graphical Notation (SBGN), biyokimyasal ve hücresel proseslerin görselleştirilmelerini standart bir şekilde ifade edebilmek amacıyla, biyokimyacılar ve bilgisayar uzmanları tarafından, yıllardır süregelen bir çalışma ile geliştirilmektedir. SBGN, etkileşim ağlarının kesin ve belirsizliğe yer vermeyecek şekilde gösterimi amacıyla, somut ve detaylı bir sembol listesi sağlayan bir görsel dil ya da notasyondur. Ayrıca, SBGN bu tarz bir grafiksel bilginin nasıl yorumlanması gerektiğini de izah eder. SBGN proses dili, biyolojik yolaklardaki entitilerin, biyolojik reaksiyonları temsil eden proseslerden nasıl etkilendiğini açıklar. Aynı entitiyi birkaç defa aynı diyagramda göstererek, biyokimyasal entiti ağında yer alan bütün moleküler etkileşimlerin gösterimi yapılır. Bu çalışmada, SBGN-PD diyagramlarının etkin bir şekilde gösterimi ve analizi için metotlar ve araçlar geliştirdik. Özellikle, büyük SBGN-PD diyagramlarının karmaşıklıklarının etkin bir şekilde yönetilmesi amacıyla yeni algoritmalar sunduk. Bu algoritmalar, karmaşıklık y ̈onetimi teknikleri uygulandıgında SBGN-PD diyagramlarının bütünlüğünü koruyacak şekilde tasarlandı. Ek olarak, bu metotların kullanıldığı yazılım bileşenleri ve web tabanlı araçlar üretildi. Bu araçlar gelişmiş ve modern web teknolojilerini ve kütüphanelerini kullanmaktadırlar.","Graphs are commonly used to model relational information in many areas such as relational databases, software engineering, biological and social networks. In vi- sualization of graphs, automatic layout, interactive editing and complexity man- agement of crowded graphs are essential for effective utilization of underlying information. Advances in graphical user interfaces have given rise and value to interactive editing and diagramming techniques in graph visualization. As the size of the information to be visualized vastly increased, it became harder to analyze such networks, making use of relational information needed to be acquired. To over- come this problem, sophisticated and domain-specific complexity management techniques should be provided. The Systems Biology Graphical Notation (SBGN) has been developed over a number of years by biochemists and computer scientists to standardize visual representation of biochemical and cellular processes. SBGN introduces a concrete, detailed set of symbols for scientists to represent network of interactions, in a way that is not open to more than one interpretation. It also describes the manner, in which such graphical information should be interpreted. The SBGN Process Description (PD) language shows how entities are influ- enced by processes, which are represented by several reaction types in a biological pathway. It can be used to show all the molecular interactions taking place in a network of biochemical entities, with the same entity appearing multiple times in the same diagram. We developed methods and tools to effectively visualize and manage SBGN- PD diagrams. Specifically, we introduced new algorithms for proper manage- ment of complexity of large SBGN-PD diagrams. These algorithms strive to keep SBGN-PD diagrams intact as complexity management takes places. In ad- dition, we provided software components and web-based tools that implement these methods. These tools use state-of-the-art web technologies and libraries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğrudan hacim boyama birçok uygulamada kullanılan bir yöntemdir. Biz bu problemi değişik boyutları ile inceledik. Öncelikle, yüksek işlemci gereksinimini resim kalitesini önemli ölçüde bozmadan azaltmak amacıyla, bir bakış açısına bağlı seçici sadeleştirme mekanizması önerdik. Daha sonra doğrudan hacim görüntüleme probleminin paralel uygulamalarını inceledik. Ve son olarak, doğrudan hacim görüntüleme tekniklerini kullanarak MaterialVis aracını geliştirdik. Hacim görüntüleme uygulamalarını interaktif seviyelere çıkarmak kolay bir iş değildir. Biz, hacim veri kümesini bakış açısına bağlı olarak dinamik bir şekilde seçici sadeleştiren bir mekanizma önerdik. Hacım kümesinin farklı kısımlarının önemleri otomatik olarak belirlenir ve buna göre detaylandırılır. Görüş bağımlı sadeleştirme mekanizmamızın düşük ek iş yükü sayesinde, yaygın veri kümelerinde yeterli çözünürlükler için interaktif seviyelere çıkmayı başardık. CUDA kullanarak grafik işlem ünitesi üzerinde çalışacak, hücre izdüşümü ve ışın fırlatım tabanlı bir hacim görüntüleme algoritması önerdik. Algoritmada, sonuç resim kalitesini işleme hızının önünde tuttuk. Algoritmalarımızın düşük hafıza kullanımları büyük veri kümelerini ciddi hız artışlarıyla işleyebilmemize olanak sağladı. Materyallerin görüntülenmesi analizlerinin önemli bir parçasını oluşturur. Bu amaçla, MaterialVis adında bir araç geliştirdik. MaterialVis, materyalleri düz atomik koordinatlara ek olarak hem hacim hem de yüzey manifoldu olarak tanımlar. Direk hacim görüntüleme teknikleri materyallerin hacimsel özelliklerini görüntülemek için idealdir. Kristal hataları, kristal özellikleri olarak tanımlanıp görüntülenebilir. MaterialVis aracı aynı zamanda yüzey görüntüleme tekniklerini de destekler. Kullanıcıların görüntülemeyi kontrol etmesini sağlayan zengin parametre ve seçenekler sayesinde, materyallerin çeşitli özellikleri etkili bir şekilde görüntülenebilir. Bu sayede, amorf ve kristal yapıları çeşitli işleme biçimlerinde interaktif olarak işleyip topoloji ve kristal hataları gibi önemli materyal özellikleri ortaya konulabilir.","Direct volume rendering is widely used in many applications. We explored several aspects of the problem. First, we proposed a view-dependent selective refinement scheme in order to reduce the high computational requirements without affecting the image quality significantly. Then, we explored the parallel implementations of direct volume rendering. Finally, we used direct volume rendering approaches to create a material visualization tool. Achieving interactive rates in volume-rendering applications is a real challenge. We present a selective refinement scheme that dynamically defines the mesh according to the camera parameters. The importance of different parts of the mesh is automatically determined and the mesh is refined accordingly. Thanks to low overhead dynamic view-dependent refinement, we achieve interactive frame rates for rendering common datasets at decent image resolutions. Using CUDA, we propose a GPU-based volume rendering algorithm that is based on a cell projection-based ray-casting algorithm. We favor image quality over rendering speed. Our algorithm has a low memory footprint, allowing us to render large datasets with significant speed-ups. Visualization of materials is an indispensable part of their structural analysis. We developed a visualization tool called MaterialVis. Unlike existing tools, MaterialVis represents materials as a volume and a surface manifold, in addition to plain atomic coordinates. MaterialVis provides a wide range of functionality to visualize topological structures and crystal defects interactively. Direct volume rendering techniques are used to visualize crystal defects. In addition, the tool provides surface visualization to extract hidden topological features within the material."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gerçek hayatta, tekmeleme, atış ve yakalama gibi çarpışma gerektiren activiteler sırasında vücudu kinematik ve kinetik açıdan hazırlayan biyomekanik stratejiler öğrenmek oldukça önemlidir. Buna dayanarak, son derece karmaşık bir görev olan top yakalama görevi için doğal ve fiziksel açıdan makul hareketler oluşturabilen gerçek zamanlı ve fizik tabanlı bir yaklaşım sunuyoruz. Biz diğer birçok karmaşık görevler gibi yakalama activitesinin, ayakta durma, yürüme, uzanmak gibi oldukça basit motor becerilerinin doğru bir şekilde birleşimi ile elde edilebilir olduğunu gösterdik. öğrenilen biyomekanik stratejilerin motor kontrolünde bilinçli davranışları artırabiliceğinden dolayı, planlama gerektirebilen çeşitli konular ile ilgilendik. Bunların arasında, zamanlama kavramının yoğun orarak üzerinde duruyoruz. Karakter zamanı doğru bir şekilde kullanabilmek için pekiştirmeli öğrenme tekniği ile nasıl ve ne zaman davranabiliceğini gösteren bazı politikalar öğrenir. Farklı yakalama stratejileri ile gerçekleştirilen bazı animasyon sonuçları sunarak yöntemimizin etkinliğini gösterilmektedir. Her bir simülasyon sırasında, toplar farklı uçuş süreleri ve yükseklik kouşullarını göz önünde bulundurmak için rasgele bir biçimde, ancak belirli limitler aralığında, fırlatıldı.","In real world, it is crucial to learn biomechanical strategies that prepare the body in kinematics and kinetics terms during the interception tasks, such as kicking, throwing and catching. Based on this, we presents a real-time physics-based approach that generate natural and physically plausible motions for a highly complex task- ball catching. We showed that ball catching behavior as many other complex tasks, can be achieved with the proper combination of rather simple motor skills, such as standing, walking, reaching. Since learned biomechanical strategies can increase the conscious in motor control, we concerned several issues that needs to be planned. Among them, we intensively focus on the concept of timing. The character learns some policies to know how and when to react by using reinforcement learning in order to use time accurately. We demonstrate the effectiveness of our method by presenting some of the catching animation results executed in different catching strategies.In each simulation, the balls were projected randomly, but within a interval of limits, in order to obtain different arrival flight time and height conditions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris dizi çarpımı, denklem çözücülerde kullanılan anahtar işlemdir. Seyrek matrix tarafından yapılan düzensiz hafıza erişimleri nedeniyle, buyruk ön yükleyicisi, işlemci ön belleği ve dizi buyrukları gibi bir çok donanım etkili bir şekilde kullanılamamaktadır. Buda paralel verimliliğin düşmesine neden olur. Bu çalışmada, paylaşımlı hafıza sistemlerinde kullanılmak üzere, • Öğrenme yetisine sahip planlayıcı ve yük dengeleyici algoritmalar, • Dizi buyruklarını etkili bir şekilde kullanmaya olanak sağlayan melez bir seyrek veri yapısı ve • Bu veri yapısını oluşturmada kullanılan bir algoritma geliştirilmiştir. Bu çalışmada belirtilen teknikler, hem ön yapılandırmalı hemde direkt olarak seyrek matrix-dizi çarpımında kullanılabilir. Testler Intel tarafından üretilen Xeon Phi adlı, x86 tabanlı çekirdeklere ve bu çekirdekleri birbirine bağlayan halka ağ protokolüne sahip, yardımcı kartlar üzerinde yapılmıştır. Önerilen teknikler ekran kartlarında da kullanılabilir.","SpMxV (Sparse matrix vector multiplication) is a kernel operation in linear solvers in which a sparse matrix is multiplied with a dense vector repeatedly. Due to random memory access patterns exhibited by SpMxV operation, hardware components such as prefetchers, CPU caches, and built in SIMD units are under-utilized. Consequently, limiting parallelization efficieny. In this study we developed; • an adaptive runtime scheduling and load balancing algorithms for shared memory systems, • a hybrid storage format to help effectively vectorize sub-matrices, • an algorithm to extract proposed hybrid sub-matrix storage format. Implemented techniques are designed to be used by both hypergraph partitioning powered and spontaneous SpMxV operations. Tests are carried out on Knights Corner (KNC) coprocessor which is an x86 based many-core architecture employing NoC (network on chip) communication subsystem. However, proposed techniques can also be implemented for GPUs (graphical processing units)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek üçgensel sistem çözümü, bir çok bilimsel ve mühendislik uygulamalarında yaygın olarak kullanılan bir çekirdek işlemdir. Bu çekirdek işlemin çok seviyeli önbellekler üzerinde etkili bir şekilde yürütülmesi, yüksek performans elde etmek açısından önemlidir. Biz bu çalışmada, seyrek üçgensel sistem çözümünde kullanılmak üzere etkili bir çerçeve sunuyoruz. Seyrek lineer sistemlerin direkt metod ile çözümü, L alt üçgensel faktör ve U üst üçgensel faktör olmak üzere, LUz = b biçimindeki bir üçgensel denklem çözümünü gerektirir. Önbelleği kullanmak için, üçgensel sistemdeki veri bağlılığını da göz önüne alarak L faktörünün satır ve sütunlarını yeniden sıraladık. Üçgensel sistemdeki veri bağlılıklarını yönlü bir hiperçizge olarak kodlayıp, bu yapı üzerinde sıralı bir bölümleme modeli inşa ettik. Bu amaçla, bağlılık sınırlamalarına riayet edecek biçimde Fiduccia-Mattheyses (FM) algoritmasını farklı bir şekilde yeniden geliştirdik. Ayrıca, bu işlemde daha fazla esneklik elde etmek için, L faktörlerini seyrek ve yoğun parçalara ayırma fikrini benimsedik ve her bir parçayı otomatik ayarlanmış farklı çekirdek yöntemlerle çözdük. Farklı depolama yöntemleri kullanarak L faktörünün ve buna karşılık gelen seyrek ve yoğun parçaların performans değişimini inceledik. İşlemci ve hafıza arasındaki hız farklarından kaynaklanan performans kayıplarını önlemek için, OSKI tarafından sağlanan otomatik ayarlama yöntemlerinden faydalandık. Gerçek veriler üzerinde yürütülen deneyler, önerilen modelin etkinliğini doğrular niteliktedir.","Sparse Triangular Solve (SpTS) is a commonly used kernel in a wide variety of scientific and engineering applications. Efficient implementation of this kernel on current architectures that involve deep cache hierarchy is crucial for attaining high performance. In this work, we propose an effective framework for cache-aware SpTS. Solution of sparse linear symmetric systems utilizing the direct methods require the triangular solve of the form LUz = b, where L is lower triangular factor and U is upper triangular factor. For cache utilization, we reorder the rows and columns of the L factor regarding the data dependencies of the triangular solve. We represent the data dependencies of the triangular solve as a directed hypergraph and construct an ordered partitioning model on this structure. For this purpose, we developed a variant of Fiduccia-Mattheyses (FM) algorithm which respects the dependency constraints. We also adopt the idea of splitting L factors into dense and sparse components and solving them seperately with different autotuned kernels for achieving more flexibility in this process. We investigate the performance variation of different storage schemes of L factors and the corresponding sparse and dense components. We utilize autotuning provided by Optimized Sparse Kernel Interface (OSKI) to reduce performance degradation that incurs due to the gap between processors and memory speeds. Experiments performed on real-world datasets verify the effectiveness of the proposed framework."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsan hareketlerini anlamak ve analiz etmek bilgisayarlı görü alanında ne kadar popüler bir konu olsa da, yapılan arastırmaların çoğu yürümek ve zıplamak gibi olağan hareketleri tanımak uzerine yoğunlasmaktadır. Bu yöntemleri yardımcı teknolojiler gibi daha spesifik alanlarda uygulamak her zaman kolay olmayabilir. Bu uygulamalar çoğu zaman sınıflar arası çok az değişim gösteren ve aynı sınıf icinde fazla değişim gösterebilen aktivitelerden oluşmaktadır. Bu tezde videoların küçük parçalarından hareket bilgisini kullanarak günlük aktiviteleri tanımayı öneriyoruz. Önerilen yöntem, gösterilen hareketi tanımlamak için hareket yörüngesinin varyans ve uzunluk gibi istatistiksel bilgilerini kullanmaktadır. Aynı zamanda, hareketlerin konumsal bilgileri de uzaysal grid kullanılarak elde edilir.Deneylerimizde bu yöntemin özellikle içinde hızlı hareketler bulunduran tıbbi cihaz kullanımı alanında yardımcı olduğunu görüyoruz. Bu tezdeki diğer katkı ise, görsel bilginin yeterli olmadığı zamanlarda hareketlerin sıralamasını modellemenin sistemin aktivite tanıma performansı arttırdığı göstermektir. Problemin görsel kısmı içinse, farklı öznitelik gruplarının tahminlerini birleştiren bir öneriyle, birden fazla görsel bilgi kullanabiliyoruz. Deneylerimizin de gösterdiği üzere, önerilen yöntemler tıbbi cihaz kullanımı ve yemek yapma aktivitelerini tanıma gibi yardımcı teknoloji cihazları alanlarında performansı ciddi bir şekilde yukseltmiştir.","Although understanding and analyzing human actions is a popular research topic in computer vision, most of the research has focused on recognizing ""ordinary"" actions, such as walking and jumping. Extending these methods for more specific domains, such as assistive technologies, is not a trivial task. In most cases, these applications contain more fine-grained activities with low inter-class variance and high intra-class variance. In this thesis, we propose to use motion information from snippets, or small video intervals, in order to recognize actions from daily activities. Proposed method encodes the motion by considering the motion statistics, such as the variance and the length of trajectories. It also encodes the position information by using a spatial grid. We show that such approach is especially helpful for the domain of medical device usage, which contains actions with fast movements Another contribution that we propose is to model the sequential information of actions by the order in which they occur. This is especially useful for fine-grained activities, such as cooking activities, where the visual information may not be enough to distinguish between different actions. As for the visual perspective of the problem, we propose to combine multiple visual descriptors by weighing their confidence values. Our experiments show that, temporal sequence model and the fusion of multiple descriptors significantly improve the performance when used together."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Arama motorlarının performansı indeksleyici, arka uç işlemcileri ve belge toplama botları gibi parçalarının performansına bağlıdır. Sorguların gecikme süresi, sorgu sonuçlarının doğruluğu ve güncellikleri performansı belirlemede önemli rol oynar. Arama motorlarında performans, güçlü donanımlarla sağlanabilir, fakat arama motorlarının ticari devamlılığı açısından operasyonel giderlerin kontrol altında tutulması gerekir. Bu nedenle, bu tez arama motorlarının performansını arka uca giden sorgu sayısını ve bir sorgu akımının sonuçlarının hesaplama maliyetini azaltarak iyileştiren tekniklere odaklanır. Bu, sorgulardaki zamansal lokalite özelliğinden yararlanılarak sağlanabilir. Yakın zamanda verilen sorguların sonuçları önbelleklenerek, bu sorguların aynı veya farklı kullanıcılar tarafından tekrarlanması durumunda oluşacak tekrar hesaplama maliyeti ortadan kaldırılabilir. Dolayısıyla, sorgu sonuç önbelleği eldeki kaynaklardaki yükü azaltarak hesaplama güçlerini artırır. Bu tez temel olarak sonuç önbelleğinin üretkenliğini geliştirerek arama motorunun performansını yükseltmeyi amaçlar. Frekans, sorgu zamanı ve sorgu maliyeti gibi sorgu istatistikleri kullanılarak önbellek isabet oranını artırarak ve toplam maliyeti düşürerek bu amaca ulaşılabilir. Arama motorlarının verimliliğini artırırken önbellekteki sonuçların taze tutulması kullanıcı memnuniyeti açısından önemlidir; bundan dolayı arama motorları tarafından gözardı edilemez. Sonuçların tazeliğini sınırlandırmak için çeşitli teknikler önerilerek, önbelleğin performansını düşürmeden bu çalışmada verimli çözümler bulunmaya çalışılmıştır.","The performance of a search engine depends on its crawler, indexer and processor components. The query latency, accuracy and recency of the results play crucial role in determining the performance. High performance can be provided with powerful hardware, but keeping the operational costs restrained is mandatory for commercial durability. This thesis focuses on techniques to boost the performance of search engines by means of reducing the number of queries issued to the backend and the cost to process a query stream. This can be accomplished by taking advantage of the temporal locality of the queries. Caching the result for a recently issued query removes the need to reprocess this query when it is issued again. Therefore, deploying query result cache decreases the load on the resources of the search engine which increases the processing power. The main objective of this thesis is to improve search engine performance by enhancing productivity of result cache. This is done by endeavoring to maximize the cache hit rate and minimizing the processing cost by using the per query statistics such as frequency, timestamp and cost. While providing high hit rates and low processing costs improves performance, the freshness of the queries has to be considered as well for user satisfaction. Therefore, a variety of techniques are examined in this thesis to bound the staleness of cache results without blasting the backend with refresh queries. The o ered techniques are demonstrated to be effcient by using real query log data from a commercial search engine."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"The scale of collaboration between people and computers has expanded leading to new era of computation called crowdsourcing. A variety of problems can be solved with this new approach by employing people to complete tasks that cannot be computerized. However, the existing approaches are focused on simplicity and independency of tasks that fall short to solve complex and sophisticated problems. We present Crowdy, a general-purpose and extensible crowdsourcing platform that lets users perform computations to solve complex problems using both computers and human workers. The platform is developed based on the stream-processing paradigm in which operators execute on the continuos stream of data elements. The proposed architecture provides a standard toolkit of operators for computation and configuration support to control and coordinate resources. There is no rigid structure or requirement that could limit the problem-set, which can be solved with the stream-based approach. The stream- based human-computation approach is implemented and verified over different scenarios. Results show that sophisticated problems can be easily solved without significant amount of work for implementation. Also possible improvements are discussed and identified that is a promising future work for the existing work.","İnsanlar ve yazılım bileşenleri arasındaki işbirliği gelişerek kitle kaynağın ortaya çıkmasını sağlamıştır. Kitle kaynak kullanılarak yazılım tarafından çözülemeyen veya çözülmesi zor birçok sorun insanlar aracılığı ile çözülmüş ve bir sonuca ulaşılmıştır. Fakat günümüzdeki kitle kaynak odaklı yaklaşımlar ve çözüm süreçleri yapılan işlerin basitliğine ve birbirlerinden bağımsız olmalarına ağırlık vermektedir. Bu sebepten dolayı zor ve çok yönlü sorunların çözümü mevcut yaklaşımlarla mümkün değildir. Bu çalışmada kullanıcıların sorun çözümü konusunda hem insanları hem de yazılım bileşenlerini kullanabilecekleri, genel amaçlı ve geliştirilebilir bir yaklaşım ve bu yaklaşımın uygulandığı bir altyapı sunulmaktadır. Yaklaşım küçük işlemcilerin sürekli olarak akan veriler üzerinde çalışması mantığına dayanmaktadır. Sunulan altyapı bünyesinden barındırdığı temel işlemciler sayesinde işlem kaynakları arasındaki kontrol ve eşgüdümü kolaylıkla sağlamaya elverişli şekilde tasarlanmıştır. Sunulan yaklaşım kısıtlı bir sorun listesini hedeflememektedir ve kullanıcılar açısından herhangi bir kısıtlama getirmemektedir. Çeşitli örnekler yapılan incelemeler sunulan yaklaşımın sorunları kayda değer bir iş yükü getirmeden çözülebileceğini göstermiştir. Ayrıca, çeşitli iyileştirme önerileri de tartışılmış ve bazıları gelecek çalışmalara eklenmek üzere belirlenmiştir."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde, çevrimiçi veri katarı formatında bulunan kullanılabilir dijital bilgi gittikçe artan bir orana sahiptir. Birçok uygulama alanında, bu tür verilerin yüksek üretilen iş kapasiteli olarak işlenmesi, yükselen girdi oranlarına ayak uydurmak için kritik bir gerekliliktir. Veri katarı işleme, bu zorluğu veriyi anında işleme tarzı ile ele almayı amaçlayan bir hesaplama örneklemidir. Bu tez, veri katarı işleme uygulamalarının otomatik bir şekilde paralelleştirilme problemini, üretilen işi arttırarak nasıl çözüleceğini gösterir. Paralelleştirme işlemi, veri katarı uygulamalarının, uygulama geliştiricileri tarafından sırasıyla yazılması ve sistem tarafından paralelleştirilmesi şeklinde otomatiktir. Bu tezde, devingen seçici ve durumsal işleçler kullanılan eşzamansız veri akış modeli benimsenmiştir. Ardışık düzenlenmiş fizyon problemi, orijinal sıralı programda ardışık düzenlenmiş ve veri paralelleştirmesinden faydalanarak çözülmüştür. Ardışık düzenlenmiş fizyon çözümü, bölümlü durumsal veri paralelleştirmeyi desteklemektedir ve paylaşımlı bellekli çok çekirdekli makineler için tasarlanmıştır. İlk olarak ardışık düzenlenmiş fizyon problemi, maliyet tabanlı formülasyonla optimizasyon problemine indirgenmiştir. Bu problemin kapsamlı çözümü çok zaman aldığı için, bu problemi hızlı ve yaklaşık olarak çözen bulgusal çözüm önerilmiştir. Tezde önerilen yaklaşımın, simülasyonlarla ve endüstriyel Veri Katarı İşleme Sistemleri (VKİS) ile kapsamlı olarak değerlendirilmesi yapılmıştır. Elde edilen sonuçların, yeterli paralelleştirme içeren programlar için iyi bir ölçeklenebilirlik ve optimum performansa yakınlık sağladığı görülmüştür.","There is an ever increasing rate of digital information available in the form of online data streams. In many application domains, high throughput processing of such data is a critical requirement for keeping up with the soaring input rates. Data stream processing is a computational paradigm that aims at addressing this challenge by processing data streams in an on-the-fly manner. In this thesis, we study the problem of automatically parallelizing data stream processing applications to improve throughput. The parallelization is automatic in the sense that stream programs are written sequentially by the application developers and are parallelized by the system. We adopt the asynchronous data flow model for our work, where operators often have dynamic selectivity and are stateful. We solve the problem of pipelined fission, in which the original sequential program is parallelized by taking advantage of both pipeline and data parallelism at the same time. Our solution supports partitioned stateful data parallelism with dynamic selectivity and is designed for shared-memory multi-core machines. We first develop a cost-based formulation to express pipelined fission as an optimization problem. The bruteforce solution of this problem takes a very long time for moderately sized stream programs. Accordingly, we develop a heuristic algorithm that can quickly, but approximately, solve this problem. We provide an extensive evaluation studying the performance of our solution, including simulations and experiments with an industrial-strength Data Stream Processing Systems (DSPS). Our results show good scalability for applications that contain sufficient parallelism, closeness to optimal performance for the algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özet Fonksiyonları daha çok veri bütünlüğünde ve elektronik imza gibi kimlik doğrulamada kullanılan kriptolojinin en önemli araçlardan biridir. Bu fonksiyonlar, herhangi bir uzunluktaki girdiyi belli özelliklerle sıkıştırarak sabit bir uzunluktaki çıktıya götürler. Bu fonksiyonlar ayrıca hızlı hesaplanabilir olmalı ve bazı kriptografik güvenlik gereksinimlerini sağlamalıdırlar. SHA ve MD gibi özet fonksiyonları aileleri, kriptolojik uygulamalarda kullanılmak üzere standartlaştırılmış özet fonksiyonlarıdır. Bunların en önemli yapısal özelliği yinelemeli (iterative) olmalarıdır. Bu özellik büyük boyutlu verilerde verimlilik problemine neden olur, çünkü girdide ufak bir değişiklik olsa bile özet fonksiyonu bütün girdi üzerinde tekrar çalışmalıdır. Bu yüzden sormamız gereken soru ""Veri üzerindeki ufak bir değişiklik olduğunda özet fonksiyonun hesaplama maliyetini düşürmek mümkün müdür?"" olacaktır. 1995 yılında Bellare, Goldriech ve Goldwasser tarafından artımlılık isimli yeni bir konsept sunulmuştur: eğer x girdisi üzerinde yapılan küçük bir değişiklik sonucu f(x) değeri değişiklikle doğru orantılı olacak bir zamanda güncellenebiliyorsa f fonksiyonuna artımlı denir. Bu konsept verimlilik açısından çok önemli iki tane avantaj saglamıstır: artımlılık ve paralellestirilebilirlik. Ayrıca ayrık logaritma problemi gibi çözülmesi zor olan problemlere dayalı bir güvenlik sağlamıştır. Bu özelliği kullanan özet fonksiyonlarına Artımlı Özet Fonksiyonları diyoruz. Ayrıca, Dan Brown 2008 yılında artımlı özet fonksiyonlarına örnek olacak Elliptik Eğrilerde Özet Fonksiyonu (ECOH) adlı özet fonksiyonunu önermiştir. Bu tezde, artımlı özet fonksiyonları örnekleriyle (özellikle de ECOH örneğiyle) birlikte incelenmiş ve bunların güvenlik ispatları çözülebilirliği zor problemlerle karşılaştırılırak gösterilmiştir.","Hash functions are one of the most important cryptographic primitives. They map an input of arbitrary finite length to a value of fixed length by compressing the input, that is why, they are called hash. They must run efficiently and satisfy some cryptographic security arguments. They are mostly used for data integrity and authentication such as digital signatures. Some hash functions such as SHA family (SHA1-SHA2) and MD family (MD2-MD4-MD5) are standardized to be used in cryptographic schemes. A common property about their construction is that they are all iterative. This property may cause an efficiency problem on big size data, because they have to run on the entire input even it is slightly changed. So the question is ""Is it possible to reduce the computational costs of hash functions when small modifications are done on data?"" In 1995, Bellare, Goldreich and Goldwasser proposed a new concept called incrementality: a function f is said to be incremental if f(x) can be updated in time proportional to the amount of modification on the input x. It brings out two main advantages on efficiency: incrementality and parallelizability. Moreover, it gives a provable security depending on hard problems such as discrete logarithm problem (DLP). The hash functions using incrementality are called Incremental Hash Functions. Moreover, in 2008, Dan Brown proposed an incremental hash function called ECOH by using elliptic curves, where DLP is especially harder on elliptic curves, and which are therefore quite popular mathematical objects in cryptography. We state incremental hash functions with some examples, especially ECOH, and give their security proofs depending on hard problems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mikroblog hizmetleri bilginin üretilmesi ve yayılmasında temel araçlar haline gelmiştir. Dolayısıyla, bu hizmetlerin gözlemlenmesi, gerekli bilgiye ulaşmada kritik bir yetenektir. Bu izleme genelde mikroblog hizmetlerine içerik tabanlı abonelikler kaydedilmesi sayesinde yapılmaktadır. Abonelikler, akan yayınlar üzerinde sürekli çalışan sorgular olarak düşünülebilinir. Bu aboneliklerin değerlendirilmesi, Twitter ve Weibo gibi sistemlerin popülerliği ve ölçeği düşünüldüğünde, oldukça önemli bir sorundur. Bu sorunu aşmak için biz, akan kısa metin eşleme sistemi olan S3-TM'i sunuyoruz. S3-TM akan veri işleme uygulaması olarak organize edilmiş ve veri merkezi ortamında çalışacak şekilde, veri paralelliği sağlayan bir akış ağı olarak tasarlanmıştır. Yayınların ve aboneliklerin yapısını avantaja çevirerek eşlemeyi ölçeklenebilir olarak yapan S3-TM, yayınları ve abonelikleri tüm uçlara aktarmamakta, yayınları birden fazla uca aktarırken, abonelikleri sadece bir uca aktarmaktadır. Ayrıca, sunduğumuz algoritmalar, verimliliği daha da artırmak için, yayınların aktarıldığı uç sayısını minimuma indirmektedir. Tezde önerdiğimiz ilk algoritmalar, kelimelerin ortak görünme grafiklerini bölümlere ayırarak ortak kelimelerin sıkça geçtiği yayınları tüm eşleme operatörlerinin küçük bir kümesine yollamayı hedeflemektedir. Bu algoritmalar verimli olmalarına rağmen yükü eşitlemede yetersiz kalmışlardır. Bu problemi aşmak için, kelime ve yayınların çift taraflı grafiğini verimli bir şekilde bölümleyerek modelleme yapan ve daha dengeli yük dağılımı sağlayan SALB algoritmasını geliştirdik. Aynı zamanda, benzer abonelikleri aynı uçlara yönlendirerek gruplayan ve eşleme işleminin yükünü minimuma indiren LASP algoritmasını ekledik. Ayrıca, artan uç sayısında daha iyi bir ölçeklenebilirliğe ulaşmak için iş yükündeki çarpıklıkları çözen basit ama verimli teknikler geliştirdik. Son olarak eşleme doğruluğu üzerinde çok az bir etki yapan yük azaltma teknikleriyle, beklenmeyen yük artışlarını çözdük. Deneysel sonuçlarımız S3-TM algoritmasının ölçeklenebilir olduğunu göstermektedir. Buna ek olarak, SALB algoritması temel algoritmadan 2.5 kat olmak üzere, kelimelerin ortak görünme grafiği bölümleme algoritmalarından da daha yüksek performanslı olduğu gözlemlenmektedir.","Micro-blogging services have become major venues for information creation, as well as channels of information dissemination. Accordingly, monitoring them for relevant information is a critical capability. This is typically achieved by reg- istering content-based subscriptions with the micro-blogging service. Such sub- scriptions are long running queries that are evaluated against the stream of posts. Given the popularity and scale of micro-blogging services like Twitter and Weibo, building a scalable infrastructure to evaluate these subscriptions is a challenge. To address this challenge, we present the S3-TM system for streaming short text matching. S3-TM is organized as a stream processing application, in the form of a data parallel flow graph designed to be run on a data center environment. It takes advantage of the structure of the publications (posts) and subscriptions to perform the matching in a scalable manner, without broadcasting publications or subscriptions to all of the matcher instances. The basic design of S3-TM uses a scoped multicast for publications and scoped anycast for subscriptions. To fur- ther improve throughput, we introduce publication routing algorithms that aim at minimizing the scope of the multicasts. The first set of algorithms we de- velop are based on partitioning the word co-occurrence frequency graph, with the aim of routing posts that include commonly co-occurring words to a small set of matchers. While effective, these algorithms fell short in balancing the load. To address this, we develop the SALB algorithm, which provides better load balance by modeling the load more accurately using the word-to-post bipartite graph. We also develop a subscription placement algorithm, called LASP, to group together similar subscriptions, in order to minimize the subscription matching cost. Fur- thermore, to achieve good scalability for increasing number of nodes, we introduce simple yet effective techniques to handle workload skew. Finally, we introduce load shedding techniques for handling unexpected load spikes with small impact on the accuracy. Our experimental results show that S3-TM is scalable. Further- more, the SALB algorithm provides more than 2.5× throughput compared to the baseline multicast and outperforms the graph partitioning based approaches."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsanlığın varoluşundan bu yana tıptaki araştırmalar, hastaları tedavi edebilmek ve hastalıkların çaresini bulabilmek adına bunların altında yatan sebepleri bulmak için yapılmıştır. Bu amaç son zamanlarda tüm genom ve tüm ekzom dizilemede yaşanan gelişmeler sayesinde gerçekleştirilebilmektedir. Dizileme maliyetlerinde yaşanan azalmalara rağmen, daha doğru sonuçlar elde edebilmek adına çok sayıda insan genomunun dizilenme ihtiyacı olduğundan tüm genom dizileme halen yüksek maliyetli bir yöntemdir. Bu sebeple, sadece protein kodlayan bölgeleri dizileyen tüm ekzom dizileme yöntemi nispeten daha az maliyetli bir alternatiftir. Tüm ekzom dizileme yaklaşımlarının yardımıyla, fonksiyonel önem taşıyan varyantların çoğu bulunabilmektedir. Buna ek olarak, Mendeliyen (tek mutasyon kaynaklı) hastalıkların en büyük sebebi olan tek nükleotid polimorfizmlerinden, ekzon bölgelerinde yer alanlar da bulunabilmektedir. Ayrıca tüm ekzom dizilemeye dayalı yaklaşımlar, insan genomun sadece %1'lik kısmını kapsadığından diğer yaklaşıma göre analiz yaparken daha az çaba gerektirmektedir. Ancak doğru sonuçlar elde edebilmek için ekzom dizileme datasında varolan prob etkinliği ve GC içeriği gibi sapma etkilerinin düzeltilmesi gerekmektedir. Bunlardan GC içeriği sapmasını düzeltmek için yapılmış bazı çalışmalar bulunmaktadır. Ancak literatürde, prob etkinliği sapmasını düzeltmek amacıyla yapılan bir çalışma bulunmamaktadır. Bu tezde ekzom dizileme datasına ait dizi derinlemesi dağılımında varolan prob etkinliliği ve GC içeriği sapmaları üzerinde çalışılmıştır. Prob etkinliği sapmasının düzeltilmesiyle birlikte, ekzom dizileme datası kullanan yeni kopya sayısı varyantı bulma metotları geliştirilmek mümkün olacaktır.","Medical research has striven for identifying the causes of disorders with the ultimate goal of establishing therapeutic treatments and finding cures since its early years. This aim is now becoming a reality thanks to recent developments in whole genome (WGS) and whole exome sequencing (WES). Despite the decrease in the cost of sequencing, WGS is still a very costly approach because of the need to evaluate large number of populations for more concise results. Therefore, sequencing only the protein coding regions (WES) is a more cost effective alternative. With the help of WES approach, most of the functionally important variants can be detected. Additionally, single nucleotide polymorphisms (SNPs) that are located within coding regions are the most common causes for Mendelian diseases (i.e. diseases caused by a single mutation). Moreover, WES approaches require less analysis effort compared to whole genome sequencing approaches since only 1% of whole genome is sequenced. Besides the advantages, there are also some shortcomings that need to be addressed such as biases in GC−content and probe efficiency. Although there are some previous studies on correcting GC−content related issues, there are no studies on correcting probe efficiency effect. In this thesis, we provide a formal study on the effects of both GC−content and probe efficiency on the distribution of read depth in exome sequencing data. The correction of probe efficiency will make it possible to develop new CNV discovery methods using exome sequencing data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizgeler sosyal ya da biyolojik ağlar gibi ilişkisel bilgilerin görselleştirilmesinde sıklıkla kullanılmaktadır. Basit çizgelerin otomatik olarak görselleştirilmesi ve yerleştirilmesi için şimdiye dek pek çok yöntem önerilmesine karşın ilişkisel bilgiler genellikle kümelenmiş olduğundan basit çizgeler için geliştirilen yöntemler genellikle yetersiz kalmaktadır. Kümeleme modelleri geleneksel olarak her veri noktasının yalnızca bir kümeye ait olduğunu varsayar, ancak karmaşık ağlarda bu kümeler genellikle kesişmektedir. Kümelenmiş çizgelerin etkin bir şekilde görselleştirilmesi için köşelerin çakışmaması, kenar kenar kesme sayılarının az olması ve toplam yerleşim alanının küçük olması gibi genel çizge gösterim kriterlerinin sağlanmasnın yanı sıra, aynı küme içinde yer alan köşelerin birbirine yakın olacak şekilde yerleştirilmesi gerekir. Kümelerin kesişimleri de göz önüne alındığında kümelenmiş çizge görselleştirme problemi daha da karmaşıklaşmaktadır. Bu çalışmada kesişen kümeler içeren çizgeler için kuvvet yönelimli yeni bir otomatik yerleştirme algoritması sunulmuştur. Bu algoritmayla kümeler kesişimlerine göre bölgelere bölunür ve geleneksel kuvvet yönelimli modele aynı küme içerisinde kalan köşeleri bir arada tutacak ve ayrı kümelerdeki komşu köşeleri güvenli bir mesafede yerleştirecek yeni kuvvetler eklenir. Ayrca, hızlı ve etkin bir görselleştirme için yay sabiti ayarlamaları yapılması zorunlu olmuştur. Algoritma, ChiEd görselleştirme aracı içerisinde Cluster Layout adında yeni bir yerleştirme stili olarak denendi ve uygulamaya kondu.","Graphs are often used for visualizing relational data such as social or biological networks. Numerous methods have been proposed for automatic layout of simple graphs. However, simple graphs are usually insufficient in displaying relational information, since relational information is often clustered. Clustering models traditionally assume that each data point belongs to one and only one cluster; however, in complex networks, these clusters often overlap. For effective visualization of clustered graphs, the nodes in the same cluster should be placed together, respecting general graph drawing criteria such as avoiding node-node overlaps, minimizing edge crossings, and minimizing the total drawing area. Clustered graph layout problem becomes even more challenging when cluster overlaps are allowed. Here, we present a new algorithm for automatic layout of graphs with overlapping clusters based on force directed layout approach. The graph is fi rst divided into zones according to clusters and their intersections, and new additional forces are introduced to the traditional spring embedder algorithm to keep nodes in the same cluster together, trying to keep neighboring nodes in separate clusters at a safe distance. Spring constants had to be fine-tuned to achieve a fast and eff ective layout operation. The algorithm was implemented and validated within a new layout style named Cluster Layout in the layout module of ChiEd visualization tool."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gelişen teknolojiyle birlikte genom bilimi üzerindeki ilgi arttı. Günümüzdeki ileri düzey çalışmalar ve yüzyıllardır süregelen moleküler biyoloji araştırmaları birleşince yüksek miktarda yolak verisi oluştu. Bu modeller, bağıntıları nedenlere bağlayıp, yüksek hacimli veri analizinde kullanılarak, birçok karmaşık hastalığı aydınlatabilir. Bu çalışmalardaki anlam belirsizliğini önlemek ve düzenliliği sağlamak amacıyla standart bir notasyona ihtiyaç duyuldu. Systems Biology Graphical Notation (SBGN), bir grup biyokimyacı, modelleme uzmanı ve bilgisayar bilimcisi tarafından, bilim adamlarının hücresel süreçler de dahil olmak üzere pek çok ağ yapısını standart ve belirsizliğe yer vermeyen bir şekilde yansıtması için geliştirilmiştir. SBGN üç dilden oluşur: proses, varlık ilişki ve etkinlik akış. Bu araştırma, bu dilin proses diyagramları kolu üzerine yoğunlaşmaktadır. Otomatik yerleştirme, çizgelerle temsil edilen bilginin açıkça görselleştirilmesini sağlamakta yaygın şekilde kullanılır. Biyolojik yolakların içiçe yapılar (nükleoplazma vb.) içerdiğini göz önünde bulundurarak, bu yapıları destekleyen kuvvet-yönlendirilmiş bir otomatik yerleştirme metodu olan Compound Spring Embedder (CoSE) algoritmasını kullandık. Bu algoritma yapısını kullanarak, özelleştirilmiş bir SBGN-PD yerleştirme algoritması geliştirdik. SBGN-PD yerleştirme algoritması genel olarak; kompleks üyelerinin ve bağlantısı olmayan moleküllerin düzgünce döşenmesi, proses düğümlerinin substrat ve ürün bağlantılarının, ilgili proses düğümlerinin zıt taraflarına yerleştirilmesi ve bu işlemlerin algoritmanın kuvvet-yönlendirilmiş yapısını bozmadan yapılmasını içerir.","Evolving technology has increased the focus on genomics. The combination of today's advanced studies with decades of molecular biology research yield in huge amount of pathway data. These models can be used to improve high-throughput data analysis by linking correlation to the causation, shedding light on many complex diseases. In order to prevent ambiguity and ensure regularity of the research, a need for using a standard notation has emerged. Systems Biology Graphical Notation (SBGN) is a visual language developed by a community of biochemists, modellers and computer scientists with the intention of enabling scientists to represent networks, including models of cellular processes, in a standard, unambiguous way. SBGN is formed of three languages: process, entity relationship and activity flow. This research is focused on its process diagram branch. Automated layout is commonly used to clearly visualize the information represented by graphs. Considering the fact that, biological pathways includes nested structures (e.g., nucleoplasms), we have made use of a force-directed automatic layout algorithm called Compound Spring Embedder (CoSE), which supports the compound graph structures. On top of this layout structure, we have developed a specialized layout algorithm called SBGN-PD layout. SBGN-PD layout enhancements mainly include properly tiling of complex members and disconnected molecules, placement of product and substrate edges on the opposite sides of a process node without disturbing the force-directed structure of the algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gerçekçi insan hareketleri sinema filmleri, video oyunları ve sanal ortamlar gibi farklı medya alanlarının önemli bir parçasıdır. Hareket yakalama sensör teknolojisini kullanarak gerçekçi insan hareketi verileri sağlar. Ancak, hareket yakalama verileri esnek değildir. Bu dezavantaj uygulamada hareket yakalama teknolojisinden yararlanmayı kısıtlar. Bu tezde, hareket grafikleri ile hareket yakalama verilerini yeniden kullanarak gerçek zamanlı hareket sentezleyen iki aşamalı bir yaklaşım öneriyoruz. Birinci aşamada, çeşitli hareketleri içeren bir veri kümesinden başlayarak benzer hareket çizgeleri içeren hareket grafiği oluşturulmaktadır ve harmanlama parametreleri gibi ikinci aşamada gerekli parametreler hesaplanmaktadır. İkinci aşamada, seçilen harmanlama tekniklerine bağlı olarak, gerçek zamanlı yeni insan hareketi sentezlenir. Doğrusal harmanlama, kübik harmanlama ve önceden hazırlanmış harmanlama olmak üzere üç farklı harmanlama tekniği kullanıcıya sağlanır. Buna ek olarak, hareket arama algoritmasına uygulanan hareket klip tercihi yaklaşımı, kullanıcıya sonuç hareketteki hareket klibi tiplerini kontrol etme olanağı sağlar.","Realistic human motions is an essential part of diverse range of media, such as feature films, video games and virtual environments. Motion capture provides realistic human motion data using sensor technology. However, motion capture data is not flexible. This drawback limits the utility of motion capture in practice. In this thesis, we propose a two-stage approach that makes the motion captured data reusable to synthesize new motions in real-time via motion graphs. Starting from a dataset of various motions, we construct a motion graph of similar motion segments and calculate the parameters, such as blending parameters, needed in the second stage. In the second stage, we synthesize a new human motion in real-time, depending on the blending techniques selected. Three different blending techniques, namely linear blending, cubic blending and anticipation-based blending, are provided to the user. In addition, motion clip preference approach, which is applied to the motion search algorithm, enable users to control the motion clip types in the result motion."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Teknolojik gelişmelerle birlikte, işlemciler boyut olarak daha küçültülüyor ve üretim sürecinde daha sık ve küçük boyutlu transistorler kullanılarak üretiliyorlar. Bu üretim süreci işlemcileri daha ucuz, daha güç tasarruflu ve daha güçlü kılıyor. Bu süreç son kullanıcı için son derece faydalı olmasına karşın, bu süreç işlemcileri dış ortamdan kaynaklı radyasyona karşı daha zayıf kılıyor ve bunun sonucunda, genellikle veri üstünde tek bir bitin değer değiştirmesi formunda oluşan, yumuşak hatalar oluşuyor. Yumuşak hatalara karşı uygulamaların güvenilirliğini arttırabilmek adına, literatürde ECC (Hata Düzeltme Kodu) özellikli veya Parite özellikle hafıza gibi donanımsal hata tolerans teknikleri geliştirilmiştir. Donanımsal hata tolerans teknikleri etkili çözümler üretmesine karşın, donanımsal altyapının bulunmadığı ya da var olan sisteme eklenmesi mümkün olmadığı durumlarda, yazılımsal hata toleransı teknikleri daha ucuz ve esnek bir alternatif sunabilir. Yazılımsal Hata Toleransı teknikleri, güçlü bir alternatif olmasına rağmen, genellikle yedekleme mantığına dayalı çalıştıklarından, performans düşüşüne ve hataya neden olmaya açık olan bit sayısını arttırmaktadır. Uygulama güvenilirliğinin bir endişe olduğu sistemlerde, performans ve erişiliebilirlik daha büyük bir sistem endişesi ve gereksinimi durumunda olduğundan, bu çoklu objektifli bir yaklaşım gerektirmektedir. Belirli ölçüde bir hatanın kabul edilebilir olduğu ve erişilebilirliğin önemli olduğu uygulamalarda, sisteme getirdikleri performans yükünden ötürü, literatürdeki yazılımsal hata toleransı tekniklerini olduğu gibi kullanılamayabilir. Bu noktada bizim tekniğimiz seçimsel yazılım hata toleransı (SYHT) erişilebilirlik ve güvenilirliği eş zamanlı sağlamayı hedefler. SYHT bunu, uygulamanın sadece ihtiyaç duyduğu ölçüde hata toleransı kullanarak ve uygulamanın ürettiği verilerde kaliteyi koruyarak sağlamaktadır. SYHT yazılım profil bilgisini kullanarak, uygulamanın yumuşak hatalara karşı hassasiyetlerini anlamaya çalışır. Yumuşak hatalar yüksek sayıda çalışan komut satırlarında oluşmaya meyillidir. Ayrıca, yumuşak hatalara maruz kaldığında, uygulama tarafından oluşturulan verilerde daha fazla hataya sebep olan komut satırlarının uygulamanın güvenilirlik açısından hassas komut satırları olduğu söylenebilir. SYHT bu bilgileri kullanarak, yumuşak hatalara maruz kalma olasılığı düşük ve uygulama tarafından oluşturulan verilerde daha az hataya sebep olan komut satırları için hata toleransını kaldırır. Bu yaklaşım, performansı arttırırken ve güvenilirliği yeterli seviyede tutarken, enerji tüketimlerini ve yedeklenen veri sayısını (dolayısıyla yumuşak hatalara maruz kalan bit sayısını) azaltır.Bu teknik kolaylıkla literatürde yaygın olan yazılımsal hata toleransı tekniklerine adapte edilebilir. Bu adaptasyonu yaparken de, uygulamanın çeşitli endişelerine uygun olacak şekilde bir hata toleransı kullanır. Benzeri şekilde, hibrit ve donanımsal hata toleransı yaklaşımları da bizim yaklaşımımızın sağladığı iyileştirmelerden faydalanabilirler.","As technology advances, the processors are shrunk in size and manufactured using higher density transistors which makes them cheaper, more power efficient and more powerful. While this progress is most beneficial to end-users, these advances make processors more vulnerable to outside radiation causing soft errors which occur mostly in the form of single bit flips on data. For protection against soft errors, hardware techniques like ECC (Error Correcting Code) and Ram Parity Memory are proposed to provide error detection and even error correction capabilities. While hardware techniques provide effective solutions, software only techniques may offer cheaper and more flexible alternatives where additional hardware is not available or cannot be introduced to existing architectures. Software fault detection techniques -while powerful- rely mostly on redundancy which causes significant amount of performance overhead and increase in the number of bits susceptible to soft errors. In most cases, where reliability is a concern, the availability and performance of the system is even a bigger concern, which actually requires a multi objective optimization approach. In applications where a certain margin of error is acceptable and availability is important, the existing software fault tolerance techniques may not be applied directly because of the unacceptable performance overheads they introduce to the system. Our technique Selective Software Fault Tolerance (SSFT) aims at providing availability and reliability simultaneously, by providing only required amount of protection while preserving the quality of the program output. SSFT uses software profiling information to understand application's vulnerabilities against transient faults. Transient faults are more likely to occur in instructions that have higher execution counts. Additionally, the instructions that cause greater damage in program output when hit by transient faults, should be considered as application weaknesses in terms of reliability. SSFT combines these information to eliminate the instructions from fault tolerance, that are less likely to be hit by transient errors or cause errors in program output. This approach reduces power consumption redundancy (therefore less data bits susceptible to soft errors), while improving performance and providing acceptable reliability. This technique can easily be adapted to existing software fault tolerance techniques in order to achieve a more suitable form of protection that will satisfy different concerns of the application. Similarly, hybrid and hardware only approaches may also take advantage of the optimizations provided by our technique."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Verilerden olasılık yoğunluk fonksiyonu parametrelerini kestirme işlemine yoğunluk kestirimi denir. Gauss karışım modeli (GKM) en çok tercih edilen yoğunluk ailelerinden biridir. Gauss karışımlarından örneklenmiş ilginç noktalar ile Gauss dağılımına sahip olmayan ilginç olmayan noktalardan oluşan veri öbeklerinden Gauss karısım kestiriminin nasıl yapıldığını inceliyoruz. Expectation-Maximization algoritması gibi geleneksel GKM kestirim teknikleri, ilginç olmayan noktalara olan hassaslığından dolayı, türdeş olmayan verilerdeki ilginç noktaları etkin olarak modelleyememektedir. Bir diğer olası sorun ise, iyi bir kestirim yapılabilmesi için ̈oncesinde gerçek bileşen sayısının genellikle bilinmesinin gerektiğidir. Biz, ilginç olmayan noktaların varlığına karşı sağlam, ilginç noktaların sayısını, karışımdaki Gauss sayısını ve gerçek karışım parametrelerini yinelemeli kestiren bir GKM kestirim algoritması tasarlıyoruz. Yöntemimiz, her yinelemede bir Gauss bileşenini sağlam bir formulasyonla kestirir. Bu bileşene ait ilginç nokta sayısı, çoklu-çözünürlük arama yöntemi kullanılarak adaylar arasından kestirilir. Eğer bu noktaların Gauss dağılımından gelme hipotezi kabul edilirse, kestirilen Gauss, karışım bileşeni olarak tutulur, ilgili noktalar veri öbeğinden ̧cıkarılır ve yinelemeler kalan noktalarla devam eder. Aksi durumda, kestirim süreci durdurulur ve kalan noktalar ilginç olmayan şeklinde etiketlenir. Bu ̧sekilde, durdurma kriteri herhangi ek bir bilgi olmadan gerçek bileşen sayısını belirler. Sentetik ve gerçek-ddünya veri öbekleri üzerinden yapılan karşılaştırmalı deneyler gösteriyor ki, algoritmamız gerçek bileşen sayısını belirleyebiliyor ve diğer iki algoritmaya göre daha iyi yoğunluk kestirimi yapabiliyor.","Density estimation is the process of estimating the parameters of a probability density function from data. The Gaussian mixture model (GMM) is one of the most preferred density families. We study the estimation of a Gaussian mixture from a heterogeneous data set that is defined as the set of points that contains interesting points that are sampled from a mixture of Gaussians as well as non-Gaussian distributed uninteresting ones. The traditional GMM estimation techniques such as the Expectation-Maximization algorithm cannot effectively model the interesting points in a heterogeneous data set due to their sensitivity to the uninteresting points as outliers. Another potential problem is that the true number of components should often be known a priori for a good estimation. We propose a GMM estimation algorithm that iteratively estimates the number of interesting points, the number of Gaussians in the mixture, and the actual mixture parameters while being robust to the presence of uninteresting points in heterogeneous data. The procedure is designed so that one Gaussian component is estimated using a robust formulation at each iteration. The number of interesting points that belong to this component is also estimated using a multi-resolution search procedure among a set of candidates. If a hypothesis on the Gaussianity of these points is accepted, the estimated Gaussian is kept as a component in the mixture, the associated points are removed from the data set, and the iterations continue with the remaining points. Otherwise, the estimation process is terminated and the remaining points are labeled as uninteresting. Thus, the stopping criterion helps to identify the true number of components without any additional information. Comparative experiments on synthetic and real-world data sets show that our algorithm can identify the true number of components and can produce a better density estimate in terms of log-likelihood compared to two other algorithms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hesaplamanın satın alınabilir bir üründen İnternet üzerinde kullanıcılara sunulabilir bir hizmete dönüşmesinin ardından, Bulut Bilişim yeni bir model olarak hizmetli hesaplamayı gerçekleştirmede eşsiz bir başarısıyla ortaya çıkmıştır. Gelişmekte olan herhangi bir teknoloji gibi, Bulut Bilişim de gelişimiyle beraber çözülmesi gereken yeni zorlukları ortaya çıkarmıştır. Bu çalışmada mevcut ağ koşullarını göz önüne alan Sanal Makine (SM) yerleştirme problemini sağlayıcı açısından özel bir senaryoda inceliyoruz. Adı geçen senaryoda, belli altyapı parçaları SM'lerden kaynaklanan yoğun trafik akımlarının son hedefi olmaktadır. Odaklandığımız senaryoda, SM'lerin verimliliği yüksek oranda mevcut altyapı tarafından yoğun trafik isteklerinin karşılanmasına bağlıdır. İlk olarak, SM'lerin verimliliğini yansıtan memnuniyet (satisfaction) olarak adlandırdığımız bir metriği tanımlayıp, bu metriği en yükseğe çıkarmaya çalışıyoruz. Tanımlanan problem NP-hard olup, probleme en uygun çözümü sağlayan polinom zamanda çalışan bir algoritma mevcut değildir. Bu nedenle, SM'lerin iletişim örneği ve akım istek profillerine dayanarak tahmini optimum çözüm sağlayan bir kaç çevrim dışı sezgisel algoritma öneriyoruz. Son bölümde, simülasyon deneyleri ile, önerilen algoritmaların etkisini değerlendirip performanslarını karşılaştırıyoruz.","Following a shift from computing as a purchasable product to computing as a deliverable service to the consumers over the Internet, Cloud Computing emerged as a novel paradigm with an unprecedented success in turning utility computing into a reality. Like any emerging technology, with its advent, Cloud Computing also brought new challenges to be addressed. This work studies network and traffic aware virtual machine (VM) placement in Cloud Computing infrastructures from a provider perspective, where certain infrastructure components have a predisposition to be the sinks or sources of a large number of intensive-traffic flows initiated or targeted by VMs. In the scenarios of interest, the performance of VMs are strictly dependent on the infrastructure's ability to meet their intensive traffic demands. We first introduce and attempt to maximize the total value of a metric named ``satisfaction"" that reflects the performance of a VM when placed on a particular physical machine (PM). The problem is NP-hard and there is no polynomial time algorithm that yields an optimal solution. Therefore we introduce several off-line heuristics-based algorithms that yield nearly optimal solutions given the communication pattern and flow demand profiles of VMs. We evaluate and compare the performance of our proposed algorithms via extensive simulation experiments."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris-vektör ve devrik-matris-vektör çarpımları (Seyrek AATx) yinelemeli çözücülerde kullanılan çekirdek işlemlerdir. Girdi matrisi A'nın ve devriğinin seyreklik deseni yinelemeler boyunca aynı kalmaktadır. Matrisin düzensiz seyreklik deseni nedeniyle, bu Seyrek AATx operasyonları sırasında CPU önbelleği tam anlamıyla kullanılamaz. Seyrek AATx operasyonu için iki paralelleştirme stratejisi öneriyoruz. Metotlarımız A matrisini bölümleyerek matris sıfır dışı girdileri ve vektör girdileri için önbellek yerelliği sağlamaktadır. Deneylerimizi çok çeşitli seyrek matrisler kullanarak piyasaya yeni sunulmuş Intel Xeon Phi yardımcı işlemci üzerinde yürüttük. Deneysel sonuçlar önerdiğimiz metotların literatürdeki en gelişmiş metotlardan daha yüksek performans geliştirmesi elde ettiğini göstermektedir.","Sparse matrix-vector and matrix-transpose-vector multiplications (Sparse AATx) are the kernel operations used in iterative solvers. Sparsity pattern of the input matrix A, as well as its transpose, remains the same throughout the iterations. CPU cache could not be used properly during these Sparse AA T x operations due to irregular sparsity pattern of the matrix. We propose two parallelization strategies for Sparse AA T x. Our methods partition A matrix in order to exploit cache locality for matrix nonzeros and vector entries. We conduct experiments on the recently-released Intel Xeon Phi coprocessor involving large variety of sparsematrices. Experimental results show that proposed methods achieve higher performance improvement than the state-of-the-art methods in the literature."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu calışmada gorsel konseptlerin otomatik olarak internet kaynaklı imgeler kullanılarak oöğrenilmesi üzerine çalısılmıstır. Sunulan iki yeni yontem ile bir konsept için edinilmiş imge koleksiyonundaki ortak özellikleri kullanarak, ilgisiz imgeleri elemek ve imgeleri gorsel bütünlük içinde gruplanmak amaçlanmıştır. ̇Ilk olarak, yeni bir veri öbekleme ve ilintisiz veri eleme yontemi Konsept Haritası (Concept Map - CMAP) sunulmştur. CMAP verileri öbeklere ayırırken, ilgisiz verileri bu öbeklere olan benzerliklerine göre eler. Daha sonrasında, CMAP'in bir konsept için ürettigi her bir veri öbeginden, konseptin değişik bir alt kümesini tanımlayan, birer model öğrenilir. Diger bir yontem, Model Evrimi ile Eşleme (Association through Model Evolution - AME), imgelerin rasgele alınmış büyük bir imge kümesi ile farklarını yinelemeli bir yöntem ile ölçer. Bu ölçümlere dayanarak, her yinelemede, yeni bir grup ilintisiz imge elenir. AME her bir imgenin, rasgele alınmış büyük bir imge kümesine karşı, ait olduğu konsept için ayrımsallık ve temsil edebilirlik özelliklerini teşhis eder. Bu özellikleri göz önüne alarak, ilitisiz imgeleri bulur. En son aşamada, temizlenmiş imge setleri üzerinden hesaplanmıs ̧ modeller ile, yeni imgeler üzerinde konseptsel sınıflandırma yapılır. Sunulan iki yeni yöntem de bilindik veri setleri ve problemler üzerinde sınanmıs ̧tır. Sonuçar bilindik en iyi yöntemler ile kıyaslanabilir değerler vermektedir.","We attack the problem of learning concepts automatically from noisy Web image search results. The idea is based on discovering common characteristics shared among category images by posing two novel methods that are able to organise the data while eliminating irrelevant instances. We propose a novel clustering and outlier detection method, namely Concept Map (CMAP). Given an image collection returned for a concept query, CMAP provides clusters pruned from outliers. Each cluster is used to train a model representing a different characteristics of the concept. One another method is Association through Model Evolution (AME). It prunes the data in an iterative manner and it progressively finds better set of images with an evaluational score computed for each iteration. The idea is based on capturing discriminativeness and representativeness of each instance against large number of random images and eliminating the outliers. The final model is used for classification of novel images. These two methods are applied on different benchmark problems and we observed compelling or better results compared to state of art methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde gerçek zamanlı fizik tabanlı bir artırılmış giyinme kabini ortamı için bir çalışma çerçevesi önerilmektedir. İnsan avatarı ve kıyafet için gerekli üç boyutlu modeller özel sınırlar çerçevesinde modellenmiştir. Bu modeller daha sonra bir derinlik alıcısı tarafından takip edilen bir kullanıcıdan alınan girdi ile gerçek zamanlı olarak hareket ettirilmektedir. Simülasyon kalitesini arttırmak amacı ile hareketler çeşitli filtrelerden geçirilmektedir. Eylemsizlik, dış kuvvetler ve çarpışma gibi dış etkenler kıyafet modeline uygulanmaktadır. Avatar ve kıyafet modelleri, kullanıcının boyutlarına göre özelleştirilebilir. Sistem üst kalite bir kişisel bilgisayar üzerinde gerçek zamanlı olarak gerçekçi görüntüler oluşturarak çalışmaktadır.","This thesis proposes a framework for a real-time physically-based augmented cloth fitting environment. The required 3D meshes for the human avatar and apparels are modeled with specific constraints. The models are then animated in real-time using input from a user tracked by a depth sensor. A set of motion filters are introduced in order to improve the quality of the simulation. The physical effects such as inertia, external and forces and collision are imposed on the apparel meshes. The avatar and the apparels can be customized according to the user. The system runs in real-time on a high-end consumer PC with realistic rendering results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Histopatolojik doku incelemesi kanser teşhis ve derecelendirmesinde rutin olarak uygulanan yöntemdir. Fakat, bu inceleme pataloğun uzmanlığına ve deneyimine bağlı olan görsel çıkarımlar gerektirdiği için öznellik içerir. Sonuçlardaki öznelliğin etkisini azaltmak için doku görüntüsünü nicel özelliklerle temsil eden ve bu özellikleri kullanarak doku sınıflandırması ve derecelendirmesi yapan otomatik kanser tanı ve derecelendirme sistemleri önerilmiştir. Bu tezde, etkili bir şekilde histopatolojik doku görüntülerini temsil etmek ve sınıflandırmak için yeni bir yaklaşım sunulmuştur. Bu yaklaşımda, doku görüntülerinin histolojik bileşenlerine ayrılması önerilmiş ve bu bileşenlerin doku içerisindeki dağılımını modellemek için lokal nesne desenleri olarak adlandırdığımız yeni bir grup örgüsel tanımlayıcı ortaya konulmuştur. Bu tanımlayıcılar, lokal ikili desenler yönteminin mantığı kullanılarak tanımlanmıştır. Ancak, pikseli, komşularının göreceli yoğunluğuna göre ikili bir dizi kurarak niceleyen piksel seviyesindeki lokal ikili desenlerin aksine, doku bileşenlerini nicelemek amacıyla, lokal nesne desen tanımlayıcıları bileşen seviyesinde tanımlanmıştır. Bu amaçla, değişik lokallik alanındaki komşuluklar belirlenmiş ve belirlenen komşuluklardaki bileşenlerin uzaydaki düzeni kodlanmıştır. Sonrasında, histolojik bileşenleri karakterize etmek amacıyla, bu dizilerden örgüsel tanımlayıcılar çıkartılmış ve bu şekilde karakterize edilmiş bileşenlerden resmin kelime-torbası temsili oluşturulmuştur. Bu tezde, bileşenlerin seçilmesi için iki yaklaşım kullanılmıştır: İlk yaklaşım kelime-torbası temsilini çıkartmak için tüm bileşenleri kullanırken, ikinci yaklaşım çizge yürümesi ile birden fazla bileşen alt kümesi seçmiş ve bunlardan birden fazla kelime-torbası temsili oluşturmuştur. Mikroskopik histopatolojik kolon doku görüntüleri üzerinde yaptığımız deneyler, önerilen bileşen seviyesindeki örgüsel tanımlayıcıların önceki örgüsel yaklaşımlara göre daha yüksek doğruluk oranları verdiğini göstermektedir.","Histopathological examination of a tissue is the routine practice for diagnosis and grading of cancer. However, this examination is subjective since it requires visual interpretation of a pathologist, which mainly depends on his/her experience and expertise. In order to minimize the subjectivity level, it has been proposed to use automated cancer diagnosis and grading systems that represent a tissue image with quantitative features and use these features for classifying and grading the tissue. In this thesis, we present a new approach for effective representation and classifi cation of histopathological tissue images. In this approach, we propose to decompose a tissue image into its histological components and introduce a set of new texture descriptors, which we call local object patterns, on these components to model their composition within a tissue. We de fine these descriptors using the idea of local binary patterns. However, we de fine our local object pattern descriptors at the component-level to quantify a component, as opposed to pixel-level local binary patterns, which quantify a pixel by constructing a binary string based on relative intensities of its neighbors. To this end, we specify neighborhoods with diff erent locality ranges and encode spatial arrangements of the components within the specifi ed local neighborhoods by generating strings. We then extract our texture descriptors from these strings to characterize histological components and construct the bag-of-words representation of an image from the characterized components. In this thesis, we use two approaches for the selection of the components: The first approach uses all components to construct a bag-of-words representation whereas the second one uses graph walking to select multiple subsets of the components and constructs multiple bag-of-words representations from these subsets. Working with microscopic images of histopathological colon tissues, our experiments show that the proposed component-level texture descriptors lead to higher classi fication accuracies than the previous textural approaches."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hiperçizgeler, bir kenarın herhangi bir sayıda düğümü bağlayabilme özelliği olduğu, çizgelerin genelleştirilmis bir versiyonudur. Bu genelleme ile hiperçizgeler yüksek bir modelleme gücüne sahiptir öyle ki kombinatöriyel bilimsel hesaplama alanında birçok önemli problem hiperçizgeler ile güçlü bir şekilde modellenebilmektedir. Bu tez ise hiperçizge tabanlı yöntemler kullanılarak veri bölümleme problemlerinin çözülmesini araştırmaktadır. Bu tez üç ana bölümden oluşmaktadır. Birinci bölümde, özyinelemeli çizge ikiye-bölümleme kullanarak, verimli bir hiperçizge bölümlere aracının nasıl oluşturulduğu gösterilmektedir. İkinci ve üçüncü bölümlerde, paralel hesaplamadaki iki önemli veri bölümleme probleminin hiperçizge bölümleme ile nasıl modellendiği gösterilmektedir. Birinci problem paralel sorgu hesaplama için indeksin terim-tabanlı bölümlenmesi problemidir. İkincisi ise yeni önerilen bir paralel matriks vektör çarpımında kullanılmak üzere yine yeni önerilen bir seyrek matriks bölümleme problemidir. Bu tezde, hiperçizge tabanlı modelleri ile daha kaliteli veri bölümleme elde edildiği gösterilmektedir. Anahtar sozcukler: hipercizge, veri bolumleme, kombinatoriyel algoritmalar.","A hypergraph is a general version of graph where the edges may connect any number of vertices. By this flexibility, hypergraphs has a larger modeling power that may allow accurate formulaion of many problems of combinatorial scientific computing. This thesis discusses the use of hypergraph-based approaches to solve problems that require data partitioning. The thesis is composed of three parts. In the first part, we show how to implement hypergraph partitioning efficiently using recursive graph bipartitioning. The remaining two parts show how to formulate two important data partitioning problems in parallel computing as hypergraph partitioning. The first problem is global inverted index partitioning for parallel query processing and the second one is row-columnwise sparse matrix partitioning for parallel matrix vector multiplication, where both multiplication and sparse matrix partitioning schemes has novelty. In this thesis, we show that hypergraph models achieve partitions with better quality."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Araç tasarsız ağları (VANET) akademik ve ticari alandan birçok araştırmanın ilgisini çeken yeni bir iletişim teknolojisidir. Daha güvenli ve etkin bir trafik koşulları sağlamak için umut veren bir teknolojidir. Bu, araçlar arası güvenlik mesajı alışverişi sağlayarak gerçekleştirilir. Bu mesajlaşma komşu araçların pozisyonları ile ilgili araçların farkındalığını artırırken tehlikeli durumlar hakkında da sürücüleri uyarır. Fakat bu tür bilgilerin varlığı araçların takip edilmesine imkan verir. Dolayısı ile bu bilgiler potansiyel kötüye kullanmalara karşı korunmalıdır. Bu tezde, küresel bir pasif saldırgan varlığında rastgele değişen takma adlar ile VANET'deki konum gizliliğini sağlama problemini ele aldık. Karma bölgelerde, araçların mesaj yayın sürelerine, takma adlarını güncelledikten sonra rastgele bir gecikme ekleyemeyi öngördük. Bu yöntemle, takma adlar arası bağlantı ihtimalini azaltmayı amaçladık. Bu yöntem, araçların takip edilmesini zorlaştırırken, VANET'e katılan sürücülerin güvenini artırmaya yardımcı oldu. Bir adım öteye giderek gecikmeleri tamamen rastgele değil de sessiz araç sayısına göre eklemeyi öngördük. Bunlara ek olarak, ağdaki karma nokta sayısının kullanıcıların konum gizliliğine etkisini de araştırdık. Sistemlerinin performansını değerlendirmek için çeşitli simülasyonlar yaptık. Deney sonuçları, mesaj yayın dönemine gecikme ekleyerek VANET sürücülerinin gizliliğinin arttığını göstermekte ve aynı zamanda takma ad değiştirerek elde edilen gizlilik düzeyi ile saldırgan gücü arasındaki ilişki hakkında bilgi vermektedir. Keywords: pseudonym, mix zone, location privacy, vehicular ad hoc network.","VANET is an emerging new communication technology which has attracted a lot of research attention from academic community and industry. It is promising technology to provide safer and efficient traffic conditions. This is realized by letting vehicles to exchange safety messages. This messaging increases the awareness of vehicles about their neighboring vehicles positions and warns drivers about dangerous situations. However, availability of such information facilitates the tracking of vehicles. So, this information must be protected against the potential misuse. In this thesis, we address the problem of achieving location privacy in VANETs with randomly changing identifiers in the presence of a global passive adversary. We suggest adding a random delay to message broadcast period after the vehicle update its pseudonym in mix zones. By this way, we want to mitigate the linkability between pseudonyms. This could help to make tracking more difficult and increase safety and confidence of drivers using VANET. Instead of adding delay to safety messages completely random, one step taken further and delay is added according to the silent vehicle number in mix zone. We also investigated the effects of different number of mix-zone placements in the network to the location privacy of users. Several simulations have been performed to evaluate the performance of the systems. The results of experiments show that adding delay to message broadcast period improves location privacy of drivers in VANET and also provide information about the relationship between the strength of the adversary and the level of privacy achieved by changing pseudonyms. Anahtar sozcukler: takma ad, karma bolge, konum gizligi, arac tasarsz ag ."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Orgu modellerinin animasyonu srasnda, iskeletlendirilmis model, deform ola- bilen model, veya zik tabanl simulasyon gibi bir cok teknik kullanlabilir. Tumbu teknikler ile animasyon olusturma islemi zaman alc ve acemi kullanclar icinzahmetli olmakla birlikte; halihazrda mevcut insan hareket yakalama verilerininuyarlanmas, sureci onemli olcude kolaylastrabilir. Bu tezde, mumkun olduguncaaz kullanc etkilesimi ile, insan hareketi verilerinin key 3B orgu modellerineuyarlanmas/aktarlmas icin bir yontem sunulmaktadr. Daha once gelistirilmishareket aktarma sistemleri, verilen cesitli hareket kstlarn saglayarak, orijinalhareketi oldugu gibi korumaya calsr. Bizim yaklasmmzda, kullanc tarafndansaglanan birkac poza-poz ornek kullanlarak, aktarma surecinin arkasndakimantksal yapnn bulunmas amaclanmaktadr. Boylece insan iskeletinden farklyapya ve/veya hareket mantgna sahip olan modeller olas girdiler halinegetirilmektedir.Ayrca yaygn olarak bulunan ve herhangi bir ek yap (orn. iskelet)icermeyen orgu modelleri dusunulerek, dahili yuzey tabanl deformasyon sistemisunulmaktadr. Bu deformasyon sisteminde, animasyon amacl deformasyonlarkat davranstan daha fazlasn gerektirebilecegi icin, hacim korumal ve cizgi- lm benzeri sonuclarn elde edilmesini saglayabilecek bir teknik onerilmektedir.Gelistirilen yontemin nasl calstgn gostermek icin, cesitli hareket yakalama verileri uc adet tannms modele aktarlmstr. Ek olarak, sadece insans modellerihede eyen otomatik hareket aktarma tekniklerinin bizim modellerimiz uzerindenasl calstklar da gosterilmistir.Anahtar sozcukler: Orgu deformasyonu, hacim korunumu, animasyon, hareket aktarm.","Animation of mesh models can be accomplished in many ways, including character animation with skinned skeletons, deformable models, or physic-based simulation. Generating animations with all of these techniques is time consumingand laborious for novice users; however adapting already available wide-rangehuman motion capture data might simplify the process signi cantly. This thesispresents a method for retargeting human motion to arbitrary 3D mesh modelswith as little user interaction as possible. Traditional motion retargeting systemstry to preserve original motion as is, while satisfying several motion constraints.In our approach, we use a few pose-to-pose examples provided by the user toextract desired semantics behind retargeting process by not limiting the transferto be only literal. Hence, mesh models, which have dierent structures and/ormotion semantics from humanoid skeleton, become possible targets. Also considering mesh models which are widely available and without any additional structure (e.g. skeleton), our method does not require such a structure by providinga build-in surface-based deformation system. Since deformation for animationpurpose can require more than rigid behaviour, we augment existing rigid deformation approaches to provide volume preserving and cartoon-like deformation.For demonstrating results of our approach, we retarget several motion capturedata to three well-known models, and also investigate how automatic retargetingmethods developed considering humanoid models work on our models.Keywords: Mesh deformation, volume preservation, animation, motion retargeting"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağlar, belli bir amacı gerçekleştirmek için ilgilenilen bölgeye dağıtılmış algılayıcı düğümlerden oluşur. Bu düğümler çevresel ve fiziki şartları izleyerek toplanan veriyi belli bir merkezi düğüme gönderir. Kablosuz algılayıcı ağlar ile ilgili çoğu çalışmada algılayıcı düğümlerin gücünün ağ yaşam süresini sınırlayan değiştirilemez piller ile sağlandığı varsayılmaktadır. Diğer taraftan, algılayıcı düğümlerin en az bir kısmına güç sağlayabilecek ve böylece ağ yaşam süresini uzatabilecek şebeke elektriği, enerji hasadı mekanizmaları gibi daimi güç kaynakları da bulunmaktadır. Burada amacımız pil ve şebeke elektriği ile beslenen farklı tipteki algılayıcı düğümlere sahip bu tarz heterojen kablosuz algılayıcı ağların yaşam süresini merkezi ve dağıtık yol atama algoritmaları kullanarak ve bu esnada fazla enerji gerektiren işleri şebeke elektriği ile beslenen düğümlere atayarak uzatmaktır. İlk olarak bir sınıf yol atama algoritması üretiminde kullanılabilecek bir çerçeve öneriyoruz. Üretilen algoritmaların ortak özelliği şebeke elektriği ile beslenen düğümleri, hedef düğümü ve gerekiyorsa pil ile beslenen bazı düğümleri içeren bir omurga oluşturarak veriyi aktarmak için bu omurgayı kullanmalarıdır. Simülasyon sonuçlarımız bu çerçeveyi kullanarak ürettiğimiz merkezi algoritmaların ağ yaşam süresinde iki kata kadar artış sağladığını göstermiştir. Ayrıca bu çalışmada veri iletimi için şebeke elektriği ile beslenen düğümlere öncelik veren güç kaynağı bilinçli, omurga temelli ve tam dağıtık bir yol atama algoritması da öneriyoruz. Önerdiğimiz bu algoritmayı geçerlemek ve değerlendirmek amacıyla ns-2 ortamını kullanarak elde ettiğimiz simülasyon sonuçları göstermektedir ki, algoritmamız düşük bir ek haberleşme yükü ile ağ yaşam süresini belirgin şekilde arttırmaktadır. Bu çalışmada, bahsi geçen teknoloji bağımsız yol atama çözümlerimizin yanısıra, 802.15.4/ZigBee kablosuz ağ teknolojisine özel, güç kaynağı bilinçli, tam dağıtık, ağaç tabanlı ve trafiğe uyumlanabilen bir yol atama çözümü de önermekteyiz. Önerdiğimiz yol atama çözümü pil ile beslenen düğümlerin haberleşme yolları üzerinde yer almasını mümkün olduğunca önlemek amacıyla, dağıtık ve hiyerarşik ağ adresi atama mekanizması gibi ZigBee protokolüne özel yetenekleri kullanmaktadır. ZigBee teknolojisine özel algoritmamızı geçerlemek ve değerlendirmek amacıyla ilk olarak ZigBee protokolünün ihtiyaç duyduğumuz kısımlarını, daha sonra da önerdiğimiz algoritmayı ns-2 simülasyon ortamında gerçekledik. Hazırladığımız ns-2 simülasyon ortamını kullanarak elde ettiğimiz sonuçlar, ZigBee tanımında yer alan yol atama yöntemi ile karşılaştırıldığında, önerdiğimiz algoritmanın yol uzunluklarını arttırmadan ağ yaşam süresini arttırabildiğini göstermiştir.","A wireless sensor network (WSN) is a collection of sensor nodes distributed over an area of interest to accomplish a certain task by monitoring environmental and physical conditions and sending the collected data to a special node called sink. Most studies on WSNs consider nodes to be powered with irreplaceable batteries, which limits network lifetime. There are, however, perpetual power source alternatives as well, including mains electricity and energy harvesting mechanisms, which can be utilized by at least some portion of the sensor nodes to further prolong the network lifetime. Our aim here is to increase the lifetime of such WSNs with heterogeneous power sources by centralized or distributed routing algorithms that distinguish battery- and mains-powered nodes in routing, so that energy consuming tasks are carried out mostly by mains-powered nodes. We first propose a framework for a class of routing algorithms, which forms and uses a backbone topology consisting of all mains-powered nodes, including the sinks, and possibly some battery-powered nodes, to route data packets. We propose and evaluate a set of centralized algorithms based on this framework, and our simulation results show that our algorithms can increase network lifetime by up to more than a factor of two. We also propose a fully distributed power-source-aware backbone-based routing algorithm (PSABR) that favors mains-powered nodes as relay nodes. We validate and evaluate our distributed algorithm with extensive ns-2 simulations and our results show that the proposed distributed algorithm can enhance network lifetime significantly with a low control messaging overhead. Besides wireless technology independent routing solutions, we also propose a technology specific power-source-aware routing solution (PSAR) for sensor and ad hoc networks which use 802.15.4/ZigBee as the wireless technology. Our solution is fully distributed, tree-based, and traffic-adaptive. It utilizes some protocol specific properties of ZigBee, such as distributed and hierarchical address assignment, to eliminate battery-powered nodes on the routing paths as much as possible. To validate and evaluate our ZigBee-specific algorithm, we first implemented ZigBee extensions to ns-2 simulator and then implemented and simulated our protocol in this extended ns-2 environment. Our results show that the proposed algorithm operates efficiently and can increase network lifetime without increasing the path lengths significantly, compared to the default ZigBee routing algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada, gerçek dünya heykeltıraşlığına benzeyen ön / kavramsal modelleri tasarlamak için kullanılabilir bir sanal gerçeklik uygulaması sunuyoruz. Önerilen sistem, insan-bilgisayar etkileşimi deneyimi artıran yenilikçi bir işaret tabanlı arayüz kullanmaktadır. İşaret tabanlı arayüz, el ve parmak hareketleri yakalamak için, klasik girdi aygıtları olan fare ve klavye yerine, veri eldiven ve altı derece serbestlikte veri toplayan bir manyetik konum izleyicilerden faydalanmaktadır. Tasarımlar; hacimce deforme edilebilir model, tasarım araçları ve sanal el içeren bir sanal ortamda gerçekleştirilmektedir. Sanal tasarım ortamında yer alan bu sanal el, veri eldiven ve pozisyon izleyici sayesinde kullanıcının el hareketlerini taklit ederek yönlendirilmektedir. Sistem, kullanıcıların tasarım araçları ve sanal el yardımıyla deforme edilebilir modeli işleyerek şekil vermesine olanak tanımaktadır. Model üzerinde deformasyon, tasarım araçları veya doğrudan sanal el ile, modele dışardan malzeme (hacim hücreleri) doldurma veya modelden malzeme oyularak yapılmaktadır. Tasarım sürecinde sistem ?kuvvet geribildirim göstergesi? sayesinde kullanıcılara görsel yardım sağlamaktadır. Ayrıca kullanıcılar, veri eldiveni ve pozisyon izleyiciyi tarafından yönlendirilen ?el faresi? ile geleneksel grafik kullanıcı arayüzü öğeleri ile etkileşime girebilirmektedirler. Kullanıcılar aynı zamanda gerçek zamanlı yörünge tabanlı el hareket/jest tanıyan algoritma sayesinde uygulamayı kontrol edebilmektedirler. Sunulan el hareketi tanıma tekniği, kapsamlı ve büyük eğitim verilerine ihtiyaç duymadan, sisteme yeni hareketler öğretmeye olanak sağlamaktadır. Sunulan teknikte, el hareketleri, 2 boyutlu yönlü hareketlerin sıralı dizisi olarak temsil edilir. Öğrenme aşamasında, sisteme sunulan örnek jestler/el hareketleri filtrelenerek işlenir. Daha sonra bu işlenmiş veri birer sonlu durum makinesi dizi tanıyıcıları olan el hareketi tanıyıcı makinaları oluşturmak için kullanılmaktadır. El hareketleri, jestlerin başlangıç ve bitiş noktaları belirtmeye gerek kalmadan bu tanıyıcılar tarafından gerçek zamanlı olarak sistem tarafından tanınabilmektedirler. Tez kapsamında yapılan kullanıcı çalışmasının sonucunda, önerilen yöntem, sürekli bir hareket akışı içersinde belirli el hareketlerini/jestleri \% 73 doğruluk ile algılama ve tanıma performansı göstermiştir. Ayrıca kullanıcı tutum anketinin sonuçlarına göre, işaret tabanlı arayüz kullanıcılar tarafından çok yararlı ve tatmin edici bulunmuştur. Önerilen yaklaşımın en önemli faydalarından biri de kullanıcıların seçilen görevler için kendi tercihlerine göre hareket komutları oluşturma özgürlüğü veriyor olmasıdır. Böylece, sunulan jest tanıma yaklaşımı insan-bilgisayar etkileşim sürecini daha sezgisel ve kullanıcıya özel hale getirmektedir.","In this study, we propose a virtual reality application that can be utilized to design preliminary/conceptual models similar to real world clay sculpting. The proposed system makes use of the innovative gestural interface that enhances the experience of the human-computer interaction. The gestural interface employs advanced motion capture hardware namely data gloves and six-degrees-of-freedom position tracker instead of classical input devices like keyboard or mouse. The design process takes places in the virtual environment that contains volumetric deformable model, design tools and a virtual hand that is driven by the data glove and the tracker. The users manipulate the design tools and the deformable model via the virtual hand. The deformation on the model is done by stuffing or carving material (voxels) in or out of the model with the help of the tools or directly by the virtual hand. The virtual sculpting system also includes volumetric force feedback indicator that provides visual aid. We also offer a mouse like interaction approach in which the users can still interact with conventional graphical user interface items such as buttons with the data glove and tracker. The users can also control the application with gestural commands thanks to our real time trajectory based dynamic gesture recognition algorithm. The gesture recognition technique exploits a fast learning mechanism that does not require extensive training data to teach gestures to the system. For recognition, gestures are represented as an ordered sequence of directional movements in 2D. In the learning phase, sample gesture data is filtered and processed to create gesture recognizers, which are basically finite-state machine sequence recognizers. We achieve real time gesture recognition by these recognizers without needing to specify gesture start and end points. The results of the conducted user study show that the proposed method is very promising in terms of gesture detection and recognition performance (73\% accuracy) in a stream of motion. Additionally, the assessment of the user attitude survey denotes that the gestural interface is very useful and satisfactory. One of the novel parts of the proposed approach is that it gives users the freedom to create gesture commands according to their preferences for selected tasks. Thus, the presented gesture recognition approach makes the HCI process more intuitive and user specific."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Rassallık, kriptoloji için çok önemli bir kavramdır ve rassal sayı üreteçleri, hemen hemen tüm kriptografik sistemlerde kullanılan temel yapı taşlarındandır. Bu nedenle, rassal sayı üretimi, güvenli iletişimin anahtar noktalarındandır. Rassal sayı üretimi güvenli iletişimin güvencesini vermez. Sorunlu rassal sayı üretim işlemi, zayıf şifreleme anahtarları oluşturacağından güvenli iletişim hatlarının kırılmasına sebep olabilir. Bilgisayarlar ve akıllı cihazlarda rassal sayı üretimi, işletim sistemleri tarafından gerçekleştirilir. Uygulamalar, çalışmaları esnasında ihtiyaç duydukları rassal sayıları, işletim sistemlerinden talep ederler. Rassal sayı üretiminin çok hassas ve önemli bir süreç olmasından ötürü, bu sürecin farklı işletim sistemleri için derinlemesine ve kriptografik olarak incelenmesi gerekmektedir. Bu noktadan yola çıkarak, Android işletim sisteminin kaynak kodlarına bakarak rassal sayı üretim sürecini inceledik ve Android işletim sisteminin güvenli rassal sayı üretiminin Linux işletim sisteminin rassal sayı üretiminin güvenliğine bağlı olduğunu tespit ettik. Ardından Android işletim sisteminin çekirdeğinin kaynak kodlarını değiştirerek rassal sayı üretecini test ettik ve entropi tahminleri üzerinde farklı testler gerçekleştirdik. Son olarak, Android cihazların açılışı esnasında, rassal sayı üretimi merkezli ortaya çıkabilecek zayıflıkları araştırdık. Anahtar sozcukler: SecureRandom, rassal say uretimi/uretecleri, Linux Rassal Say Ureteci, Android Rassal Say  Ureteci, entropi tahmini.","Randomness is a crucial resource for cryptography, and random number generators are critical building blocks of almost all cryptographic systems. Therefore, random number generation is one of the key parts of secure communication. Random number generation does not guarantee security. Problematic random number generation process may result in breaking the encrypted communication channel, because encryption keys are obtained by using random numbers. For computers and smart devices, generation of random numbers is done by operating systems. Applications which need random numbers for their operation request them from the operating system they are working on. Due to the importance of random number generation, this process should be analyzed deeply and cryptographically for different operating systems. From this perspective, we studied Android random number generation process by looking at the source codes and found that security of random number generation done by Android relies on the security of random number generation of Linux. Then we analyzed Android random number generator by modifying the kernel source code and applying some tests on its entropy estimator. Finally, we looked for possible weaknesses of random number generator during startup of Android devices. Keywords: SecureRandom, random number generation/generators, Linux RNG,Android RNG, entropy estimator."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Zaman bölmeli çoklu erişim (TDMA) tabanlı zamanlanmış kanal erişimi kullanan kablosuz algılaycı ağlarında (KAA) zaman dilimlerinin uzaysal yeniden kullanımı çakışmayan düğüm kümeleri için mümkündür. Zaman dilimlerinin uzaysal yeniden kullanımının yanı sıra bazı algılayıcı düğüm düzlemlerinde de yer alan kullanılabilir birden fazla kanal özelliği eş zamanlılığı artırmak ve bir tur iletişimde gereken zaman dilimi sayısını azaltmak için kullanılabilir. Bu tezde, çok kanallı kablosuz algılayıcı ağları için TDMA-tabanlı zamanlama algoritmaları önerilmektedir. Çok kanallı ortamda çakışma tekrar tanımlanarak mevcut iki adet tek-kanallı TDMA zamanlama algoritması çok kanallı yapıya genişletilmektedir. Ayrıca, genişletilmiş çok-kanallı zamanlama algoritmaları ile kullanıma uygun NCA ve LCA adı verilen kanal atama yöntemleri önerilmektedir. Önerilen yöntemler ayrıntılı benzetim ve deneylerle değerlendirilmektedir ve literatürde bilinen diğer tek-kanallı ve çok-kanallı algoritmalarla karşılaştırılmaktadır. Elde edilen sonuçlar önerdiğimiz algoritmaların geniş ağlarda karşılaştırılan diğer yöntemlerden daha iyi başarım ve eş zamanlılık gösterdiğini ve %50'ye varan ölçüde daha az gecikme sağladığını göstermiştir. Anahtar sozcukler: Kablosuz alglayc aglar, cok-kanall, zamanlama, kanal atama.","In wireless sensor networks (WSNs) that use TDMA-based scheduled channel access, spatial re-use of time-slots is possible among a non-conflicting set of nodes. In this way, data gathering delays can be reduced and aggregate network throughput can be increased. Besides spatial re-use, available multiple channels, which is already an available feature in some sensor node platforms, can be utilized to increase concurrency and minimize the number of time-slots required for a round of communication. In this thesis, we propose TDMA-based scheduling algorithms for multi-channel wireless sensor networks. By rede fining the conflicts in a multi-channel environment, we extend two existing single-channel TDMA scheduling algorithms into multi-channel structure. We also present two channel assignment schemes (called NCA and LCA) appropriate to use with the extended multi-channel scheduling algorithms. We evaluate our proposed schemes by extensive simulation experiments and compare them with other single-channel and multi-channel algorithms from literature. The results show that in large networks our proposed algorithms can provide better performance, more concurrency, and up to 50% less delay compared to other methods. Keywords: Wireless sensor networks, multi-channel, TDMA, scheduling, channel assignment."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seçilmiş temel veri madenciligi görevlerini iyileştirmek için bir çok yaklaşım üzerinde yoğunlaştık. Şu andaki tez büyük miktardaki veri için paralel işleme metodlarının iyileştirilmesiyle alakalıdır. Hem seyrek hem yoğun verikümeleri için yeni koşut veri madenciliği algoritmaları geliştirdik, ve bütün-çiftler benzerlik problemi için 1-B ve 2-B koşut algoritmalar önerdik. NoClique ve NoClique2 adında iki yeni koşut veri madenciliği algoritması bitdrill adındaki kendi ardışık dikey sık kalemkümesi madenciliği (SKM) algoritmamızı koşutlaştırmaktadır, ve düğüm ayracı ile çizge bölümleme (DAÇB) kullanan bir metodla kalemleri dağıtmakta ve seçici biçimde yinelemektedir. Metod düğümlerin sık kalemlere ve kenarların iki boyutundaki sık kalem kümelerine karşılık geldiği bir çizge üzerinde çalışmaktadır. Bu çizgenin düğüm ayracının bulunmasının kalem dagıtımı tarafından tespit edilen alt-veritabanlarının bağımsız bi ?çimde işlenmesi için yeterli olduğunu gösterdik. Bu dağıtım uygun ağırlıkların düğümlere verilmesiyle minimize edilen bir veri yinelemesine yol açmaktadır. Veri dağıtımı şeması iki yeni koşut sık kalemkümesi madenciliği algoritmasının tasarımında kullanılmaktadır. Iki algoritma da ayraca karşılık gelen kalemleri yineler. NoClique ayracın sebep olduğu işi yineler ve NoClique2 ayni işi kolektif olarak hesaplar. Hesapsal yük dengeleme ve yinelenen yahut kolektif işin minimizasyonu uygun yük tahminlerinin düğümlere atanmasıyla başarılabilir. Başarım bü ?tün kalemleri yineleyen başka bir koşutlaştırmayla ve ParDCI algoritmasıyla karşılaştırılır. Seçici kalem yinelemeyle kalem dağıtımını kullanan başka bir koşut SKM algoritması tanıtıyoruz. Daha önce önerdiğimiz koşut SKM için DACB modelini, bağımsız madencilik koşulunu gevşetme suretiyle genişletiyoruz. Bağımsız keşfedilen kalem kümeleri bulmak yerine, iletişim miktarını minimize edebiliriz ve adayları ince-gözenekli biçimde bölümleyebiliriz. Koşut hesaplamanın düğümlerin adaylara ve hiperkenarların kalemlere karşılık geldiği bir hiperçizge bölümleme modelini öneriyoruz. Her adaya düğüm ağırlıklarıyla bir yük tahmini atanır, ve kalem sıklıkları hiperkenar ağırlıkları olarak atanır. Modelin veri yinelemesini minimize ettiği ve yükleri yüksek kesinlikle dengelediği gösterilir. Aynı zamanda sadece belli bir sayıda seviyenin adaylarını u ?retebileceğimiz için, önceki kalem dağıtımını temsil eden sabit düğümlerin olduğu bir yeniden bölümleme modeli de tanıtıyoruz. Deneyler NoClique2?nin daha yüksek yük dengesizliğine göre aynı problem örnekleri için, ek koşut fazla hesaplama bedeliyle, hatırı sayılır iyileştirme elde ettiğimizi göstermektedir. Bütün-çiftler benzerlik problemi için, yakın zamandaki etkin ardışık algoritmaları koşut çerçeveye genişletiyoruz, ve hızlı bir ardışık algoritmanın vektör-başı ve boyut-başı koşutlaştırılmalarını, ve aynı zamanda iki algoritmanın 2-B bir algoritma üreten zarif bir birleşimini elde ediyoruz. Boyut-başı durumu için iki etkin algoritmik optimizasyonun boyut-başı koşutlaştırmayı yeterince etkin hale getirdiği gösterilmektedir. Bu optimizasyonlar iletişim bedellerini, aday sayısını ve hesaplama/iletişim dengesizliğini azaltmak için yerel budama ve belli bir sayıdaki vektörun blok işlemesini hedeflemektedir. Yerel budamanın doğruluğu ispatlanır. Ayrıca, özyinelemeli boyut-başı koşutlaştırma sunulur. Geniş deneylerde, algoritmaların başarımının olumlu çıktıgı, ve iki önemli optimizasyonun faydası gösterilmiştir. s¨ozc¨ukler: ko¸sut veri madencili?gi, d¨u?g¨um ayracı ile ¸cizge b¨ol¨umleme, hiper¸cizge b¨ol¨umleme, b¨ut¨un-¸ciftler benzerlik, veri da?gıtımı, veri yineleme.","We have embarked upon a multitude of approaches to improve the efficiency of selected fundamental tasks in data mining. The present thesis is concerned with improving the efficiency of parallel processing methods for large amounts of data. We have devised new parallel frequent itemset mining algorithms that work on both sparse and dense datasets, and 1-D and 2-D parallel algorithms for the all-pairs similarity problem. Two new parallel frequent itemset mining (FIM) algorithms named NoClique and NoClique2 parallelize our sequential vertical frequent itemset mining algorithm named bitdrill, and uses a method based on graph partitioning by vertex separator (GPVS) to distribute and selectively replicate items. The method operates on a graph where vertices correspond to frequent items and edges correspond to frequent itemsets of size two. We show that partitioning this graph by a vertex separator is sufficient to decide a distribution of the items such that the sub-databases determined by the item distribution can be mined independently. This distribution entails an amount of data replication, which may be reduced by setting appropriate weights to vertices. The data distribution scheme is used in the design of two new parallel frequent itemset mining algorithms. Both algorithms replicate the items that correspond to the separator. NoClique replicates the work induced by the separator and NoClique2 computes the same work collectively. Computational load balancing and minimization of redundant or collective work may be achieved by assigning appropriate load estimates to vertices. The performance is compared to another parallelization that replicates all items, and ParDCI algorithm. We introduce another parallel FIM method using a variation of item distribution with selective item replication. We extend the GPVS model for parallel FIM we have proposed earlier, by relaxing the condition of independent mining. Instead of finding independently mined item sets, we may minimize the amount of communication and partition the candidates in a fine-grained manner. We introduce a hypergraph partitioning model of the parallel computation where vertices correspond to candidates and hyperedges correspond to items. A load estimate is assigned to each candidate with vertex weights, and item frequencies are given as hyperedge weights. The model is shown to minimize data replication and balance load accurately. We also introduce a re-partitioning model since we can generate only so many levels of candidates at once, using fixed vertices to model previous item distribution/replication. Experiments show that we improve over the higher load imbalance of NoClique2 algorithm for the same problem instances at the cost of additional parallel overhead. For the all-pairs similarity problem, we extend recent efficient sequential algorithms to a parallel setting, and obtain document-wise and term-wise parallelizations of a fast sequential algorithm, as well as an elegant combination of two algorithms that yield a 2-D distribution of the data. Two effective algorithmic optimizations for the term-wise case are reported that make the term-wise parallelization feasible. These optimizations exploit local pruning and block processing of a number of vectors, in order to decrease communication costs, the number of candidates, and communication/computation imbalance. The correctness of local pruning is proven. Also, a recursive term-wise parallelization is introduced. The performance of the algorithms are shown to be favorable in extensive experiments, as well as the utility of two major optimizations. Key Words: parallel data mining, graph partitioning by vertex separator, hypergraph partitioning, all pairs similarity, data distribution, data replication."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu örnekle öğrenme paradigmasının birçok uygulama alanında yararları görülmekte beraber bu öğrenme yöntemi etiketlemenin zor olduğu bilgisayarlı görü problemlerine özellikle uygundur. Çoklu örnekle öğrenmenin bilgisayarlı görüde nesne tanıma ve bulma, izleme sahne sınıflandırma, resim sınıflandırma vb. gibi birçok zorlu öğrenme problemlerine uygulamaları bulunmaktadır. Geleneksel gözetimli öğrenmede teksel etiketlerin kullanılmasından farklı olarak, çoklu örnekle öğrenme örnek torbaları üzerinden çalışır. Bir torba eğer en az bir pozitif örnek içeriyorsa pozitif olarak etiketlenir diğer türlü torba pozitif örnek içermiyorsa negatif olarak etiketlenir. Çoklu örnekle öğrenmenin amacı torba olarak organize edilmiş eğitim verisini kullanarak bazı konseptler için bir model öğrenmektir. Bilgisayarlı görüye çoklu örnekle öğrenmeyi uygulayabilmenin önemli bir aşaması da görsel problemler için torba tanımının yapılması ve torbaların içindeki örneklerin ne olacağının belirlenmesidir. Bu bağlamda üç farklı bilgisayarlı görü problemi ile çalışmakta ve özgün çözümlerimizi sunmaktayız. İlk olarak resim geri getirme ve sıralama problemine çalıştık ve ilgili resimleri içeren aday çoklu örnek torbalarını otomatik olarak oluşturduğumuz yöntemimizi sunduk. İkinci olarak resimlerden hareket tanıma problemine çalıştık. Resimlerden nesne içeren aday pencerelerin otomatik olarak çıkararak zayıf gözetimli bir yaklaşımla nesnelerin tanınması problemine araştırdık. Son olarak videolardan insan etkileşimlerini tanımayı bir çoklu örnekle öğrenme çatısı içerisinde amaçladık. İnsan etkileşimi tanımada videolar farklı aktiviteleri içeren hareketlerden oluşurlar ve amacımız video içerisine dağılmış olan bu ilgisiz aktivitelere rağmen etkileşimi tanımaya çalışmaktır. Bu problemi çözmek için, videolarda bulunan bu ilgisiz hareketleri ele alacak şekilde çoklu örnekle öğrenme yöntemini kullandık. Bahsettiğimiz çalışmalarımızı veri kümeleri üzerinde test ettik ve en iyi çözümlerin sonuçları ile karşılatırdık. Veri kümeleri üzerindeki deneysel sonuçlarımız sunduğumuz algoritmaların performansını doğrulamaktadır.","The Multiple Instance Learning (MIL) paradigm arises to be useful in many application domains, whereas it is particularly suitable for computer vision problems due to the difficulty of obtaining manual labeling. Multiple Instance Learning methods have large applicability to a variety of challenging learning problems in computer vision, including object recognition and detection, tracking, image classification, scene classification and more. As opposed to working with single instances as in standard supervised learning, Multiple Instance Learning operates over bags of instances. A bag is labeled as positive if it is known to contain at least one positive instance; otherwise it is labeled as negative. The overall learning task is to learn a model for some concept using a training set that is formed of bags. A vital component of using Multiple Instance Learning in computer vision is its design for abstracting the visual problem to multi-instance representation, which involves determining what the bag is and what are the instances in the bag. In this context, we consider three different computer vision problems and propose solutions for each of them via novel representations. The first problem is image retrieval and re-ranking; we propose a method that automatically constructs multiple candidate Multi-instance bags, which are likely to contain relevant images. The second problem we look into is recognizing actions from still images, where we extract several candidate object regions and approach the problem of identifying related objects from a weakly supervised point of view. Finally, we address the recognition of human interactions in videos within a MIL framework. In human interaction recognition, videos may be composed of frames of different activities, and the task is to identify the interaction in spite of irrelevant activities that are scattered through the video. To overcome this problem, we use the idea of Multiple Instance Learning to tackle irrelevant actions in the whole video sequence classification. Each of the outlined problems are tested on benchmark datasets of the problems and compared with the state-of-the-art. The experimental results verify the advantages of the proposed MIL approaches to these vision problems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birçok telekom analizi geçmiş arama desenlerine dayalı arama profillerine gereksinim duyar. Bu arama profilleri değişik zamanlardaki müşteri etkileşimlerinin yığılması ile oluşmaktadır. Telekom şirketlerinin pazarlama ve satış gibi operasyonlarını iyileştirecek analizler müşteri arama profilleri üzerinden yapılmaktadır. Örnek uygulamalar olarak tarife iyileştirme, müşteri bölümleme ve kullanım öngörüsü gösterilebilir. Bu tezde güncelleme katarları ile oluşan müşteri profillerinin kümelenmesi için bir yöntem sunulmaktadır. Profil kümeleri yüksek sayıda müşteri olması nedeniyle yüksek bellek ve işlemci gücü gerektir. Bu gereksinimleri karşılayabilmek için çözümümüzde dağıtık veri katarı işleme yöntemleri kullandık. Ancak profillerin makinalara dağılımını kümeleme kalitesini yüksek tutarken, her makinanın eşit miktarda profil saklamasını ve işlemesini sağlamak, dağıtık sistemlerde önemli bir zorluk. Buna ek olarak, müşterilerin arama deseni değiştirmesi ihtimali nedeniyle, profillerin makinalara dağılımı düzenli olarak güncellenmeli. Bu güncelleme işlemi çevirimiçi işleme sürecini aksatmamak için asgari miktarda yer değişimi gerçekleştirmeli. Bu tezde tüm bu ihtiyaçları karşılayan bir tekrar dağıtım tekniği sunulmuştur. Her makina kendi içerisinden mikro-kümeler oluşturmakta ve onların özetlerini merkezi makinaya göndermektedir. Merkezi makina mikro-küme özetlerini üzerinde yeni aitlik buluşsal yöntemleri içeren açgözlü bir işlemsel süreçten geçirerek profil dağıtımını güncellemektedir. Tezde ayrıca sunulan çözümün Storm ve Hbase tabanlı gerçekleştirmesini gösteren, telekom şirketleri için müşteri bölümleme amacıyla kullanılabilecek bir demo uygulaması sunulmuştur.","Many telco analytics require maintaining call profiles based on recent customer call patterns. Such profiles are typically organized as aggregations computed at different time scales over the recent customer interactions. Clustering these profiles is needed to group customers with similar calling patterns and to build aggregate models for them. Example applications include optimizing tarifs, segmentation, and usage forecasting. In this thesis, we present an approach for clustering profiles that are incrementally maintained over a stream of updates. Due to the large number of customers, maintaining profile clusters have high processing and memory resource requirements. In order to tackle this problem, we apply distributed stream processing. However, in the presence of distributed state, it is a major challenge to partition the profiles over machines (nodes) such that memory and computation balance is maintained, while keeping the clustering accuracy high. Furthermore, to adapt to potentially changing customer calling patterns, the partitioning of profiles to machines should be continuously revised, yet one should minimize the migration of profiles so as not to disturb the online processing of updates. We provide a re-partitioning technique that achieves all these goals. We keep micro-cluster summaries at each node, collect these summaries at a centralized node, and use a greedy algorithm with novel affinity heuristics to revise the partitioning. We present a demo application that showcases our Storm and Hbase based implementation in the context of a customer segmentation application."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, heterojen sistemler için büyüklüğü farklılık gösteren işlerin işlemcilere dağıtılması problemleri üzerinde çalıştık. Bu bağlamda iki ayrı problemi inceledik. İlk olarak farklı işlem büyüklüğüne sahip iş katarlarının heterojen işlemcilere bir boyutlu dağıtılması problemi üzerinde durduk. İkinci olarak ise, farklı işlem büyüklüğüne sahip bağımsız işlerin heterojen sistemlerde atanması problemi üzerinde çalıştık. Farklı büyüklükteki iş katarlarının bir boyutlu parçalanması probleminde iki alt problem üzerinde çalıştık. Birincisi, zincir-zincir parçalama (ZZP) olarak bilinen bir boyutlu sıralı iş zincirinin bir boyutlu sıralı işlemci zinciri üzerine parçalama problemi, ikincisi ise zincir parçalama (ZP) olarak tanımladığımız, bir boyutlu iş zincirinin sıra önemli olmadan işlemcilere parçalama problemi. ZZP problemi için heterojen sistemlerde polinom zamanda optimal çözüm sunan algoritmalar sunduk. ZP probleminin ise NP-tam olduğunu ispatladık. Yaptığımız çalışmalar sonucunda ZZP probleminde sunduğumuz optimal çözümlerin sezgisel yöntemlerden çok daha iyi sonuçları karşılaştırılabilir sürelerde bulabildiğini ortaya koyduk. Bağımsız iş atama probleminde, bilinen ve çok kullanılan yapıcı sezgisel algoritmalardan MinMin, MaxMin ve Sufferage algoritmalarının iyileştirilmesi üzerinde çalıştık. BU sezgisel metotların N işi K işlemciye dağıtırken O(KN2) zamanda çalıştığı biliniyordu. Bu tezde, MinMin algoritmasında, çözümünü ve çözüm kalitesini değiştirmeden, çalışma zamanını O(KN log N) zamana dönüştürecek algoritmik iyileştirmeler yaptık. Ayrıca, Minmin algoritması ile MaxMin ve Sufferage algoritmalarını birleştirerek, iki adet daha hibrit algoritma elde ettik. MaxMin ile MinMin hibritlemesi, MaxMin algoritmasının özellikle kuvvet kanunu gibi özellikleri taşıyan dağılımlardaki dezavantajlarını gidermenin yanında, MaxMin algoritmasının çalışma hızını da iyileştirdi. Sufferage ile MinMin hibritlemesi ise Sufferage algoritmasının çözüm kalitesini düşürmeden çözüm hızını iyileştirdi. Algoritmalar için verdiğimiz detaylı akışlar sunduğumuz algoritmaların kolay gerçekleştirilebilir olduğunu göstermektedir. Gerçek hayattan alınan çok sayıdaki örnek veri üzerinde yaptığımız deneyler sunduğumuz MinMin ve hibrit algoritmaların klasik versiyonlarından ve diğer çok kullanılan sezgisel algoritmalardan çok daha iyi çalıştığını gösterdi. Deneylerde kullandığımız büyük ölçekli veriler için, MinMin, MaxMin ve Sufferage algoritmaları ve diğer çok kullanılan sezgisel algoritmalar günler, haftalar hatta aylar mertebesinde çalışırken, sunduğumuz algoritmalar sonuçları iki-üç dakika içinde hesaplayabildiler. Bağımsız iş atama probleminde ayrıca, graf ve hipergraf parçalama gibi uygulamalarda başarıyla kullanılmış çok katmanlı mimari yöntemlerin probleme adaptasyonu üzerinde çalıştık. Çok katmanlı mimarinin katlama aşamasında kullanılmak üzere etkili, çoğu zaman O(KN) sürede çalışan bir algoritma tasarladık. Çok katmanlı mimarinin açma kısmında kullanılmak üzere, iki adet iyileştirme algoritması tasarladık: O(KN) sürede çalışan kaydırma temelli iyileştirme algoritması ve O(K2N) sürede çalışan değiştirme temelli iyileştirme algoritması. Yaptığımız çalışmalar çok katmanlı yaklaşımların, özellikle büyük örnek veriler için hem iş atama kalitesini hem de çalışma süresi performansını ciddi olarak iyileştirdiğini ortaya koymaktadır. Bağımsız iş atama probleminin gerçekçi vir dağıtık uygulamasını göstermek üzere, iş atama eşleştirme problemini inceledik. Bu problem, Internet üzerindeki çok sayıda web sitesinin birden fazla yerde konuşlanmış dağıtık indirici sistemleri vasıtası ile en az sürede tarama işleminin gerçekleştirilmesini hedeflemektedir. Bu problemin bağımsız iş atama problemi olarak modellemesini gerçekleştirdik. Günümüzde kullanılan bağımsız iş atama algoritmalarını sunduğumuz iyileştirilmiş algoritmaları ve çok katmanlı algoritmamızı problem üzerinde deneyerek karşılaştırdık. Karşılaştırmalarımızda gerçek hayattan alınan çok büyük örnek kümeler kullandık. Sonuçlarımız, kolay gerçekleştirilebilen sezgisel yöntemler yerine, bağımsız iş atama yaklaşımının dağıtık indirici sistemlerin verimliliğini ciddi olarak arttırdığını gösterdi.","We study the problem of assigning nonuniform tasks onto heterogeneous systems. We investigate two distinct problems in this context. The first problem is the one-dimensional partitioning of nonuniform workload arrays with optimal load balancing. The second problem is the assignment of nonuniform independent tasks onto heterogeneous systems. For one-dimensional partitioning of nonuniform workload arrays, we investigate two cases: chain-on-chain partitioning (CCP), where the order of the processors is specified, and chain partitioning (CP), where processor permutation is allowed. We present polynomial time algorithms to solve the CCP problem optimally, while we prove that the CP problem is NP complete. Our empirical studies show that our proposed exact algorithms for the CCP problem produce substantially better results than the state-of-the-art heuristics while the solution times remain comparable. For the independent task assignment problem, we investigate improving the performance of the well-known and widely used constructive heuristics MinMin, MaxMin and Sufferage. All three heuristics are known to run in O(KN2) time in assigning N tasks to K processors. In this thesis, we present our work on an algorithmic improvement that asymptotically decreases the running time complexity of MinMin to O(KN logN) without affecting its solution quality. Furthermore, we combine the newly proposed MinMin algorithm with MaxMin as well as Sufferage, obtaining two hybrid algorithms. The motivation behind the former hybrid algorithm is to address the drawback of MaxMin in solving problem instances with highly skewed cost distributions while also improving the running time performance of MaxMin. The latter hybrid algorithm improves the running time performance of Sufferage without degrading its solution quality. The proposed algorithms are easy to implement and we illustrate them through detailed pseudocodes. The experimental results over a large number of real-life datasets show that the proposed fast MinMin algorithm and the proposed hybrid algorithms perform signi cantly better than their traditional counterparts as well as more recent state-of-the-art assignment heuristics. For the large datasets used in the experiments, MinMin, MaxMin, and Sufferage, as well as recent state-of-the-art heuristics, require days, weeks, or even months to produce a solution, whereas all of the proposed algorithms produce solutions within only two or three minutes. For the independent task assignment problem, we also investigate adopting the multi-level framework which was successfully utilized in several applications including graph and hypergraph partitioning. For the coarsening phase of the multi-level framework, we present an efficient matching algorithm which runs in O(KN) time in most cases. For the uncoarsening phase, we present two refinement algorithms: an efficient O(KN)-time move-based refinement and an effcient O(K2N logN)-time swap-based refinement. Our results indicate that multi-level approach improves the quality of task assignments, while also improving the running time performance, especially for large datasets. As a realistic distributed application of the independent task assignment problem, we introduce the site-to-crawler assignment problem, where a large number of geographically distributed web servers are crawled by a multi-site distributed crawling system and the objective is to minimize the duration of the crawl. We show that this problem can be modeled as an independent task assignment problem. As a solution to the problem, we evaluate a large number of state-of-the-art task assignment heuristics selected from the literature as well as the improved versions and the newly developed multi-level task assignment algorithm. We compare the performance of different approaches through simulations on very large, real-life web datasets. Our results indicate that multi-site web crawling effciency can be considerably improved using the independent task assignment approach, when compared to relatively easy-to-implement, yet naive baselines."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"BLAST gibi sekans arama araçları, bir sorgu sekansı icin, seçilen veritabanındaki en benzer sonuçları bulmayı amaçlar. Sorguya benzer sonuçlar, kendi içinde de benzerlik göstermektedir. Biyoenformatikteki bir çok analiz yeni aramalar icin daha geniş bir yaklaşım gerektirir ve ilk sıralardaki sonuçların daha farklı çeşitler sunarak yol gösterici olması beklenir. Fakat, su anki arama sistemlerinde çeşitlilik henuz tamamlayıcı bir parça olarak sunulmamaktadır. Tekrar eden sonuçların azaltılması adına, sekans veritabanları oluşturulurken belli bir gereklilik seviyesine bakılmaktadır. Ama, bu durum dinamik olarak oluşturulmuş sonuç kümelerinin gereklilik seviyelerini kontrol etmek icin uygun değildir. Bu tezde, öncelikle, sekans araması için çeşitlilik arama problemi üzerinde durduk. Tüm sorgular ve sonuçlar icin kullanılabilecek çözümler geliştirmeye calıştık. Sekans arama araçlarında alınan sonuçlara uygulanabilecek, olası ceşitlilik olçekleri geliştirdik. Bunların yanı sıra, deneyleri değerlendirmek icin de objektif bir değerlendirme olceği tanımladık. Çeşitlilik algoritmalarının etkinliğini PSI-BLAST aracı kullanılarak alınmış sonuçlar üzerinde değerlendirdik. Ayrıca, sonuçların biyolojik açıdan anlamlı olup olmadığını kontrol etmek için gen ontolojilerinin kullanıldığı bir fonksiyonel çeşitlilik olçeği belirledik. Yaplan deneyler, önerdiğimiz metotların orijinal arama sonuçlarından, hem fonksiyonel hem sekans tabanlı analizlerde istatistiksel olarak daha üstün olduğunu gösterdi. Bunların dışında, gelistirdiğimiz yöntemlerin kullanımını sağlamak için BLAST web servislerini kullanan Div-BLAST adında bir web arama aracı geliştirdik. Bahsi geçen araç öncelikle verilen paramatreleri kullanarak BLAST üzerinde arama yapmakta; daha sonra bu aramada elde edilen sonuçları çeşitlilik unsurunu hesaba katarak yeniden sıralamakta ve BLAST kullanıcılarının alıştığı bir arayüze benzer şekilde sonuçları sunmaktadır.","Sequence similarity tools, such as BLAST, seek sequences from a database most similar to a query. They return results signi cantly similar to the query sequence that are typically also highly similar to each other. Most sequence analysis tasks in bioinformatics require an exploratory approach where the initial results guide the user to new searches. However, diversity has not been considered as an integral component of sequence search tools yet. Repetitions in the result can be avoided by introducing non-redundancy during database construction; however, it is not feasible to dynamically set a level of non-redundancy tailored to a query sequence. We introduce the problem of diverse search and browsing in sequence databases that produces non-redundant results optimized for any given query. We de ne diversity measures for sequences, and propose methods to obtain diverse results extracted from current sequence similarity search tools. We propose a new measure to evaluate the diversity of a set of sequences that is returned as a result of a similarity query. We evaluate the e ectiveness of the proposed methods in post-processing PSI-BLAST results. We also assess the functional diversity of the returned results based on available Gene Ontology annotations. Our experiments show that the proposed methods are able to achieve more diverse yet similar result sets compared to static non-redundancy approaches. In both sequence based and functional diversity evaluation, the proposed diversi cation methods outperform original BLAST results signi cantly. We built an online diverse sequence search tool Div-BLAST that supports queries using BLAST web services. It re-ranks the results diversely according to given parameters."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Deflasyonal teori, günümüz felsefesinin en etkili doğruluk teorilerinden birisidir. Bu teori, doğruluğun bir özellik olmadığını ve ""doğru"" kelimesinin cümle içinde kullanımının gereksiz olduğunu; diğer bir deyişle, bu cümleleri kullanarak ifade ettiğimiz şey üzerinde kesinlikle hiçbir etkisi olmadığını savunur. Fakat, deflasyonal teorisyenlerin kitaplarında, makalelerinde bu teoriyi örneklendirmek için kullandıkları varsayımsal cümlelerin tamamı bağlamlarından çıkarılmıştır. Dolayısıyla, ""doğru"" kelimesinin bağlam içerisinde, kullanıldığı cümleye anlamsal bakımdan ne kattığını analiz etmek imkansız bir hal almaktadır. Biz bu teoriye felsefe bir zeminde değil, empirik bir zeminde, gündelik dil felsefesi yaklaşımıyla karşı çıkmaktayız. ""Doğru"" kelimesinin yüklem olarak açık şekilde kullanıldığı 7610 adet örnek, ABD'de basılan 10 gazete ve derginin arşivlerinden bilgisayar yardımıyla toplandı. Bu örnekler önce bilgisayarla, daha sonra okunarak içerdikleri bağlaçların konumuna göre sınıflandırıldı. Bu sınıflandırmanın amacı ""doğru"" kelimesinin kullanıldığı önermenin, etrafındaki önermelerle bağlamsal ilişkilerini irdelemektir. 34 farklı sözdizimsel örüntü ile karşılaşıldı. Bu araştırmadaki argümanımız, ""doğru"" kelimesinin bazı kullanımlarında, bu kelimenin varlığının edimsel bir fiille aynı minvalde vurgu yaptığı ve edim gerçekleştirdiğidir. Bu tezde, dilsel olarak güvenilir medyada kullanılmış ve deflasyonal teoriye karşı örnek oluşturan, ""doğru"" kelimesinin açık şekilde kullanıldığı gündelik dil örnekleri sunulmaktadır.","The deflationary theory has been one of the most influential theories of truth in contemporary philosophy. This theory holds that there is no property of truth at all, and that overt uses of the predicate ""true"" in our sentences are redundant, having absolutely no effect on what we express. However, all hypothetical examples used by deflationary theorists in exemplifying the theory, in papers, books, have been taken out of context. Thus, there is no way to examine and analyze what the predicate adds to the sentence within context. We oppose this theory not on philosophical grounds, but on empirical grounds, with an ""ordinary language philosophy"" approach. We computationally collect 7610 occurrences of overt uses of the predicate ""true"" in the form ""it is true that"", from 10 influential periodicals (newspapers and magazines) published in the United States. We classify and annotate these examples with respect to coordinating and subordinating conjunctions' positions they contain. We investigate contextual relations of the proposition following the phrase ""it is true that"" with its surrounding propositions. We encounter 34 different syntactical patterns. We propose that in some occurrences of overt uses of the predicate ""true"", existence of the predicate makes an emphasis, performs an action in the same manner as a performatory verb does. We provide ordinary language appearances of overt uses of the predicate ""true"", which have been used in linguistically reliable media and constitute pragmatic ""counter-examples"" to the deflationary theory of truth."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Map/Reduce, büyük veri uygulamalarının hızlı bir şekilde geliştirilebilmesi için ilk kez Google tarafından ortaya atılan bir uygulama çatısıdır. Map/Reduce paradigmasının, bilgisayar bilimlerinin veri madenciliği, bilgi sistemleri gibi alanlarında büyük etkisi olmasına rağmen, bilimsel hesaplama alanına böyle bir etkisi olmamıştır. Mevcut Map/Reduce uygulamaları özellikle hata oranı yüksek olan ve iletişim hızı düşük olan dağıtık bellekli bilgisayar kümeler için geliştirilmiştir. Bununla birlikte, bilimsel hesaplama uygulamaları genellikle yüksek performanslı bilgisayar sistemleri üzerinde çalıştırılmaktadır ve bu sistemler yüksek bant genişlikli ve düşük gecikmeli iletişim sağlarlar ve bu sistemlerde hata oranı azdır. Bu yüzden, Map/Reduce paradigması bilimsel hesaplama alanında performans azalmasına neden olmaktadır ve bu yüzden daha az tercih edilmektedir. Bu nedenlerden dolayı, bilimsel hesaplama uygulamaları için özel Map/Reduce uygulamaları gerekmektedir. Mevcut olan uygulamalar arasından biz dikkatimizi Sandia Ulusal laboratuvarları tarafından geliştirilen MapReduce-MPI (MR-MPI) kütüphanesi üzerine odakladık. Bu tezde, MR-MPI kütüphanesinden faydalanarak Map/Reduce paradigmasının ölçeklenebilirlik ve performans gerektiren bilimsel hesaplama alanında da kullanılabilecegini savunduk. MR-MPI kütüphanesini bilimsel hesaplama ve veri madenciliğinde sıklıkla kullanılan algoritmalarla yüksek performanslı bilgisayar sistemlerinde test ettik. Tatbik ettiğimiz algoritmalar arasında APSS, APSP, ve PR algoritmalrı vardır. Bu algoritmaların ölçeklenebilirliğini ve hızlanmasını incelemek için yaptığımız testler IBM BlueGene/Q (Juqueen) ve Cray XE6 (Hermit) sistemlerinde gerçekleştirildi.","Map/Reduce is a framework first introduced by Google in order to rapidly develop big data analytic applications on distributed computing systems. Even though the Map/Reduce paradigm had a game changing impact on certain fields of computer science such as information retrieval and data mining, it did not have such an impact on the scientific computing domain yet. The current implementations of Map/Reduce are especially designed for commodity PC clusters, where failures of compute nodes are common and inter-processor communication is slow. However, scientific computing applications are usually executed on high performance computing (HPC) systems and such systems provide high communication bandwidth with low message latency where failures of processors are rare. Therefore, Map/Reduce framework causes performance degradation and becomes less preferable in scientific computing domain. Due to these reasons, specific implementations of Map/Reduce paradigm are needed for scientific computing domain. Among the existing implementations, we focus our attention on the MapReduce-MPI (MR-MPI) library developed at Sandia National Labs. In this thesis, we argue that by utilizing MR-MPI Library, the Map/Reduce programming paradigm can be successfully utilized for scientific computing applications that require scalability and performance. We tested MR-MPI Library in HPC systems with several fundamental algorithms that are frequently used in scientific computing and data mining domains. Implemented algorithms include all-pair-similarity-search (APSS), all-pair-shortest-path (APSP), and page-rank (PR). Tests were performed on well-known large-scale HPC systems IBM BlueGene/Q (Juqueen) and Cray XE6 (Hermit) to examine scalability and speedup of these algorithms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ozellikle son yirmi yıl i¸cerisinde dijital i¸ceri?gin kul ¨ lanımı olduk¸ca arttı. CD?ler, DVD?ler, TV yayınları, ?Internet gibi ¸cok sayıda dijital urun formları hayatımıza girdi. S¸ifrelemenin tek bir kullanıcıdan tek bir kullanıcıya ¸seklinde modellendi?gi klasik kriptogra? bu de?gi¸sime tam anlamıyla ayak uyduramadı, ¸c¨unk¨u do?grudan klasik kriptogra? kullanımı ¸coklu alıcı k¨umesine g¨onderimlere uygun de?gildi ve bu g¨onderilerin fazla b¨uy¨umesine yol a¸cmakta. Dahası, i¸sin ticari yanı da d¨u¸s¨un¨uld¨u?g¨unde, yeni kaygılar ortaya ¸cıkıyor. Ticari dijital i¸cerikler ¸cok sayıda m¨u¸steriye satılabildi?gi ve kopyalanmaları kolay oldu?gu i¸cin izinsiz kopyalanmalarını engellemek ¨onem arzediyor. Tam da bu nedenlerle dijital hak y¨onetimi adında yeni bir ara¸stırma alanı ortaya ¸cıktı. Ve bu alanın ¸cer¸cevesinde yeni kriptogra?k primitif y¨ontemler ¨onerildi. Bu tezde, bu y¨ontemlerden ¨u¸c¨un¨u, yayın ¸sifreleme, hain tespiti, ve izleme ve iptal y¨ontemlerini, ele alıyoruz ve bu y¨ontemlerin performanslarını ve yapabildiklerini artırmaya y¨onelik metotlar ortaya koyuyoruz. Oncelikle en ¨ pop¨uler yayın ¸sifreleme y¨ontemlerinde kullanıcıların pro?llerini hesaba katarak g¨onderi maliyetinin d¨u¸s¨ur¨ulmesini ¨oneriyoruz. Daha sonra, halen en verimli yayın ¸sifreleme y¨ontemlerden bir tanesi i¸cin en iyi bedava alıcı yerle¸stirme algoritması vererek maliyetin ¨onemli ¨ol¸c¨ude d¨u¸s¨ur¨ulebilece?gini g¨osteriyoruz. Bir sonraki ¸calı¸smamızda yayın ¸sifreleme y¨ontemlerine hain tespit mekanizması eklemenin jenerik bir yolunu vererek yayın ¸sifreleme ile izleme ve iptal y¨ontemleri arasındaki bo¸slu?gu ortadan kaldırıyoruz. Son olarak izleme ve iptal y¨ontemlerinde uzun zamandır g¨ozardı edilmi¸s gizlilik problemini inceliyoruz. S¸a¸sırtıcı bi¸cimde yayın ¸sifreleme y¨ontemleri i¸cin dahi gizlilik ¸cok yakında yayınlanan birka¸c makale dı¸sında g¨ozardı edildi. Dolayısıyla halihazırdaki yayın ¸sifreleme y¨ontemleri, g¨onderinin yapıldı?gı ki¸silerin kim oldu?gunu gizlemiyor ve kimin hangi dijital i¸ceri?ge ula¸sabildi?gi yayınla birlikte a¸cıktan g¨onderiliyor. Bu konuda gizlili?gi sa?glayan ilk anonim izleme ve iptal y¨ontemini de ¨oneriyoruz.Anahtar s¨ozc¨ukler: Yayın ¸sifreleme, hain tespiti, dijital hak y¨onetimi.","In the last few decades, the use of digital content increased dramatically. Many forms of digital products in the form of CDs, DVDs, TV broadcasts, data over the Internet, entered our life. Classical cryptography, where encryption isdone for only one recipient, was not able to handle this change, since its direct use leads to intolerably expensive transmissions. Moreover, new concerns regarding the commercial aspect arised. Since digital commercial contents aresold to various customers, unauthorized copying by malicious actors became a major concern and it needed to be prevented carefully. Therefore, a new research area called digital rights management (DRM) has emerged. Within the scope of DRM, new cryptographic primitives are proposed. In this thesis, we consider three of these: broadcast encryption (BE), traitor tracing (TT), and trace and revoke (T&R) schemes and propose methods to improve the performances and capabilities of these primitives. Particularly, we ?rst consider pro?ling the recipient set in order to improve transmission size in the most popular BE schemes. We then investigate and solve the optimal free rider assignment problem for one of the most e?cient BE schemes so far. Next, we attempt to close the non-trivial gap between BE and T&R schemes by proposing a generic method for adding traitor tracing capability to BE schemes and hus obtaining a T&R scheme. Finally, we investigate an overlooked problem: privacy of the recipient set in T&R schemes. Right now, most schemes do not keep the recipient set anonymous, and everybody can see who received a particular content. As a generic solution to this problem, we propose a method for obtaining anonymous T&R scheme by using anonymous BE schemes as a primitive.Keywords: Broadcast encryption, traitor tracing, digital rights management."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tıp alanında, bir tedavi sonucunda başarıya ulaşma şansının karar verilmesi çok önemlidir. Bu tez çalsması, tüp bebek tedavisinde dikkate alınması gereken iki önemli aşama üzerine odaklanmıştır. Bu aşamalardan birincisi gelen hastanın tüp bebek tedavisi için uygun olup olmadığıdır. Hastanın tedaviye uygun olduğu kararı verildikten sonra ikinci aşama hastaya uygulanacak olan en uygun tedavi yönteminin belirlenmesidir. Hem doktorlar, hem de tedavi uygulanacak olan aday hasta çifti için ilk değerlendirmeden sonra hastaya uygulanacak olan tedavi sonucunda başarıya ulaşma şansı çok önemlidir. Eğer başarı şansı düşük ise, hasta çifti bu pahalı ve stresli tedaviye devam etmek istemeyebilir. Tedavi uygulama karar verildikten sonra doktorlar için karar verilmesi gerekilen ikinci konu hasta çifti icin en uygun olan tedavi yöntemini seçmektir. Bu tez çalışmasındaki ilk amacımız tedavi için gelen bir hasta çifti icin başarı şansını tahmin etme ve tüp bebek tedavisindeki başarı oranını etkileyen faktörleri bulmak ve amacıyla teknikler geliştirmektir. Bu amaçlar doğrultusunda sıralama algoritmaları kullanılmaktadır. Kullanılan metodlar RIMARC (Ranking Instances by Maximizing the Area under the ROC Curve), SVMlight (Support Vector Machine Ranking Algorithm) ve RIkNN (Ranking Instances using k Nearest Neighbour)'dir. Bu algoritmaların her uçu de örnek hastaları onlara atanmış olan skor değerlerine göre sıralamaya dayalı bir model öğrenir. RIMARC, Receiver Operating Characteristics (ROC) eğrisi altında kalan alanı maksimize ederek örnekleri sıralayan bir metoddur. SVMlight, destek vektör makinesi algoritmasının örnek sıralaması için geliştirilmiş bir versiyonudur. RIkNN, en yakın komşu algoritmasını esas alan ve örnek sıralamasında benzerlik olçütünü kullanan bir algoritmadır. Bunlara ek olarak, bu tez calışmasında RIkNN algoritmasının bir versiyonu olan ve her bir öznitelik için konunun uzmanları tarafından belirlenmiş olan öznitelik ağırlıklarını da dikkate alan RIwkNN algoritmasını da kullandık. Bu algoritmaları değerlendirmek için ROC eğrisi altındaki alan (AUC) değeri ve katmanlaştırılmış 10'lu çapraz geçerlilik yöntemlerini kullandık. Ek olarak, tasarlanan sıralama algoritmalarını birer sınıflandırma algoritması haline getirdik ve bu algoritmaları değerlendirmek için accuracy değeri ve katmanlaştırılmış 10'lu çapraz geçerlilik yöntemlerini kullandık. Yan ürün olarak RIMARC algoritması tüp bebek tedavisinde başarı şansını etkileyen faktörleri öğrenmektedir. Bu amaçla öznitelik ağırlıklarını hesaplar ve insanların kolaylıkla anlayıp yorumlayabilecekleri kurallar üretir. Gelen hasta çifti icin ilk değerlendirmeden sonra tedavi sonrası şansının yüksek olduğuna karar verilir ise ikinci aşamaya geçilir. Bu aşama hasta için en uygun olan tedavi yönteminin belirlenmesi aşamasıdır. Tüp bebek tedavisi içerisinde çok sayıda ilaç yer almaktadr fakat bu ilaçlardan hangisinin hasta için en uygun olduğu kesin olarak bilinememektedir. Doktorlar genellikle hasta için ilaç seçimi yaparken geçmişte tedavi ettikleri hastaların değerlerine bakarak karar verirler. Bu karar her zaman olumlu bir şekilde sonuçlanmayabilir çünkü insan hafızası gereği doktorların geçmişte tedavi ettikleri bütün hasta pro fillerini doğru bir şekilde hatırlayabilmeleri mümkün değildir. Bildiğimiz kadarıyla, istenilen sonucu elde etme şansını arttırmak amacıyla en iyi öznitelik değerini önermek için model öğrenen bir method bulunmamaktadır. Biz bu tur bir sistemi Önerme Sistemi olarak adlandıracağız. Doktorlara, uygun tedavi yöntemlerini belirleme aşamasında yardımcı olmak için bilinen makine öğrenmesi tekniklerine dayalı üç önerme sistemi geliştirdik. Bu çalsmanın bir parçası olarak geliştirilen önerme sistemlerini NSNS (Nearest Successful Neighbour Based Suggestion), kNNS (k Nearest Neighbour Based Suggestion) ve DTS (Decision Tree Based Suggestion) olarak adlandıracağız. Bunlara ek olarak, bu tez çalışmasında NSNS algoritmasının bir versiyonu olan ve her bir öznitelik için RIMARC algoritması tarafından belirlenmiş olan öznitelik ağırlıklarını da dikkate alan wNSNS algoritmasını da kullandık. Ayrıca, önerme algoritmalarının doğruluğunu değerlendirmek için performans kriterleri tasarladık. Bu amaçla, bu tez çalışmasında, pessimistic metric (mp), optimistic metric (mo), validated optimistic metric (mvo) ve validated pessimistic metric (mvp) olarak adlandırılan dört adet değerlendirme kriteri sunuyoruz. Geliştirilen bu algoritmalardan doktorların faydalanmasını sağlamak amacı ile RAST (Risk Analysis and Suggestion for Treatment) adı verilen bir karar destek sistemi geliştirdik. Sistem şuanda Ankara Etlik Zübeyde Hanım Kadın Hastalıkları Eğitim ve Araştırma Hastanesi Tüp Bebek Merkezi'nde aktif olarak kullanılmaktadır.","In medicine, the chance of success for a treatment is important for decision making for the doctor and the patient. This thesis focuses on the domain of In Vitro Fertilization (IVF), where there are two issues: the first one is the decision on whether or not go with the treatment procedure, the second one is the selection of the proper treatment protocol for the patient. It is important for both the doctor and the couple to have some idea about the chance of success of the treatment after the initial evaluation. If the chance of success is low, the patient couple may decide not to proceed with this stressful and expensive treatment. Once a decision for treatment is made, the next issue for the doctors is the choice of the treatment protocol which is the most suitable for the couple. Our fi rst aim is to develop techniques to estimate the chance of success and determine the factors that aff ect the success in IVF treatment. So, we employ ranking algorithms to estimate the chance of success. The ranking methods used are RIMARC (Ranking Instances by Maximizing the Area under the ROC Curve), SVMlight (Support Vector Machine Ranking Algorithm) and RIkNN (Ranking Instances using k Nearest Neighbour). All of these three algorithms learn a model to rank the instances based on their score values. RIMARC is a method for ranking instances by maximizing the area under the ROC curve. SVMlight is an implementation of Support Vector Machine for ranking instances. RIkNN is a k Nearest Neighbour (kNN) based algorithm that is developed for ranking instances based on similarity metric. We also used RIwkNN, which is the version of RIkNN where the features are assigned weights by experts in the domain. These algorithms are compared on the basis of the AUC of 10-fold strati ed cross-validation. Moreover, these ranking algorithms are modi ed as a classi cation algorithm and compared on the basis of the accuracy of 10-fold strati ed cross-validation. As a by-product, the RIMARC algorithm learns the factors that a ect the success in IVF treatment. It calculates feature weights and creates rules that are in a human readable form and easy to interpret. After a decision for a treatment is made, the second aim is to determine which treatment protocol is the most suitable for the couple. In IVF treatment, many di erent types of drugs and dosages are used, however, which drug and the dosage are the most suitable for the given patient is not certain. Doctors generally make their decision based on their past experiences and the results of research published all over the world. To the best of our knowledge, there are no methods for learning a model that can be used to suggest the best feature values to increase the chance that the class label to be the desired one. We will refer to such a system as Suggestion System. To help doctors in making decision on the selection of the suitable treatment protocols, we present three suggestion systems that are based on well-known machine learning techniques. We will call the suggestion systems developed as a part of this work as NSNS (Nearest Successful Neighbour Based Suggestion), kNNS (k Nearest Neighbour Based Suggestion) and DTS (Decision Tree Based Suggestion). We also implemented the weighted version of NSNS using feature weights that are produced by the RIMARC algorithm. Moreover, we propose performance metrics for the evaluation of the suggestion algorithms. We introduce four evaluation metrics namely; pessimistic metric (mp), optimistic metric (mo), validated optimistic metric (mvo) and validated pessimistic metric (mvp) to test the correctness of the algorithms. In order to help doctors to utilize developed algorithms, we develop a decision support system, called RAST (Risk Analysis and Suggestion for Treatment). This system is actively being used in the IVF center at Etlik Zubeyde Hanm Woman's Health and Teaching Hospital."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Biyolojik yolaklar bizlere bir organizmanın devamlılığını sağlayan etkileşimlerin mekansal ve zamansal organizasyonu hakkında bilgiler sunabilir. Fakat yolaklar büyük ve karmaşık olabilecekleri için, onlardan bilgi elde etmek basit bir işlem değildir. Ayrıca sadece görselleştirme odaklı analizler bizlere sınırlı miktarda bilgi sağlayabilirken, bu görsellerin deneysel sonuçlar ile entegre edilmesi farklı ve ilginç etkileşimleri açığa çıkarabilir. Bu nedenle, biyolojik yolakların analizi ve daha detaylı anlaşılması üzerine yoğunlaşmış yazılım araçlarına sahip olmanın önemi büyüktür. ChiBE, BioPAX formatında saklanan yolak verilerinin görselleştirilmesi, düzenlenmesi ve analizi üzerine odaklanmış bir yazılım aracıdır. Yazılımın ikinci versiyonu geliştirilirken çizge üzerinde aramalar, yüksek çıktılı deney sonuçlarının entegrasyonu ve veritabanı bağlantılarının sağlanması üzerine yoğunlaşan eklemeler yapıldı. Ayrıca ChiBE'nin görsel sembolleri de SBGN tarafından belirlenmiş standartları takip edecek şekilde güncellendi. Daha önceden tanımlanmış olan yolak sorgulama algoritmaları BioPAX modelleri üzerinde kullanılabilecek şekilde uyarlandı. Bunlara ek olarak yeni yolak sorgulama türleri de geliştirildi. Bu algoritmaların eklenmesiyle birlikte, ChiBE artık yolakları daha küçük parçalara ayrıştıran ve karmaşık çizgelerin detaylı analizini sağlayan birçok farklı yola sahip oldu. Mikrodizi deneyleri sonuçlarına kolay erişim sunmak için GEO veritabanına ulaşım noktası eklendi. Ayrıca, kanser hücrelerinin genomik durumu hakkında bilgi sunmak için `cBio Cancer Genomics Portal?ı da ChiBE içerisinden erişilebilir hale getirildi. Bunlara ek olarak, DAVID veritabanına da kolay bir erişim sunulmaya başlandı. Böylelikle, kullanıcılar bir gen listesini bilinen biyolojik terimlerle eşleştirme şansına sahip olacaklar. Bu yeni özellikler ve iyileştirmelerle birlikte ChiBE 2 genomik odaklı iş akışına elverişli, geniş çapta analiz seçenekleri sunan ve biyolojik yolaklar hakkındaki bilgi birikimimizi derinleştirebilecek kapsamlı bir yazılım aracı haline geldi.","Biological pathways store information about spatial and temporal organization of interactions taking place in an organism. However, extracting knowledge from these pathways is not trivial as they can be huge and complicated. Additionally, simple visualization of pathways will only reveal limited knowledge. Therefore, it is critical to have tools that are specialized in analyzing and understanding biological pathways. ChiBE is one such tool that can visualize, manipulate and analyze pathway data stored in BioPAX format. There have been improvements in ChiBE regarding pathway searches, high throughput data integration, and database connections. Visual notation has been updated to follow standards in visualizations defined by the SBGN. Previously defined pathway query algorithms have been adapted to be compatible with the BioPAX model. New query types have also been designed to offer a wider range of options. With these queries, ChiBE now offers a variety of ways of pathway decomposition and thorough analysis of complex pathway views. To offer easy access to expression microarrays, a gateway to the GEO database has been added. The cBio Cancer Genomics Portal is also now reachable within ChiBE to obtain information about genomic status of various cancer cells. Furthermore, a connection to DAVID database is available, in case users want to annotate a list of genes with respect to biological terms associated with them. With these new features and improvements, ChiBE 2 has become a comprehensive tool that offers a wide range of analysis options with a genomics-oriented workflow to deepen our understanding of biological pathways."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Büyük ölçekli ticari web arama motorunun kalitesini belirleyen en önemli faktörlerden biri arama motorunun bulduğu web arama sonuçlarının kullanıcıya sunulduğu sıralamadır. Modern web arama motorlarında, web arama sonuçlarının sıralamasının iskeleti sonuç sayfaların önemi ve sonuç sayfalarının verilen arama sorgusuyla ilişki bilgileri bir arada kullanılarak oluşturulmaktadır. Bu tez web sayfalarının küresel öneminin tahmin edilmesi ile ilgilidir. Şimdiye kadar, web sayfalarının önemini tahmin etmek için, iki farklı veri kaynağı birbirinden bağımsız bir şekilde ele alınmıştır: web sayfalarının arasındaki köprü bilgisi (PageRank) ve web kullanıcıların tarama bilgileri (BrowseRank). Ne yazık ki, her iki veri kaynağının da bazı sınırlamaları vardır. Web sayfalarının arasındaki köprü bilgisi pek güvenilir değildir, çünkü bu köprü bilgisi web içeriği yaratıcıları tarafından kolayca düzenlenebilmektedir ve kötü niyete karşı savunmasızdır. Öte yandan, web kullanıcıların tarama bilgilerinin en önemli sınırlamaları seyreklik ve düşük web kapsamasıdır. Bu tezde, yukarıda belirtilen sınırlamaları kaldırmak için yukarıda bahsedilen iki tür veri kaynağının karışımını kullanarak web sayfalarının küresel öneminin tahmin eden model tasarlanmıştır. Yahoo! web arama motorunun sorgu günlüklerinden elde edilen kullanıcı tıklama bilgilerini gerçek sıralama olarak kullanan bir değerlendirme metriğine göre iki farklı veri kaynağının bir arada kullanılması sayfa öneminin daha iyi tahmin edilebildiğini göstermektdir. Deneyler sırasında çok büyük ölçekli web sayfa veri seti (yaklaşıl 6.5 milyar web sayfası) ve Yahoo! araç çubuğu üzerinden toplanan web tarama veri seti (iki milyar web sayfa ziyareti) kullanılmıştır.","One of the most crucial factors that determines the effectiveness of a large-scale commercial web search engine is the ranking (i.e., order) in which web search results are presented to the end user. In modern web search engines, the skeleton for the ranking of web search results is constructed using a combination of the global (i.e., query independent) importance of web pages and their relevance to the given search query. In this thesis, we are concerned with the estimation of global importance of web pages. So far, to estimate the importance of web pages, two different types of data sources have been taken into account, independent of each other: hyperlink structure of the web (e.g., PageRank) or surfing behavior of web users (e.g., BrowseRank). Unfortunately, both types of data sources have certain limitations. The hyperlink structure of the web is not very reliable and is vulnerable to bad intent (e.g., web spam), because hyperlinks can be easily edited by the web content creators. On the other hand, the browsing behavior of web users has limitations such as, sparsity and low web coverage. In this thesis, we combine these two types of feedback under a hybrid page importance estimation model in order to alleviate the above-mentioned drawbacks. Our experimental results indicate that the proposed hybrid model leads to better estimation of page importance according to an evaluation metric that uses the user click information obtained from Yahoo! web search engine's query logs as ground-truth ranking. We conduct all of our experiments in a realistic setting, using a very large scale web page collection (around 6.5 billion web pages) and web browsing data (around two billion web page visits) collected through the Yahoo! toolbar."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kullanıcılarının gözünde federe kimlik sistemleri çeşitli web sitelerine önceden kayıt olma derdi olmaksızın giriş yapmada büyük kolaylık sağlıyor. Daha yukarıdan bakarsak, federe kimlik yönetimi bir IT sisteminin kullanıcılarına tam bir kullanıcı bilgi yönetimine ihtiyaç kalmadan diğer IT sisteminin kaynaklarına güvenli ve kesintisiz erişim imkanı verir. Tek oturum açma mekanizmaları bu tür birbirinden bağımsız sistemlerin kullanıcı kimlik doğrulama sürecini giriş yapılmasını tek bir kez isteyip devamında erişime izin verilmesini temin ederek sağlamış olur. OpenID genellikle kullanıcı adı-parola kimlik doğrulamalı olarak sıkça kullanılan federe kimlik/tek oturum açma sistemidir. Bu çalışmada, OpenID'nin kullanıcı kimlik doğrulama kısmını akıllı kart teknolojisi kullanarak sertifika tabanlı kullanıcı kimlik doğrulama ile güçlendiriyoruz. Çözümümüz kullanıcının akıllı kartındaki elektronik sertifikayı kullanarak güvenli bir kimlik doğrulama metodu sağlıyor. Anahtar sozcukler: OpenID, elektronik serti ka, federe kimlik, tek oturum acma,serti ka tabanl kullanc kimlik dogrulama, akll kart, akll kart tabanl OpenID","From the point of its users, federated identity systems provide great convenience to log in to varied web sites without bothering of registration in advance. Looking from a vantage point, federated identity management gives the opportunity to users of one IT system to access data and sources of another IT system seam-lessly and securely without handling a complete user administration. Single sign-on mechanisms manage user authentication process of these systems prompting log in once and assure access control across those multiple independent systems. OpenID is a widely used federated identity/single sign-on scheme generally implemented with username-password authentication. In this work, we augment the user authentication phase of OpenID with certificate-based authentication using smartcard technology. Our solution provides a secure method to authenticate the user with user's digital certificate written on the smartcard. Keywords: OpenID, digital certi cate, federated identity, single sign-on,certi cate-based user authentication, smartcard, smartcard-based OpenID."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İstatistiksel tahminleme konusunda yapılan araştırmalar geleneksel olarak daha çok verilen zaman serisi için daha doğru modeller oluşturmaya odaklanmaktadır. Bu modeller verimlilik ve ölçeklenebilirlik kısıtlarından dolayı sadece sınırlı sayıda zaman serisine uygulanabilmektedir. Ancak, Müşteri İlişkileri Yönetimi (MİY) ve Müşteri Deneyimi Yönetimi (MDY) gibi bazı kurumsal uygulamalar büyük veri seti üzerinde çalışabilen ölçeklenebilir tahminleme gerektirmektedir. Örnek olarak, telekomünikasyon ?rmaları müşterilerin ihtiyaçlarını ve davranışlarını anlamak veya müşteriye özel kampanya üretmek için her bir müşterinin ayrı ayrı tra?k yükünün tahminine ihtiyaç duyarlar. Tahminleme modelleri, gelir tahmini veya kaynak planlaması için gerekli olan toplam tra?k hacmi tahmininde toplu trak verisi üzerinde kolayca kullanılabilir. Bununla birlikte, çok sayıda kullanıcı için ayrı ayrı model oluşturmak çok fazla zaman aldığı için bu durumda tahminleme modelleri uygulabilirliğini yitirmektedir. Tahminleme sürecinin sürekli olduğu ve modellerin periyodik olarak güncellenmesi gerektiği durumlarda problem daha da içinden çıkılmaz hale gelmektedir. Biz bu çalışmada, birden fazla zaman serisi için tahminleme modellerini oluşturma ve sürekli bir şekilde güncelleme problemini ele almaktayız ve tahminleme için optimize edilmiş dinamik kümeleme modellemesini sunmaktayız. Çalışmada küme merkezleri için temsili model oluşturmayı ve bu modelleri tekrarlı doğrusal olmayan optimizasyon kullanarak her bir zaman serisine ayrı ayrı uygulamayı önermekteyiz. Öne sürülen yaklaşım, modelleme ve kümeleme işlemlerini eş zamanlı olarak yerine getirmekte, temsili modelleri her bir zaman serisine uygulayarak tahminleme yapmakta ve sürekli tahminleme süreci için model parametrelerini güncellemektedir. Elde ettiğimiz bulgular bireysel model davranışlarını kendi segment modeli üzerinden değerlendirmenin bireysel modeller hesaplamaya göre daha ölçeklenebilir ve daha doğru olduğunu göstermektedir. Gerçek bir telekom MİY uygulaması üzerinde yapılan deneyler, önerilen yöntemin her bir kişiyi ayrı ayrı modellemeye göre yüksek verimli, ölçeklenebilir ve daha doğru olduğunu ortaya koymaktadır.","Research on statistical forecasting has traditionally focused on building more accurate models for a given time-series. The models are mostly applied only to limited data due to their limitation on eciency and scalability. However, many enterprise applications such as Customer Relationship Model (CRM) and Customer Experience Management (CEM) require scalable forecasting on large number of data series. For example, telecommunication companies need to forecast each of their customers' trac load individually to understand their needs and behavior, and to tailor targeted campaigns. Forecasting models are easily applied on aggregate trac data to estimate the total trac volume for revenue estimation and resource planning. However, they cannot be applied to each user individually as building accurate models for large number of users would be time consuming. The problem is exacerbated when the forecasting process is continuous and the models need to be updated periodically. We address the problem of building and updating forecasting models continuously for multiple data series and propose dynamic clustered modeling optimized for forecasting. We introduce representative models as an analogy to cluster centers, and apply the models to each individual series through iterative nonlinear optimization. The approach performs modeling and clustering simultaneously, makes forecasts by applying representative models to each data, and updates the model parameters for a continuous forecasting process. Our ndings indicate that understanding an individual's behavior within its segment's model provides more scalability and accuracy than computing the individual model itself. Experimental results from a real telecom CRM application show the method is highly ecient and scalable, and also more accurate than having separate individual models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tarihsel arşivler dünyanın pek çok yerinden akademisyenlerin ve konuyla ilgilenen araştırmacıların ilgisini çekmektedir. Bu belgelere erişim isteğinin artması otomatik erişim ve tanıma sistemlerini zorunlu kılmaktadır. Osmanlıca belgeler tarihsel belgeler arasında önemli ve büyük bir yer kaplamaktadır. Osmanlıca günümüzde halen konuşulan bir dil olmamasına rağmen bir çok tarihçinin ilgisini çekmektedir. Bu tezde de iki adet Osmanlıca belge analizi çalışması sunulmaktadır. İlki Osmanlıca belgelerin bölütlenmesi olup; bölge, satır ve kelime bölütleme çalışılmıştır. Bölgelere ayırma Log-Gabor filtreleme yöntemi ile sağlanmıştır. Satırlara bölütleme içinse 4 farklı yöntem sunulmaktadır. Son olarak ise belgeler morfolojik yöntemler ile kelimelere ayrılmıştır. Veri kümelerine Osmanlıca?nın yanında farklı dillerden oluşan belgeler (İngilizce, Yunanca ve Bangla) da eklenmiştir. Deneylerden elde edilen sonuçlar bölütleme algoritmalarının iyi çalıştığını göstermektedir. Tezin ikinci kısmında ise Kufi resimlerinde İslami motiflerin tespiti amaçlanmıştır. Motiflerin temsili için grafikler kullanılmıştır. Eşyapılı grafikler ve altçizgeler incelenerek motifler eşleştirilmeye çalışılmıştır. Kufi imgeleri farklı deneyler ile incelenmiştir. İlki verilen bir sorgu motifinin veri kümesinden geri getirilmesidir. İkinci deney Kufi resimlerinin otomatik etiketlenmesidir. Son olarak, her resimdeki tekrarlanan motifler incelenmiştir. İnternet üzerinden toplanan resimlerle bir veri kümesi oluşturulmuştur. önerilen yöntem bu veri kümesi ile test edilmiş ve umut verici sonuçlar elde edilmiştir.","Large archives of historical documents attract many researchers from all around the world. The increasing demand to access those archives makes automatic retrieval and recognition of historical documents crucial. Ottoman archives are one of the largest collections of historical documents. Although Ottoman is not a currently spoken language, many researchers from all around the world are interested in accessing the archived material. This thesis proposes two Ottoman document analysis studies; first one is a crucial pre-processing task for retrieval and recognition which is segmentation of documents. Second one is a more specific retrieval and recognition problem which aims matching Islamic patterns is Kufic images. For the first segmentation task, layout, line and word segmentation is studied. Layout segmentation is obtained via Log-Gabor filtering. Four different algorithms are proposed for line segmentation and finally a simple morphological method is preferred for word segmentation. Datasets are constructed with documents from both Ottoman and other languages (English, Greek and Bangla) to test the script-independency of the methods. Experiments show that our segmentation steps give satisfactory results. The second task aims to detect Islamic patterns in Kufic images. The sub-patterns are considered as basic units and matching is used for the analysis. Graphs are preferred to represent sub-patterns where graph and sub-graph isomorphism are used for matching them. Kufic images are analyzed in three different ways. Given a query pattern, all the instances of the query can be found through retrieval. Going further, through known patterns images can be automatically labeled in the entire dataset. Finally, patterns that repeat inside an image can be automatically discovered. As there is no existing Kufic dataset, a new one is constructed by collecting images from the Internet and promising results are obtained on this dataset."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsanlar ve bazı hayvanlar aletleri erişim alanını genişletmek, mekanik kuvvetlerini arttırmak, toplumsal değerini oluşturmak ve arttırmak, kamuflaj, vücudu rahatlatma ve sıvı kontrolleri gibi amaçlar için kullanırlar. Robotik alanında alet kullanımı genelde robotun erişim alanını arttırmak için kullanılır. Bu amaç doğrultusunda ""Hangi alet hangi duruma uygundur?"" sorusu cok önemlidir. Sağlarlık kavramının önemi bu soru ile ortaya çıkmaktadır. Çünkü farklı aletler, hedef nesneler üzerinde farklı kabiliyetlere sahip olabilirler. Alet sağlarlığının öğrenimi için robotlar farklı davranışları farklı nesneler üzerinde deneyerek ortaya çıkan sonuçları gözlemlemelidirler. Bu çalışmadaki amaç, insansı robot iCub'a farklı davranışları nesneler üzerinde uygulatıp, çıkan sonuçları gözlemleterek alet sağlarlıklarını öğretmektir. Göz kamerası ve Kinect kullanarak, eğitim verisi oluşturmak için, her etkileşimde alet ve nesne nitelikleri elde edilir. Bir davranışın başarısı alet niteliklerine, nesnenin pozisyon ve özelliklerine ve robotun kullandığı ele bağlıdır. Davranış eğitimlerinin ardından, verilen bir alet ve nesneye göre, robot farklı davranışların sonuçlarını tahmin edip, sağlarlıkları çıkarabilmektedir. Herhangi bir sağlarlık istendiğinde, robot kendisine verilen alet ve nesneye göre uygun davranışı uygulayabilmektedir, herhangi bir nesne gösterildiğinde robot farklı aletler arasından en uygun aleti seçebilmektedir. Bu çalışma ayrıca nesnelerin farklı pozisyonlar ve özelliklerinin sağlarlık ve davranış sonuçlarını nasıl etkilediğini ve bir aletin herhangi bir parçası çıkarıldığında, değiştirildiğinde veya yeni bir parça eklendiğinde sağlarlıkların ve davranış sonuçlarının nasıl etkilendiğini göstermektedir. Anahtar sozcukler: Saglarlk, Alet Saglarlg, _Insans Robot, Alet Kullanm.","Humans and some animals use di fferent tools for diff erent aims such as extending reach, amplifying mechanical force, create or augment signal value of social display, camouflage, bodily comfort and e ffective control of fluids. In robotics, tools are mostly used for extending the reach area of a robot. For this aim, the question ""What kind of tool is better in which situation?"" is very signi cant. The importance of a ffordance concept rises with this question. That is because, di fferent tools aff ord variety of capabilities depending on target objects. Towards the aim of learning tool a ffordances, robots should experience e ffects by applying behaviors on di fferent objects. In this study, our goal is to teach the humanoid robot iCub, the a ffordances of tools by applying di fferent behaviors on a variety of objects and observing the e ffects of these interactions. Using eye camera and Kinect, tool and object features are obtained for each interaction to construct the training data. Success of a behavior depends on the tool features, object position and properties and also the hand that the robot uses the tool with. As a result of the training of each behavior, the robot successfully predicts e ffects of diff erent behaviors and infers the a ffordances when a tool is given and an object is shown. When an a ffordance is requested, the robot can apply the appropriate behavior given a tool and an object, the robot can select the best tool among di fferent tools when a speci fic aff ordance is requested and an object is shown. This study also demonstrates how diff erent positions and properties of objects aff ect the aff ordance and behavior results, and how a ffordance and behavior results are aff ected when a part of a tool is removed, modifi ed or a new part is added. Keywords: A ordance, Tool A ordance, Humanoid Robot, Tool Use."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hızla gelişen sosyal ağ servisleri sayesinde, viral pazarlama alanında yapılan araştırmalarda bir patlama yaşandı. Sosyal ağlarda yeni bir fikrin, ürünün benimsenmesi veya marka bilinirliğinin geliştirilmesi için çokça uygulanan bir yöntem, küçük bir çekirdek kullanıcı kümesi seçerek, sonraki benimsenmelerin maksimize edilmesidir. Literatürdeki çözüm ve formülasyonlar genel olarak tek bir şirket durumunu göz önüne alır. Fakat, herbiri ağdaki bir kısım kullanıcılara (reklam aktarıcı) belli bir ücret ödeyerek bir viral pazarlama kampanyası oluşturmak isteyen birden fazla şirket (reklam veren) olduğu durumda problem daha zor bir hal almaktadır. Reklam aktarıcıların amacı ağ üzerinde reklam verenin seçtiği içeriği de barındıran ilgi çekici ve eğlenceli mesajlar göndermektir. Herbir reklam verenin önceden belirlenmiş belli bir bütçesi bulunmaktadır. Ayrıca, herbir reklam aktarıcının kaç tane reklam veren tarafından kullanılabileceğini sınırlayan bir limiti bulunmaktadır. Bu tezde, reklam verenler ile reklam aktarıcılar arasında bir aracı sistem tasarlamaktayız. Amacımız, reklam verenlerin bütçelerini muhafaza ederek, reklamların ağdaki sıradan kullanıcılar (son kullanıcılar) arasında yayılımını maksimize etmektir. Tasarladığımız sistem, reklam aktarıcılarının aşırı yüklenimine ve son kullanıcıların de reklamlarla boğulmasına engel olmaktadır. Bu problemi kombinatoryal optimizasyon problemi üzerinden bütçe kısıtlarını entegre ederek tasarlıyoruz. Bu problemin çözümü için büyük çaptaki ağlarda optimale yakın performanslı, maliyet-etkili bir algoritma tasarladık. Ayrıca son kullanıcıların aşırı-yüklenimini modellemek için klasik Bağımsız Yayılım Modelini (BYM) tekrar gözden geçirip, bu modele bir eklenti sunuyoruz: Aşırı-Yüklenim etkili Bağımsız Yayılım Modeli (ABYM). Yayılım maksimizasyon problemini bu model üzerinde çalışıyoruz. Birkaç gerçek büyük sosyal ağ verisi üzerindeki deneylerimizde gösteriyoruz ki sunulan algoritma optimal performansa yakın ve büyük sosyal ağlar üzerinde de çalıştırılabilecek kadar zaman açısından verimli.","With the rapid growth of social networking services, there has been an explosion in the area of viral marketing research. The idea is to explore the marketing value of social networks with respect to increasing the adoption of a new innovation/product, or generating brand awareness. A common technique employed is to target a small set of users that will result in a large cascade of further adoptions. Existing formulations and solutions in the literature generally focus on the case of a single company. Yet, the problem gets more challenging if there are a number of companies (the advertisers), each one aiming to create a viral advertising campaign of its own by paying a set of network users (the endorsers). The endorsers are asked to post intriguing and entertaining ad messages that contain the content selected by the advertising company. The advertiser has a predefined budget on how much it is going to spend on this effort. Also each endorser has a limit on the number of companies for which it serves as an endorser. In this thesis, we design a broker system as an intermediary between advertisers and endorsers. We seek to maximize the spread of advertisements over regular users (the audience), while considering the budget constraints of advertisers. Our system avoids overburdening of the endorsers and overloading of the audience. We model the problem through a combinatorial optimization framework with budget constraints. We develop a cost-effective algorithm called CEAL, which is designed for solving the problem with close to optimal performance on large-scale graphs. We also revisit the traditional Independent Cascade Model (ICM) to account for overloaded users. We propose an extension of ICM called Independent Cascade Model with Overload (ICMO).We study the influence maximization problem on variations of this model. We perform experiments over multiple real-world social networks and empirically show that the proposed CEAL algorithm performs close to optimal in terms of coverage, yet is sufficiently lightweight to execute on large-scale graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Her geçen gün sistemlerin işlem kapasitelerini artırmak için, daha fazla çok-çekirdekli hızlandırıcı piyasaya sunulmaktadır. Bu gibi sistemlerde, uygulamaların var olan donanım mimarisi ile eşleştirilmesi işlemi daha fazla önem kazanmaktadır. Çünkü, daha uygun eşleştirmeler sonucunda donanımdan kazanılan faydanın arttırılması hedeflenmektedir. Özellikle grafik işleme birimlerinde, çekirdek fonksiyonların donanıma doğru bir şekilde eşleştirilmesinin nihai performans üzerine etkisi büyüktür. Bunun nedeni, çeşitli çekirdek fonksiyonların çeşitli karakteristiksel özelliklere sahip olmalarıdır. Bu karakteristiksel özellikler nedeniyle bazı fonksiyonlar merkezi işlem biriminde (CPU) iyi sonuçlar verirken, ötekiler ise grafik işleme birimlerinde (GPU) daha iyi performans sonuçları ortaya koymaktadırlar. Bu nedenle, uygulamaların heterojen ortamlarda heterojen bir şekilde çalıştırılması, uygulamanın sadece CPU ya da sadece GPU üzerinde çalıştırılmasından daha iyi performans sonuçları verecektir. Bu tezde iki farklı yaklaşım araştırılmıştır: ilki, çekirdekleri uygun aygıta atamak için geliştirdiğimiz özgün profil temelli uyarlanabilir çekirdek eşleştirme algoritması, ve ikincisi, ideal eşleştirmeleri bulmak için ürettiğimiz karışık tam sayılı programlama modelidir. Geliştirdiğimiz algoritmada, bir uygulamanın nihai performansını artırmak ya da enerji tüketimi azaltmak hedeflenmektedir. Bu hedef doğrultusunda uygulamaların çekirdek fonksiyonlarının doğru bir şekilde aygıtlara dağılımı yapılmakta, ve bu işlem süresince çekirdek fonksiyonların sistemde bulunan çeşitli aygıtlar üzerinde elde edilen profil bilgileri kullanılmaktadır. Deneyler göstermektedir ki, yaklaşımlarımız verimli bir şekilde çekirdek fonksiyonların CPU?lara ve GPU?lara dağılımını yapmakta, ve sadece CPU ya da sadece GPU yaklaşımlarından daha iyi sonuçlar ortaya koymaktadır. Bu tezde bahsi geçen işin bir bölümü 41. Uluslararası Paralel İşleme Çalıştayında (ICPPW, 2012) yayınlanmış, ve Paralel Hesaplama dergisine (ParCo) gönderilmiştir.","Many-core accelerators are being more frequently deployed to improve the system processing capabilities. In such systems, application mapping must be enhanced to maximize utilization of the underlying architecture. Especially, in graphics processing units (GPUs), mapping kernels that are part of multi-kernel applications has a great impact on overall performance, since kernels may exhibit different characteristics on different CPUs and GPUs. While some kernels run faster on GPUs, others may perform better in CPUs. Thus, heterogeneous execution may yield better performance than executing the application only on a CPU or only on a GPU. In this thesis, we investigate on two approaches: a novel profiling-based adaptive kernel mapping algorithm to assign each kernel of an application to the proper device, and a Mixed Integer Programming (MIP) implementation to determine optimal mapping. We utilize profiling information for kernels on different devices and generate a map that identifies which kernel should run where in order to improve the overall performance or energy consumption of an application. Initial experiments show that our approach can efficiently map kernels on CPUs and GPUs, and outperforms CPU-only and GPU-only approaches. Some part of this work is published in 41st International Conference on Parallel Processing Workshops (ICPPW), 2012, and submitted to Parallel Computing journal (ParCo)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İşletim sistemlerinde kullanılan mimari-kayıtsız iş parçacığı çizelgeleyicileri ve iş parçacığı-kayıtsız harici ana bellek erişim kontrol birimi çizelgeleyicileri nedeniyle çok çekirdekli işlemcilerin potansiyelleri tam olarak değerlendirilememektedir. Bu problemin çözümüne yönelik olarak iş parçacığı çizelgeleme için, iş parçacıkları arası çekişmeyi en aza indirmeyi amaçlayan ve bu doğrultuda iş parçacıkları çizelgeleyen uyarlamalı önbellek-sıradüzeni-farkında iş parçacığı çizelgeleyicisi sunulmuştur. Süreçlerin birinci seviye önbellek kullanım özelliklerini belirten yeni çok-metrikli puanlama tekniği geliştirilmiş ve kullanılmıştır. Sunulan çizelgeleyicide, çizelgeleme iş parçacıklarının çok-metrikli puanlamaları esas alınarak gerçekleştirilir. Harici ana bellek erişim çizelgelemesi için ise, iş parçacıkları etkin olarak yürütme özelliklerine göre gruplandıran ve iş parçacıklarının hassas olarak önceliklendirebilen uyarlamalı işlem fazı tahmini ve iş parçacığı önceliklendirme tekniği sunulmuştur. Anahtar sozcukler: Uyarlamal cizelgeleme, cok cekirdekli islemciler, is parcacklar-aras cekisme, is parcacg faz ongorusu, cok-metrikli puanlama.","The full potential of chip multiprocessors remains unexploited due to architecture oblivious thread schedulers used in operating systems, and thread-oblivious memory access schedulers used in o -chip main memory controllers. For the thread scheduling, we introduce an adaptive cache-hierarchy-aware scheduler that tries to schedule threads in a way that inter-thread contention is minimized. A novel multi-metric scoring scheme is used that specifies the L1 cache access characteristics of a thread. The scheduling decisions are made based on multi-metric scores of threads. For the memory access scheduling, we introduce an adaptive compute-phase prediction and thread prioritization scheme that efficiently categorize threads based on execution characteristics and provides fine-grained prioritization that allows to differentiate threads and prioritize their memory access requests accordingly. Keywords: Adaptive scheduling, chip multiprocessors, inter-thread contention, thread phase prediction, multi-metric scoring."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Temel olarak, kablosuz bağlar ile birbirlerine bağlanmış örgüsel yönelticilerden oluşan omurga ağlar olan kablosuz örgüsel ağlar, tasarısız ağ oluşturabilme, öz-oluşum, öz-düzenleşim, öz-iyileşme gibi özelliklere sahip oldukları için kendilerine son mil geniş bant İnternet erişiminden olağanüstü durum ağlarına yahut eşler arası ağlara kadar çok geniş bir yelpazede uygulama alanı bulmaktadır. Kablosuz örgüsel ağların çoklu atlamalı doğası akışlar-arası girişimi arttırır ve akış-içi girişime sebebiyet verir. Bu etmenler de ağ kapasitesini ciddi ölçüde azaltır. Girişimi azaltıp ağ kapasitesini arttırmak için sıkça başvurulan bir yöntem örgüsel yönelticileri birden fazla iletişim kanalında çalışabilen birden fazla radyo ile donatmaktır. Böylelikle, bir örgüsel yönelticinin eş zamanlı olarak birden fazla kablosuz iletişim kanalını kullanması ve birden fazla kanal üzerinden koşut olarak paket alıp vermesi mümkün olmaktadır. Fakat birden fazla radyonun ve kanalın verimli olarak kullanılabilmesi için akış-radyo ve kanal atamayı da içeren dikkatli ve akıllı bir radyo kaynak planlaması gereklidir. Bu ise öncelikle, kablosuz örgüsel ağlar bağlamında kanal-içi girişimin ve komşu kanal girişiminin doğasını çözümlemeyi ve modellemeyi gerektirir. Kanal-içi girişimi ve komşu kanal girişiminin etkilerini anlamak ve modellemek için, çok-radyolu 802.11b/g örgüsel yönelticilerden mütevellit, adını BilMesh koyduğumuz bina içi sınama ortamımız üzerinde deneyler ve gözlemler yaptık. Ayrıca, çok-radyolu örgüsel yönelticiler kullanmanın ve böylelikle çoklu atlamalı bir akışın ardışık atlamalarını farklı kanallardan geçirmenin ağ ve uygulama katmanı metrikleri üzerindeki etkilerini inceledik. Çok-radyolu örgüsel yönelticilerde sadece örtüşmeyen kanallar kullanmanın başarımını, örtüşmeyen kanalların yanında örtüşen kanallar kullanmanın başarımı ile mukayese ettik. Daha sonra, komşu kanal girişimini modellemeye ve ölçmeye yöneldik. Bu amaçla BilMesh sınama ortamına IEEE 802.15.4 radyoları ekleyerek deneyler yaptık ve hem bir kablosuz iletişim standardının kanalları arasındaki girişimi hem de Wi-Fi ve ZigBee gibi farklı iki standardın kanalları arasındaki girişimi hesaplayabildiğimiz iki yöntem önerdik. Literatürdeki kanal atama üzerine olan çalışmaların birçoğu, çok-radyolu kablosuz örgüsel ağlar için sadece örtüşmeyen kanalları kullanmaktadır. Girişim için nicel modeller geliştirdikten sonraki adım olarak, kablosuz örgüsel ağlarda birleşik akış-radyo ve kanal atama problemi için örtüşen kanalları da kullanan eniyileme modelleri önerdik. Daha sonra, yine birleşik akış-radyo ve kanal atama problemini çözmeye yönelik olarak, örtüşen kanalları da kullanabilen verimli merkezi ve dağıtık algoritmalar önerdik. Önerdiğimiz bu algoritmaların başarımını çeşitli gerçekçi girişim ve ağ kapasitesi metriklerini kullanarak, ayrıntılı benzetim modelleri ile gerçekleştirdiğimiz deneylerde ölçtük ve önerdiğimiz algoritmaların örgüsel kablosuz ağlarda tek kanal kullanarak veya rastgele yapılacak akış-radyo ve kanal atamaya göre büyük iyileşme sağladıklarını gözlemledik.","Wireless mesh networking, which is basically forming a backbone network of mesh routers using wireless links, is becoming increasingly popular for a broad range of applications from last-mile broadband access to disaster networking or P2P communications, because of its easy deployment, self-forming, self-configuration, and self-healing properties. The multi-hop nature of wireless mesh networks (WMNs) aggravates inter-flow interference and causes intra-flow interference and severely limits the network capacity. One technique to mitigate interference and increase network capacity is to equip the mesh routers with multiple radios and use multiple channels. The radios of a mesh router can then simultaneously send or receive packets on different wireless channels. However, careful and intelligent radio resource planning, including flow-radio and channel assignment, is necessary to efficiently make use of multiple radios and channels. This first requires analyzing and modeling the nature of co-channel and adjacent channel interference in a WMN. Through real-world experiments and observations made in an indoor multi-hop multi-radio 802.11b/g mesh networking testbed we established, BilMesh, we first analyze and model the nature of co-channel and adjacent channel interference. We conduct extensive experiments on this testbed to understand the effects of using multi-radio, multi-channel relay nodes in terms of network and application layer performance metrics. We also report our results on using overlapping in addition to orthogonal channels for the radios of the mesh routers. We then turn our attention to modeling and quantifying adjacent channel interference. Extending BilMesh with IEEE 802.15.4 nodes, we propose computational methods to quantify interference between channels of a wireless communication standard and between channels of two different standards (such as Wi-Fi and ZigBee). Majority of the studies in the literature on channel assignment consider only orthogonal channels for the radios of a multi-radio WMN. Having developed quantitative models of interference, next we propose two optimization models, which use overlapping channels, for the joint flow-radio and channel assignment problems in WMNs. Then we propose efficient centralized and distributed heuristic algorithms for coupling flows and assigning channels to the radios of a WMN. The proposed centralized and distributed schemes make use of overlapping channels to increase spectrum utilization. Using solid interference and capacity metrics, we evaluate the performances of the proposed schemes via extensive simulation experiments, and we observe that our schemes can achieve substantial improvement over single-channel and random flow-radio and channel assignment schemes."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Derinlik kavramı ve derinliğin nasıl algılandığı psikolojide, fizyolojide, hatta sanatsal çalışmalarda uzun süredir incelenmektedir. İnsanlardaki görsel algı sistemi, dış dünyanın yerleşimini görsel derinlik ipuçlarını kullanarak anlamaktadır. Bu derinlik ipuçlarından biri olan binoküler disparite iki göz tarafından yakalanan iki farklı görüntü arasındaki ayrılığa dayalı olarak oluşmaktadır. Gelişen teknolojiler 3B yanılsamasını sağlamak ve stereoskopik görüntüleri oluşturabilmek amacıyla binoküler disparite prensiplerini kopyalamayı denemektedirler. Bu prensiplerin uygulanabilirliğinin karmaşıklığı, araştırmacıları yanlış şekilde üretilen stereoskopik içerikler oluşturmaları problemiyle karşı karşıya getirmiştir. Çalışmamızda disparite kontrolünü sağlayan yeni bir yaklaşım ile yol oluşturmayı sağlayan bir modelden oluşan kamera kontrol mekanizması sunulmuştur. Kullanıcılara konforlu bir seyir deneyimi sunmak adına stereoskopik 3B üretimi esnasında karşılaşılan sorunların çözülmesi amaçlanmıştır. Akomodasyon ve yakınsama uyuşmazlığı 3B sistemlerde karşılaşılan göz yorgunluğuna neden olan en büyük problemdir. Sunduğumuz disparite sistemi akomodasyon ve yakınsama kavramlarının uyuşmamasından doğan problemi, sahne elemanlarının önem derecelerini dikkate alarak ele almaktadır. Stereo kamera parametreleri bu evrede optimizasyon işleminden geçirilerek otomatik olarak hesaplanmaktadır. Kontrol mekanizmamızın ikinci kısmında ise verilen bir 3B ortam için kameranın izleyeceği yol oluşturulmaktadır. Objelerin dikkat çeken kısımlarına bakarak sahneyi incelemek tercih edilen bir sahne analiz yöntemidir. Sahne elemanları etrafındaki bakış noktalarının seçilebilmesi için objelerin dikkat çekerlilikleri kullanılmıştır. Seçilen bakış noktaları Bezier eğrileri formunda birbirlerine bağlanarak yol yapısı oluşturulmuştur. Kullanıcı testleriyle kendi metodumuz var olan diğer iki disparite kontrol modelleriyle karşılaştırılmıştır. Deneyler metodumuzun görsel kalite, derinlik ve rahatlık üzerine üstün sonuçlar gösterdiğini doğrulamaktadır.","Depth notion and how to perceive depth have long been studied in the field of psychology, physiology, and even art. Human visual perception enables to perceive outside world by using visual depth cues. Binocular disparity among these depth cues, is based on the separation between two different views that are observed by two eyes. Disparity concept constitutes the base of the construction of the stereoscopic vision. Emerging technologies try to replicate disparity principles to provide 3D illusion. However, complexity of applying the underlying principles of 3D perception, confronted researchers the problem of wrongly produced stereoscopic contents. In this work, we present a camera control mechanism: a novel approach for disparity control and a model for path generation. We address stereoscopic 3D production by presenting comfortable viewing experience to users. Therefore, our disparity system approaches the accommodation/convergence conflict problem, which is the most known issue that causes visual fatigue in stereo systems, by considering objects' importance. Stereo camera parameters are calculated automatically with an optimization process. In the second part of our control mechanism, the camera path is constructed for a given 3D environment. Moving around important regions of objects is a desired scene exploration task. In this respect, object saliencies are used for viewpoint selection around scene elements. Selected viewpoints are linked in the form of Bezier curves to generate path structure. Our method and existing two other disparity control models are compared with user studies. Results indicate that our method shows superior results in quality, depth, and comfort."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek işlem hacimli içerik taraması, floresan mikroskop görüntüler kullanarak, karmaşık biyolojik sistemlerin yüksek hız ve başarı oranıyla analizine ve sayısal veri elde edilmesine olanak sağlar; böylelikle, moleküler hücresel biyoloji araştırmalarının kalitesinin arttırılması hedeflenir. Daha hızlı ve daha hatasız tarama, otomatik mikroskobik görüntü analizi sistemlerindeki gelişmelerle mümkündür. Bu sistemlerde genellikle ana adım, görüntülerdeki hücrelerin doğru bir şekilde bölütlenmesidir. Bölütleme işleminin sonuçları, sistemin sonraki adımlarını doğrudan etkileyeceğinden, verimli bölütleme algoritmaları geliştirmek büyük bir önem taşımaktadır. Literatürde, tekil ve az kalabalık hücrelerden oluşan görüntüleri bölütlemek üzere tasarlanmış umut verici yöntemler olsa da, üst üste büyüyen, daha kalabalık hücreleri bölütlemek halen çözüm bekleyen bir problem olarak yerini korumaktadır.Bu tezde, bu problemi çözmek üzere, insan algısını hücre bölütleme ile bağdaştıran yeni bir işaretçi-kontrollü su-seddi algoritması sunulmaktadır. Bu bağlamda, bir insanın bir hücrenin doğru kenarlarını algılayıp, bunları bir araya getirmek suretiyle hücrenin yerini saptaması, hücre bölütleme probleminin çözümüne ilham kaynağı olmuştur. Bu amaçla sunulan su-seddi algoritması, farklı tipteki kenarları (sol, sağ, üst ve alt) temsil eden dört farklı tipte primitif tanımlar ve bir özellikli ilişkisel çizge ile primitiflerin birbirleriyle olan konumsal ilişkilerinimodeller. Böylece işaretçi bulma problemi, çizge içerisinde önceden tanımlanmış yapısal örüntüleri arama problemine indirgenmiş olur. Ayrıca geliştirilen yöntem, kenar primitiflerinden faydalanarak su-seddi algoritmasında suyun akışını kontrol eder. Floresan görüntüler üzerinde yapılan deneyler, sunulan algoritmanın, hem az kalabalık hem de çok kalabalık hücre görüntülerinde, önceki algoritmalara kıyasla işaretçileri daha iyi tanımladığını ve hücreleri daha iyi bölütlediğini göstermiştir.","High content screening aims to analyze complex biological systems and collect quantitative data via automated microscopy imaging to improve the quality of molecular cellular biology research in means of speed and accuracy. More rapid and accurate high-throughput screening becomes possible with advances in automated microscopy image analysis, for which cell segmentation commonly constitutes the core step. Since the performance of cell segmentation directly affects the output of the system, it is of great importance to develop effective segmentation algorithms. Although there exist several promising methods for segmenting monolayer isolated and less confluent cells, it still remains an open problem to segment more confluent cells that grow in aggregates on layers.In order to address this problem, we propose a new marker-controlled watershed algorithm that incorporates human perception into segmentation. This incorporation is in the form of how a human locates a cell by identifying its correct boundaries and piecing these boundaries together to form the cell. For this purpose, our proposed watershed algorithm defines four different types of primitives to represent different types of boundaries (left, right, top, and bottom) and constructs an attributed relational graph on these primitives to represent their spatial relations. Then, it reduces the marker identification problem to the problem of finding predefined structural patterns in the constructed graph. Moreover, it makes use of the boundary primitives to guide the flooding process in the watershed algorithm. Working with fluorescence microscopy images, our experiments demonstrate that the proposed algorithm results in locating better markers and obtaining better cell boundaries for both less and more confluent cells, compared to previous cell segmentation algorithms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çizgeler, karmaşık ilişkisel bilgilerin gösteriminde sıklıkla kullanılmaktadır. Çizge görselleştirmesi, bilginin etkin bir şekilde analizinde oldukça önemlidir. Basit çizgelerde, köşeler genelde aynı büyüklükte kabul edilir ve iç içe geçemezler. Bu durum, genellikle karmaşık ilişkilerin görselleştirilmesinde yeterli değildir, çünkü ilişkisel bilgi genelde kümelenmiş ya da gruplar veya iç içe geçmiş yapılar şeklinde hiyerarşik olarak düzenlenmiştir.Web-tabanlı çizge görselleştirme alanında, ücretsiz, açık kaynak bir çok yazılım bulunmaktadır. Ancak, bunların hiçbiri bileşik ya da kümelenmiş çizgeleri tam olarak desteklememektedir. Üstelik, bu yazılımların uyarlanabilirliği, genelde köşe ve kenarların temel görsel özellikleriyle sınırlıdır. Bu yazılımlarla, görsel özellikler ve etkileşimsel işlevsellik açısından gelişmiş bir uyarlama yapmak çok fazla emek istemektedir.Bu tezde, Chisio Web (ChiWeb) adında, genel amaçlı, web-tabanlı, ücretsiz ve açık kaynak bir çizge görselleştirme uygulama çatısı sunuyoruz. ChiWeb, hem basit hem de bileşik çizgelerin görselleştirilmesine, yerleştirilmesine ve etkileşimli olarak düzenlenmesine olanak sağlamaktadır. ChiWeb, ActionScript programlama diliyle gerçekleştirilmiştir ve Flare adında açık kaynak bir ActionScript kütüphanesini temel almıştır.ChiWeb görselleştirme ve işlevsellik yönünden kolay uyarlanabilir olması için özel olarak tasarlanmıştır. ChiWeb kütüphanesi, belirli bir alana özgü ihtiyaçlar doğrultusunda özelleşmiş bir görselleştirme ve gelişmiş bir uygulama işlevselliği yaratmak için kullanılabilir. Görsel biçimler, köşe yaratmak gibi etkileşimli eylemler için denetimler, klavye ve fare işlevselliği, menüler, araç çubukları ve denetim pencereleri, ChiWeb ile kolayca özelleştirilebilecek işlevsellik ve öğeler arasındadır. Bunun yanı sıra, ChiWeb'in mimarisi yeni yerleştirme algoritmalarının kolaylıkla eklenmesine izin verecek yapıdadır.","Graphs are widely used to represent complex relational information. Graph visualization is crucial for effective analysis of information. In simple graphs, nodes are generally considered as uniform-sized components and they cannot be nested. This is often not sufficient to visualize complex relationships, because relational information is often clustered or hierarchically organized into groups or nested structures.There exist many free, open source software in the field of web-based graph visualization. However, none fully supports compound or clustered graphs. Moreover, customization provided by such software is often limited to the basic visual properties of nodes and edges. It requires a lot of effort to build an advanced customization of visual properties and interactive functionality with these software.In this thesis, we introduce a free, open source, general-purpose, web-based graph visualization framework, named Chisio Web (ChiWeb). ChiWeb supports visualization, interactive editing and layout of both simple and compound graphs. ChiWeb is implemented in ActionScript language and based on Flare, which is an open source ActionScript library designed for data visualization.ChiWeb is specifically designed for easy customization with respect to visualization and functionality. ChiWeb can be used as a library to create a custom graph visualization with an advanced application behavior for particular needs of a specific domain. The elements and functionality that can be easily customized with ChiWeb are: visual styles, controls for interactive events such as node creation, key and mouse functionality, context menus, toolbars, and inspector windows. Furthermore, ChiWeb's architecture allows easy integration of new graph layout algorithms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnsanlar bir fikri veya hikayeyi anlatırken birbiriyle anlam olarak ilişkili kelimeleri kullanmaktan kaçamazlar. Bu fenomenden iki farklı bakış açısıyla faydalanmak mümkündür. Kelimeler açısından bakıldığında, anlam olarak ilişkili kelimelerin istatistiksel dağılımı ve anlatımda kullanımlarına bakarak anlam olarak ilişkili kelimeleri tanımlamak mümkün olabilir. Anlam bütünlüğüne anlatım açısından baktığımızda da kelimelerin anlam ilişkilerindeki değişime bakarak bir metnin yapısını modellemek ve bu modeli farklı doğal dil işleme problemlerinde kullanmak mümkündür. Bu araştırmada anlam bütünlüğü, bu iki açıdan da incelenmektedir. Önce kelimeler arası anlam ilişikliğinin ölçülmesi için anlam bütünlüğü kullanılmış daha sonra bu kelime ilişkileri konu bölümleme, özet çıkarma ve anahtar kelime çıkarma problemlerinde kullanılmıştır.Kelimelerin anlam ilişikliğinin ölçülmesi için bir bilgi dağarcığı gerekmektedir. Araştırma kapsamında iki farklı bilgi dağarcığından faydalanılmaya çalışılmıştır. Birinci kelime dağarcığı kelime ilişkilerinin elle girildiği bir anlam ağıdır. Ikinci yöntem ise kelimelerin düz metin derlemindeki kullanım dağılımlarını kullanmaktadır. Araştırma kapsamında bu yöntemlerin birbirine göre başarımı ölçülmekte ve kapsamlı bir analiz yapılmaktadır.Konu bölümleme probleminde kelime bütünlüğü kullanan farklı yöntemler literatürde kullanılmaktadır. Bunların bazıları sadece kelime tekrarlarından faydalanırken, bazıları da eş anlam gibi güçlü anlamsal ilişkilerden faydalanmaktadır. Fakat şu ana kadar ?çok daha kapsamlı olan kelime ilişikliği yöntemleri bu problemde kullanılmamıştır. Yapılan deneyler göstermektedir ki konu bölümleme probleminin başarımı kelime ilişikliği kullanılarak arttırılabilmektedir. Ayrıca deneyler farklı kelime ilişikliği ölçüm yöntemlerini karşılaştırmak için kullanılabilmektedir. Konulara göre bölümlenmiş metinler otomatik özet çıkarma probleminde kullanılmış ve kelime zinciri tabanlı yöntemlere göre daha başarılı sonuçlar elde etmiştir.Son olarak kelime bütünlüğü analizi anahtar kelime bulma probleminde araştırılmaktadır. Geçmiş araştırmalar anahtar kelimelerin belge getirme ve navigasyon için başarılı araçlar olduğunu göstermektedir. Her ne kadar bu araştırmalar anahtar kelime ve belge getirme arasında bir ilişki olduğunu gösterse de, başka bir çalışmada anahtar kelimeleri bulmak için onların belge getirme başarım tahmini kullanılmamıştır. Bu araştırmada sorgu başarım tahmini yöntemlerinin anahtar kelime bulmada kullanımı incelenmiştir. Bunun için sorgu başarı tahmininde kullanılan öznitelikler anahtar kelime bulma probleminde Naive Bayes sınıflandırıcı ile birlikte kullanılmıştır. Yapılan deneyler bu özniteliklerin farklı boyuttaki belgelerde başarımı arttırdığını göstermektedir. Daha da önemlisi bu özniteliklerin yaygın olarak kullanılan deyim geçme frekansı ve belgede ilk kullanım yeri özniteliklerinin tersine kısa belgelerde daha başarılı olduğunu göstermektedir.Anahtar sozcukler: Kelime butunlu~gu, Anlamsal ilisiklilik, Konu Bolumleme,Ozetleme, Anahtar Kelime C karma.","When we express some idea or story, it is inevitable to use words that are semantically related to each other. When this phenomena is exploited from the aspect of words in the language, it is possible to infer the level of semantic relationship between words by observing their distribution and use in discourse. From the aspect of discourse it is possible to model the structure of the document by observing the changes in the lexical cohesion in order to attack high level natural language processing tasks. In this research lexical cohesion is investigated from both of these aspects by first building methods for measuring semantic relatedness of word pairs and then using these methods in the tasks of topic segmentation, summarization and keyphrase extraction.Measuring semantic relatedness of words requires prior knowledge about the words. Two different knowledge-bases are investigated in this research. The first knowledge base is a manually built network of semantic relationships, while the second relies on the distributional patterns in raw text corpora. In order to discover which method is effective in lexical cohesion analysis, a comprehensive comparison of state-of-the art methods in semantic relatedness is made. For topic segmentation different methods using some form of lexical cohesion are present in the literature. While some of these confine the relationships onlyto word repetition or strong semantic relationships like synonymy, no other work uses the semantic relatedness measures that can be calculated for any two word pairs in the vocabulary. Our experiments suggest that topic segmentation performance improves methods using both classical relationships and word repetition. Furthermore, the experiments compare the performance of different semantic relatedness methods in a high level task. The detected topic segments are used in summarization, and achieves better results compared to a lexical chains based method that uses WordNet.Finally, the use of lexical cohesion analysis in keyphrase extraction is investigated. Previous research shows that keyphrases are useful tools in document retrieval and navigation. While these point to a relation between keyphrases and document retrieval performance, no other work uses this relationship to identify keyphrases of a given document. We aim to establish a link between the problemsof query performance prediction (QPP) and keyphrase extraction. To this end, features used in QPP are evaluated in keyphrase extraction using a Naive Bayes classifier. Our experiments indicate that these features improve the effectiveness of keyphrase extraction in documents of different length. More importantly, commonly used features of frequency and first position in text perform poorly on shorter documents, whereas QPP features are more robust and achieve better results.Keywords: Lexical Cohesion, Semantic Relatedness, Topic Segmentation, Summarization, Keyphrase Extraction."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüz hesaplama sistemlerinin işlemesi gereken veri miktarlarındakı artış ve hesaplama sistemleri altyapı ve ekonomileri sebebleri ile uygulamaların çoğunda yüksek seviyede paralleleştirme gerekmektedir. Bu paralleleştirme genellikle veri-paralel çözümlerle gercekleştirilir ki bu çözümler de efektif veri gruplama (partitioning) ve veri dağıtma (declustering) yöntemleri gerektirir. Veri gruplama ve daşıtma yöntemlerinin yanında, gerek kullanılabilirliği gerekse performansı arttırma adına veri çoklama yöntemleri de sıkça kullanılmaya başlanmıştır. Veri bölümleme ya da dağıtma ve veri çoklama problemleri genellikle iki farkli aşamada çözümlenmeye çalışılırlar. Bu tezdeki çalışmalar, veri bölüöleme/dağıtma ve veri çoklama problemlerinin tek bir aşamada yapılması sureti ile daha etkin çoklanarakbölümlenmiş/dağıtılmış sistemler elde edilmesi ? kri üzerine yoğunlaşmıştır. Bu amaçla, bölümleme sistemlerinde yaygın olarak kullanılan Fiduccia-Mattheyses (FM) yinelemeli iyileştirme algoritması çoklama işlemini de kapsayacak şekilde genişletilmiştir. Bu algoritma kullanılarak sorgu günlükleri kullanan veri tabanı uygulamalarının performansını arttıracak bir çoklamalı veri dağıtma sistemi önerilmiştir. Ayrica bu çoklamalı veri dağıtma sisteminin sorgu desenlerinde değişimler, yeni sunucu ekleme ya da çıkarma işlemleri gibi durumlar karşısında mümküm olduğunca az veri taşıması yaparak kendini adapte etmesini sağlayan genişletme ve ilaveler önerilmiştir. Daha sonra, çoklamalı bölümleme problemi için geliştirilen tek-aşamalı çoklamalı bölümleme aracı, yaygın olarak bilinen iki uygulama (kelime bazlı arama ve Twitter) üzerinde test edilmiştir. Elde edilen sonuçlar sorgu günlükleri kullanımının ve çoklama ile veribölümleme/dağıtma işlemlerinin tek aşamada yapılmasının parallel performansı arttırdıgını göstermektedir.","The growth in the amount of data in todays computing problems and the level of parallelism dictated by the large-scale computing economics necessitates high-level parallelism for many applications. This parallelism is generally achieved via data-parallel solutions that require effective data clustering (partitioning) or declustering schemes (depending on the application requirements). In addition to data partitioning/declustering, data replication, which is used for data availability and increased performance, has also become an inherent feature of many applications. The data partitioning/declustering and data replication problems are generally addressed separately. This thesis is centered around the idea of performing data replication and data partitioning/declustering simultenously to obtain replicated data distributions that yield better parallelism. To this end, we utilize query-logs to propose replicated data distribution solutions and extend the well known Fiduccia-Mattheyses (FM) iterative improvement algorithm so that it can be used to generate replicated partitioning/declustering of data. For the replicated declustering problem, we propose a novel replicated declustering scheme that utilizes query logs to improve the performance of a parallel database system. We also extend our replicated declustering scheme and propose a novel replicated re-declustering scheme such that in the face of drastic query pattern changes or server additions/removals from the parallel database system, new declustering solutions that require low migration overheads can be computed. For the replicated partitioning problem, we show how to utilize an effective single-phase replicated partitioning solution in two well-known applications (keyword-based search and Twitter). For these applications, we provide the algorithmic solutions we had to devise for solving the problems that replication brings, the engineering decisions we made so as to obtain the greatest benefits from the proposed data distribution, and the implementation details for realistic systems. Obtained results indicate that utilizing query-logs and performing replication and partitioning/declustering in a single phase improves parallel performance."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Etkileşimli çizge düzenleme, bilgi görselleştirme sistemlerinde önemli bir rol oynamaktadır. Eldeki verinin kaliteli analizini yapmak için otomatikleştirilmiş yerleşim hesaplaması yapmak gerekmektedir. Basit çizgelerin otomatik yerleşiminin yapılmasına ilişkin bir çok çalışma yapılmıştır. Genellikle bu basit çizgelerde köşeler iki ya da üç boyutlu düzlemde nokta, kenarlar da bu noktaları bağlayan doğru ya da eğriler olarak gösterilmektedir. Ancak, ilişkisel veriler, genellikle hiyerarşik veya kümelenmiş olarak organize edilmektedirler. CoSE ile, kuvvet yönelimli yerleşim şemasına dayalı bir bileşik çizge yerleştirme algoritması sunulmaktadır. CoSE ile birlikte yönsüz, birbirinden farklı büyüklükte köşelere sahip bileşik çizgelerin yerleşimi sağlanmaktadır.Kullanıcı memnuniyetini sağlamak için çizge yerleştirme işinin kısa sürede tamamlanması ve çizgenin göze hoş gelen bir biçimde ekranda yer alması gerekmetedir. Hem performans hem de görsel kalitenin iyileştrilmesi amacıyla sunulan bir çok metod bulunmaktadır. Bu tez çalışmasında CoSE'nin görsel kalitesiningeliştirilmesi amacıyla çok seviyeli ölçeklendirme stratejisini adapte ettik. Ayrıca, çalışma zamanının iyileştirilmesi için de Fruchterman ve Reingold'un kareleme yöntemi ile grafik işleme ünitesi (GPU) üzerinde eş-zamanlı programlama stratejisini CoSE'ye uyguladık. Ek olarak, yay sabiti, serinleme faktörü ve benzeri parametrelerin ayarlanması işini de fiziksel sistemin davranışını önemli ölçüde etkilediği için dikkate aldık. Yaptığımız deneyler gösterdi ki, parametre ayarlamaları ve yukarıda bahsedilen metodların adaptasyonu ile birlikte CoSE algoritmasınınçalışma süresi önemli ölçüde azalırken, sonuç çizgelerin görsel kalitesi ise önemli miktarda iyileştirildi.","Interactive graph editing plays an important role in information visualization systems. For qualified analysis of the given data, an automated layout calculation is needed. There have been numerous results published about automatic layout of simple graphs, where the vertices are depicted as points in a 2D or 3D plane and edges as straight lines connecting those points. But simple graphs are insufficient to cover most real life information. Relational information is often clustered or hierarchically organized into groups or nested structures. Compound spring embedder (CoSE) of Chisio project is a layout algorithm based on a force-directed layout scheme for undirected, non-uniform node sized compound graphs.In order to satisfy the end-user, layout calculation process has to finish fast, and the resulting layout should be eye pleasing. Therefore, several methods were developed for improving both running time and the visual quality of the layout. With the purpose of improving the visual quality of CoSE, we adapted a multi-level scaling strategy. For improving the performance of the CoSE, the grid-variant algorithm proposed by Fruchterman and Reingold and parallel force calculation strategy by using graphics processing unit (GPU) were also adopted. Additionally, tuning of the parameters like spring constant and cooling factor were considered, as they affect the behavior of the physical system dramatically. Our experiments show that after some tuning and adaptation of the methods above, running time decreased and the visual quality of the layout improved significantly."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternetin büyümesiyle birlikte arama motorları insanların hayatında önemli bir etkiye sahip olmuştur. Günümüzde, büyük miktarda bilgi içeren milyarlarca web sayfası mevcuttur. Arama motorları internetten bilgi edinmek için vazgeçilemez araçlardır. Arama sonucu kalitesini arttırmaya yönelik olarak gösterilen sürekli çabaya rağmen, kullanıcılar arama motorlarında azımsanmayacak oranda az cevaplı ya da cevapsız sorgu sonuçlarıyla karşılaşabilmektedirler. Bu tezde, gerçek sorgu ""log""undan alınan bu tarz sorgular için ilk detaylı karakterizasyon analizi sağlanmaktadır. Deneysel düzenimiz üç önemli arama motorunun az cevaplı veya cevapsız sorgularla nasıl başa çıktığını karşılaştırmamıza olanak sağlayacak şekilde kurulmuştur. Ayrıca, sorgu öneri kalıplarının ve cevapsız sorguların tahmini için makine öğrenmesi modelleri geliştirilmiştir.","Nowadays search engines have significant impacts on people's life with the rapid growth of World Wide Web. There are billions of web pages that include a huge amount of information. Search engines are indispensable tools for finding information on the Web. Despite the continuous efforts to improve the web search quality, a non-negligible fraction of user queries end up with very few or even no matching results in leading commercial web search engines. In this thesis, we provide the first detailed characterization of such queries based on an analysis of a real-life query log. Our experimental setup allows us to characterize the queries with few/no results and compare the mechanisms employed by the three major search engines to handle them. Furthermore, we build machine learning models for the prediction of query suggestion patterns and no-answer queries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Arama motorları internet üzerinden bilgi aramak için yararlanılır. Çok anlamlı sorgular için ilgili dökümanların sorgu-doküman benzerliğine göre gelmesi kullanıcıyı memnun etmez; çünkü sorgunun birbirinden farklı birçok anlamı vardır. Bu çalışmada, yeni geliştirilen çapraz entropi tabanlı kademeli arama sonuç çeşitlendirmesi (CCED) yöntemi, sorgunun farklı anlamlarını içeren dokümanları arama sonuç listesinde üst sıralara yerleştirir. Değiştirilmiş ters sıralama ve çapraz entropi ölçümlerini birleştirerek sorgu-doküman benzerliği ile doküman-doküman çeşitliliği arasındaki ilişkiyi dengeler. Sorgu-doküman benzerliğini hesaplamak için Latent Diriclet Allocation (LDA) kullanılmıştır. Çok anlamlı sorgunun anlam sayısı, tam bağlı kümeleme tekniği ile tahmin edilmiştir. İlk Türkçe arama sonuç çeşitlendirme deney derlemi, BILDIV-2012, oluşturulmuştur. CCED'in başarısı iki yöntem ile karşılaştırılmıştır, Maximum Marginal Relevance (MMR) ve IA-Select. Bu karşılaştırmada Ambient, TREC Diversity Track ve BILDIV-2012 deney derlemleri kullanılmıştır. Bu algoritmaların başarısı Bing ve Google ile karşılaştırılmıştır. Sonuçlar, CCED'in sorgunun çeşitli anlamlarıyla ilgilenen kullanıcılara en ilgili dokümanları üst sıralarda getirmesi açısından diğer yöntemlere göre daha başarılı olduğunu göstermektedir.","Search engines are used to find information on the web. Retrieving relevant documents for ambiguous queries based on query-document similarity does not satisfy the users because such queries have more than one different meaning. In this study, a new method, cascaded cross entropy-based search result diversification (CCED), is proposed to list the web pages corresponding to different meanings of the query in higher rank positions. It combines modified reciprocal rank and cross entropy measures to balance the trade-off between query-document relevancy and diversity among the retrieved documents. We use the Latent Dirichlet Allocation (LDA) algorithm to compute query-document relevancy scores. The number of different meanings of an ambiguous query is estimated by complete-link clustering. We construct the first Turkish test collection for result diversification, BILDIV-2012. The performance of CCED is compared with Maximum Marginal Relevance (MMR) and IA-Select algorithms. In this comparison, the Ambient, TREC Diversity Track, and BILDIV-2012 test collections are used. We also compare performance of these algorithms with those of Bing and Google. The results indicate that CCED is the most successful method in terms of satisfying the users interested in different meanings of the query in higher rank positions of the result list."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez insan hareketlerinin birden çok kamera görüntüsü ile tanınması üzerine yapılan çalışmaları içermektedir. Bu çalışmalarda iki farklı yöntem önerilmiştir. Birinci yöntemde kalibre edilmiş kameralardan elde edilen hacimleri eşleştiren bir sistem, ikinci yöntemde ise görüntü karelerini eşleştiren esnek bir sistem önerilmiştir. Kullandığımız iki farklı yöntemde elde ettiğimiz sonuçlar, tek kamera görüntüleri ile yapılan çalışmalarda elde edilen sonuçlarla karşılaştırılarak, farklılıkları ve performansları incelenmiştir.Tezin ilk bölümü geri çatılım yöntemi ile elde edilen hacimsel veriler için yoğun betimleyiciler önerir. Kameralar tarafından kaydedilen görüntü kareleri geri çatılım yöntemi ile birleştirilir ve elde edilen hacimler hareket pozlarının eşleniği olarak kabul edilir. Bu çalışmalarda üç boyutlu verilerin üzerinden hızlı ve ayırt edici özelliklere sahip yeni poz betimleyicileri önerilmiştir. Bu betimleyicilerden ilki farklı doğrultuda ve boyuttaki silindirlerin histogramıdır. Önerilen bir diğer poz tanımlayıcısı ise bakış açısından bağımsızdır yani poz hizalamasına ihtiyaç duymamaktadır. Poz tanımlayıcılarının önemi hareket tanımlama kısımları sade tutulan düzeneklerde gösterilmiştir. Sunulan hacim eşlenmesine dayalı hareket tanımlama literatüre göre başarılı sonuçlar ortaya çıkarmıştır.Birden çok kamera verisinin işlenmesi ve ayıklanmasında hacim geri çatılım metodu seçilen en doğal yöntem olmuştur. Ancak birbiriyle örtüşen mevcut görüntüler yeterli sayıda olmayabilir. Tezin ikinci bölümünde farklı sayıda kamera ve öznitelikle çalışabilen bir hareket tanıma sistemi önerilmektedir. Bu sistem kamera görüntülerindeki hareket bulgularını oylama tekniği ile bulmaktadır ve kameraların kalibre edilmesine gerek duyulmamaktadır. Sistemin performansı kamera ve öznitelik sayısıyla orantılı olarak artmaktadır. Eğitim ve sınama için kullanılan kamera görüntülerinin örtüşmesine gerek yoktur. Sisteme herhangi bir anda bir kameranın girişi ve çıkışı kolayca çözümlenmektedir. İnsan hareketi tanımlanmasında birden çok kameranın kullanılmasının, tek kamera kullanılmasına oranla avantajları deneylerle desteklenmiştir.","This thesis explores the human activity recognition problem when multiple views are available. We follow two main directions: we first present a system that performs volume matching using constructed 3D volumes from calibrated cameras, then we present a flexible system based on frame matching directly using multiple views. We examine the multiple view systems compared to single view systems, and measure the performance improvements in recognition using more views by various experiments.Initial part of the thesis introduces compact representations for volumetric data gained through reconstruction. The video frames recorded by many cameras with significant overlap are fused by reconstruction, and the reconstructed volumes are used as substitutes of action poses. We propose new pose descriptors over these three dimensional volumes. Our first descriptor is based on the histogram of oriented cylinders in various sizes and orientations. We then propose another descriptor which is view-independent, and which does not require pose alignment. We show the importance of discriminative pose representations within simpler activity classification schemes. Activity recognition framework based on volume matching presents promising results compared to the state-of-the-art.Volume reconstruction is one natural approach for multi camera data fusion, but there can be few cameras with overlapping views. In the second part of the thesis, we introduce an architecture that is adaptable to various number of cameras and features. The system collects and fuses activity judgments from cameras using a voting scheme. The architecture requires no camera calibration. Performance generally improves when there are more cameras and more features; training and test cameras do not need to overlap; camera drop in or drop out is handled easily with little penalty. Experiments support the performance penalties, and advantages for using multiple views versus single view."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Cubism, pioneered by Pablo Picasso and Georges Braque, was a breakthrough in art, influencing artists to abandon existing traditions. In this thesis, we present a novel approach for cubist rendering of 3D synthetic environments. Rather than merely imitating cubist paintings, we apply the main principles of Analytical Cubism to 3D graphics rendering. In this respect, we develop a new cubist camera providing an extended view, and a perceptually based spatial imprecision technique that keeps the important regions of the scene within a certain area of the output. Additionally, several methods to provide a painterly style are applied. We demonstrate the effectiveness of our extending view method by comparing the visible face counts in the images rendered by the cubist camera model and the traditional perspective camera. Besides, we give an overall discussion of final results and apply user tests in which users compare our results very well with Analytical Cubist paintings but not Synthetic Cubist paintings.","Pablo Picasso ve Georges Braque'ın öncülük ettiği kübizm, sanatçılara yüzyıllarca süregelen gelenekleri terketmeleri konusunda ilham veren önemli bir hareketti. Bu çalışmada, üç boyutlu sentetik ortamların kübist sahnelenmesi için yeni bir yaklaşım sunduk. Kübist resimleri doğrudan kopyalamak yerine, Analitik Kübizmin temel ilkelerini üç boyutlu grafik sahnelemesine uyguladık. Bu doğrultuda, genişletilmiş bir görüntü sağlayan yeni bir kübist kamera ve sahnenin önemli bölümlerini sonucun belirli bir alanında tutan algıya dayalı bölgesel belgisizlik tekniğini geliştirdik. Ayrıca resim etkisini sağlamak için çeşitli yöntemlere başvurduk. Geleneksel kamera ve kübist kamera modeli ile oluşturulmuş resimlerde görünen yüz sayılarını karşılaştırarak, genişletilmiş görüntü yöntemimizin geçerliliğini gösterdik. Bunun yanında kesin sonuçlar üzerine genel bir tartışmaya yer verdik ve kullanıcı deneyleri gerçekleştirdik. Bu deneylerde, denekler sonuçlarımızı Analitik Kübist resimlere benzer bulmalarına rağmen Sentetik Kübist resimler ile eşleştirmediler."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Evrim ve tecrübe tabanlı öğrenme süreci hayvanlara doğada hayatta kalabilmeleri için birçok karmaşık görevi yerine getirebilen vücut yapıları ve hareket kabiliyetleri vermiştir. Bu hayvanlar arasında özellikle bacaklı omurgalılar, yüksek hızlara erişmek ve keskin manevralar yapabilmek için kaslı iskelet vücut yapılarını kullanırlar. Buna benzer dinamik davranışlarda esnek omurga, havyana fazladan güç ve esneklik desteği vererek önemli bir rol oynamaktadır. Robotik araştırmaları buna benzer dinamik becerileri mekanik sistemlerde gösterebilmeyi amaçlar. Buna rağmen günümüzde benzeri dinamik hareketleri yapabilen robotlar esnek omurga sistemlerine sahip değillerdir. Biz bu tez ile kontrol edilebilir omurga sisteminin dört bacaklı robotlarda sıçrayarak koşma hareketi üzerindeki etkilerini araştırıyoruz. Biyolojik kaynaklara dayanarak, ilk olarak yeni bir kontrol edilebilir omurgaya sahip dört bacaklı robot modeli sunuyor ve bu modeli yine yeni olarak sunduğumuz sıçrayarak koşma kontrolü modeli ile bağdaştırıyoruz. Çok tekrarlı değişkenli yöntemler kullanarak, sunduğumuz esnek robot ve standart robot modelini eniyileştiriyor ve bununla beraber omurga kontrolünün sunduğumuz esnek robot modeli üzerindeki başarım etkilerini araştırıyoruz. Daha yüksek hız ve zıplama yükseklikleri elde ederek esnek vücut yapısı ile dinamik sıçrayarak koşma hareketinin adım özellikleri arasındaki bağı inceliyoruz. Ayrıca tezimizde, sunduğumuz robot modelinin ve robottaki omurga sisteminin analitik incelemesini yaparak, bütün sistemdeki dinamikleri ve kuvvetleri araştırıyoruz. Simülasyon sonuçlarına bakarak esnek bir omurga sisteminin, robotlardan daha yüksek verim alabilmek için ne dereceye kadar geliştirilebileceğini sorguluyoruz.","Evolution and experience based learning have given animals body structures and motion capabilities to survive in the nature by achieving many complicated tasks. Among these animals, legged vertebrates use their musculoskeletal bodies up to the limits to achieve actions involving high speeds and agile maneuvers. Moreover the flexible spine plays a very important role in providing auxiliary power and dexterity for such dynamic behaviors. Robotics research tries to imitate such dynamic abilities on mechanical platforms. However, most existing robots performing these dynamic motions does not include such a flexible spine architecture. In this thesis we investigate how quadrupedal bounding can be achieved with the help of an actuated flexible spine. Depending upon biological correspondences we first present a novel quadruped robot model with an actuated spine and relate it with our proposed new bounding gait controller model. By optimizing our model and a standard stiff backed model via repetitive parametric methods, we investigate the role of spinal actuation on the performance enhancements of the flexible model. By achieving higher ground speeds and hopping heights we discuss the relations between flexible body structure and stride properties of a dynamic bounding gait. Furthermore, we present an analytical model of the proposed robot structure along with the spinal architecture and analyze the dynamics and active forces on the overall system. By gathering simulation results we question how such a flexible spine system can be improved to achieve higher performances during dynamic gaits."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kullanıcı yardımı sistemleri, yazılım ürünü kullanıcılarına kılavuzluk ederler. Kullanıcılara görevleri boyunca yardım ederek, başarılı bir kullanıcı deneyimini garantilemeye çalışırlar. Eskiden, en çok kullanılan kullanıcı yardımı araçları, çevrimdışı kullanma talimatı dokümanlarıydı. Bu dokümanlar bağlı oldukları sistemden tamamen bağımsız basılı şekilde bulunuyordu. Bu konuda son zamanlarda yükseliş gösteren eğilim ise, yardımın sağlanmasını otomatize eden teknolojiler kullanmaktır. Gömülü kullanıcı yardımı, eğitici ve kavramsal bilgiler içeren ve kullanıcı arayüzlerinde görülen bir yardım tipidir. Bu tip kullanıcı yardımında, yardım başlıkları, alan etiketleri ve sayfa açıklamaları en çok görülen çözüm yöntemleridir.Bu çalışmanın genel amacı, kullanıcı yardımı alanındaki en gelişkin teknolojileri ortaya çıkararak, bunların sonuçlarına bağlı bir yazılım çerçevesi geliştirmektir. Bu doğrultuda, otomatize kullanıcı yardımı ve gömülü kullanıcı yardımı alanlarının ikisi için de ayrı bir sistematik literatür incelemesi yapılmıştır. Araştırma bulguları, literatürde yayınlanmış detaylı çalışmalara paralel bir şekilde sunularak, bu konuda çok çeşitli çözüm yöntemlerinin ortaya çıkarılması amaçlanmıştır. Sistematik literatür incelemeleri, belirli araştırma stratejilerine dayalı yüzlerce farklı çalışmayı, çok aşamalı bir yayın seçme sürecine tabi tutmaktadır. Şu ana kadar kullanıcı yardımı sistemlerinin durumunu sistematik bir şekilde ele alan bir çalışma yapılmadığı için, bu çalışma güncel akımları göz önüne sererek, öncü değerde bir içerik sunmaktadır.Sistematik literatür incelemelerinin ve yardım yaratma araçları üzerinde yaptığımız bir diğer araştırmanın sonuçlarını analiz ettiğimizde, genellenmiş içerik-duyarlı kullanıcı yardımı sunan çözümlerin eksikliği ortaya çıkmıştır. Ayrıca yöntem, algoritma ve araçların kullanımı oldukça dağınıktır. Biz bu çalışmada, aslında hiç de kolay olmayan ve muhtelif zorluklar içeren, gömülü içerik-duyarlı kullanıcı yardımı sistemleri geliştirmeyi amaçladık. Ne yazık ki kullanıcı yardımı işleri, tek modüller içinde kolaylıkla lokalize edilemezler ve bu şekilde birden fazla modülü enine kesmeye eğimlidirler. Her münferit uygulamaya özel kullanıcı yardımı geliştirmek zahmetli olduğu için, kullanıcı yardımı araçlarının farklı uygulamalarda yeniden kullanılabilir olması gerekmektedir. Sonuç olarak, içerik-duyarlı kullanıcı yardımı geliştirmenin önündeki engeller, bu amaca yönelik bir araçlar çerçevesi fikrini beraberinde getirmiştir. Biz bu konulara çözüm yaratmak amacıyla, birden çok uygulamada içerik-duyarlı gömülü kullanıcı yardımı sunmak için kullanılabilen ve ilgiye-yönelik bir araçlar çerçevesi olan Assistant-Pro'yu geliştirdik. Bu çerçeve, süreç modeli tanımlamak, süreç adımlarına ilişkin yardım içeriğini tanımlamak ve yardım gerektiren hedef uygulamada yardım içeriğini modularize etmeye ve dokumaya imkan veren araçlar sunmaktadır. Bu araçlar çerçevesi orijinal olarak, büyük bir savuna sanayii firması olan Aselsan'ın kapsamında geliştirilmiş ve doğrulanmıştır.","User assistance systems act as a guide for the users of software products. These systems aim to guarantee a successful user experience by helping in performing tasks. Early on, off-line user manuals were mostly the mediums of user assistance,and technically, they were independent of the systems they belong to. The upward trend in user assistance systems is that the provision of assistance is automated through some attached mechanisms to the software systems. There have been numerous proposals introducing fresh and novel methods for the purpose of automated user assistance. Specically, embedded user assistance consists of instructional or conceptual information that appears within a software application window. It includes embedded help that appear within the application, field labels, and page overviews.The overall objective of this thesis is to reveal the state of the art advances in user assistance systems, and to propose a tool framework for developing context-sensitive user assistance systems. Firstly, we conducted two systematic literature reviews for both automated and embedded user assistance systems. The systematic literature reviews are required for acquiring solid background on embedded user assistance systems as well as for exploring the main obstacles to automated user assistance systems. The research findings are presented in parallel with the work published in the literature, and we aim at revealing a variety of techniques used for automated and embedded user assistance. The systematic reviews are conducted by a multiphase study selection process under a lot of articles obtained by dedicated search strategies. Since there has been no study to systematically undertake the state of user assistance systems, our work has a pioneering value of contents providing a road-map of current trends for further researchers in the field of user assistance.Having analyzed the results of systematic reviews, we conducted a survey of help authoring tools that revealed the lack of generalized context-sensitive user assistance solutions. Also, the utilization of methods, algorithms and tools differs from domain to domain, being rather scattered. We aimed at developingembedded context-sensitive user assistance systems, which is not trivial and has to meet several challenges. Unfortunately, user-assistance concerns such as help content and related weaving information cannot be easily localized in single modules and as such tend to crosscut multiple modules. The reuse of user assistance tools for different applications is required because developing custom-based user assistance for each separate application is laborious. Consequently, the obstacles related to the development of context-sensitive user assistance systems have brought out the idea of a tool framework for this purpose. To address these issues we developed an aspect-oriented tool framework Assistant-Pro that can be used to develop context-sensitive embedded user assistance for multiple applications. The framework provides tools for defining the process model, defining guidance related to process steps, and modularizing and weaving help concerns in the target application for which user guidance needs to be provided. The tool has been originally developed and validated in the context of Aselsan, a large Turkish defense electronics company."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Veri gridleri, büyük veri setleri üreten ve kullanan uygulamalar için coğrafi olarak dağıtılmış kaynaklar sağlar. Halbuki, veri gridlerinde veriye hızlı erişim ve işler için düşük yanıt süresi temin etme durumları, çeşitli sebeplerden dolayı engellenmektedir. Bu sorunları ele almak için, yüksek veri elverişliliği, düşük bant genişliği tüketimi ve indirgenmiş yanıt süresi sağlayan değişik veri çoklama ve iş çizelgeleme stratejileri sunulmuştur. Veri çoklama sayesinde, veri farklı konumlarda çok kopyalı şekilde muhafaza edilmektedir. Ayrıca, grid üzerinde etkili bir şekilde iş çizelgeleme yaparak, sistem verimliliğinin arttırılması amaçlanmıştır. Çoklama stratejileri genelde statik ve dinamik olarak sınıflandırılır. Statik çoklama stratejilerinde, çoklama kararları çoğunlukla grid sistemindeki veri erişim maliyetlerini, bant genişliği özelliklerini ve saklama kısıtlarını kapsayan bir maliyet modeline dayanarak verilir ve çoklama işlemi sistemin tasarlanması sırasında yapılmaktadır. Dinamik çoklama stratejilerinde çoklama işlemi, kullanıcı isteği deseninindeki değişiklikleri sisteme uyarlamak için çalışma zamanında yapılmaktadır. İş çizelgeleme stratejileri, çevrimiçi mod ve toplu mod olmak üzere iki genel kategorinin içinde yer alırlar. Çevrimiçi mod çizelgeleyicisi, bir işi ulaşır ulaşmaz bir makineye atar. Toplu mod yönteminde, bütün grid bilgisini kullanarak, bütün işler aynı anda ele alınır ve çizelgelenir.Biz bu çalışmada, grid sistemindeki işleri ve verileri temsil eden bir ""iki kısımlı çizge"" modeli önermekteyiz. Veri yerleştirme ve iş çizelgeleme stratejisi elde etmek için bu çizgeyi bölüntülüyoruz. Elde edilen bölüntüler, yerleşkeler arasındaki bant genişliğini ve hoplama bilgisini hesaba katan KL-tabanlı buluşsal bir çizge bölüntüleme yöntemi kullanarak, grid yerleşkelerine atama yapmak için yeniden iyileştirilmektedir. Çoklama, bölüntüleme sürecinden önce seçilen en çok erişilen dosyaların belli bir miktarını kopyalarak gerçekleştirilir. Deneysel sonuçlar göstermektedir ki, bölüntüleme kalitesindeki artış, atama kalitesine olumlu şekilde yansımaktadır. Buna ek olarak, veri çoklama uygulandığında iletişim maliyetinin dikkate değer ölçüde düştüğü gözlemlenmiştir.","Data grids provide geographically distributed resources for applications that generate and utilize large data sets. However, there are some issues that hinder to ensure fast access to data and low turnaround time for the jobs in data grids. To address these issues, several data replication and job scheduling strategies have been introduced to offer high data availability, low bandwidth consumption, and reduced turnaround time for grid systems. Multiple copies of existing data are maintained at different locations via data replication. Data replication strategies are broadly categorized as static and dynamic. In static replication strategies, replication is performed during the system design, and replica decisions are generally based on a cost model that includes data access costs, bandwidth characteristics and storage constraints of the grid system. In dynamic replication strategies, the replication operation is managed at runtime so that the system adapts to the changes in user request patterns dynamically. Job scheduling strategies fall under two main categories: online mode and batch mode. The online mode scheduler assigns tasks to sites as soon as they arrive. In the batch mode, the complete set of jobs are taken into account and scheduled at the same time by using all the grid information.In this thesis, we propose a bipartite graph model for tasks and files in the grid system, and then we partition this graph to obtain a data placement and job scheduling strategy. The obtained parts are further refined in order to be assigned to grid sites by using a KL-based heuristic that takes the bandwidth and hop information between sites into account. Replication is achieved by replicating a certain amount of most accessed files chosen prior to the partitioning process. Experimental results indicate that the increase in the partitioning quality reflects positively on the mapping quality. Morever, it is observed that the communication cost is notably decreased when the data replication is applied. Hence, our results show that by replicating a small amount of data files and placing files onto sites using bipartite graph model, we can obtain performance improvement for scheduling jobs compared to no replication."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağları (KAA) kurmak için kullanılan birçok algılayıcı düğüm düzlemleri kablosuz iletişim için birden çok radyo kanalını destekleyebilmektedir. Bu yüzden, tek bir radyo kanalı kullanıp tüm ağ için onu paylaşmak yerine, toplam üretilen işi arttırmaya ve paket çarpışmasını ve gecikmeyi azaltmaya yardımcı olabilecek toplam girişimi azaltmak için, eş zamanlı olarak birden çok kanaldan algılayıcı ağlarında yararlanılabilir. Bu ancak, çok-kanallı iletişimde kullanılacak düğümlere kanal atamak için uygun kanal atama yöntemleri kullanılmasını gerektirir. Genellikle algılayıcı düğümlerinde oluşturulan veri, ağda bulunan bir ya da daha çok alıcıya yol atama ağaçlarıyla taşındığı için, KAA'da kanal atamak için ağaç-tabanlı kanal atama yöntemlerinin kullanılması doğal bir yaklaşımdır. Çok-kanallı KAA'lar için iki adet hızlı ağaç-tabanlı kanal atama yöntemi sunulmaktadır (BUCA ve NCCA). Ayrıca, algoritmalarımızda karar verirken kullanılan yeni bir ağ girişim ölçevi de önerilmektedir. Önerilen yöntemler ayrıntılı benzetim ve deneylerle değerlendirilmektedir ve literatürde iyi bilinen başka bir ağaç-tabanlı iletişim kuralı ile karşılaştırılmaktadır. Sonuçlar önerdiğimiz algoritmaların diğer yöntem ile karşılaştırıldığında daha iyi başarım gösterdiğini ve %40'a kadar başarım artışı olduğunu göstermektedir. Ayrıca, hangi durumlarda başarım artışına ulaşıldığı tartışılmaktadır.","A lot of sensor node platforms used for establishing wireless sensor networks (WSNs) can support multiple radio channels for wireless communication. Therefore, rather than using a radio single channel and sharing it for the whole network, multiple channels can be utilized in a sensor network simultaneously to decrease the overall interference in the network, which may help increasing the aggregate throughout in the network and decrease packet collisions and delay. This requires, however, appropriate channel assignment schemes to be used for assigning channels to the nodes for multi-channel communication in the network. Since, data generated by sensor nodes are usually carried to one or more sinks in the network using routing trees, tree-based channel assignment schemes are a natural approach for assigning channels in a WSN. We present two fast tree-based channel assignment schemes (called BUCA and NCCA) for multi-channel WSNs. We also propose a new network interference metric that is used in our algorithms while making decisions. We evaluate our proposed schemes by extensive simulation experiments and compare them with another well-known tree-based protocol from the literature. The results show that our proposed algorithms can provide better performance, up to 40% performance increase in some cases, compared to the other method. We also discuss in which cases the performance improvement can be achieved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otomatik hücre görüntüleme sistemleri, hücre seviyesindeki biyolojik olayların hızlı ve güvenilir bir şekilde analizine olanak sağlar. Bu sistemlerde genellikle ilk basamak, daha sonraki sistem basamaklarını büyük ölçüde etkileyen hücre bölütlemesidir. Öte yandan, diğer görüntü bölütleme problemlerine benzer şekilde, hücre bölütleme eksik tanımlanmış bir problemdir. Bu problem, tipik olarak, insanların başarılı bölütlemeler elde edebilmesi için bile, alan bilgisinin kullanılmasını gerektirir. Bu bilgiyi bölütleme algoritmalarına dahil eden yaklaşımların, bölütleme sonuçlarını büyük ölçüde iyileştirme potansiyelleri vardır.Biz bu çalışmamızda, faz-kontrast mikroskobu ile alınan canlı hücrelerin etkin bölütlenmesi için yeni bir yaklaşım önermekteyiz. Bu yaklaşım, işaretçilerinin tespitinin kritik olduğu işaretçi-kontrollü su-seddi algoritmaları için yeni bir ?akıllı işaretçi? kümesi tanımlar.Önerilen yaklaşım, hücrelerin görsel karakteristikleri biçiminde ifade edilen alana özel bilginin işaretçi tanımında kullanılmasına dayanır. Bu yaklaşım, toplam 1954 hücre üzerinde değerlendirilmiştir. Deneysel çalışmalar, önerilen yaklaşımın, diğer yaklaşımlarla karşılaştırıldığında daha iyi işaretçiler bulunmasında etkin olduğunu göstermiştir. Bu ise, işaretçi-kontrollü su-seddi algoritmalarının bölütleme performanslarının iyileştirilmesinde etkili olacaktır.","Automated cell imaging systems facilitate fast and reliable analysis of biological events at the cellular level. In these systems, the first step is usually cell segmentation that greatly affects the success of the subsequent system steps. On the other hand, similar to other image segmentation problems, cell segmentation is an ill-posed problem that typically necessitates the use of domain specific knowledge to obtain successful segmentations even by human subjects. The approaches that can incorporate this knowledge into their segmentation algorithms have a potential to greatly improve the segmentation results.In this study, we propose a new approach for the effective segmentation of live cells from phase-contrast microscopy. This approach introduces a new set of ``smart markers'' for a marker-controlled watershed algorithm, for which the identification of its markers is critical. The proposed approach relies on using domain specific knowledge, in the form of visual characteristics of the cells, to define the markers. We evaluate our approach on a total of 1954 cells. The experimental results demonstrate that the proposed approach is quite effective in identifying better markers compared to its counterparts. This will in turn be effective in improving the segmentation performance of a marker-controlled watershed algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet haber sayfaları, reklamlar, bağlantılar, ve kullanıcı yorumları gibi fazladan elemanlar içermektedirler. Bu elemanlar, haber içeriklerinin çıkartılmasını zorlu kılmaktadırlar.Günümüzdeki haber içeriği çıkartma (HİÇ) yöntemleri genellikle şablon bağımlı olarak çalışmaktadırlar. Haber sağlayıcılar, internet sayfası şablonlarını sıklıkla değiştirdikleri için,bu yöntemler düzenli bakım gerektirmektedirler. Bu nedenle, haber içeriklerini internet sayfası şablonlarına bağımlı olmaksızın doğru bir şekilde çıkartabilecek HİÇ yöntemlerine gereksinim duyulmaktadır. Bu tez çalışmasında, bir şablon bağımsız haber içeriği çıkartma yöntemi (N-EXT) önerilmiştir. N-EXT ilk olarak, bir haber sayfasını HTML etiketlerine göre bloklara ayrıştırır. Daha sonra haber içeriğinin çoğunluğunu ya da tamamını içeren bloğu tespit etmek için ayrıştırdığı tüm blokları inceler. Bu amaçla, bloklara metinsel boyutlarını ve haber başlığına olan benzerliklerini göz önünde tutarak birer ağırlık tahsis eder. Bu iki ağırlık bileşenlerinin önemini belirlemek için k-kat çapraz doğrulama yaklaşımı ve olası farklı benzerlik ölçülerinin etkilerini değerlendirmek için de tek yönlü varyans analizi (ANOVA) ve Scheffe çoklu karşılaştırma testi birlikte kullanılmıştır. En yüksek ağırlığa sahip blok, haber bloğu olarak düşünülür. Haber bloğu içerisinde yer alan fakat haber içeriğiyle ilgisi olmayan cümleler, önerilen yöntem tarafından haber bloğuna olan benzerlikleri değerlendirilerek haber bloğundan elenir. Son olarak, önerilen yöntem olası haber içeriği kalıntılarını tespit etmek için, haber bloğu dışındaki blokları da inceler. Farklı haber sitelerinin internet sayfalarını içeren iki farklı deney koleksiyonu üzerinde yapılan deneylerce, önerilen yöntemin doğruluğu ve dayanıklılığı gösterilmiştir.","News web pages contain additional elements such as advertisements, hyperlinks, and reader comments. These elements make the extraction of news contents a challenging task. Current news content extraction (NCE) methods are usually template-dependent. They require regular maintenance, since news providers frequently change their web page templates. Therefore, there is a need for NCE methods that extract news contents accurately without depending on web page templates. In this thesis, a template-independent News content EXTraction approach, called N-EXT, is introduced. It first parses a web page into its blocks according to the HTML tags. Then, it examines all blocks to detect the one that contains the major part of the news content. For this purpose, it assigns weights to the blocks by considering both their textual sizes and similarities to the news title. For quantifying the importance of these two weight components, we use the k-fold cross validation approach; and for assessing the impact of different possible similarity measures, we use a one-way Analysis of Variance (ANOVA) with a Scheff\'{e} comparison. The block with the highest weight is considered as the news block. Our approach eliminates the sentences in the news block that are not related to the news content by considering similarities of sentences to the news block. Finally, it also examines other blocks to detect the rest of the news content. The experimental results show the accuracy and robustness of our method by using two test collections whose web pages are obtained from several different news websites."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kürk, hayvanların önemli görsel unsurlarından biridir ve bilgisayar grafiklerinde bunu modellemek oldukça zordur. Bunun en büyük nedeni ise, kürkün çok fazla geometrik tüyden oluşuyor olması ve kişisel bilgisayarlarımızda bu kadar fazla geometrik öğeyi gerçek zamanda hem imgeye dönüştürmek, hem de hareket ettirmemizin zor olmasıdır. Bundan dolayı güncel bilgisayar oyunlarında, hayvanların çok basit bir kürk yapısı vardır ama bu mevcut yöntemler yeterli gerçekliği sağlamamaktadır.Araştırmacılar hayvan kürkünü modellemek için bir çok farklı yöntem sunmuş olmasına rağmen, çoğu tüyün doğal özelliklerini yansıtmayı başaramamışlardır. Bunlara tüyün aydınlatılması, gölgelendirmesi, ve animasyonu örnek olarak gösterilebilir. Bu yüzden ortaya çıkan sonuç gerçekçi bir oyun deneyimi için yeterli olmamıştır.Bu tezde 3B nesneler üzerinde kullanılabilir gerçek zamanlı bir kürk modelleme yöntemi gösterilmektedir. Ayrıca tezimizde bu yönteme aydınlatmanın, gölgelendirmenin, animasyonun ve hatta yakmanın nasıl entegre edilebileceğini gösteriyoruz.","Fur is one of the important visual aspects of animals and it is quite challenging to model it in computer graphics. This is due to rendering and animating high amounts of geometry taking excessive time in our personal computers. Thus in computer games most of the animals are without fur or covered with a single layer of texture. But these current methods do not provide the reality and even if the rendering in the game is realistic the fur is omitted.There have been several models to render a fur, but the methods that incorporate rendering are not in real-time, on the other hand most of the real-time methods omit many of the natural aspects , such as; texture lighting, shadow and animation. Thus the outcome is not sufficient for realistic gaming experience.In this thesis we propose a real-time fur represantation that can be used on 3D objects. Moreover, we demonstrate how to; render, animate and burn this real-time fur."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Robotların özerk hareketi için konumlanma ve haritalama çok önemli birimlerdir. Lakin, bu tip robotlar genelde yerleşimden uzak, dış alanlarda çevre hakkında bir önsel bilgi olmadan çalışmak zorundadır. Buna bağlı olarak, saha robotları algılayıcıları doğrultusunda kendi haritalarını çıkarmak zorundadırlar. Bu doğrultuda, görsel algılayıcılar ile haritalamanın çeşitli avantajları vardır. Yüksek bant genişliğine sahip görsel algılayıcılar ile, insanların da anlayabileceği haritalar anlamlı ve birbirinden ayırt edilebilir nesneler tanımlanarak çıkartılabilir.Bu tezde, Eşzamanlı Konumlanma ve Haritalama (EKH) için uygun işaretler grubu olarak, dış alanlarda ağaçların işaret olarak kullanılması üzerine yoğunlaştık. Ağaçlar göreceli olarak kolay, neredeyse dikey bir yapıya sahiptir ve bu, onların kolay ve istikrarlı olarak tesbit edilebilmelerini sağlar. Daha önemlisi, ağacın kalınlığı farklı görüş açılarından hassas ve istikrarlı olarak belirlenebilmektedir. Bizim ana katkımız, ağaç kalınlığı bilgisini algılayıcılar ile algılayarak, haritadaki her ağaca yarıçap bilgisini de dahil edebilmektir. Bu doğrultuda, görüntüdeki ağaç kalınlığı ile onun yarıçapını birbirine bağlayan yeni bir algılayıcı modeli geliştirdik. Bu modelin matematiksel formüllerini çıkarıp, EKH için gerekli olan türevlerini aldik. Sonra da, çalışan bir EKH üzerine algılayıcı modelini ve türevleri ekledik. Yeni algılayıcı modelinin yalnızca bir kamera kullanarak, konumlanma ve haritalama hassasiyetini artırdığını simülasyonlar ile gösterdik.","Localization and mapping are crucial components for robotic autonomy. However, such robots must often function in remote, outdoor areas with no a-priori knowledge of the environment. Consequently, it becomes necessary for field robots to be able to construct their own maps based on exteroceptive sensor readings. To this end, visual sensing and mapping through naturally occurring landmarks have distinct advantages. With the availability of high bandwidth data provided by visual sensors, meaningful and uniquely identifiable objects can be detected. This improves the construction of maps consisting of natural landmarks that are meaningful for human readers as well.In this thesis, we focus on the use of trees in an outdoor environment as a suitable set of landmarks for Simultaneous Localization and Mapping (SLAM). Trees have a relatively simple, near vertical structure which makes them easily and consistently detectable. Furthermore, the thickness of a tree can be accurately determined from different viewpoints. Our primary contribution is the usage of the width of a tree trunk as an additional sensory reading, allowing us to include the radius of tree trunks on the map. To this end, we introduce a new sensor model that relates the width of a tree landmark on the image plane to the radius of its trunk. We provide a mathematical formulation of this model, derive associated Jacobians and incorporate our sensor model into a working EKF SLAM implementation. Through simulations we show that the use of this new sensory reading improves the accuracy of both the map and the trajectory estimates without additional sensor hardware other than a monocular camera."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Arazi görselleştirme, bilgisayar oyunları ve görsel benzetimler gibi gerçek-zamanlı bilgisayar grafikleri uygulamalarının çok önemli bir parçasıdır. Arazi görselleştirme, izleyiciye temel bir referans-çerçevesi sağlamasının yanında izleyiciyi saran hayali veya benzetimli dünyanın temelini oluşturur. Gerçek-zamanlı uygulamalarda arazi modellerinin saklanması ve görselleştirmesi genellikle veri boyutu büyüklüğü ve talep edilen detay seviyesi nedeniyle bu iş için özelleşmiş bir yaklaşım gerektirir. Böyle büyük boyutlu verilerin gerçek-zamanlı olarak işlenmesi ve görselleştirilmesi için izlenecek en kolay yol arazi modellerinin birçok yönden kısıtlanmasıdır. Bu kalıba uydurma işlemi, arazi modellerinin ifade edilebilirliğinin ve ilginç arazi modelleri yaratma olanaklarının kısıtlanması pahasına işlenmesi gereken veri boyutunu ve ihtiyaç duyulan işlem gücünü azaltır.Çağdaş gerçek-zamanlı grafik uygulamaları tarafından en çok kullanılan arazi gösterimi yükseklik haritasının üçüncü boyutta uygulandığı düzenli bir 2B ızgaradır. Bu gösterim, mümkün olan en basit ve hızlı işlem olanağı sağlayan arazi gosterimidir; ancak mağara, asılı kaya, uçurum ve kemer gibi ilginç arazi özelliklerinin temsil edilmesine olanak sunmaz. Biz, hacimsel gösterim ve yükseklik haritası yaklaşımlarını birleştiren, mağara, asılı kaya, uçurum ve kemer gibi arazi özelliklerini içeren karmaşık arazi modellerini temsil edebilecek ifade yeteneğine sahip ve gerçek-zamanlı olarak arazi düzenleme, şekil değişikliği ve görselleştirmesine izin verecek kadar verimli yeni bir arazi gösterimi öneriyoruz. Aynı zamanda, önerilen arazi gösterimine ışıklandırma, kaplama, gölgelendirme ve detay seviyesi belirleme işlemlerinin nasıl uygulanabileceğini inceliyoruz.","Terrain rendering is a crucial part of many real-time computer graphics applications such as video games and visual simulations. It provides the main frame-of-reference for the observer and constitutes the basis of an imaginary or simulated world that encases the observer. Storing and rendering terrain models in real-time applications usually require a specialized approach due to the sheer magnitude of data available and the level of detail demanded. The easiest way to process and visualize such large amounts of data in real-time is to constrain the terrain model in several ways. This process of regularization decreases the amount of data to be processed and also the amount of processing power needed at the cost of expressivity and the ability to create interesting terrains.The most popular terrain representation, by far, used by modern real-time graphics applications is a regular 2D grid where the vertices are displaced in a third dimension by a displacement map, conventionally called a height map. It is the simplest and fastest possible terrain representation, but it is not possible to represent complex terrain models that include interesting terrain features such as caves, overhangs, cliffs and arches using a simple 2D grid and a height map. We propose a novel terrain representation combining the voxel and height map approaches that is expressive enough to allow creating complex terrains with caves, overhangs, cliffs and arches, and efficient enough to allow terrain editing, deformations and rendering in real-time. We also explore how to apply lighting, texturing, shadowing and level-of-detail to the proposed terrain representation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışma Gecikme Toleranslı Ağlar (GTA) üzerinde yönlendirme sorununa odaklanmaktadır. GridRoute ağ elemanlarının hareket ve pozisyon bilgilerinden yaralanarak her yönlendirme adımında mesajın hedefine ulaşma şansını artıran olasılıksal bir yönlendirme protokolüdür. Bu çalışmanın literatüre en büyük karkısı GTA'larda akıllı yönlendirme adımları uygulayabilmek için ağ elemanları üzerinde diğer elemanlarla alakalı hiç bir bilgi depolaması gerektirmeyen bir protokolün sunumudur.Aynı zamanda, GridRoute gereksiz mesaj trafiğini var olan protokoller arasında en aza indirgemekte, kabul edilebilir mesaj gecikme sürelerine ulasabilmekte ve kimlik gizliliği gibi bazı güvenlik avantajları sağlamaktadır. Benzeştirim sonuçlarına göre, GridRoute var olan yönlendirme protokollerinin hepsinden hafıza gereksinimi açısından üstündür. Bununla birlikte GridRoute,yüksek ulaştırma oranlarına, başarılı gönderici-hedef arası gecikmeye ve önemli derecede az gereksiz mesaj tragine ulaşmıştır.","This work proposes a new routing protocol for delay-tolerant mobile networks (DTMNs) called GridRoute. The proposed protocol can be adopted considering network requirements such as low message delay or low resource usage. GridRoute is a probabilistic routing protocol that takes advantage of mobility and location information of nodes. It uses a multi-layered grid for contact probability maximization. It requires almost no memory storage of contact or location probabilities for intelligent routing decisions. GridRoute also minimizes the number of redundant messages throughout the network with feasible delay on message delivery, and provides some security advantages like identity secrecy. Our simulation results show that GridRoute outperforms existing routing protocols in terms of memory requirement. It also achieves high delivery ratio, reasonable end-to-end delay and signicantly lower message overhead."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, birbirine benzer doküman tespitinin genelleştirilmiş versiyonu olan bir dokümanıniçerdiği bilgilerin başka bir doküman tarafından içerilip içerilmediğini ortaya koyan kapsamatespiti konusu incelenmiştir. Yazılı dokümanlarda dokümanların birbirini kapsaması üç farklışekilde karşımıza çıkmaktadır: ilk durum dokümanların tamamen aynı olması, ikinci durumdokumanların oldukça benzer olması, üçüncü ve ilk iki durumun daha geniş kapsamlı haliise bir dokumanın diğerini içermesi.Kapsama tespiti için CoDet ismini verdiğimiz özelliklepeşisıra gelmekte olan haberler için kullanışlı yeni bir algoritma önermekteyiz. Ayrıcahavuzlama tekniği vasıtasıyla sınırlı insan yardımı kullanarak algoritmaların etkinliğini veverimliliğini güvenilir bir şekilde ölçmemizi sağlayan bir test koleksiyonu oluşturduk.CoDet'in performansını oldukça benzer doküman tespitinde kullanılan ve alanındabaşarılı kabul edilen dört farklı algoritma (DSC, full fingerprinting, I-Match ve SimHash)ile karşılaştırdık. Deneysel çalışmalarımızdan edindiğimiz bulgulara göre CoDet genelliklealternatif algoritmalardan daha iyi sonuç vermekte ve yazılı dokümanlar üzerindekapsama tespiti konusunda kaydadeğer sonuçlar üretmektedir.","In this thesis, we investigate containment detection, which is a generalized versionof the well known near-duplicate detection problem concerning whether adocument is a subset of another document. In text-based applications, there arethree ways of observing document containment: exact-duplicates, near-duplicates,or containments, where first two are the special cases of containment. To detectcontainments, we introduce CoDet, which is a novel algorithm that focusesparticularly on containment problem. We also construct a test collection using anovel pooling technique, which enables us to make reliable judgments for the relativeeffectiveness of algorithms using limited human assessments. We compare itsperformance with four well-known near duplicate detection methods (DSC, fullfingerprinting, I-Match, and SimHash) that are adapted to containment detection.Our algorithm is especially suitable for streaming news. It is also expandable todifferent domains. Experimental results show that CoDet mostly outperforms theother algorithms and produces remarkable results in detection of containments intext corpora."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kanser tanısı ve derecelendirilmesi için doku örneklerinin pataloglar tarafından incelenmesi gereklidir. Fakat, bu inceleme patalogların dokuyu görsel olarak incelemesinden dolayı oldukça fazla öznelliğe sebep olur. Bu problemi azaltmak için bilgisayar ortamında ölçülebilir değerler ile çalışan sistemler geliştirilmelidir ve imge bölütleme işlemi de bu sistemlerin temel taşıdır. Literatürdeki imge bölütleme yöntemleri dokuları nicelendirmek için genel olarak piksel düzeyindeki renk/yapı tanımlayıcılarını kullanır. Ancak piksel düzeyindeki bu bilgiler doku bileşenlerinin uzamsal organizasyonu gibi patolojiye özgü bilgiyi ifade edebilmek ve bu bilgiyi kullanabilmek için yeterli değildir. Bunun yanında, genel olarak doku resimlerinde oldukça fazla varyasyon ve piksel düzeyinde parazit bulunur; farklı doku bileşenlerinin benzer renk dağılımları, hücre dizilimlerindeki dağılmalar ve fazla boyanmadan dolayı oluşan bölgesel renk karşıtlıkları gibi. Önceki bölütleme yöntemleri piksel düzeyinde tanımlayıcılar kullandıkları için belirtilen bu problemlere karşı daha fazla duyarlıdır.Bu sorunları başarılı bir biçimde çözebilmek için, bu tezde üç yeni yapısal tanımlayıcı sunduk -- bunlar ObjSEG, GraphRLM ve ObjCooc yapıları olarak adlandırıldı -- ve histopatolojik doku resimlerinin bölütlenmesi için bu tanımlayıcıları kullanan yöntemler geliştirdik. Bu tanımlayıcıları yaklaşık olarak dairesel nesnelerle betimlenmiş doku bileşenleri üzerinde hesapladık. Belirtilen nesneye dayalı tanımlayıcılar doku bileşenleri üzerinden çıkarıldıkları için bu bileşenlerin uzamsalorganizasyonunu ve dolaylı olarak patolojiye özgü bilgileri literatürdeki emsallerine kıyasla daha iyi temsil edebilirler. Böylece bu tanımlayıcıları kullanarak geliştirdiğimiz yöntemler de daha verimli ve güçlü sonuçlar çıkarabilır. Ayrıca, bu tanımlayıcılar direk olarak imge pikselleri üzerinden hesaplanmadıkları için piksel düzeyindeki problemleri de azaltma konusunda daha etkilidirler.Deneylerimizde, sunduğumuz nesneye dayalı tanımlayıcıları kullanan imge bölütleme yöntemlerimizi 200 kolon doku imgesinde test ettik. Deneylerimiz, nesneye dayalı bu yeni tanımlayıcıların hem yüksek oranda doğruluk veren bölütleme sonuçları çıkardığını hem de bölütlenmiş alan sayısını makul seviyelerde tutmayı başardığını kanıtladı. Deneysel sonuçlarımız, önceki imge bölütleme yöntemleriyle de karşılaştırıldığında, sunduğumuz yöntemlerin histopatolojik resimlerin bölütlenmesinde daha etkili olduğunu gösterdi.","The histopathological examination of tissue specimens is essential for cancer diagnosis and grading. However, this examination is subject to a considerable amount of observer variability as it mainly relies on visual interpretation of pathologists. To alleviate this problem, it is very important to develop computational quantitative tools, for which image segmentation constitutes the core step. The segmentation algorithms in literature commonly use pixel-level color/texture descriptors that they define on image pixels for quantizing a tissue. On the other hand, it is usually harder to express domain specific knowledge about tissues, such as the spatial organization of tissue components, using only the pixel-level descriptors. This may become even harder for tissue images, which typically consist of a considerable amount of variation and noise at their pixel-level, such as similar color distribution of different tissue components, distortion in cell alignments, and color contrast caused by too much stain in a particular region. The previous segmentation algorithms are more susceptible to these problems as they work on pixel-level descriptors.In order to successfully address these issues, in this thesis, we introduce three new texture descriptors, namely ObjSEG, GraphRLM, and ObjCooc textures, and implement algorithms that use these descriptors for segmenting histopathological tissue images. We extract these texture descriptors on tissue components that are approximately represented by circular objects. Since these object-oriented texture descriptors are defined on the tissue components, and hence domain specific knowledge, they represent the spatial organization of the components better than their previous counterparts. Thus, our algorithms based on these descriptors give more effective and robust segmentation results. Furthermore, since the descriptors are not directly defined on image pixels, they are effective to alleviate the pixel-level problems.In our experiments, we tested our algorithms that use the proposed object-oriented descriptors on a dataset of 200 colon tissue images. Our experiments demonstrated that our new object-oriented feature descriptors led to high segmentation accuracies, also providing a reasonable number of segmented regions. Compared with its previous counterparts, the experimental results also showed that our proposed algorithms are more effective in segmenting histopathological images."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kişiler ve markalar hakkındaki görüşleri anlamak, itibar yönetimi ve karar verme konularında yardımcı olur. Sosyal medyann gelişiyle, daha çok insan tavsiye ve görüşlerini aleni şekilde paylaşmaya istekli hale gelmiştir. Sosyal alanların tip ve miktarı arttığı için, metinsel kaynaklardaki duygu analizini otomatize etmek, zaruri bir veri madenciliği görevi haline gelmiştir. Duygu sınıflandırma, metindeki duygu kutupluluğunu belirlemeyi amaçlar. Kutupluluk, duygunun güçlülüğu belirtildiği kadar, ya ikili (pozitif, negatif) ya da cok değişkenli skalada tahmin edilir. Metinler çoğu kez pozitif ve negatif duyguların karışımını ihtiva ederler; dolayısıyla, bu iki tip duyguyu sıkça aynı anda saptamak gereklidir. Metni duygu kutupluluğuna göre sınıflandırmak ana bir görev iken, duyguları her alt konulariçin ayrı ayrı analiz etmek, çoğu uygulama için daha faydalı olabilir.Biz bu çalışmada, bir kısa metin koleksiyonu üzerinde kişi ve başlıklar hakkındaki alt konuları çıkararak, düşünce analizi problemi üzerinde inceleme yapmaktayız. Resmi olmayan kısa mesajlar içeren Turkçe tweetler üzerinde yoğunlaşmaktayız. Düşünce analizi üzerinde literatürde yer alan kelime sözlüğü ve etiketli veriler gibi kaynakların çoğu ingilizce içindir. Bizim yaklaşımımızın, böyle zengin kaynakların olmadığı diğer diller için duygu analizini geliştirmeye yardımcı olması mümkündür. Birtakım önişleme adımlarından sonra, veridenürün(ler) hakkındaki alt konuları çıkarıp, bu konulara dayanarak tweetleri gruplamaktayız. Elle işletilerek oluşturduğumuz Türkçe duygusal kelime listesine ek olarak, bir kelime seçme algoritması kullanıp, kelimelerin duygu güçlülüğü ilebirlikte bir otomatize oluşum yöntemi geliştirildi. Daha sonra, duygu tabanlı metin gösterim şekli olarak ifade edilen, kelimelerin duygu güçlülüğüne göre metnin yeni bir gösterim şekli oluşturuldu. Metinlerin öznitelik vektörü, bu yenigösterim şekline göre oluşturulmaktadr. Pozitif ve negatif duygu karışımını belirlemek için çok değişkenli skalada öznitelik vektörlerine dayanan sınıflandırıcılar oluşturmak ve bunların performansını Twitter API vasıtasıyla zamanla toplananTürkçe tweet verisinde test etmek için makine öğrenmesi yöntemlerini uyarlamaktayız.","Understanding opinions about entities or brands is instrumental in reputation management and decision making. With the advent of social media, more people are willing to publicly share their recommendations and opinions. As the type and amount of such venues increase, automated analysis of sentiment on textualresources has become an essential data mining task. Sentiment classication aims to identify the polarity of sentiment in text. The polarity is predicted on either a binary (positive, negative) or a multi-variant scale as the strength of sentimentexpressed. Text often contains a mix of positive and negative sentiments, hence it is often necessary to detect both simultaneously. While classifying text based on sentiment polarity is a major task, analyzing sentiments separately for each aspect can be more useful in many applications.In this thesis, we investigate the problem of mining opinions by extracting aspects of entities/topics on collection of short texts. We focus on Turkish tweets that contain informal short messages. Most of the available resources such as lexicons and labeled corpus in the literature of opinion mining are for the English language. Our approach would help enhance the sentiment analyses to other languages where such rich sources do not exist. After a set of preprocessing steps, we extract the aspects of the product(s) from the data and group the tweets based on the extracted aspects. In addition to our manually constructed Turkish opinion word list, an automated generation of the words with their sentiment strengths is proposed using a word selection algorithm. Then, we propose a new representation of the text according to sentiment strength of the words, which we refer to as sentiment based text representation. The feature vectors of the text are constructed according to this new representation. We adapt machine learning methods to generate classiers based on the multi-variant scale feature vectors to detect mixture of positive and negative sentiments and to test their performance on Turkish tweets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek çıktılı dizileme (YÇD) yöntemleri, düşük maliyeti ve yüksek çıktı vermesiyle, daha şimdiden genom araştırmaları alanında temelden bir devrim gerçekleştirdi. Ancak, elde edilen verinin büyük olması, çeşitli hesaplama tabanlı sorunları da beraberinde getirdi. Örneğin, Illumina HiSeq2000 modelinde, her bir çalışma sonrası, 7-8 milyardan fazla küçük DNA bölütü ve 600 Gb dan fazla baz çifti 10 gün içinde elde edilebilmektedşr. Birçok uygulama için, YÇD verilerinin çözümlenmesi küçük DNA bölütlerinin eşlenmesiyle başlar. Örneğin, küçük DNA parçalarının kaynak DNA'daki yerlerinin tespit edilmesi gibi.İki dizinin arasındaki benzerlik, en uygun genel hizalamalarının Needleman-Wunsch algoritması yarımıyla hesaplanmasıyla bulunur. Needleman-Wunsch algoritması yüksek duyarlılığı sebebiyle, karma tablo tabanlı küçük DNA bölütlerinin eşlenmesi algoritmalarında kullanılır. Ancak bu algoritmanın ikilenik karmaşıklıktaki yapısı, fazla zaman harcamasına ve analizlerde darboğaz oluşturmasına sebep olur. Bu engelin yanında DNA bölütlerinin küçüklüğü (yaklaşık 100 baz çifti) ve memeli genomlarının büyüklüğü (3.1 Giga baz çifti), her bir küçük DNA bölütü için yüzlerce ila onbinlerce arası hesaplama yapılmasını gerektirerek, durmu daha da kötü hale getirmektedir. Needleman-Wunsch algoritmasını kullanmadan çalışan ve yukarıdaki veriyi kullanan en hızlı uygulama 70 MİB gününde, az duyarlılıkta çalışmaktadır. Daha duyarlı olan yaklaşımlar ise daha da yavaş çalışmaktadır. Bu tezde, etkili bir paralel dizi karşılaştırma yapısı geliştirirek, bu uygulamanın başarımını ciddi seviyelerde arttırıldığını önerdik. Bu güdülenmeyle yola çıkarak, grafik işlem birimlerinin paralel mimarisinin kullanan gelişmiş bir yaklaşım ortaya koyduk.","The high throughput sequencing (HTS) methods have already started to fundamentally revolutionize the area of genome research through low-cost and high-throughput genome sequencing. However, the sheer size of data imposes various computational challenges. For example, in the Illumina HiSeq2000, each run produces over 7-8 billion short reads and over 600 Gb of base pairs of sequence data within less than 10 days. For most applications, analysis of HTS data starts with read mapping, i.e. finding the locations of these short sequence reads in a reference genome assembly.The similarities between two sequences can be determined by computing their optimal global alignments using a dynamic programming method called the Needleman-Wunsch algorithm. The Needleman-Wunsch algorithm is widely used in hash-based DNA read mapping algorithms because of its guaranteed sensitivity. However, the quadratic time complexity of this algorithm makes it highly time-consuming and the main bottleneck in analysis. In addition to this drawback, the short length of reads ( ~100 base pairs) and the large size of mammalian genomes (3.1 Gbp for human) worsens the situation by requiring several hundreds to tens of thousands of Needleman-Wunsch calculations per read. The fastest approach proposed so far avoids Needleman-Wunsch and maps the data described above in 70 CPU days with lower sensitivity. More sensitive mapping approaches are even slower. We propose that efficient parallel implementations of string comparison will dramatically improve the running time of this process. With this motivation, we propose to develop enhanced algorithms to exploit the parallel architecture of GPUs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez normal kağıt ve kalem kullanıyormuşcasına kavisli şekiller çizmeyi sağlayan bir yöntem öne sürmektedir. Sistem ?ozellikle akla gelen 3 boyutlu fikirleri zaman kaybetmeden dijital ortama aktarabilmek için kullanışlıdır. Kullanıcılar, basınca duyarlı grafik-tablet ve kamera yardımıyla sistemle etkileşim haline geçerler. Kullanıcıların tablet yüzeyine dokunuşları bir ?cizim düzlemine aktarılır ve bu düzlem 3 boyutlu sahnede herhangi bir yere yerleştirilebilir. Sistem tek gözle ilgili derinlik ipuçlarını ve kullanıcının kafa pozisyonunundan elde ettiği hareket paralaksını uygulayarak 3 boyutlu çizime derinlik anlamı katar. Sistemin kullanışlılığı üzerine çizim tecrübesi olmayan kullanıcılarla yaptığımız testler, bu sistemin geniş bir kitlenin 3 boyutlu çizimler yapabilmesi için uygun olduğunu göstermektedir. Ayrıca profesyonel bir kişinin sistemin daha anlamlı ve etkili özelliklerinden nasıl yararlanabileceğini göstermek için bir mimarın katılımıyla daha ileri seviyede bir inceleme de yaptık.","This thesis proposes a method that resembles a natural pen and paper interface to create curve based 3D sketches. The system is particularly useful for representing initial 3D design ideas without much effort. Users interact with the system by the help of a pressure sensitive pen tablet, and a camera. The input strokes of the users are projected onto a drawing plane, which serves as a paper that they can place anywhere in the 3D scene. The resulting 3D sketch is visualized emphasizing depth perception by implementing several monocular depth cues, including motion parallax performed by tracking user?s head position. Our evaluation involving several naive users suggest that the system is suitable for a broad range of users to easily express their ideas in 3D. We further analyze the system with the help of an architect to demonstrate the expressive capabilities of the system that a professional can benefit."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar grafiği teknolojisinin hızlı gelişimi ve 3B modellerin yaygın kullanımıyla birlikte 3B grafik nesnelerini kullanan hesaplamalı model ve yöntemler de gelişmeye devam etmektedir. Gerçek zamanlı bilgisayar grafiği uygulamalarında, hesaplama ve aktarım süresini geliştirmek için birçok 3B model modifikasyon yöntemi bulunmaktadır. 3B modifikasyon işlemlerinde, kullanıcıya modelin görsel olarak en az deforme olmuş halini sunmak gerekmektedir.Bu tez kapsamında, hareketli 3B modeller üzerinde görsel olarak algılanan farklılıkları hesaplamak için bir yöntem sunulmaktadır. Yöntem, insan görme sistemini, görsel algıyı taklit ederek modellemektedir. Ayrıca bu yöntem, modifikasyonların uygulanması sırasında yardımcı olması amacıyla modelin 3B duyarlılık haritasının oluşturulmasında da kullanılabilir. Bu çalışmada sunulan yaklaşım, İnsan Görme Sistemi'nin iki etmenini birleştirerek 3B geometrik gösterimin algılanan kalite ölçümünü sağlamaktadır. Birincisi, insan görme modelinin uzamsal işlenmesi, yüzeydeki bozulmaları öngörmeyi mümkün kılmaktadır. İkincisi ise, hareket hızının zamansal etkileridir. Her iki etmen için de psikofizik deney verileri kullanılmıştır. Ayrıca kişiye özel deneyler ile de sunulan yöntemin geçerliliği desteklenmiştir.","COmputational models and methods to handle 3D graphics objects continue to emerge with the wide-range use of 3D models and rapid development of computer graphics technology. Many 3D model modification methods exist to improvecomputation and transfer time of 3D models in real-time computer graphics applications. Providing user with the least visually-deformed model is essential for 3D modification tasks. In this thesis, we propose a method to estimate the visually perceived differences on animated 3D models. The model makes use of Human Visual System modelsto mimic visual perception. It can also be used to generate a 3D sensitivity map for a model to act as a guide during the application of modifications. Our approach gives a perceived quality measure using 3D geometric representation by incorporating two factors of Human Visual System (HVS) that contribute to perception of differences. First, spatial processing of human vision model enables us to predict deformations on the surface. Secondly, temporal effects of animation velocity are predicted. Psychophysical experiment data is used for both of these HVS models. We used subjective experiments to verify the validity of our proposed method."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yer gözlem uyduları tarafından dünyaya gönderilen büyük miktarda veri bulunmaktadır ve bu uydular günden güne dünyadaki alıcı istasyonlara yeni veriler göndermektedir. Bu nedenle, bu verilerin madenciliği toplanan multispektral görüntülerin etkin işlenmesi için daha önemli hale gelmektedir. Bu sorun için en popüler yaklaşımlar, bu görüntülerin coğrafi koordinatları gibi bazı üst verileri kullanmaktadır. Ancak bu yaklaşımlar, bu görüntülerin ne içerdiğini tespit etmek konusunda iyi bir çözüm sunamamaktadır. Bazı araştırmalar, bu alandaki çalışmaların odağını, üst veri tabanlı yaklaşımlardan toplanan görüntülerin içerik bilgisini kullanan yaklaşımlara çevirerek, bu alanda büyük bir adım teşkil etmektedir. Bu araştırmalar, genellikle görüntülerdeki bölge bilgisini kullanmaktadır.Bu tezde, bölge bilgisi ile bölgelerin birbirleri ile olan uzamsal ilişkilerini kullanan kapsamlı ve genişletilebilir yeni bir görüntü madenciliği sistemi önerilmektedir. Bu sistemde, sadece bölge içeriği değil, bu bölgelerin birbiri arasındaki ilişkileri de kullanılmaktadır. Bunun için ilk olarak, görüntülerdeki bölge bilgileri çıkartılır ve sonra bu bölgelerin sol, sağ, yukarı, aşağı, yakın, uzak ve uzaklık gibi ikili ilişkileri çıkartılır. Bu öznitelik çıkarma işlemi, görüntünün bölütlenmesinden bağımsız genel bir süreç olarak tanımlanmıştır. Bunlara ek olarak, görüntü madenciliği araştırmacıları tarafından sürekli yeni öznitelikler ve yeni yaklaşımlar geliştirilmekte olduğu için, sistemin genişletilebilir özellikte olması, sistem tasarımında büyük rol oynamıştır.Ayrıca bu tezde, yeni bir öznitelik vektörü yapısı önerilmektedir. Bu yapıda, herhangi bir öznitelik vektörü, alt-öznitelik vektörlerinden oluşabilmektedir. Önerilen bu öznitelik vektörü yapısında, her alt-öznitelik vektörü arama işleminde kullanılmak üzere seçilebilir ve diğer öznitelik vektörü yapılarının aynı alt-öznitelik vektörleri arasındaki karşılaştırmalarda kullanılan farklı uzaklık ölçütüne sahip olabilir. Böylelikle, bu sistem, kullanıcılara, bölgelerin çeşitli öznitelikleri ve bölgelerin birbirleri ile olan çeşitli ilişkilerinin özniteliklerinden hangilerini arama yapmak için kullanacaklarını seçme olanağı sağlamaktadır. Önerilen sistem, çok yüksek çözünürlüklü uydu görüntüleri üzerinde, bölge tabanlı erişim senaryoları kullanılarak gösterilmiştir.","There is a huge amount of data which is collected from the Earth observation satellites and they are continuously sending data to Earth receiving stations day by day. Therefore, mining of those data becomes more important for effective processing of collected multi-spectral images. The most popular approaches for this problem use the meta-data of the images such as geographical coordinates etc. However, these approaches do not offer a good solution for determining what those images contain. Some researches make a big step from the meta-data based approaches in this area by moving the focus of the study to content based approaches such as utilizing the region information of the sensed images.In this thesis, we propose a novel, generic and extendable image information mining system that uses spatial relationship constraints. In this system, we use not only the region content, but also relationships of those regions. First, we extract the region information of the images and then extract pairwise relationship information of those regions such as left, right, above, below, near, far and distance etc. This feature extraction process is defined as a generic process which is independent from how the region segmentation is obtained. In addition to these, since new features and new approaches are continuously being developed by the image information mining researchers, extendibility feature of our system plays a big role while we are designing our system.In this thesis, we also propose a novel feature vector structure in which a feature vector consists of several sub-feature vectors. In the proposed feature vector structure, each sub-feature vector can be exclusively selected to be used for search process and they can have different distance metrics to be used in comparisons between the same sub-feature vector of the other feature vector structures. Therefore, the system gives ability to users to choose which information about the region and its pairwise relationship with other regions to be used when they perform a search on the system. The proposed system is illustrated by using region based retrieval scenarios on very high spatial resolution satellite images."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gün geçtikte Internet büyümekte ve Web arama motoru kullanımı sürekli artmaktadır. Kullanıcıların internet tarayıcılarını başlatıkları ana sayfa, genellikle bir arama motorunun giriş sayfasıdır. Kullanıcıların çoğunluğu belirli bir siteye erişmek için, Internet tarayıcısının adres çubuğunu kullanmak yerine, arama motorunun ara yüzüne Web sayfasının ismini yazmayı tercih etmektedir. Arama motorlarının Web'e giriş noktasındaki bu önemli rolünü göz önüne alarak, kullanıcıların zaman içerisinde ortaya çıkan Web arama eğilimlerini anlamaya yönelik bir ihtiyaç olduğunu söyleyebiliriz. Arama motorları tarafından dönen sorgu sonuçlarının zamana göre değişiminin analizi, Web üzerinde gerçekleştirilen arama işleminin mevcut durumu ve gelecekteki yönelimleri ile ilgili önemli noktaları açığa çıkaracaktır.Tez çalışmamızda, 630000 gerçek sorgu seti için gerçek bir arama motoru tarafından 2007 ve 2010 yıllarında sağlanan iki ayrı zamana ait sorgu sonuçlarının büyük ölçekli analizini gerçekleştirdik. Yaptığımız analizler Web arama sonuçlarının gelişimi konusundaki bir kaç kritik soruya cevap aramaktadır. Çalışmamız, sorgu cevaplarının büyük ölçekli boylamsal analizi açısından, bu kritik sorulara ışık tutacaktır.","The internet is growing day-by-day and the usage of web search engines is continuously increasing. Main page of browsers started by internet users is typically the home page of a search engine. To navigate a certain web site, most of the people prefer to type web sites? name to search engine interface instead of using internet browsers? address bar. Considering this important role of search engines as the main entry point to the web, we need to understand Web searching trends that are emerging over time. We believe that temporal analysis of returned query results by search engines reveals important insights for the current situation and future directions of web searching.In this thesis, we provide a large-scale analysis of the evolution of query results obtained from a real search engine at two distant points in time, namely, in 2007 and 2010, for a set of 630000 real queries. Our analyses in this work attempt to find answers to several critical questions regarding the evolution of Web search results. We believe that this work, being a large-scale longitudinal analysis of query results, would shed some light on those questions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım mimarisi tasarımında yaygın pratiklerden biri yazılım mimarisini çeşitli paydaş ilgilerine yönelik tasarlayabilmek için mimari görünümlerini kullanmaktır. Mimari görünümleri genellikle bu görünümleri oluşturmayı, yorumlamayı ve analiz etmeyi sağlayan kuralları tanımlayan mimari bakış açılarını temel alarak geliştirilir. Şimdiye kadar çoğu mimari bakış açısının esasen paydaşlar arasındaki iletişimi desteklemek ya da en iyi ihtimalle detaylı tasarım için bir plan sağlamak amacıyla kullanıldığı görülmektedir.Bu tezde mimari bakış açılarını alana özgü dil olarak tanımlamak için bir yazılım dil mühendisliği yaklaşımı sunuyoruz. Bu, mimari bakış açılarının formalliğini iyileştirirken bir yandan da araçlar tarafından yorumlanıp analiz edilebilen çalıştırılabilir görünüm modellerine öncülük ediyor. Mimari bakış açılarını alana özgü dil olarak tanımlama çalışmamızı Görünümler ve Ötesi yaklaşımı için gösterdik. Yaklaşımımız çeşitli görünümleri modellemeyi destekleyen Eclipse eklentisi SAVE-Bench yazılım aracı olarak geliştirildi. Araç aynı zamanda görünüm modellerinden otomatik mimari dökümantasyonu üretmeyi de destekliyor.","A common practice in software architecture design is to apply so-called architectural views to design software architecture for the various stakeholder concerns. Architectural views are usually developed based on architectural viewpoints which define the conventions for constructing, interpreting and analyzing views. So far most architectural viewpoints seem to have been primarily used either to support the communication among stakeholders, or at the best to provide a blueprint for the detailed design.In this thesis, we provide a software language engineering approach to define viewpoints as domain specific languages. This enhances the formal precision of architectural viewpoints and leads to executable views that can be interpreted and analyzed by tools. We illustrate our approach for defining domain specific languages for the viewpoints of the Views and Beyond framework. The approach is implemented as an Eclipse plug-in, SAVE-Bench tool, which can be used to define different views based on the predefined software architecture viewpoints. The tool also supports automatic generation of architecture documentation from view models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"3-B bilgisayar grafikleri, gelişen teknolojik imkanların da etkisiyle, çok yüksek seviyelere ulaştı ve günümüzde gerçeğe oldukça yakın görüntüler, bilgisayar oyunları ve diğer kullanıcı etkileşimi içeren uygulamar, gerçek zamanlı olarak üretilebiliyor. Fakat bilgisayar grafikleri alanındaki araştırmaların limitlerine ulaştığını iddia edemeyiz. Foto gerçekçi görüntüleme halen gerçek zamanlı olarak başarılamamakta olup, görsel kaliteyi artırmak ve görüntüleme maliyetlerini azaltmak araştırma alanı olarak ilgi odağı olmayı sürdürmektedir.Son zamanlarda bilgisayar grafikleri alanındaki uğraşlar, görüntüleme kalitesini artırmak amacıyla görsel algı prensiplerini kullanmaya yöneldi. Bu, bilgisayar grafiklerinde, temel değerlendirme kriterinin insanların yargıları ve dolayısıyla algıları olmasının doğal bir sonucu. Bu tezde hedefimiz, görsel algı prensiplerinin bilgisayar grafikleri için kullanımını artırmaktır. Bu tezin literatüre katkısı iki alanda incelenebilir: Birincisi, 3-B sahnelerde görsel olarak önemli, dikkat-çeker, kısımları tespit etmeye yönelik sunulan modeller; ikincisi de, dikkat-çekerlik ölçütlerinin bilgisayar grafiklerinde kullanımına yapılan katkılar.İnsanlarda görsel dikkat mekanizmasının iki kısmı vardır. İlk kısım, uyaranlara bağlı, aşağıdan yukarıya görsel dikkat olup; ikinci kısım göreve bağlı, yukarıdan aşağıya görsel dikkat olarak adlandırılmaktadır. Bu iki kısım arasındaki en önemli fark izleyicinin rolüdür. Yukarıdan aşağıya görsel dikkatte, aşağıdan yukarıya dikkatten farklı olarak, izleyicinin niyeti ve görevi sahnenin nasıl algılandığını etkiler. çalışmalarımızda daha çok, içerisinde dikkat-çekerliği de barındıran aşağıdan yukarıya görsel dikkati araştırdık.İki türlü grafiksel içerik türü için dikkat-çekerlik ölçütleri tanımladık. İlk ölçüt, 3-B hareketli grafiksel modeller için geliştirilmiş olup, modelin her düğümüne bir dikkat-çekerlik değeri atamaktadır. İkinci model ise birden çok nesne barındıran bir animasyon sahnesinde, hareketlerinden dolayı görsel olarak önemli hale gelen nesneleri tespit etmeye yöneliktir. üçüncü bir model de ise, ikinci modelde önerilen modelin ilk modelde kullanılan grafiksel içeriklere nasıl uygulanacağı gösterilmiştir.Tezde, dikkat-çekerlik ölçütlerinin yanısıra, ölçütlerin muhtemel kullanım alanları ve ikili (stereo) görüntüleme için algıya bağlı bir optimizasyon yöntemi de sunulmuştur. Bu yöntem ikili görme prensiplerine dayanmakta olup, sahnenin dikkat-çekerlik bilgisinden yararlanmaktadır.Sunulan yöntemlerin her biri, deneyler vasıtasıyla değerlendirildi. Dikkat-çekerlik ölçütlerinin değerlendirmesinde göz takip cihazı kullanıldı ve dikkat-çeker olarak belirtilen kısımlara daha çok bakıldığı tespit edildi. İkili görüntüleme için önerilen yöntem de, ayrıntılı bir kullanıcı testi ile doğrulandı.Sonuç olarak, sunulan tez görsel sisteme dair prensiplerin, özellikle dikkat-çekerlik ile ilgili olanların, 3-B bilgisayar grafiklerinde kullanımını genişletmektedir.","3D computer graphics, with the increasing technological and computational opportunities, have advanced to very high levels that it is possible to generate very realistic computer-generated scenes in real-time for games and other interactive environments. However, we cannot claim that computer graphics research has reached to its limits. Rendering photo-realistic scenes still cannot be achieved in real-time; and improving visual quality and decreasing computational costs are still research areas of great interest.Recent efforts in computer graphics have been directed towards exploiting principles of human visual perception to increase visual quality of rendering. This is natural since in computer graphics, the main source of evaluation is the judgment of people, which is based on their perception. In this thesis, our aim is to extend the use of perceptual principles in computer graphics. Our contribution is two-fold: First, we present several models to determine the visually important, salient, regions in a 3D scene. Secondly, we contribute to use of definition of saliency metrics in computer graphics.Human visual attention is composed of two components, the first component is the stimuli-oriented, bottom-up, visual attention; and the second component is task-oriented, top-down visual attention. The main difference between these components is the role of the user. In the top-down component, viewer's intention and task affect perception of the visual scene as opposed to the bottom-up component. We mostly investigate the bottom-up component where saliency resides.We define saliency computation metrics for two types of graphical contents. Our first metric is applicable to 3D mesh models that are possibly animating, and it extracts saliency values for each vertex of the mesh models. The second metric we propose is applicable to animating objects and finds visually important objects due to their motion behaviours. In a third model, we present how to adapt the second metric for the animated 3D meshes.Along with the metrics of saliency, we also present possible application areas and a perceptual method to accelerate stereoscopic rendering, which is based on binocular vision principles and makes use of saliency information in a stereoscopic rendering scene.Each of the proposed models are evaluated with formal experiments. The proposed saliency metrics are evaluated via eye-tracker based experiments and the computationally salient regions are found to attract more attention in practice too. For the stereoscopic optimization part, we have performed a detailed experiment and verified our model of optimization.In conclusion, this thesis extends the use of human visual system principles in 3D computer graphics, especially in terms of saliency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Benzerlik birleşimi, veri madenciliğinin önemli işlemlerindendir ve çeşitli alanlardan birçok uygulamada kullanılmaktadır. Bir benzerlik birleşimi işleci bir ya da iki veri noktası kümesi alır ve veri uzayında birbirleri arasındaki uzaklık belirli bir eşik değeri, epsilon, arasında olan veri noktası ikililerini çıktı olarak verir. Baz alınan iç içe geçmiş döngü algoritması bütün veri nesneleri arası uzaklık hesabı yapar. İç içe geçmiş döngü algoritması için çok fazla sorgu zamanı tutan, büyük veri kümeleri dikkate alındığında, böyle bir operasyonu hızlandırmak daha da önemli olmaktadır. Günümüz grafik işlemcilerinin, genel amaçlı paralel hesaplama mimarisinin de (CUDA) katkısıyla, hesaplama kapasiteleri birçok araştırmaya ön ayak olmaktadır. Bu motivasyonla, iki tane Grafik İşleme Birimi (GİB) tabanlı benzerlik birleşimi algoritması önermekteyiz. İlk olarak, genel amaçlı GİB programlamanın avantajlarından faydalanmak için, GİB'in kendine özgü özelliklerine uygun olan, geliştirilmiş iç içe geçmiş döngü algoritması (GPU-INLJ) önermekteyiz. Ayrıca herhangi bir birleşim ikilisi kaybına yol açmadan, her bölümün birbirinden bağımsız olarak benzerlik birleşimini sağlamayı garanti eden, bölümleme tabanlı benzerlik birleşimi algoritması (KMEANS-JOIN) önerilmektedir. Deneylerimiz büyük performans kazancı ve algoritmamızın büyük veri kümelerine uygunluğunu göstermektedir.","The similarity join is an important operation in data mining and it is used in many applications from varying domains. A similarity join operator takes one or two sets of data points and outputs pairs of points whose distances in the data space is within a certain threshold value, epsilon. The baseline nested loop approach computes the distances between all pairs of objects. When considering large set of objects which yield too long query time for nested loop paradigm, accelerating such operator becomes more important. The computing capability of recent GPUs with the help of a general purpose parallel computing architecture (CUDA) has attracted many researches. With this motivation, we propose two similarity join algorithms for Graphics Processing Unit (GPU). To exploit the advantages of general purpose GPU computing, we first propose an improved nested loop join algorithm (GPU-INLJ) for the specific environment of GPU. Also we present a partitioning-based join algorithm (KMEANS-JOIN) that guarantees each partition can be joined independently without missing any join pair. Our experiments demonstrate massive performance gains and the suitability of our algorithms for large datasets."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, Web tabanlı iletis¸im metotlarının farklı ¨ozelliklerini inceleyip, de?gis¸ikiletis¸im metotlarının ortak karakteristikleri oldu?gunu ¨one s¨urd¨uk. Bu tezimizikanıtlayabilmek ic¸in bu ortak ¨ozelliklerden iki tanesinin ¨uzerinde yo?gunlas¸acak vebu ¨ozellikleri derinlemesine inceleyece?giz. Bu ¨ozellikler: B¨ut¨un Web tabanlı iletis¸immetotları yazarlarına,alıcılarına, veya mesajların kendilerine atfedilebilecek ¨ozelliklertas¸ırlar. Ve b¨ut¨un Web tabanlı iletis¸im metotları benzer da?gılımsal ¨ozellikler g¨osterirler.Bu iki hipotezi kanıtlayabilmek amacıyla ¨uc¸ farklı, pratik, gerc¸ek yas¸amla ilgiliaras¸tırma problemi ¨uzerinde durduk ve bu iki hipotezi kullanarak sunulan aras¸tırmaproblemlerini c¸ ¨ozmeye c¸alı¨stık. Bu problemlerden ilkinde, halihazırda kullanılmaktaolan bir sorgu motoru ic¸in sorgu ¨ozelliklerine dayanan bir otomatik ¨o?grenme yaklas¸ımı¨one s¨urd¨uk. Bu c¸alıs¸mada, kullanıcı sorgularından c¸es¸itli ¨ozellikler c¸ıkartarak bu¨ozellikleri otomatik ¨o?grenilmis¸ bir model olus¸turmak ic¸in kullandık. Bu modele g¨ore her sorguya bir kalite metri?gi atayarak, arama motoru ¨on belle?gine kabulve atılma kararlarını bu metrik sayesinde yaptık. ?Ikinci problemde, kullanıcı vemesaj ¨ozelliklerini tahmin etmek amacı ile bir chat sunucusunun verilerini inceledik.Sonuc¸larımız birc¸ok kullanıcı ve mesaj bazlı ozelli?gin tahmin edilebilirli?gine ıs¸ıktuttu. Uc¸ ¨unc¨u c¸alıs¸mamızda, terim bazlı ters indekslerin ha ¨ fıza bazlı ve paralelolarak olus¸turulmalarını inceledik. Bu aras¸tırmada ise, is¸lemciler arası toplam iletis¸imzamanını minimize edebilmek amacı ile, Web sayfalarındaki terimlerin da?gılımsal¨ozelliklerini temel alan bir guruplama metodu ¨onerdik. Bu ¨ozellikleri kullanarak,is¸lemciler arası iletis¸im zamanını, is¸lemci g¨orev da?gılımını da dikkate alacak s¸ekildenasıl azaltabilece?gimiz y¨on¨unde aras¸tırmalar yaptık.Anahtar sozcukler ¨ : Arama Motoru, Sonuc¸ ¨onbellegi, ¨on bellek, Chat madencili?gi, verimadencili?gi, indeks tersleme, ters dizin.","In this thesis, we analyze different aspects of Web-based textual communicationsand argue that all such communications share some common properties. In order toprovide practical evidence for the validity of this argument, we focus on two common properties by examining these properties on various types of Web-based textualcommunications data. These properties are: All Web-based communications containfeatures attributable to their author and reciever; and all Web-based communicationsexhibit similar heavy tailed distributional properties.In order to provide practical proof for the validity of our claims, we provide threepractical, real life research problems and exploit the proposed common properties ofWeb-based textual communications to ?nd practical solutions to these problems. Inthis work, we ?rst provide a feature-based result caching framework for real life searchengines. To this end, we mined attributes from user queries in order to classify queriesand estimate a quality metric for giving admission and eviction decisions for the queryresult cache. Second, we analyzed messages of an online chat server in order to predictuser and mesage attributes. Our results show that several user- and message-basedattributes can be predicted with signi?cant occuracy using both chat message- andwriting-style based features of the chat users. Third, we provide a parallel frameworkfor in-memory construction of term partitioned inverted indexes. In this work, in orderto minimize the total communication time between processors, we provide a bucketingscheme that is based on term-based distributional properties of Web page contents.Keywords: Web search engine, result caching, cache, chat mining, data mining, indexinversion, inverted index, posting list."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sonlu Elemanlar Yöntemi (SEY), analitik yöntemlerle çözülemeyen matematiksel fizik ve mühendisliğin karmaşık problemlerinin uygun çözümlerini bulmak için yaygın bir şekilde kullanılan sayısal bir tekniktir. Simülasyonun hızlı olmasını gerektiren uygulamaların çoğunda, Doğrusal Sonlu Elemanlar Yöntemi tercih edilmektedir. Doğrusal Sonlu Elemanlar Yöntemi, küçük deformasyonlarla doğruya yakın çalışır. Ancak bu yöntem büyük deformasyonlarla kullanıldığı zaman doğrudan uzak sonuç verir. Bu nedenle doğrusal olmayan SEY ameliyat simülatörleri gibi önemli uygulamalar için uygun bir yöntemdir. Bu tezde, doğrusal olmayan 3 boyutlu esneklik teorisine, yeni bir formülasyon ve sonlu eleman yöntemi önermekteyiz. Doğrusal olmayan direngenlik matrisleri, Green- Lagrange gerinimleri (büyük deformasyon) kullanarak inşa edilirler. Green-Lagrange gerinimleri, küçük gerinim teorisinde gözardı edilen doğrusal olmayan terimlerin küçük gerinimlere ilave edilmesiyle türetilmektedirler. önerilen çözüm, doğrusal SEY hakkında bilgisi olanlar için daha anlaşılır bir doğrusal olmayan SEY'dir çünkü önerilen yöntem doğrudan küçük gerinimlerden türetilmektedir. Biz hem doğrusal hem de doğrusal olmayan SEY'i, aynı dört yüzlü elemanlarla aynı malzemeyi kullanarak doğrusal olmayan SEY'in doğrusal SEY'e göre avantajlarını incelemek için uyguladık. Deneylerimizde, doğrusal olmayan SEY, doğrusal SEY ile kıyaslandığında, rotasyonlar ve yüksek dış kuvvetler gerektiğinde daha hassas sonuçlar vermektedir. Diğer taraftan önerilen doğrusal olmayan çözüm, direngenlik matrislerinin hesaplanmasını ve tüm olarak sistemin çözümünü hızlandırmaktadır.","Finite Element Method (FEM) is a widely used numerical technique for finding approximate solutions to the complex problems of engineering and mathematical physics that cannot be solved with analytical methods. In most of the applications that require simulation to be fast, linear FEM is widely used. Linear FEM works with a high degree of accuracy with small deformations. However, linear FEM fails in accuracy when large deformations are used. Therefore, nonlinear FEM is the suitable method for crucial applications like surgical simulators. In this thesis, we propose a new formulation and finite element solution to the nonlinear 3D elasticity theory. Nonlinear stiffness matrices are constructed by using the Green-Lagrange strains (large deformation), which are derived directly from the infinitesimal strains (small deformation) by adding the nonlinear terms that are discarded in infinitesimal strain theory. The proposed solution is a more comprehensible nonlinear FEM for those who have knowledge about linear FEM since the proposed method directly derived from the infinitesimal strains. We implemented both linear and nonlinear FEM by using same material properties with the same tetrahedral elements to examine the advantages of nonlinear FEM over the linear FEM. In our experiments, it is shown that nonlinear FEM gives more accurate results when compared to linear FEM when rotations and high external forces are involved. Moreover, the proposed nonlinear solution achieved significant speed-ups for the calculation of stiffness matrices and for the solution of a system as a whole."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İç mekan tanıma, insan yapımı yapıların yüksek sınıf içi varyasyonlar ve sınıfarası benzerlikler göstermesi sebebiyle klasik mekan tanıma alanının zorlu birproblemidir. Resmin bütüncül temsillerini çıkarmak gibi en ileri mekan tanımateknikleri iç mekanlarda düşük performans göstermektedirler. Nesnelerin belirlenmesive ardından onların mekanlarla ilişkilendirilmesi gibi ara kademeler kullanandiğer yöntemlerin de oldukça karmaşık bir ortamda nesnelerin başarıyla lokalizeedilmesi ve tanınması handikapları vardır.Kodkitabı olarak da bilinen görsel kelimeler kümesi tekniğinden faydalanaraken yakın komşu yöntemine dayalı bir metrik fonksiyonu ile bu zorluklarınüstesinden gelebilen bir sınıflandırma yöntemi öneriyoruz. Kodkitabı oluşumuöznitelik uzayının mozaikleştirilmesi olarak ele alınırsa, verilen bir resim için,öznitelik vektörlerinin Voronoi hücrelerinin ortalarına olan öğrenilmiş ağırlıklıuzaklıklarının resmin kategorisi için güçlü bir gösterge olduğunu gözlemledik.Yöntemimiz tek bir tanımlayıcı ile bir iç mekan testinde en gelişmiş yaklaşımlarıgeçmekte ve genel bir mekan veri kümesinde rekabetçi sonuçlar üretmektedir.Bu çalışmada her ne kadar temel sorunumuz iç mekan kategorizasyonu olsa da,önerilen metrik fonksiyonunu otomatik etiketleme problemine de bir temel uygulamaoluşturmak için kullanıyoruz. Gittikçe artan sayısal medya ile, otomatik olarak resimlereanlamsal etiketler çıkarma problemine son on yılda araştırmacılar büyük ilgigöstermişlerdir. Bu tarz içerikleri manüel olarak etiketlemek gibi geleneksel yaklaşımlarçok bıktırıcı ve zaman harcayıcı olarak değerlendirilmektedir. Bu neden resimlerianlamsal olarak başarıyla açıklayan anahtar kelimelerle otomatik olarak etiketleme,çözülmeyi bekleyen önemli bir sorundur.","Indoor scene recognition is a challenging problem in the classical scene recognitiondomain due to the severe intra-class variations and inter-class similarities ofman-made indoor structures. State-of-the-art scene recognition techniques suchas capturing holistic representations of an image demonstrate low performance onindoor scenes. Other methods that introduce intermediate steps such as identifyingobjects and associating them with scenes have the handicap of successfullylocalizing and recognizing the objects in a highly cluttered and sophisticatedenvironment.We propose a classication method that can handle such diculties of theproblem domain by employing a metric function based on the nearest-neighborclassication procedure using the bag-of-visual words scheme, the so-called codebooks.Considering the codebook construction as a Voronoi tessellation of thefeature space, we have observed that, given an image, a learned weighted distanceof the extracted feature vectors to the center of the Voronoi cells gives a strongindication of the image's category. Our method outperforms state-of-the-art approacheson an indoor scene recognition benchmark and achieves competitiveresults on a general scene dataset, using a single type of descriptor.In this study although our primary focus is indoor scene categorization, we alsoemploy the proposed metric function to create a baseline implementation for theauto-annotation problem. With the growing amount of digital media, the problemof auto-annotating images with semantic labels has received signicant interestfrom researches in the last decade. Traditional approaches where such content ismanually tagged has been found to be too tedious and a time-consuming process.Hence, succesfully labeling images with keywords describing the semantics is acrucial task yet to be accomplished."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yazılım geliştirme yaklaşımı, son yıllarda tek merkezli geliştirme yerine küresel olarak dağıtılmış geliştirmeye kaymaktadır. Küresel veya dağıtılmış olarak da adlandırılan bu tip geliştirmede, belli takımlar, coğrafi olarak dünyanın değişik yerlerine dağılmış olan değişik sitelerde çalışır. Bu takımlar yalnızca kodlama üzerine değil, aynı zamanda yazılım pazarlaması, bakım ve test gibi yazılımın diğer alanlarında da çalışmaktadır. Tüm geliştirme, değişik ülkelere dağılmış merkezleri olan bir şirket tarafından idare edilebileceği gibi iş yükü sözleşmeli alt yükleniciler ile de paylaşılabilir. Ticari kaygı gütmeyen açık kaynak geliştirme grupları ve organizasyonlar da Küresel Yazılım Geliştirme (KYG) takımları olarak sınıflandırılabilir.Dağıtılmış KYG takımlarının iletişim, koordinasyon ve kontrol gereksinimlerini karşılamak için uygun bir KYG mimarisi geliştirmek önemlidir. Bununla birlikte, KYG literatür taraması gösteriyor ki bu alandaki araştırmalar daha çok kültürler arası iletişim ve koordinasyon problemleri gibi sosyal mevzulara yoğunlaşmıştır. Yazılım mimarisinin geniş çaplı ve karmaşık yazılımların geliştirilmesinde önemli bir rol oynadığı bilinen bir gerçektir. Fakat hem KYG hem de yazılım mimarisi toplulukları, KYG mimarisinin tasarımı konusunu açık olarak konu etmemişlerdir.Bu anlamda, bu çalışma KYG mimarisinin tasarımını konu ederek bu alanda katkı sağlamayı amaçlamaktadır. Mimariyi paydaşların endişelerine göre betimlemek için değişik mimari bakışları modellemek ve dökümlemek genel bir uygulamadır. Birden çok bakışın kullanılması, farklı paydaşlar için yazılım mimarisinin modellenmesi, anlaşılması, iletişimde kullanılması ve analiz edilmesine yardım etmektedir. Mimari bakışlar, bakışların nasıl inşa edilmesi ve kullanılması gerektiğini tanımlayan bakış açılarına uyarlar. Bu çalışmada, KYG mimarisinin modellenmesi için tanımlanmış olan yedi adet mimari bakış açısı sunuyoruz. Mimari bakış açılarının tanımlanması için, öncelikle genel bir KYG meta-modeli tanımladık. Meta-model ayrıntılı bir literatür araştırması sonucu oluşturuldu. Meta-model, mimari bakış açılarının soyut dilbilgisini oluşturan altı adet parçadan oluşuyor. Meta-modelin tanımlanmasından sonra, meta-model için yazılı ve görsel somut dilbilgisi de, bakış açısı tanımlamasını tamamlamak için önerildi.Bakış açılarına dayanan mimari bakışların elde edilmesinde mimarı desteklemek amacıyla bir soru çatısı sunduk. Soru çatısı, bir KYG projesinin anahtar elementleriyle ilişkili altı adet soru kümesi içeriyor. Bu soru çatısındaki sorulara verilen yanıtlara dayanarak, çatıda önceden tanımlanmış olan tasarım hareketlerinin uygulanmasıyla KYG mimarisi elde edilebilir. Bu soru çatısını Global Architect (Küresel Mimar) adını verdiğimiz bir araç geliştirerek uyguladık. Küresel Mimar sorulara verilen yanıtları alarak yazılı biçimdeki mimari tanımlamayı oluşturuyor. Daha sonra, bu yazılı tanımlama kullanılarak KYG projesinin görsel biçimde olan uygulama mimarisi çıkartılıyor.","Current trends in software engineering show that large software projects have to operate with teams that are working in different locations. The reason behind this globalization of software development stems from clear business goals such as reducing cost of development, solving local IT skills shortage, and supporting outsourcing and offshoring. There is ample reason that these factors will be even stronger in the future, and as such we will face a further globalization of software development. To cope with these problems, the concept of Global Software Development (GSD) is introduced. GSD is a relatively new concept in software development that can be considered as the coordinated activity of software development that is not localized and central but geographically distributed.Designing a proper architecture of GSD is important to meet the requirements for the communication, coordination and control of distributed GSD teams. However, an analysis of the literature on GSD shows that research in this area has been generally focused on social issues focusing on some concerns such as intercultural communication problems and coordination. It is generally accepted that software architecture design plays a fundamental role in coping with the inherent difficulties of the development of large-scale and complex software. Unfortunately, in both GSD and software architecture design communities, the architecture design of GSD systems has not been explicitly addressed.This study aims to provide a contribution in this context by explicitly focusing on the architecture design of GSD. A common practice is to model and document different architectural views for describing the architecture according to the stakeholders? concerns. Having multiple views helps to separate the concerns and as such support the modeling, understanding, communication and analysis of the software architecture for different stakeholders. Architectural views conform to viewpoints that represent the conventions for constructing and using a view. In this study, we propose seven architectural viewpoints which have been specifically defined for modeling GSD architecture. To define architecture viewpoints, we first describe a general GSD meta-model. The meta-model has been derived after a thorough analysis of the related GSD literature. The meta-model consists of six different parts which form the abstract syntax of the architectural viewpoints. After the meta-model derivation, we also suggest textual and visual concrete syntaxes for the meta-model in order to complete viewpoint definition.Supporting the architect in deriving architectural views based on the corresponding architectural viewpoints, we present a question framework. The question framework consists of six sets of questions related to the key concerns of a GSD project. Based on the answers given to the questions in this framework, the GSD application architecture can be derived by applying predefined design actions in the question framework. We have developed the tool called Global Architect which implements the question framework. Global Architect takes as input the answers to the provided questions and subsequently generates the textual architecture description of the required viewpoint. On its turn, the textual description is used to generate the visual presentation of the application architecture for the GSD project."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternetin yüksek popülaritesi, onu saldırganlar için çekici bir ortam haline getirmiştir. Günümüzde suçlular sık sık İnternet kullanıcılarını hedef alarak yasadışı kazanç sağlamaktadırlar. Birçok yaygın İnternet saldırısı bir istismar bağlantısına tıklamak ya da bir güvenlik uyarısı diyaloğunu azletmek gibi birtakım kullanıcı etkileşimlerine ihtiyaç duymaktadır. Bundan dolayı, eldeki güvenlik problemi sadece teknik olmayıp güçlü bir insani yöne de sahiptir. Güvenlik topluluğu yaygın internet saldırılarını azaltmak için birçok teknik çözüm önermiş olmasına rağmen, kullanıcıların bu saldırılarla karşılaştıklarındaki davranışları büyük ölçüde keşfedilmemiş bir alan olmayı sürdürmektedir.Bu çalışmada, kullanıcıların yansıyan siteler arası betik yazma, oturum sabitleme, scareware ve dosya paylaşım dolandırıcılıkları gibi yaygın, somut saldırı senaryolarıyla yüzleştiklerindeki davranışlarını test etmek için kurduğumuz bir çevrimiçi deney platformunu tanımlıyoruz. Çeşitli geçmişlere sahip 160'tan fazla İnternet kullanıcısıyla deneyler yürüttük. Bulgularımız, teknik olmayan kullanıcıların görece basit ve iyi bilinen tehditleri (örneğin e-posta dolandırıcılıkları) önlemede bilgili kullanıcılarla kıyaslanabilir performanslar sergileyebildiklerini göstermektedir. Bunu yaparken tehlikeyi bilinçli bir şekilde idrak etmeyip, yalnızca sezgilerine ve geçmiş deneyimlerine (yani bir eğitim etkisi var) güvenmektedirler. Fakat, daha gelişmiş saldırılarda, bu kullanıcılar sıklıkla yapıların (mesela URL'ler) ?boyut? ve ?uzunluk?ları gibi yanıltıcı ipuçlarına dayanmakta, ve kendilerini korumakta başarısız olmaktadırlar. Bulgularımız ayrıca, dosya paylaşım sitelerinde yaygın olan hile afişlerinin ve kısaltılmış URL'lerin, teknik olmayan kullanıcıları kandırmada yüksek oranda başarılı olduklarını, bu nedenle ciddi bir güuvenlik tehlikesi yarattıklarını göstermektedir.","The Internet?s immense popularity has made it an attractive medium for attackers. Today, criminals often make illegal profits by targeting Internet users. Most common Internet attacks require some form of user interaction such as clicking on an exploit link, or dismissing a security warning dialogue. Hence, the security problem at hand is not only a technical one, but it also has a strong human aspect. Although the security community has proposed many technical solutions to mitigate common Internet attacks, the behavior of users when they face these attacks remains a largely unexplored area.In this work, we describe an online experiment platform we built for testing the behavior of users when they are confronted with common, concrete attack scenarios such as reflected cross-site scripting, session fixation, scareware and file sharing scams. We conducted experiments with more than 160 Internet users with diverse backgrounds. Our findings show that non-technical users can exhibit comparable performance to knowledgeable users at averting relatively simple and well-known threats (e.g., email scams). While doing so, they do not consciously perceive the risk, but solely depend on their intuition and past experience (i.e., there is a training effect). However, in more sophisticated attacks, these non-technical users often rely on misleading cues such as the ?size? and ?length? of artifacts (e.g., URLs), and fail to protect themselves. Our findings also show that trick banners that are common in file sharing websites and shortened URLs have high success rates of deceiving non-technical users, thus posing a severe security risk."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"y=Ax biçimindeki seyrek matris-vektör çarpımı (SMxV) bilimsel uygulamalardayinelemeli doğrusal denklem çözümleyicilerinde kullanılan bir çekirdekoperasyondur. Bu çözümleyicilerde, yinelemeler vasıtasıyla yakınsayıncaya kadaraynı seyrek matris ile SMxV operasyonu tekrarlanarak uygulanır. Paralel ortamdaparalel SMxV operasyonu matrise ve onun ayrışımına göre işlemcilerarasında haberleşmeye ihtiyaç duyar. Bu haberleşme akıllı ayrışımlar ileazaltılabilinir. Fakat, biz veri replikasyonu ve fazla hesaplama ile bu haberleşmeyidaha da fazla azaltabiliriz. Satır-paralel SMxV hesaplamada bu haberleşme x vektörelemanlarının transferi yüzünden oluşur. Bir sonraki yinelemenin girdivektörü x, bazı doğrusal operasyonlar vasıtasıyla yürürlükteki yinelemenin çiktivektörü y ile hesaplanır. Bundan dolayı, bir işlemci başka bir işlemciden bir x vektörelemanı almak yerine fazla bir y vektör elemanını, ki bu y vektör elemanıbir sonraki yinelemenin x vektör elemanına öncülük eder, hesaplayabilir. Böylece,fazla y vektör elemanı hesaplamak haberleşmenin azalmasına yol açabilir.Bu tezde, biz yukarıda bahsedilen yinelemeli denklem çözümlayiciler içinhesaplama ve haberleşme desenini doğru yakalayan yönlü çizge tabanlı modeltasarladık. Bundan başka, biz fazla y vektör elemanı hesaplaması sebebiylehaberleşme azalışını yönlü çizge modeli uzerinde bir kombinatoriyal problemolarak formülledik. Biz bu kombinatoriyal problemi çözmek için iki tane buluşsalyöntem önerdik. Deneysel sonuçlar göstermektedir ki fazla hesaplama yaparakhaberleşme azaltma stratejisi gelecek vaat etmektedir.","Sparse matrix vector multiplication (SpMxV) of the form y = Ax is a kerneloperation in iterative linear solvers used in scientific applications. In thesesolvers, the SpMxV operation is performed repeatedly with the same sparse matrixthrough iterations until convergence. Depending on the matrix and its decomposition,parallel SpMxV operation necessitates communication among processorsin the parallel environment. The communication can be reduced by intelligentdecomposition. However, we can further decrease the communication throughdata replication and redundant computation. The communication occurs due tothe transfer of x-vector entries in row-parallel SpMxV computation. The inputvector x of the next iteration is computed from the output vector of the currentiteration through linear vector operations. Hence, a processor may compute ay-vector entry redundantly, which leads to a x-vector entry in the following iteration,instead of receiving that x-vector entry from another processor. Thus,redundant computation of that y-vector entry may lead to reduction in communication.In this thesis, we devise a directed-graph-based model that correctly capturesthe computation and communication pattern for above-mentioned iterativesolvers. Moreover, we formulate the communication minimization by utilizingredundant computation of y-vector entries as a combinatorial problem on thisdirected graph model. We propose two heuristics to solve this combinatorialproblem. Experimental results indicate that the communication reducing strategyby redundantly computing is promising."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Haber portalları vb. sistemlerde haberlerin otomatik olarak sınıflandırılmasıgerekmektedir. Ancak birok haberin kategori bilgisi bulunmamakta, yanlışatanmş olmakta ya da kapsamlı olmaktadır. Bu durum otomatik haber kategorizasyonunugerekli kılmaktadır. Otomatik yazı sınıflandırma (OYS) parametreayarlama, terim ağırlıklandırma, kelime kökü bulma, ortak kelimeleri yoketme, ve özellik se¸cme gibi kararları içeren çok yönl¨u bir işlemdir. OYS'deyüksek doğruluk sonuçları sağlayan bir kategorizasyon ayarlaması yapmak T¨urkçehaber portalları için önemlidir. Bilkent Haber Portalı kullanılarak farklı karakterleresahip iki Türkçe veri kümesi yaratılmıştır. Deneyler dört kategorizasyonyöntemiyle yapılmıştır: C4.5, KNN, Naive Bayes, ve SVM (polynomial ve rbfçekirdekleri kullanılarak). Sonuçlar Türkçe haber portalları için bir yazı kategorizasyonuşablonu önermektedir. Tavsiye edilen yazı kategorizasyonu şablonugöz önünde bulundurarak etkililiği arttırmak için topluluk öğrenme yöntemlerikullanılmaktadır. Ancak bu yöntemler çok fazla hesaplama iş yükü gerektirdiğinden topluluk budama stratejileri geliştirilmiştir. Veri ayırma topluluklarıoluşturulmuş ve sıralamaya dayalı topluluk budama çeşitli otomatik öğrenmekategorizasyon algoritmalarıyla uygulanmıştır. Amaç şu soruları yanıtlamaktır:(1) Yazı kategorizasyon alanında veri ayırma kullanılarak ne kadar veriyi budayabiliriz?(2) Hangi veri ayırma ve kategorizasyon yöntemleri veri budamaiçin daha uygundur? (3) ? Ingilizce ve Türkçe dillerde topluluk budama ne kadarfark etmektedir? (4) Yazı kategorizasyonu alanında topluluk budama ile etkililiğiarttırmak mümkün müdür? Deneyler iki veri kmesinde yapılmıştır: Reuters-21578 ve BilCat-TRT. 90% oranında topluluk üyesi hassasiyette hemen hemenhiç eksilme olmadan elenmektedir.","In news portals, text category information is needed for news presentation. However,for many news stories the category information is unavailable, incorrectlyassigned or too generic. This makes the text categorization a necessary toolfor news portals. Automated text categorization (ATC) is a multifaceted difficultprocess that involves decisions regarding tuning of several parameters, termweighting, word stemming, word stopping, and feature selection. It is importantto find a categorization setup that will provide highly accurate results in ATC forTurkish news portals. Two Turkish test collections with different characteristicsare created using Bilkent News Portal. Experiments are conducted with four classificationmethods: C4.5, KNN, Naive Bayes, and SVM (using polynomial andrbf kernels). Results recommend a text categorization template for Turkish newsportals. Regarding recommended text categorization template, ensemble learningmethods are applied to increase effectiveness. Since they require many computationalworkload, ensemble pruning strategies are developed. Data partitioningensembles are constructed and ranked-based ensemble pruning is applied withseveral machine learning categorization algorithms. The aim is to answer the followingquestions: (1) How much data can we prune using data partitioning on thetext categorization domain? (2) Which partitioning and categorization methodsare more suitable for ensemble pruning? (3) How do English and Turkish differin ensemble pruning? (4) Can we increase effectiveness with ensemble pruningin the text categorization? Experiments are conducted on two text collections:Reuters-21578 and BilCat-TRT. 90% of ensemble members can be pruned withalmost no decreasing in accuracy."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Arama motorları sorgu sonuçlarını sayfalara ayrılmış uzun web doküman listesihalinde sunmaktadır. Bilgi erişim sonuçlarının istenen bilgiye daha kolayulaşmayı sağlamak amacıyla tekrar işlenmesi önemli bir araştırma konusudur.Bir tekrar işleme yöntemi de arama sonuçlarını konularına göre gruplamak ve bugrupları konularını yansıtacak şekilde etiketlemektir. Bu tezde, arama motorlarıtarafından oluşturulan uzun doküman listesini anlamlı bir şekilde gruplanmış veetiketlenmiş kümelere ayıran yeni bir arama sonucu kümeleme yaklaşımı sunuyoruz.Metodumuz kapsama katsayısına dayalı kümeleme ve sıralı k-ortalamalaralgoritmalarını kullanarak kümeleme kalitesine önem vermektedir. Diğer bir yandan,kümelerin etiketlemesi, anlamsız ya da kafa karıştıran etiketlerin kullanıcılarıyanlış kümelere yönlendirerek zaman kaybettirmesi nedeniyle önemlidir. Bunlaraek olarak, bir kümenin etiketi, kümede bulunan dokümanların içeriklerini doğrubir biçimde yansıtmalıdır. Kümeleri etiketleme görevini etkin bir şekilde yerinegetirebilmek için, terim ağırlıklandırmaya dayalı yeni bir küme etiketlemeyöntemi sunulmaktadır. Ayrıca küme etiketlemenin başarısını değerlendirmekamacıyla hassasiyet ve kesinlik ölçütlerini kullanan yeni bir etiketleme metriğisunulmaktadır. Metodumuzun Sonek Ağacıyla Kümeleme ve Lingo gibi önde gelenarama sonucu kümeleme algoritmalarına göreceli performansını saptayabilmekamacıyla karşılaştırmalı bir değerlendirme yöntemi uygulanmaktadır. Diğertaraftan, herkesin kullanımına açık olan Ambient ve ODP-239 veri setlerindetestler gerçekleştirilmiştir. Test sonuçları önerilen metodun hem kümeleme hemde etiketleme görevini başarıyla yerine getirdiğini göstermektedir.","Search engines present query results as a long ordered list of web snippets dividedinto several pages. Post-processing of information retrieval results for easier accessto the desired information is an important research problem. A post-processingtechnique is clustering search results by topics and labeling these groups to reflectthe topic of each cluster. In this thesis, we present a novel search result clusteringapproach to split the long list of documents returned by search engines intomeaningfully grouped and labeled clusters. Our method emphasizes clusteringquality by using cover coefficient and sequential k-means clustering algorithms.Cluster labeling is crucial because meaningless or confusing labels may misleadusers to check wrong clusters for the query and lose extra time. Additionally,labels should reflect the contents of documents within the cluster accurately. Tobe able to label clusters effectively, a new cluster labeling method based on termweighting is introduced. We also present a new metric that employs precision andrecall to assess the success of cluster labeling. We adopt a comparative evaluationstrategy to derive the relative performance of the proposed method with respectto the two prominent search result clustering methods: Suffix Tree Clusteringand Lingo. Moreover, we perform the experiments using the publicly availableAmbient and ODP-239 datasets. Experimental results show that the proposedmethod can successfully achieve both clustering and labeling tasks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek doğrusal denklem sistemlerinin ön hazırlık kullanılarak cözümü çizge bölümleme araçları kullanılarak etkili ve verimli bir biçimde koşut hesaplamasına uygun hale getirilebilir. Bu tez çalışmasında, çarpımsal schwarz ön hazırlayıcısının koşut hesaplanmasında kullanılmak üzere bir seyrek matrisin örtüşen blok köşegen biçimine yeniden düzenlenmesi problemi incelenmektedir. Ardışık köşegen blokları örtüşen blok köşegen matrislere örtüşen blok köşegen matrisler denir. Bu yeniden düzenleme probleminin çizge kuramı kullanılarak ifade edilebilmesi için Düğüm Ayıracı ile Çizge Bölümleme (DAÇB) probleminin kısıtlı bir çeşidi olan sıralı DAÇB (sDAÇB) problemi tanıtılmaktadır. sDAÇB probleminde amaç iki ardışık düğüm bölümünün sadece bir düğüm ayıracı ile bağlanabildiği sıralı bir düğüm bölümlemesi bulmaktır.Varolan çizge bölümleme araçları sDAÇB problemini çözememektedirler. Bu nedenle, bu tez çalışmasında, düğüm ayıraçlarını ve yeni bir düğüm sabitleme düzenini kullanan özyineli bir çizge bölümleme algoritması önerilmektedir. Bu algoritma ile sabit düğümleri destekleyen bir DAÇB aracı etkili ve verimli bir şekilde kullanılabilmektedir. Ayrıca, bir sDAÇB çözümünün uygulanabilirliği için yeterli ve gerekli koşul incelenerek önerilen yaklaşım kuramsal olarak doğrulanmıştır. Çeşitli matrisler üzerinde yapılan deneylerin sonuçları önerilen yaklaşımın geçerliliğini doğrulamaktadır.","Solving sparse system of linear equations using preconditioners can be efficiently parallelized using graph partitioning tools. In this thesis, we investigate the problem of permuting a sparse matrix into a block diagonal form with overlap which is to be used in the parallelization of the multiplicative schwarz preconditioner. A matrix is said to be in block diagonal form with overlap if the diagonal blocks may overlap. In order to formulate this permutation problem as a graph-theoretical problem, we introducea restricted version of the graph partitioning by vertex separator problem (GPVS), where the objective is to find a vertex partition whose parts are only connected by a vertex separator. The modified problem, we refer as ordered GPVS problem (oGPVS), is restricted such that the parts should exhibit an ordered form where the consecutive parts can only be connected by a separator.The existing graph partitioning tools are unable to solve the oGPVS problem. Thus, we present a recursive graph bipartitioning algorithm by vertex separators together with a novel vertex fixation scheme so that a GPVS tool supporting fixed vertices can effectively and efficiently be utilized. We also theoretically verified the correctness of the proposed approach devising a necessary and sufficient condition to the feasibility of a oGPVS solution. Experimental results on a wide range of matrices confirm the validity of the proposed approach."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde sonlu bir denetim birimi ve sonsuz uzayda tanımlı çoklu etkileşimli altsistemleri olan Markov sistemlerini ele aldık. Bu sistemler için geliştirilmiş olan düzey-bağımlı sonsuz sözde-doğum-ölüm süreci modelini, bu sistemlerin sıfırdan farklı bloklarının Kronecker çarpım toplamlarını kullanarak nasıl tanımlanabileceğini gösterdik. İki ya da daha fazla sonsuz uzayda tanımlı altsistemi olan rassal kimyasal devingen sistemler üzerinde gerçekleştirdiğimiz deneyler, matris çözümlemeli yöntemin pek çok durumda, benzetime oranla, daha hızlı ve doğruya yakın sonuç verdiğini gösterdik.","Markovian systems with multiple interacting subsystems under the influence of a control unit are considered. The state spaces of the subsystems are countably infinite, whereas that of the control unit is finite. A recent infinite level-dependent quasi-birth-and-death (LDQBD) model for such systems is extended by facilitating the automatic representation and generation of the nonzero blocks in its underlying infinitesimal generator matrix with sums of Kronecker products. Experiments are performed on systems of stochastic chemical kinetics having two or more countably infinite state space subsystems. Results indicate that, albeit more memory consuming, there are many cases where a matrix analytic solution coupled with Lyapunov theory yields a faster and more accurate steady-state measure compared to that obtained with simulation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Benzerlik taraması veri kümelerinden ilgili bilginin elde edilmesi işlemidir. İlgi dahilindeki veri kümeleri özellikle resim, görüntü, ses kaydı, protein ve DNAdizisi gibi karmaşık ve düzensiz veriler içerirler. İlgilenilen bilgi genellikle iki yaygın sorgu türünden bir tanesi kullanılarak tanımlanır: Menzil sorgusu, verilen sorgu nesnesinin belirli bir uzaklığı içerisinde kalan bütün nesnelerin elde edilmesini kapsar. Öte yandan en yakın k-komşu sorgusu, sorgu nesnesine en yakın k veritabanı nesnesinin elde edilmesi ile ilgilenir. Belirtilen sorgu türlerini uygulayabilmek amacıyla metrik uzay kavramına dayanan çeşitli indeks yapıları önerilmiştir.Önerilen bu indeks yapılarının sorgu performansları özellikle yüksek boyutlu veri kümeleri için çok tatmin edici olmamıştır. Çözüm olarak, kullanıcılara kalite/zaman ödünleşim imkanı sunan çeşili yaklaşık benzerlik taraması yöntemleri geliştirilmiştir. Bu yaklaşım, kullanıcıların sorgu doğruluğundan ödün vererek sorgu sonuçlarına görece daha hızlı erişmek istemeleri ilkesine dayanmaktadır. Önerilen yaklaşık tarama tasarıları genelde altta kullanılan veri yapılarına çok bağımlıdırlar. Bu durum, bu tasarıların dayandığı temel fikirlerin kalite açısından kıyaslanabilmesini zorlaştırmaktadır.Bu tezde, benzerlik sorgularının cevap süresini kısaltabilmek için farklı yaklaşık benzerlik yöntemleri araştırlımıştır. Bu yöntemler, elimizdeki veri kümesinden elde edilen çeşitli istatistiksel bilgileri kullanarak her sorgu nesnesine özgü dinamik (sorgulama esnasında gerçekleşen) yönlendirmeye olanaksağlamaktadırlar. Deneyler basit bir pivot-tabanlı indeks yapısı üzerinde çalıştırılarak alttaki yapının yaklaşık benzerlik tasarılarına etkisi azaltılmıştır. Sonuçlar, veri kümesine ve sorgu nesnesine duyarlı yönlendirmenin performans/doğruluk hususunda iyileştirme sağlayabileceğini göstermektedir.","Similarity searching is the task of retrieval of relevant information from datasets. We are particularly interested in datasets that contain complex and unstructured data such as images, videos, audio recordings, protein and DNA sequences. The relevant information is typically defined using one of two common query types: a range query involves retrieval of all the objects within a specified distance to the query object; whereas a k-nearest neighbor query deals with obtaining k closest database objects to the query object. A variety of index structures based on the notion of metric spaces have been offered to process these two query types.The query performances of the proposed index structures have not been satisfactory particularly for high dimensional datasets. As a solution, various approximate similarity search methods offering the users a quality/time trade-off have been proposed. The rationale is that the users might be willing to tolerate query precision to retrieve query results relatively faster. The proposed approximate searching schemes usually have strong connections to the underlying data structures, making the comparison of the quality of the essence of their ideas difficult.In this thesis we investigate various approximation approaches to decrease the response time of similarity queries. These approaches use a variety of statistics about the dataset in order to obtain dynamic (at the time of querying) and specific guidance on the approximation for each query object individually. The experiments are performed on top of a simple underlying pivot-based index structure to minimize the effects of the index to our approximation schemes. The results show that it is possible to improve the performance/precision of the approximation based on data and query object sensitive guidance."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son on yıl içinde, bilgisayar destekli teşhis sistemleri, patologların kanser tespiti için histopatolojik görüntüleri yorumlamasını artırmaya yardımcı olması yönüyle büyük bir önem kazanmıştır. Bu sistemler, kanser tanısı için mevcut histopatolojik doku muayenesi uygulamasında çok yaygın olan gözlemci-içi ve gözlemciler arası değişkenliği azaltmaya ve ortadan kaldırmaya yönelik çok değerli fırsatlar sunmaktadır. Özellikle dokusal ve yapısal doku görüntü analizine dayalı birçok çalışma, kanserin tanı ve sınıflandırması için bu tür sistemleri geliştirmeye adanmıştır. Son zamanlardaki dokusal ve yapısal yaklaşımlar, farklı tipte dokular için umut verici sonuçlar vermesine rağmen, doku bileşenleri tarafından taşınan potansiyel biyolojik bilgiyi kullanabilmekten yoksundurlar. Halbuki, bu doku bileşenleri, doku temsiline ve dolayısıyla, kanserin yol açtığı doku değişikliklerini ölçmeye daha iyi yardımcı olur.Bu tez, kolon biyopsi görüntülerini temsil etmede doku bileşenlerinin kullanımı için özellikli Nokta Modelleri olarak adlandırılan yeni bir dokusal yaklaşım sunmaktadır. Bu dokusal yaklaşım öncelikle kolon dokusunun çekirdek, stroma ve lümen bileşenlerine karşılık gelen bir dizi özellikli noktaları tanımlar. Sonra, bu belirgin noktalar etrafından doku görüntülerini ölçmede kullanılan öznitelikler çıkartılır. Son olarak, bu öznitelikleri kullanarak doku örneklerini sınıflandırır. 258 farklı hastadan alınan 3236 kolon biyopsi örneği üzerinde gerçekleştirdiğimiz deneyler, Özellikli Nokta Modelleri yaklaşımının, dokuları tanımlamada yapısal bileşenleri kullanmayan benzer çalyşmalarla karşılaştırıldığında, sınıflandırma başarı yüzdesini artırdığını ortaya koymuştur. Ayrıca gerçekleştirdiğimiz bu deneyler, doku görüntüsünün daha iyi temsil edilebilmesi için bu dokusal yaklaşım kullanılarak farklı özniteliklerin elde edilebileceğini göstermektedir.","Over the last decade, computer aided diagnosis (CAD) systems have gained great importance to help pathologists improve the interpretation of histopathological tissue images for cancer detection. These systems offer valuable opportunities to reduce and eliminate the inter- and intra-observer variations in diagnosis, which is very common in the current practice of histopathological examination. Many studies have been dedicated to develop such systems for cancer diagnosis and grading, especially based on textural and structural tissue image analysis. Although the recent textural and structural approaches yield promising results for different types of tissues, they are still unable to make use of the potential biological information carried by different tissue components. However, these tissue components help better represent a tissue, and hence, they help better quantify the tissue changes caused by cancer.This thesis introduces a new textural approach, called Salient Point Patterns (SPP), for the utilization of tissue components in order to represent colon biopsy images. This textural approach first defines a set of salient points that correspond to nuclear, stromal, and luminal components of a colon tissue. Then, it extracts some features around these salient points to quantify the images. Finally, it classifies the tissue samples by using the extracted features. Working with 3236 colon biopsy samples that are taken from 258 different patients, our experiments demonstrate that Salient Point Patterns approach improves the classification accuracy, compared to its counterparts, which do not make use of tissue components in defining their texture descriptors. These experiments also show that different set of features can be used within the SPP approach for better representation of a tissue image."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mobil robotlarla Eşzamanlı Haritalama ve Konumlandırma (EHK), robotikcamiasının en zorlu problemlerinden biridir. Geçtiğimiz birkaç yılda, üzerineyapılan yoğun çalışmalar neticesinde, bu konu teorik ve pratik açılardan doyumaulaşmıştır. Geçtiğimiz birkaç sene içerisinde, araştırmaların yönelimi EHK'den,ölçüm aygıtı olarak kameraların kullanıldığı Görsel EHK'ye doğru olmuştur.Düzlemsel uzayda çalışan birçok EHK uygulamasına kıyasen daha üstün olarak,GEHK, ortamın 3 boyutlu modelini ve robotun 6 serbestlik dereceli durumunuda kestirebilmektedir. Robotik çalışmalarına henüz uygulanmakla beraber,GEHK'nin geliştirilmesi gereken çok yönleri bulunmaktadır. Özellikle, EHK veGEHK algoritmalarının ortak problemi bilgi eşlemesidir. Hatalı bilgi eşlemesiEHK'nin kararlığını olumsuz yönde etkileyebilir ya da tamamen ıraksamasınaneden olabilir. Bu çalışmada, aykırı gözlemleri elemek için, tahmini izdüşümhatasını ve optik akı bilgisini kullanan iki yöntem öneriyoruz. İlk yöntem, haritaöğelerinin tahmini izdüşüm ve onlarla eşlenen ölçüm yerlerinin yakın olmasıgerektiği mantığını kullanmaktadır. İkinci yöntem ise, optik akı vektor alanınıreferans kabul edip, ardışık iki ölçüm ile belirlenen vektör ile, bölgesel optik akıalanını kıyas ediyor; ve optik akı alanı ile çelişen ölçümleri eliyor. Çalışmamızda,bu iki yöntemin, GEHK'nin ıraksamasını engellediğini ve genel performansınıartırdığını gösteriyoruz. Ayrıca, modüler bir EKH kütüphanesi olan SLAM++yazılımımızı açıklıyoruz.","Simultaneous Localization and Mapping (SLAM) for mobile robots has been oneof the challenging problems for the robotics community. Extensive study of thisproblem in recent years has somewhat saturated the theoretical and practicalbackground on this topic. Within last few years, researches on SLAM have beenheaded towards Visual SLAM, in which camera is used as the primary sensor.Superior to many SLAM application run with planar robots, VSLAM allows us toestimate the 3D model of the environment and 6-DOF pose of the robot.Being applied to robotics only recently, VSLAM still has a lot of room for improvement.In particular, a common issue both in normal and Visual SLAM algorithms is th data association problem. Wrong data association either disturbs stability orresult in divergence of the SLAM process. In this study, we propose two outlierelimination methods which use predicted feature location error and optical flow field.The former method asserts estimated landmark projection and its measurement locationsto be close. The latter accepts optical flow field as a reference and compares the vectoformed by consecutive matched feature locations; eliminates matches contradicting withthe local optical flow vector field. We have shown these two methods to be saving VSLAMfrom divergence and improving its overall performance. We have also described our newmodular SLAM library, SLAM++."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tarih boyunca, kaynakların yetersizliği insanoğlu için sorun olmuştur. Ne var ki günümüz bilgi dünyasında, kaynakların yetersizliğinden ziyade kaynak fazlalığının sebep olduğu yeni bir problem türüyle karşı karşıyayız. Aşırı bilgi, ihtiyaç duyulan bilginin bulunmasını ve çıkarımını gerektirmektedir. Bilgi çıkarımı, ihtiyaç duyulan nesnelerin, ilişkilerin, gerçeklerin veya olayların, doğal dildeki serbest metinler içerisinde bulunması olarak tanımlanabilir. Bu bağlamda bilgi çıkarımı, doğal dildeki yapısal olmayan metinlerin çözümlenmesi ve bu metinlerin ihtiva ettiği gerekli bilginin yapısal bir şablona aktarılması işlemidir.Bu çalışmanın amacı Türkçe serbest metinlerdeki bilgiyi otomatik olarak bulan ve çıkaran bir sistemin geliştirilmesidir. Çalışma iki temel bilgi çıkarımı görevine odaklanmaktadır: Ad Tanıma ve İlişki Bulma. En temel bilgi çıkarımı görevlerinden olan Ad Tanıma, serbest metinlerde geçen varlık isimlerinin (insan, yer, organizasyon vb.) bulunmasıdır. İlişki Bulma görevi ise, metinlerde bahsedilen varlıklar arasındaki ilişkileri bulmaya çalışır.Gözetimli öğrenme stratejisini kullanan sistem, öğrenme kümesinden seçilen örnek kümesi ile başlayıp bilgi çıkarım kurallarını üretmektedir. Ayrıca, genelleştirmenin ve doğruluğun maksimize edilmesi amacıyla kural filtreleme ve kural iyileştirme teknikleri kullanılmaktadır. Hassas genelleştirmenin sağlanması maksadıyla imla, bağlam, sözcük, biçim gibi çeşitli sözdizimsel ve anlamsal metin özelliklerinden faydalanılmaktadır. Özellikle, bitişimli bir dil olan Türkçe'den bilgi çıkarımı başarımının artırılması için biçimbilimsel özellikler etkin olarak kullanılmıştır. Sistem elle üretilen kurallar üzerine dayanmadığı için alan uyumluluğu probleminden ciddi olarak etkilenmemektedir.Yapılan test sonuçları, (1) geliştirilen sistemin Ad Tanıma ve İlişki Bulma görevlerine başarılı bir şekilde uygulandığını, ve (2) biçimbilimsel özelliklerin kullanımının, bitişimli bir dil olan Türkçe'den bilgi çıkarımı işleminin performansını önemli ölçüde artırdığını göstermiştir.","Throughout history, mankind has often suffered from a lack of necessary resources. In today's information world, the challenge can sometimes be a wealth of resources. That is to say, an excessive amount of information implies the need to find and extract necessary information. Information extraction can be defined as the identification of selected types of entities, relations, facts or events in a set of unstructured text documents in a natural language.The goal of our research is to build a system that automatically locates and extracts information from Turkish unstructured texts. Our study focuses on two basic Information Extraction (IE) tasks: Named Entity Recognition and Entity Relation Detection. Named Entity Recognition, finding named entities (persons, locations, organizations, etc.) located in unstructured texts, is one of the most fundamental IE tasks. Entity Relation Detection task tries to identify relationships between entities mentioned in text documents.Using supervised learning strategy, the developed systems start with a set of examples collected from a training dataset and generate the extraction rules from the given examples by using a carefully designed coverage algorithm. Moreover, several rule filtering and rule refinement techniques are utilized to maximize generalization and accuracy at the same time. In order to obtain accurate generalization, we use several syntactic and semantic features of the text, including: orthographical, contextual, lexical and morphological features. In particular, morphological features of the text are effectively used in this study to increase the extraction performance for Turkish, an agglutinative language. Since the system does not rely on handcrafted rules/patterns, it does not heavily suffer from domain adaptability problem.The results of the conducted experiments show that (1) the developed systems are successfully applicable to the Named Entity Recognition and Entity Relation Detection tasks, and (2) exploiting morphological features can significantly improve the performance of information extraction from Turkish, an agglutinative language."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İki boyutlu eskizlerin kullanımının, pek cok sanatçının kendisini doğal hissetmesinisağladıgını düşünürsek, üç boyutlu modellemelerde eskiz kullanımınınneden bu kadar rağbet gören bir araştırma alanı olduğunu anlayabiliriz. Üçboyutlu nesne yaratabilmek için; kullanımı kolay, aynı zamanda yetkin biraraca olanak sağlaması amacıyla pek çok teknik sunulmuştur. Bu tez de, bualanda daha önce yapılmış olan araştırmaların ışığında, detaylı üç boyutlu nesnetasarımına olanak sağlayacak bir sistem geliştirmeyi amaçlamaktadır. Tezdekullanılan sistemin amacı, kullanıcıların bazı çizgilerle pürüzsüz basit nesneleryaratabilmesini ve gölgelendirme bilgisi taşıyan eskizler aracılığıyla, yaratılan nesneninyüzeyini pürüzlü hale dönüştürebilmesini sağlamaktır. Böylelikle, sadecebasit bir silüet çizimiyle kullanıcılara, üç boyutlu pürüzsüz basit bir nesneyaratma olanağı yaratılmıştır. Sistemin işleyişi şu şekilde özetlenebilir: Sistem,çizgi girdisini alır ve basit üç boyutlu bir nesne yaratır. Ardından, kullanıcınındaha önceden çizmiş olduğu iki boyutlu silüetin istediği kısımlarınıgölgelendirmesine olanak sağlanır. Burada, yükseklik haritasının oluşumundakullanılan gölgelendirme bilgisini ve gölgelendirmenin şeklini alır ve daha öncedenoluşturulmuş örgü üzerinde pürüzlü bir yüzey oluşturmak için pürüzsüz nesneninyüzeyine gölgelendirme haritasını uygular. Tezde kullanlan sistem sayesinde, kolaycaüç boyutlu örgüler yaratılabilir ve üzerinde değişikler yapılabilir.","Using sketches for 3D modelling is a popular research area, which is expected sinceusing 2D sketches feels natural to most of the artists. Many techniques have beenproposed to enable an intuitive and competent tool for 3D object creation. Inthe light of the previous research in this area, we designed a system that enablescreation of 3D free-form objects with details. Our system aims to enable usersto easily create simple free-form objects using strokes and perturb their surfacesusing sketches that provide contours of details and shading information. Weprovide the user with the ability to create a 3D simple object just by drawingits silhouette. We take this stroke input and create a simple 3D object. Thenwe allow the user to shade the parts of the 2D silhouette drawn before. Wetake the shading information and use shape from shading techniques to create aheight map and apply the height map on the surface of the object to construct aperturbed surface for the previously created mesh. With our system, it is possibleto create and modify 3D meshes easily and intuitively."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tarihsel belgelerin otomatik erişimi ve dizinlenmesi bir çok alandan ve ülkeden araştırmacı için faydalı olacaktır. Ne yazık ki, bu belgelerdeki yıpranma ve lekeler yüzünden, Optik Karakter Tanıma (OKT) tekniklerinin bu belgelerde başarılı olması zordur. Son zamanlarda, bu belgelerde erişim problemi kelime eşleştirme yöntemleriyle çözülmeye çalışılmıştır. Bu tezde, iki tarihsel belge analizi problemi, tarihsel belgelerin kelimelere bölütlenmesi ve Kufi resimlerinde İslami motiflerin eşleştirilmesi, kelime eşleştirme tabanlı yöntemlerle çözülmeye çalışılmıştır. Birinci problemin çözümü için çapraz belgelerde kelime eşleştirme tabanlı bir yöntem önerilmiştir. Bir belgenin kelime bölütlemenin kolay olacağı bir versiyonu kaynak veri kümesi ve de diğer başka bir yazı tarzıyla yazılan ve kelime bölütlemesinin zor olacağı bir versiyonu da hedef veri kümesi olarak kullanılmıştır. Kaynak veri kümesi basit bir yöntemle kelimelerine bölütlenmiş ve elde edilen bu kelimeler sorgu kelimeleri olarak kullanılarak hedef veri kümesindeki yerleri saptanmaya çalışılmıştır. Yapılan deneyler, çapraz belgelerde kelime eşleştirme tabanlı yöntemin tarihsel belgelerde kelime bölütlemesi için umut verici sonuçlar verdiğini göstermişir. İkinci problemin çözümü için sunulan yöntemde, öncelikle resimlerdeki çizgiler çıkartılır ve alt-kelimeler otomatik olarak bulunur. Daha sonra alt-kelimeler, çizgi tabanlı zincir kod gösterimi eşleştirmesi ve şekil içeriği tanımlayıcısı eşleştirmesi yöntemleriyle eşleştirilir. Kare kufi resimlerinden oluşan bir veri kümesi üzerinde yapılan deneyler, sunulan kelime eşleştirme tabanlı yöntemin umut verici sonuçlar verdiğini göstermiştir.","Historical documents constitute a heritage which should be preserved and providing automatic retrieval and indexing scheme for these archives would be beneficial for researchers from several disciplines and countries. Unfortunately, applying ordinary Optical Character Recognition (OCR) techniques on these documents is nearly impossible, since these documents are degraded and deformed. Recently, word matching methods are proposed to access these documents. In this thesis, two historical document analysis problems, word segmentation in historical documents and Islamic pattern matching in kufic images are tackled based on word matching. In the first task, a cross document word matching based approach is proposed to segment historical documents into words. A version of a document, in which word segmentation is easy, is used as a source data set and another version in a different writing style, which is more difficult to segment into words, is used as a target data set. The source data set is segmented into words by a simple method and extracted words are used as queries to be spotted in the target data set. Experiments on an Ottoman data set show that cross document word matching is a promising method to segment historical documents into words. In the second task, firstly lines are extracted and sub-patterns are automatically detected in the images. Then sub-patterns are matched based on a line representation in two ways: by their chain code representation and by their shape contexts. Promising results are obtained for finding the instances of a query pattern and for fully automatic detection of repeating patterns on a square kufic image collection."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağları bir ortamı algılayabilen, ve ölçülen verileri merkezi bir konuma gönderebilmek için birbirleri ile kablosuz şekilde iletişim kurabilen düğümlerden oluşur. Bir çok alandaki uygulamalar için sunduğu avantajlarının yanısıra kısıtlı ve değiştirilemez enerji kaynaklarına sahip olmak kablosuz algılayıcı ağlarının önemli bir yetersizliğidir. Bu tezde, veri toplama uygulamaları çalıştıran kablosuz algılayıcı ağlarının ağ ömrünü iyileştirmek için etkili yol atama ve zaman planlama çözümleri sunulmuştur. Bu amaçla, öncelikle ağ ömrü problemi, tam veri yığışımı ve düğümler için güç ayarlayabilme yeteneğini göz önünde bulunduran teorik bir model oluşturarak incelenmiş; ve bir algılayıcı ağının fonksiyonel ömrü için bir üst sınır türetilmiştir. Daha sonra, ağ ömrünü bazı koşullarda bu teorik üst sınıra kadar iyileştiren bir yol atama protokolü önerilmiştir. L-PEDAP adındaki önerdiğimiz algoritma; yerelleştirilmiş, kendini örgütleyebilen, stabil, ve güç-farkında veri yığışım ağaçlarının oluşturulması esasına dayanmaktadır. Bununla birlikte, ağ ömrünü daha da iyileştirmek için yol atama protokolümüz ile beraber çalışabilen bir zaman planlama protokolü de önerilmiştir. PENS adını verdiğimiz bu zaman planlama protokolü, bir turda en az enerji harcanmasını sağlayacak en uygun sayıda düğümü açık tutar; ve geri kalan düğümleri uyku moduna alır. Bazı koşullarda, en uygun düğüm sayısı, tüm alanı kapsamak için gerekli en az sayıda düğüm miktarından fazla olabilir. Bu kapsamda, daha fazla düğümü açık tutmanın enerji açısından daha verimli olabileceği şartlar türetilmiştir. Önerdiğimiz PEDAP ve PENS protokollerini değerlendirmek için yapmış olduğumuz kapsamlı simulasyonlar, bu yöntemlerin düğümlerin güç ayarlama yeteneğine sahip olduğu ve tam veri yığışımının kullanılabildiği veri toplama uygulamaları için etkili olduğunu göstermiştir.","A wireless sensor network consists of nodes which are capable of sensing an environment and wirelessly communicating with each other to gather the sensed data to a central location. Besides the advantages for many applications, having very limited irreplaceable energy resources is an important shortcoming of the wireless sensor networks. In this thesis, we present effective routing and node scheduling solutions to improve network lifetime in wireless sensor networks for data gathering applications. Towards this goal, we first investigate the network lifetime problem by developing a theoretical model which assumes perfect data aggregation and power-control capability for the nodes; and we derive an upper-bound on the functional lifetime of a sensor network. Then we propose a routing protocol to improve network lifetime close to this upper-bound on some certain conditions. Our proposed routing protocol, called L-PEDAP, is based on constructing localized, self-organizing, robust and power-aware data aggregation trees. We also propose a node scheduling protocol that can work with our routing protocol together to improve network lifetime further. Our node scheduling protocol, called PENS, keeps an optimal number of nodes active to achieve minimum energy consumption in a round, and puts the remaining nodes into sleep mode for a while. Under some conditions, the optimum number can be greater than the minimum number of nodes required to cover an area. We also derive the conditions under which keeping more nodes alive can be more energy efficient. The extensive simulation experiments we performed to evaluate our PEDAP and PENS protocols show that they can be effective methods to improve wireless sensor network lifetime for data gathering applications where nodes have power-control capability and where perfect data aggregation can be used."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Cevap önbelleği, büyük ölçekli Web arama motorlarının verimi için anahtar bileşen konumundadır ve önbellekte bulunan sorgu cevaplarının tazeleğinin korunması güncel araştırma konularından birisidir. Bu probleme çözüm olarak, gerçekleştirdiğimiz çalışma önbellekte bayat cevaba sahip olan sorguların tespit edilmesi için yeni bir yöntem önermektedir. Önerdiğimiz yöntemin temelindeki ana fikir, sorgu cevaplarının taze olup olmadığına karar vermek amacıyla sorgular için cevap oluşturulma zamanının, terim listeleri ve dökümanlar için de güncellenme zamanlarının tutulmasıdır. Önerilen yöntemin başarımı, gerçek güncellenme zaman bilgisi içeren Wikipedia doküman kümesi ve yine gerçek bir sorgu kümesi kullanılarak değerlendirilmiştir. Gerçekleştirilen deneylerde, önerilen teknik literatürdeki referans yaklaşımlarla karşılaştırmalı olarak incelenmiş ve detaylı bir şekilde değerlendirilmiştir. Bu yöntem ile literatürdeki son-kullanma-süresi (SKS) yaklaşımından çok daha başarılı tahmin sonuçları elde edilmiştir. Buna ilave olarak, önerdiğimiz yöntem literatürdeki gelişmiş bir yönteme göre de daha kolay gerçeklenebilir ve sistem üzerinde merkezi bir darboğaz yaratmayacak şekildedir.","The result cache is a vital component for the efficiency of large-scale web search engines, and maintaining the freshness of cached query results is a current research challenge. As a remedy to this problem, our work proposes a new mechanism to identify queries whose cached results are stale. The basic idea behind our mechanism is to maintain and compare the generation time of query results with the update times of posting lists and documents to decide on staleness of query results. The proposed technique is evaluated using a Wikipedia document collection with real update information and a real-life query log. Throughout the experiments, we compare our approach with two baseline strategies from literature together with a detailed evaluation. We show that our technique has good prediction accuracy, relative to the baseline based on the time-to-live (TTL) mechanism. Moreover, it is easy to implement and it incurs less processing overhead on the system relative to a recently proposed, more sophisticated invalidation mechanism."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüzde aynı edebi eserin farklı versiyonlarinı detaylı bir aramayla bulabilmekmümkündür. Sezgisel olarak bu tür aynı kaynak tabanlı çeviri eserlerin birbirlerinebenzer yapıda olmaları beklenmektedir. Aynı şekilde, intihal şüphesitaşıyan bir yazı metnin, intihal yapılan orijinal eser ile de yapısal olarak benzemesiolasıdır. Yazısal intihal ile kastedilen, bir yazarın yazdığı herhangi birmetninin, üslubunun veya belirttiği fikrin, yazar lehine kaynak gösterilmedenbaşka biri tarafından yazarın onayını almadan kullanılmasıdır. Günümüzdekiiçsel ve harici yazısal intihal tespit yöntemleri var olan intihalin tespitini makulzaman dilimleri içerisinde sonuçlandırabilmek için yapılan yazısal intihalin kapsamınısınırlandırma yoluna gitmişler ve intihali arayabilmenin önkoşulu olarakbir referans doküman kümesine ihtiyaç duymuşlardır. Bu da intihal tespityönteminde referans doküman kümesinin başarıyla oluşturulması gibi başkasorunların varlığını ortaya koymuştur. Bu tez çalışmasında bir harici intihalve benzer yapı tespit ve ölçme yöntemi önerilmiştir. İntihal tespit probleminianaliz etmek ve benzerligi ölçmek için metinlerdeki yapısal benzerlikten faydanılmıştır. Bu yöntem dahilinde öncelikle şüpheli ve kaynak metinler karşılıklıbloklara bölünmüştür. Oluşturulan her bir blok sabit sayıda kelime içeren birgrup dökümandan oluşmaktadır. Daha sonra bloklar indekslenmiş ve kapsamakatsayısına dayalı kümeleme yöntemiyle kümelenmiştir. Her iki metnin oluşanküme yapıları incelenmiş ve benzerlikleri ölçülmüştür. PAN'09 intihal veri kümesive ünlü edebi eser Leyla ve Mecnun'un farklı versiyonları üzerinde yapılan testsonuçlarına göre önerilen yöntem benzer yapı tespitini ve yapısal olarak benzerlikgösteren intihal durumlarını başarıyla tespit edebilmektedir.","Today different editions and translations of the same literary text can be found.Intuitively such translations that are based on the same literary text are expectedto possess significantly similar structure. In the same way, it is possible that atext that is suspected to have plagiarism can possess structural similarities withthe text that is believed to be the source of the plagiarism. Textual plagiarismimplies the usage of an author?s text, his/her work or the idea that is inserted inanother textual work without giving a reference or without taking the permissionof the original text?s author. Today, existing intrinsic and external plagiarism detectionmethods tend to detect plagiarism cases within a given dataset in order torun these algorithms in a reasonable amount of time. Hence a reference documentset is built in order to search for plagiarism cases successfully by these algorithms.In this thesis, a method for detecting and quantifying the external plagiarism andparallel corpora is introduced. For this purpose, we use the structural similaritiesin order to analyze plagiarism detection problem and to quantify the similaritybetween given texts. In this method, suspicious and source texts are partitionedinto corresponding blocks. Each block is represented as a group of documentswhere a document consists of a fixed amount of words. Then, blocks are indexedand clustered by using the cover coefficient clustering algorithm. Cluster formationsfor both texts are then analyzed and their similarities are measured. Theresults over PAN?09 plagiarism dataset and over different versions of the famousliterary text classic Leyla and Mecnun show that the proposed method successfullydetects and quantifies the structurally similar plagiarism cases and succeedsin detecting the parallel corpora."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde videolardaki insan eylemlerini tanımak için çizgiye dayalı bir poz temsilinden faydalanılmaktadır. Her karedeki pozu çizgi-çiftleri kullanarak temsil ediyoruz; böylece el, kol ve eklem hareketlerini daha iyi tanımlamış, insan figürünü oluşturan çizgiler arasındaki geometrik ilişkileri yakalamış oluyoruz. İki poz arasındaki çizgi-çiftlerini eşleştirerek benzerliklerini hesaplayan yeni bir yöntem önererek literatüre katkıda bulunuyoruz. Dahası, poz dizilerindeki genel hareket bilgisinin saklanması için ardışık karelerdeki çizgileri eşleştirerek oluşturulan çizgi-akış histogramlarını sunuyoruz. Weizmann ve KTH veri setleri üzerindeki deneysel sonuçlar, poz temsilimizin gücünü vurgulamakta; beraber kullanıldıklarında, sıralı poz ve çizgi-akış histogramlarının bir eylemin doğasını kavrayarak birini diğerlerinden ayırt edebilme üzerindeki etkinliğini göstermektedir. Son olarak, yaklaşımımızın çoklu kamera sistemlerine uygulanabilirliğini IXMAS veri seti üzerinde göstermekteyiz.","In this thesis, we utilize a line based pose representation to recognize human actions in videos. We represent the pose in each frame by employing a collection of line-pairs, so that limb and joint movements are better described and the geometrical relationships among the lines forming the human figure is captured. We contribute to the literature by proposing a new method that matches line-pairs of two poses to compute the similarity between them. Moreover, to encapsulate the global motion information of a pose sequence, we introduce line-flow histograms, which are extracted by matching line segments in consecutive frames. Experimental results on Weizmann and KTH datasets, emphasize the power of our pose representation; and show the effectiveness of using pose ordering and line-flow histograms together in grasping the nature of an action and distinguishing one from the others. Finally, we demonstrate the applicability of our approach to multi-camera systems on the IXMAS dataset."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Büyük ölçekli arama motorları artan ağ içeriği ve artan günlük sorgu sayısı ile mücadele etmek zorundadırlar. Sorgu cevaplarının önbelleklenmesi ise sistemin belli bir zamanda sorgu cevaplama sayısını arttırabileceği kritik yöntemlerden birisidir. Bu tezde, arama motorlarında önbelleklemenin verimliliğini arttırıcı çeşitli yöntemler önerilmektedir.İlk olarak, statik ve dinamik sorgu cevabı önbellekleri için maliyet bazlı önbellekleme yöntemleri geliştirilmiştir. Sorguların ciddi oranda farklı maliyetlerinin olduğu ve bu maliyet ile frekans arasında doğru orantı olmadığı gözlemlenmiştir. Bu nedenle önbelleğe hangi öğelerin alınacağına karar verilirken frekansa ek olarak sorgu maliyetini de göz önüne alan yöntemler tasarlanmıştır. İkinci olarak, navigasyonel sorguların tanımlanarak diğer sorgulardan farklı olarak önbelleklendiği bir sorgu tipi bazlı önbellekleme yöntemi geliştirilmiştir. Sorgu cevapları genellikle her biri 10 cevap içeren sonuç sayfaları olarak sunulmakta ve önbellekte saklanmaktadır. Navigasyonel sorgularda amaç belirli bir ağ sayfasına ulaşmaktır ve bu sayfa arama motoru tarafından bulunursa yüksek sıralarda listelenmektedir. Bu sorgu tipi için cevapların bir sayfada 10 cevap olacak şekilde sunulup önbellekte saklanmasının maliyet etkinliği olmayan bir yöntem olduğu gösterilmiş ve alternatif cevap sunum modelleri önerilerek bunların önbellekleme üzerindeki etkisi araştırılmıştır. Üçüncü olarak, statik önbellekte sorgu cevapları için kümeleme bazlı bir saklama yöntemi önerilmiştir. Ortak cevap belgesi olan sorgular tek bağlantı yöntemi ile kümelenmiştir. Oluşan kümelerdeki sorgu cevaplarındaki örtüşmeden yararlanan kompakt bir saklama modeli sunulmuştur. Son olarak, bir arama motoru ortamında önbelleklenebilecek tüm veri öğelerini (sorgu cevapları, endeks parçası ve belge içeriği) içerisinde barındıran beş-seviyeli bir statik önbellek önerilmiştir. öğelerin hangi sırayla önbelleğe alınacağını belirlemek için bir açgözlü algoritma geliştirilmiştir. Bu yöntem önbelleğe alınmada öğeleri geçmiş frekans değerleri, öngörülen maliyet ve önbellekte kaplayacağı alan açısından önceliklendirmektedir. Ayrıca öğeler arasındaki bağımlılık göz önüne alınarak bir öğenin önbelleğe alınmasından sonra henüz önbelleğe alınmamış bağımlı öğelerin kazanç değerleri değiştirilmektedir.Önerilen bütün yöntemler gerçek sorgu kütüğü ve belge kolleksiyonu kullanan deneylerle test edilmiştir. Literatürde karşılık gelen referans yöntemler ile karşılaştırmalar sunulmuş ve üretilen iş, sorgu ıskalama sayısı ve sorgu cevaplarının kapladığı alan bazında gelişmeler elde edilmiştir.","Large scale search engines have to cope with increasing volume of web content and increasing number of query requests each day. Caching of query results is one of the crucial methods that can increase the throughput of the system. In this thesis, we propose a variety of methods to increase the efficiency of caching for search engines.We first provide cost-aware policies for both static and dynamic query result caches. We show that queries have significantly varying costs and processing cost of a query is not proportional to its frequency. Based on this observation, we develop caching policies that take the query cost into consideration in addition to frequency, while deciding which items to cache. Second, we propose a query intent aware caching scheme such that navigational queries are identified and cached differently from other queries. Query results are cached and presented in terms of pages, which typically includes 10 results each. In navigational queries, the aim is to reach a particular web site which would be typically listed at the top ranks by the search engine, if found. We argue that caching and presenting the results of navigational queries in this 10-per-page manner is not cost effective and thus we propose alternative result presentation models and investigate the effect of these models on caching performance. Third, we propose a cluster based storage model for query results in a static cache. Queries with common result documents are clustered using single link clustering algorithm. We provide a compact storage model for those clusters by exploiting the overlap in query results. Finally, a five-level static cache that consists of all cacheable data items (query results, part of index, and document contents) in a search engine setting is presented. A greedy method is developed to determine which items to cache. This method prioritizes items for caching based on gains computed using items' past frequency, estimated costs, and storage overheads. This approach also considers the inter-dependency between items such that caching of an item may affect the gain of items that are not cached yet.We experimentally evaluate all our methods using a real query log and document collections. We provide comparisons to corresponding baseline methods in the literature and we present improvements in terms of throughput, number of cache misses, and storage overhead of query results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, videolardaki insan eylemlerini tanımak için anahtar kareye dayalı bir poztemsilinden faydalanılmaktadır. İnsan figürünün oluşturduğu pozun, bir kare içerisinde devameden eylemi tanımlamak için çok güçlü bir kaynak olduğunu düşünüyoruz. Her eylem, oeylemin gerçekleştiği süre içerisinde insan vücudunun parçalarının oluşturduğu bütün uzamsaldüzenleşimleri içeren bir kare grubuyla temsil edilebilir. ""Anahtar Kare"" olarakadlandırdığımız bu kare grubu bir eylemi diğerlerinden ayırt eder. ""Anahtar Kare""leri seçmekiçin, insan figürünü oluşturan çizgilerle beraber bir şekil eşleme metodu kullanarak, verileniki kare üzerindeki pozların arasında bir benzerlik değeri tanımlıyoruz. Bir kümelemealgoritması kullanarak, her eylemin benzer karelerini belirli bir sayıda kümede grupluyor vebu grupların ağırlık merkezlerini ""Anahtar Kare"" olarak kullanıyoruz. Dahası, insan figürünüoluşturan çizgilerin hareketlerini video dizisi boyunca takip ederek, eylem içerisindekidevinim bilgisinden de faydalanıyoruz. Weizmann ve KTH verisetleri üzerinde elde ettiğimizsonuçlar, ""Anahtar Kare"" bazlı yaklaşımımızın insan hareketlerini temsil etme ve tanımadakietkinliğini göstermektedir.","This thesis utilizes a key-pose based representation to recognize human actions in videos. Webelieve that the pose of the human figure is a powerful source for describing the nature of theongoing action in a frame. Each action can be represented by a unique set of frames thatinclude all the possible spatial configurations of the human body parts throughout the time theaction is performed. Such set of frames for each action referred as ""key poses"" uniquelydistinguishes that action from the rest. For extracting ""key poses"", we define a similarity valuebetween the poses in a pair of frames by using the lines forming the human figure along witha shape matching method. By the help of a clustering algorithm, we group the similar framesof each action into a number of clusters and use the medoids as ""key poses"" for that action.Moreover, in order to utilize the motion information present in the action, we include simpleline displacement vectors for each frame in the ""key poses"" selection process. Experiments onWeizmann and KTH datasets show the effectiveness of our key-pose based approach inrepresenting and recognizing human actions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İşlemci ve bellek çekirdeği arasındaki artan haberleşme ihtiyacindan ötürü, Çok İşlemcili Mikrodevre Sistemler (CİMS)'deki işlemci çekirdekleri ve alt dizgeleri bağlamak için, uygulamaya özgü ve ölçeklenebilir Mikrodevre Ağlar (MA) ortaya çıktı. Uygulamaya özgü mikrodevre tasarımların zorluğu, haberleşmeden kaynaklı gecikme, güç tüketimi ve mikrodevre alanı gibi farklı ödünleşimler arasındaki doğru dengeyi bulmaktır.Bu tez, farklı özelliklere sahip işlemci çekirdeklerini, Serbest Kullanılabilir İş Yükü Grafiği (SKIYG)'nden üretilen iş yüklerine göre, belirlenen mikrodevre ağ alanina yerleştirmek için, dirimbilimsel ilhamlı evrimsel çözüm yolu ve 2 boyutlu çözüm yolunun kullanıldığı, yeni bir mikrodevre ağ tasarım yaklaşımını tanıtıyor. SKIYG, zaman çizelgelemesi ve bölüştürmede kullanılan, rastgele iş yükü üreten araçlardan bir tanesidir. Verilen iş yükü çizelgesine göre, türdeş olmayan çok işlemcili mikrodevredeki azami haberleşme maliyetini en aza indirgiyoruz. Özellikle haberleşme maliyeti üzerine yoğunlaşmamızın nedeni, çok işlemcili bir mimarideki haberleşme maliyetinin önemli bir gider olmasıdır. Deneysel sonuçlar, bizim yaklaşımımızın, kabul edilebilir bir güç tüketimiyle birlikte toplam haberleşme gecikmesini %27'lere kadar indirgediğini gösteriyor.","With increasing communication demands of processors and memory coresin Systems-on-Chips (SoCs), application-specific and scalable Network-on-Chips(NoCs) are emerged to interconnect processing cores and subsystems in MultiprocessorSystem-on-Chips (MPSoCs). The challenge of application-specific NoCdesign is to find the right balance among different trade-offs such as communicationlatency, power consumption, and chip area.This thesis introduces a novel heterogeneous NoC design approach where biologicallyinspired evolutionary algorithm and 2-dimensional rectangle packingalgorithm are used to place the processing elements with various properties intoa constrained NoC area according to the tasks generated by Task Graph forFree (TGFF). TGFF is one of the pseudo-random task graph generators usedfor scheduling and allocation. Based on a given task graph, we minimize themaximum execution time in a Heterogeneous Chip-Multiprocessor. We specifically emphasize on the communication cost as it is a big overhead in a multi-corearchitecture. Experimental results show that our approach improves total communicationlatency up to 27% with modest power consumption."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İnternet, her türden bilgiye ulaşmak için önemli bir kaynaktır. Teknolojik yenilikler, çok sayıda insanın internette bilgi paylaşabilmesini sağlamıştır. Bu durum, internetteki gereksiz bilgi miktarının artmasına yol açmış ve insanların ihtiyaç duydukları bilgiye ulaşması zorlaşmıştır. İnsanların gereksinim duydukları bilgiye kolayca ulaşabilmeleri için Katılımcı Filtrelemesini Kullanan Tavsiye Sistemleri ortaya çıkmıştır. Bu sistemler, kullanıcıların geçmiş tercihlerini göz önünde bulundurarak onları gruplandırır ve aralarındaki benzerlikleri dikkate alarak ilgili içerikleri onlara önerir. Güvenirlik Tabanlı Sistemler kullanıcı benzerliklerinin yanı sıra onların dürüstlük seviyelerini de hesaba katar. Çünkü bir kullanıcının birkaç konuda güvenilir olması o kişinin her konuda güvenilir olacağı anlamına gelmez. İnternetteki içeriklerin kalitesi de kullanıcılara uygun ve güncel içerikleri sunabilmek için önemlidir. Bu tezde, içerik kalitesi ve kullanıcı kullanıcı güvenirliği kavramlarını bir arada kullanan bir çalışmayı baz aldık. Bu çalışmaya göre, içeriği değerlendiren kişilerin kaliteleri hesaba katılmadan, o içeriğin kalitesi belirlenemez. Çalışmada, içerik ve kullanıcı kalitelerinin bir arada belirlenmesine imkan veren bir Bayes yöntemi kullanılmıştır. Bayes yöntemi, bu kalite değerlerinin zaman içerisindeki değişimlerinin hesaplanmasına da olanak sağlar. Bu tez için, yukarıda bahsedilen modeli geliştirip, kullanıcı ve içerik kalitelerini değerlendirmek için kullanılacak bir yazılım geliştirdik. Bir film seti üzerinde yaptığımız deneyler, modelimizin kullanıcı kalitelerini göz önünde bulundurmayan klasik yöntemden daha iyi sonuç verdiğini göstermiştir. Yöntemimiz, kullanıcıları uzmanlık seviyelerine göre sınıflandırmada da başarılı olmuştur.","The internet provides unlimited access to vast amounts of information. Technical innovations and internet coverage allow more and more people to supply contents for the web. As a result, there is a great deal of material which is either inaccurate or out-of-date, making it increasingly difficult to find relevant and up-to-date content. In order to solve this problem, recommender systems based on collaborative filtering have been introduced. These systems cluster users based on their past preferences, and suggest relevant contents according to user similarities. Trust-based recommender systems consider the trust level of users in addition to their past preferences, since some users may not be trustworthy in certain categories even though they are trustworthy in others. Content quality levels are important in order to present the most current and relevant contents to users. The study presented here is based on a model which combines the concepts of content quality and user trust. According to this model, the quality level of contents cannot be properly determined without considering the quality levels of evaluators. The model uses a Bayesian approach, which allows the simultaneous co-evaluation of evaluators and contents. The Bayesian approach also allows the calculation of the updated quality values over time. In this thesis, the model is further refined and configurable software is implemented in order to assess the qualities of users and contents on the web. Experiments were performed on a movie data set and the results showed that the Bayesian co-evaluation approach performed more effectively than a classical approach which does not consider user qualities. The approach also succeeded in classifying users according to their expertise level."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dokuların patologlar tarafından histopatolojik incelemesinin yapılması, kanser tanı veerecelendirmesinde altın standart olarak kabul edilir. Bu işlemde gözlemcilerin değişkenliköstermesi, tanı sonuçlarında öznelliğe sebep olur. Bu tarz sorunların üstesinden gelebilmekçin, nicel veriler kullanan hesaplamasal teknikler ileri sürülmüştür. Bu teknikler, dokuesimlerinin homojen bölgelerden oluştuğunu varsayarak bu resimlerden matematikselzellikler çıkarır ve resimleri sınıflandırır. Fakat bu varsayım her zaman doğru değildir veınıflandırmadan önce resimlerin bölütlenmesi gerekir. Resimleri bölütlemek için çeşitlieknikler ileri sürülmüştür, fakat bu tekniklerin çoğu imgecikler üzerinde çalışır vegenel resimler için geliştirilmiştir. Son zamanlarda birkaç algoritma doku resimleribölütlemede ıbbi bilgileri kullanmıştır. Bu tekniklerin yüksek seviye özellik tanımlarıçok ümit vericidir. ncak, bu teknikler bölütleme safhalarında, çok kararlı olmayanve yerel çözümlere kaçabilen ölge büyütme yaklaşımını kullanmıştır.Bu tezde, histopatolojik resimlerin bölütlenmesi için yüksek kalite sonuçlar üreten, verimli ve ararlı bir yöntem sunuyoruz. Doku resimlerini bölütlemek için var olanyüksek seviye özellik tanımlarını kullandık. Bölütleme yöntemimiz, bizimle aynı özelliktanımını kullanan diğer yöntemlerin bölütleme başarısını ve kararlılığını önemli derecedearttırıyor. Resim bölütleme problemini bir kümeleme problemi olarak kabul ettik.Kümeleme sonuçlarının kalitesini ve kararlılığını arttırmak için farklı kümeleme sonuçlarınıbir araya getirip birleştirdik. Bu teknik, kümeleme bileşimi olarak da bilinir. Biz ayrıcakümeleme problemini çizge bölümleme problemine dönüştürdük. Birbirinden farklı veyüksek kaliteli kümeleme sonuçları elde etmek için, iyi bilinen çok seviyeli çizgebölümleme tekniği üzerinde değişiklikler ve iyileştirmeler yaptık. Yöntemimiz tıbbi olarak bir anlamı olan nesneleri ayrı bölgelere toplayarak sonuç bölütlemeyi elde eder.Yaptığımız deneyler, önerdiğimiz çok seviyeli kümeleme bileşimi tekniğinin, genel resimlerve doku resimleri için daha önceden önerilmiş bölütleme tekniklerinden çok daha iyisonuçlar ürettiğini gösterdi. Deneylerde kullandığımız doku resimlerinin çoğu resimelde etme aşamasında ortaya çıkan bozulmalar içermesine rağmen, önerdiğimizyöntem yüksek kaliteli sonuçlar üretti.","In cancer diagnosis and grading, histopathological examination of tissues bypathologists is accepted as the gold standard. However, this procedure has observervariability and leads to subjectivity in diagnosis. In order to overcome suchproblems, computational methods which use quantitative measures are proposed.These methods extract mathematical features from tissue images assuming theyare composed of homogeneous regions and classify images. This assumption isnot always true and segmentation of images before classification is necessary.There are methods to segment images but most of them are proposed for genericimages and work on the pixel-level. Recently few algorithms incorporated medicalbackground knowledge into segmentation. Their high level feature definitionsare very promising. However, in the segmentation step, they use region growingapproaches which are not very stable and may lead to local optima.In this thesis, we present an efficient and stable method for the segmentationof histopathological images which produces high quality results. We use existinghigh level feature definitions to segment tissue images. Our segmentation methodsignificantly improves the segmentation accuracy and stability, compared to existingmethods which use the same feature definition. We tackle image segmentationproblem as a clustering problem. To improve the quality and the stabilityof the clustering results, we combine different clustering solutions. This approachis also known as cluster ensembles. We formulate the clustering problem as agraph partitioning problem. In order to obtain diverse and high quality clusteringresults quickly, we made modifications and improvements on the well-knownmultilevel graph partitioning scheme. Our method clusters medically meaningfulcomponents in tissue images into regions and obtains the final segmentation.Experiments showed that our multilevel cluster ensembling approach performedsignificantly better than existing segmentation algorithms used for genericand tissue images. Although most of the images used in experiments, containnoise and artifacts, the proposed algorithm produced high quality results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Doğru kanser tanısı ve derecelendirilmesi, etkili bir tedavi planı için önemlidir. Ancak, biyopsi görüntüleri üzerinde yapılan kanser tanısı, patologların görsel olarak yorumlamasına dayanır, bu ise öznellik taşır. Bu öznellik, etkili olmayan tedavi planlarının uygulanmasına yol açabilir. Tanıdaki öznelliği azaltmak amacıyla, ölçülebilir değerler üzerinden otomatik kanser tanısı ve derecelendirmesi yapan sistemler önerilmiştir. Biyopsi görüntülerinde varolan değişim bu tür sistemlerin tasarlanmasında en büyük sorunlardan birini oluşturur. Bu tür sistemlerin biyopsi görüntülerini doğru sınıflandırabilmesi için fazla sayıda öğrenme örneği ile eğitilmesi gerekir. Ancak, histopatolojik görüntü alanındaki öğrenme kümeleri, görüntü toplama ve bu görüntüleri etiketlemedeki zorluklardan ötürü genelde az sayıda örnek içerir. Biz bu çalışmamızda bu probleme karşı, öğrenme kümesindeki örnek sayısını ve varyansını artırarak sınıflandırıcının genelleme kapasitesini artıran, yeni bir tekrar örnekleme yöntemi sunmaktayız. Bunu yapabilmek için, görüntü üzerinden her biri, görüntünün değiştirilmiş örneğine denk gelen diziler oluşturulur. Bu değiştirilmiş örneklerin her biri, görüntü üzerinde değişik alt bölgeleri nitelendirir ve dolayısıyla birbirinden farklıdır. Bu örneklerin öğrenmede kullanılması ise öğrenme kümesinin büyüklüğünü ve varyansını artırır. Markov modeller ile bu örnekler modellenir ve etiketlenmemiş örneklerin sınıflandırılmasında kullanılır. Histopatolojik görüntüler üzerinde yapılan testlerde, sunulan bu yöntemin hem büyük hem de küçük boyutlu öğrenme kümelerinde diğer yöntemlere göre daha başarılı olduğu görülmektedir. Ayrıca değiştirilmiş örneklerin oylama yönteminde kullanılması sınıflandırıcının performansını artırmaktadır.","Correct diagnosis and grading of cancer is very crucial for planning an effective treatment. However, cancer diagnosis on biopsy images involves visual interpretation of a pathologist, which is highly subjective. This subjectivity may, however, lead to selecting suboptimal treatment plans. In order to circumvent this problem, it has been proposed to use automatic diagnosis and grading systems that help decrease the subjectivity levels by providing quantitative measures. However, one major challenge for designing these systems is the existence of high variance observed in the biopsy images due to the nature of biopsies. Thus, for successful classifications of unseen images, these systems should be trained with a large number of labeled images. However, most of the training sets in this domain have limited size of labeled data since it is quite difficult to collect and label histopathological images. In this thesis, we successfully address this issue by presenting a new resampling framework. This framework relies on increasing the generalization capacity of a classifier by augmenting the size and variation in the training set. To this end, we generate multiple sequences from an image, each of which corresponds to a perturbed sample of the image. Each perturbed sample characterizes different parts of the image, and hence, they are slightly different from each other. The use of these perturbed samples for representing the image increases the size and variability of the training set. These samples are modeled with Markov processes which are used to classify unseen image. Working with histopathological tissue images, our experiments demonstrate that the proposed framework is more effective for both larger and smaller training sets compared against other approaches. Additionally, they show that the use of perturbed samples is effective in a voting scheme which boosts the performance of the classifier."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezin ana kapsamı, robotik planlama problemleri için, odaklanma, kaynak yöönetimi ve kısıtlamaların da dahil edilerek, sezgisel doğrusal mantıkta, hedefe yönelik bir teorem ispatlama çatısı oluşturmak. Bu amaçla, hedefe yöönelik formöulasyon şekli, uygulama ve test aşamasında daha anlaşılır bir içerik sunmaktadır. Bununla beraber, mevcut hedefe yöonelik teorem ispatlayıcılar, ispat aramada ya etkili bir yööntem sunamamaktadırlar ya da kullandıkları dili Doğgrusal Hereditary Harrop Formöülleri gibi daha köücük parçalara kısıtlayarak etkili bir yööntem sağlayabilmektedirler. Bahsedilen yaklaşımlardan ilki, sonuç sisteminin öölçeklenebilirliğine öönemli derecede zarar verdiği için uygun değildir. İkinci bahsedilen teorem ispatlama yaklaşımlarında ise öölçeklenebilirlik konusu çözüulebilir fakat ifade edebileceği dili kısıtlar ve belirli olmayan planlama elemanlarını ele alamayabilir. Bu tezde tanımladığımız ispatlama teorisi, robotik planlama problemlerindeki dinamik durum elemanlarının ifade edilmesinde, doğrusallığın ve söürekli kısıtlamaların etkili bir biçimde kullanılmasını sağlıyor. Bu amaçla, tanımladığımız sistemin SWI-Prolog dilinde bir uygulamasını gerçekleştirdik. Bu uygulamaya kısıtlamaları da dahil ederek sistemi genişlettik. Sistemimizin ifade gücünü ve verimliliğini, bazı robot planlama öörnekleri vererek destekledik.","The main scope of this thesis is implementing a backwards theorem prover with focusing, resource management and constraints within the intuitionistic rst-order linear logic for robotic planning problems. To this end, backwards formulations provide a simpler context for experimentation. However, existing backward theorem provers are either implemented without regard to the eficiency of the proofsearch,or when they do, restrict the language to smaller fragments such as Linear Hereditary Harrop Formulas (LHHF). The former approach is unsuitable since it signicantly impairs the scalability of the resulting system. The latter family of theorem provers address the scalability issue but impact the expressivity of the resulting language and may not be able to deal with certain non-deterministic planning elements. The proof theory we describe in this thesis enables us to efectively experiment with the use of linearity and continuous constraints to encode dynamic state elements characteristic of robotic planning problems. Tothis end, we describe a prototype implementation of our system in SWI-Prolog, and also incorporate continuous constraints into the prototype implementation of the system. We support the expressivity and eficiency of our system with some examples."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışma sıvı ve kumaş benzeri, biçimi bozulabilen nesnelerin fizik tabanlı yöntemler kullanılarak modellenmesi ve benzetimiyle ilgilidir. Benzetimi yapılan nesneler parçacıklar kullanılarak modellenmektedir ve nesnelerin birbirleriyle ve çevreleri ile olan etkileşimleri parçacıklar arasındaki kuvvetlerle tanımlanmaktadır. Çalışmada, varolan teknikler üzerine bir çok iyileştirme önerilmektedir. Komşu parçacıkların doğru ve hızlı bir şekilde bulunması parçacık tabanlı benzetim sistemleri için çok önemlidir. Bu çalışmada, paralel hesaplamaya uygun, birbiçimli ızgara üzerinde çalışan, sıralama tabanlı komşu bulma yöntemi önerilmektedir. Sıvı yüzeyinin oluşturulması için önerilen yöntem, varolan yöntemlerden daha ayrıntılı bir yüzey oluşturmaktadır. Bunun sebebi parçacıkların sıvı yüzeyine göreceli konumlarının dikkate alınmasıdır. Sıvı-sıvı, sıvı-kumaş ve sıvı-sınır etkileşimlerini tanımlamak üzere hesaplamalı fizikte de kullanılmakta olan bir çok yöntem araştırılmıştır. Ayrıca örgü tipindeki kumaşları ve bunların sıvılarla etkileşimini benzetmek amacıyla kullanışlı bir yöntem de önerilmektedir. Sıvıların örgüler tarafından emilmesi yüzey gerilimlerini kullanan bir yöntemle benzetilmektedir. Önerilen parçacık sistemi birbirinekarışabilen sıvıların benzetmesini de yapabilmektedir. Bu çalışmada anlatılan parçacık tabanlı benzetme yöntemleri, günümüzde yaygın hale gelen paralel bilgisayarlarda (çok çekirdekli işlemciler ve grafik işlemciler gibi) çalışabilecek şekilde uygulanmıştır. Deneyler önerdiğimiz yöntemin işlemsel olarak verimli olduğunu ve gerçekçi sonuçlar ürettiğini göstermektedir.","This thesis is about modeling and simulation of fluids and cloth-like deformable objects by the physically-based simulation paradigm. Simulated objects are modeled with particles and their interaction with each other and the environment is defined by particle-to-particle forces. We propose several improvements over the existing particle simulation techniques. Neighbor search algorithms are crucialfor the performance efficiency and robustness of a particle system. We present a sorting-based neighbor search method which operates on a uniform grid, and can be parallelizable. We improve upon the existing fluid surface generation methods so that our method captures surface details better since we consider the relative position of fluid particles to the fluid surface. We investigate several alternatives of particle interaction schema (i.e. Smoothed Particle Hydrodynamics, the Discrete Element Method, and Lennard-Jones potential) for the purpose of defining fluid-fluid, fluid-cloth, fluid-boundary interaction forces. We also propose a practical way to simulate knitwear and its interaction with fluids. We employ capillary pressure?based forces to simulate the absorption of fluid particles by knitwear.We also propose a method to simulate the flow of miscible fluids. Our particle simulation system is implement to exploit parallel computing capabilities of the commodity computers. Specifically, we implemented the proposed methods on multicore CPUs and programmable graphics boards. The experiments show that our method is computationally efficient and produces realistic results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Serviks kanseri, erken teşhis ile tedavi edilerek önlenebilmektedir. Pap smear testi, rahim ağzında meydana gelen kanser ve kanser öncüsü değişiklikleri belirlemek üzere uygulanan manüel bir tarama yöntemidir. Ancak bu yöntem gözlemci tutarsızlığı ve her bir test için harcanması gereken çaba gibi dezavantajlar içermektedir. Bilgisayar destekli bir tarama sistemi, başarılı bir algoritma ile serviks kanserinin önlenmesinde yararlı olacaktır.Bu tezde, verilen bir Pap test lamında yer alan hücreleri anormallik derecesine göre sıralayarak sitologlara yardımcı olacak bilgisayar destekli tanılayıcı bir sistem önerilmektedir. Böyle bir sitemi oluşturan üç temel bileşen vardır. İlk başta, hücreler ve çekirdekleri mikroskop kullanılarak elde edilen bir görüntü üzerinde bir bölütleme yöntemi yardımıyla tespit edilir. Sonra, bölütlenmiş olan hücreleri betimleyen özellikler çıkarılır. En sonunda, hücreler çıkarılan özellikler temel alınarak anormallik derecesine göre sıralanır.Bir tek serviks hücresi içeren görüntüleri gerektiren ilgili çalışmalardan farklı olarak, örtüşen hücrelerin görüntülerini de işleyebilen parametrik olmayan genel bir bölütleme algoritması önerilmektedir. İlk aşama olarak, arka plan alanlarını çıkararak geriye kalan hücre alanlarını elde etmek amacıyla eşikleme yöntemi kullanılmıştır. İkinci aşama, spektral, şekil ve gradyan bilgisinden faydalanan parametrik olmayan hiyerarşik bir bölütleme yöntemi ile hücre alanlarının bölütlenmesinden oluşmaktadır. Son aşama, elde edilen bölütleri çekirdek ya da sitoplazma olarak sınıflandırmak suretiyle hücre alanını her bir çekirdeğe ait doğru yapılara ve bütün sitoplazma alanına ayırmayı amaçlamaktadır. Önerilen bölütleme yöntemi iki farklı veri kümesi kullanılarak nicel ve nitel olarak değerlendirilmiştir.Öğreticisiz bir tarama sistemi önerilerek, sınıflandırma üzerine yoğunlaşmış ilgili çalışmalara göre probleme farklı bir yönden yaklaşmak amaçlanmıştır. Bir Pap lamında yer alan hücreleri sıralamak için, ilk önce, hücrelerden çıkarılan 14 farklı özelliğe göre hiyerarşik kümeleme uygulanmıştır. Hücrelerin ilk sıralaması oluşturulan hiyerarşik ağacın yaprak sıralaması olarak belirlenmiştir. Sonra, bu ilk sıralama bir en iyi yaprak sıralama algoritması ile iyileştirilmiştir. Referans veri kullanılarak yapılan deneyler önerilen yaklaşımın etkinliğini farklı deneysel ayarlar altında göstermektedir.","Cervical cancer can be prevented if it is detected and treated early. Pap smear test is a manual screening procedure used to detect cervical cancer and precancerous changes in an uterine cervix. However, this procedure is costly and it may result in inaccurate diagnoses due to human error like intra- and inter-observer variability. Therefore, a computer-assisted screening system will be very beneficial to prevent cervical cancer if it increases the reliability of diagnoses.In this thesis, we propose a computer-assisted diagnosis system which helps cyto-technicians by sorting cells in a Pap smear slide according to their abnormality degree. There are three main components of such a system. Firstly, cells along with their nuclei are located using a segmentation procedure on an image taken using a microscope. Then, features describing these segmented cells are extracted. Finally, the cells are sorted according to their abnormality degree based on the extracted features.Different from the related studies that require images of a single cervical cell, we propose a non-parametric generic segmentation algorithm that can also handle images of overlapping cells. We use thresholding as the first phase to extract background regions for obtaining remaining cell regions. The second phase consists of segmenting the cell regions by a non-parametric hierarchical segmentation algorithm that uses the spectral and shape information as well as the gradient information. The last phase aims to partition the cell region into true structures of each nucleus and the whole cytoplasm area by classifying the final segments as nucleus or cytoplasm region. We evaluate our segmentation method both quantitatively and qualitatively using two data sets.By proposing an unsupervised screening system, we aim to approach the problem in a different way when compared to the related studies that concentrate on classification. In order to rank the cells in a Pap slide, we first perform hierarchical clustering on 14 different cell features. The initial ordering of the cells is determined as the leaf ordering of the constructed hierarchical tree. Then, this initial ordering is improved by applying an optimal leaf ordering algorithm. The experiments with ground truth data show the effectiveness of the proposed approach under different experimental settings."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gizlilik paylaşım şemalari bir takım katılımcılar arasında gizli olan bir değeri sadece bazı koalisyonların bulabileceği şekilde dağıtma yöntemidir.Bu çalışmada, birçok erişim yapılarını inceledik. Alternatifli hiyerarşik erişim yapıları için ideal ve mükemmel bir çözüm önerdik. Kompartmanlı erişim yapıları ve birleşik hiyerarşik erişim yapılarını da özel durum olarak içine alan kesişebilir kompartmanlı erişim yapılarını tanımladık, ve bu yapılar için ideal ve mükemmel bir paylaşım şeması önerdik. Son olarak da alternatif bir eşik değer gizlilik paylaşım şeması önerdik ve bu şema ile alternatifli hiyerarşik erişim yapılarına yönelik başka bir şema tasarladık.","A secret sharing scheme is a method of assigning shares for a secret to some participants such that only authorized coalitions of these participants can recover the secret.In this work, we study several access structure types: we give an ideal perfect secret sharing scheme for disjunctive multilevel access structures. We introduce joint compartmented access structures, which covers compartmented access structures and conjunctive hierarchical access structures as special cases. We provide an almost surely perfect scheme for those joint compartmented access structures that can be realized by an ideal perfect secret sharing scheme. Lastly, we suggest an alternative threshold secret sharing scheme, and we use this scheme to construct a disjunctive multilevel secret sharing scheme."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Taşınabilir cihazları kullanmak hayatımızın kaçınılmaz bir parçasıdır. Onları kullanmanın en kaçınılmaz kısmı ise onlarla iletişimde bulunmaktır. Bugünün kullanıcı ara yüzleri çoğunlukla masa üstü cihazlardan esinlenilerek tasarlanmıştır.Bunun sonucu olaraksa taşınabilir cihazlarla iletişim kurmak zorlaşmıştır. Ancak işlemci gücü, sensörler ve kameralar gibi eklentiler çoktan bu cihazlarda yerlerini almışlardır. Bunlar bize standardın dışında yeni iletişim kurma yöntemleri için olanak sağlamaktadır. Bu tez, parmak tan_ma, parmak takibi ve nesne hareketleri konusunda bazı yeni yaklaşımlar sunarak, mobil cihazlar ile etkileşimin daha iyi geliştirilebileceğini öne süren bir çalışmadır.Tezimin sonucu olarak, taşınabilir cihazlar ile kullanıcı arasında yeni bir iletişim yöntemi tasarlanmıştır. Bu, taşınabilir cihazlarla iletişim kurmanın tamamen yeni bir yöntemidir. Bu, nesneleri doğrudan değiştirme imkanı vermektedir. Sistem bu cihazlarda bulunan donanımlara ilave bir donanım gerektirmemektedir. Etkileşim yöntemi, kameranın önünde hareket eden bir nesnenin hareketini (bir parmak veya önceden tanımlanmış bir işaretçinin hareketini) sanal bir ortama yansıtarak nesneleri doğrudan değiştirme imkanı vermektedir.Parmak tanımlama için, taşınabilir cihazların ve baş parmağın yapısına uygun olarak yeni bir yöntem geliştirilmiştir. Problemi çözmek için iki boyutlu, hızlı, renk tabanlı bir sahne analizi yöntemi uygulanmıştır.Parmak takibi için, taşınabilir cihaz elde tutulurken baş parmağın hareket yapısına uygun yeni bir yöntem geliştirilmiştir. İki boyutlu Kırmızı-Yeşil-Mavi (KYM) renk verisinden üç boyutlu hareket verisinin çıkarılması çalışmanın bu kısmı için önemli bir bölümdür.","Using handheld devices is a very important part of our daily life. Interacting with them is the most unavoidable part of using them. Today's user interface designs are mostly adapted from desktop computers. The result of this was difficulties of using handheld devices. However processing power, new sensing technologies and cameras are already available for mobile devices. This gives us the possibility to develop systems to communicate through different modalities. This thesis proposes some novel approaches, including finger detection, finger tracking and object motion analysis, to allow efficient interaction with mobile devices.As the result of my thesis, a new interface between users and mobile devices is created. This is a new way of interaction with the mobile device. It enables direct manipulation on objects. The technique does not require any extra hardware. The interaction method, maps an object's motion (such as a finger's or a predefined marker's motion) to a virtual space to achieve manipulation which is moving in front of the camera.For Finger Detection, a new method is created based on the usage of the mobile devices and structure of thumb. A fast two dimensional color-based scene analysis method is applied to solve the problem.For Finger Tracking, a new method is created based on the movement ergonomics of thumb when holding the mobile device on hand. Extracting the three dimensional movement from the two dimensional RGB data is an important part of this section of the study.A new 3D pointer data and pointer image is created for usage with 3D input and 3D interaction of 3D scenes. Also direct manipulation for low cost is achieved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Üç boyutlu elektromanyetik saçılım ve ışınım problemlerinin çalışılmasında yoğun doğrusal sistemlere yol açan ayrıklaştırılmış yüzey integral denklemlerini çözmek yaygın bir yöntemdir. Çözümün karmaşıklığının azalmasından dolayı, bu doğrusal denklemlerin Krylov altuzayı ve çok seviyeli hızlı çokkutup (ÇSHÇY) yöntemleri kullanılarak iteratif çözümü son derece çekici hale gelmiştir. Fakat bu yaklaşım sadece yakınsama için gereken iterasyon sayısı aşırı derecede yüksek olmadığı sürece işe yaramaktadır. Maalesef, pek çok pratik durumda bu geçerli olmamaktadır. Özellikle, açık yüzey ve karmaşık gerçek hayat problemleri kötü koşullu doğrusal sistemlere yol açmaktadır. Bu tarz problemlerin iteratif çözümleri, kabaca sistem matrislerine yaklaşan tersi alınabilir matrisler olarak tanımlanan öniyileştiriciler olmadan mümkün olmamaktadır.Bu doktora tezinde, büyük ölçekli yüzey integral denklemi problemleri için geliştirdiğimiz etkin öniyileştiricileri sunmaktayız. İlk olarak, en yaygın ve oturmuş bir öniyileştirme yöntemi olan eksik LU (ELU) öniyileştirmesini ele aldık. Bu öniyileştiricilerin nasıl bir kara kutu formunda ve güvenli olarak kullanılabileceklerini gösterdik. Önemli avantajlarına rağmen, ELU öniyileştiricileri temel olarak sıralı bir yapıda oldukları için, paralel çözümlerde kullanılmak üzere bir seyrek yaklaşık ters (SYT) öniyileştiricisi geliştirdik. Paralel ölçeklenebilirlik için önemli olan özgün bir yük dengeleme yöntemi geliştirdik. Daha sonra SYT öniyileştiricilerini, yoğun sistemi bir iç-dış çözümü şeklinde öniyileştiren yakın alan matris sisteminin iteratif çözümünde kullanarak geliştirdik. Mükemmel iletkenler için geliştirdiğimiz son öniyileştirici, benzer bir iç-dış çözümü kullanmakta, ama iç çözümler için ÇSHÇY'nin yaklaşık bir versiyonunu kullanmaktadır. Bu yolla, helikopterler ve metamalzemeler içeren çok sayıda karmaşık gerçek hayat problemini makul iterasyon sayılarında çözmeyi başardık.Son olarak, diyelektrik problemlerinin ayrıklaştırılmasından elde edilen doğrusal sistemlerin öniyileştirilmelerini hedefledik. Mükemmel iletkenlerden farklı olarak, bu sistemler bölünmüş yapıdadırlar.Schur tümleyenine indirgemeyle bu bölünmüş yapıyı öniyileştirme için kullandık. Bu yolla, diyelektrik fotonik kristaller gibi, çözümü zor gerçek hayat problemlerinin makul sürelerde çözümünü mümkün kılan etkin öniyileştiricilerin geliştirilmesi mümkün olmuştur.","A popular method to study electromagnetic scattering and radiation of threedimensionalelectromagnetics problems is to solve discretized surface integralequations, which give rise to dense linear systems. Iterative solution of suchlinear systems using Krylov subspace iterative methods and the multilevel fastmultipole algorithm (MLFMA) has been a very attractive approach for largeproblems because of the reduced complexity of the solution. This scheme workswell, however, only if the number of iterations required for convergence of theiterative solver is not too high. Unfortunately, this is not the case for manypractical problems. In particular, discretizations of open-surface problems andcomplex real-life targets yield ill-conditioned linear systems. The iterative solutionsof such problems are not tractable without preconditioners, which can beroughly defined as easily invertible approximations of the system matrices.In this dissertation, we present our efforts to design effective preconditioners forlarge-scale surface-integral-equation problems. We first address incomplete LU(ILU) preconditioning, which is the most commonly used and well-establishedpreconditioning method. We show how to use these preconditioners in a blackboxform and safe manner. Despite their important advantages, ILU preconditionersare inherently sequential. Hence, for parallel solutions, a sparseapproximate-inverse (SAI) preconditioner has been developed. We propose anovel load-balancing scheme for SAI, which is crucial for parallel scalability.Then, we improve the performance of the SAI preconditioner by using it for theiterative solution of the near-field matrix system, which is used to preconditionthe dense linear system in an inner-outer solution scheme. The last preconditionerwe develop for perfectly-electric-conductor (PEC) problems uses the sameinner-outer solution scheme, but employs an approximate version of MLFMA forinner solutions. In this way, we succeed to solve many complex real-life problemsincluding helicopters and metamaterial structures with moderate iteration countsand short solution times. Finally, we consider preconditioning of linear systemsobtained from the discretization of dielectric problems. Unlike the PEC case,those linear systems are in a partitioned structure. We exploit the partitionedstructure for preconditioning by employing Schur complement reduction. In thisway, we develop effective preconditioners, which render the solution of difficultreal-life problems solvable, such as dielectric photonic crystals."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar grafiği alanında, akışkanların davranışlarının simülasyonu önemli bir problemdir. Çok miktarda bulunan ve artmaya devam eden animasyonlar, bu fenomeni daha önemli bir problem haline getirmiştir. Bir çok araştırmacı, bu konuyla ilgili çözümler sunmuşlardır. Fakat, son zamanlarda, hızlı ve kolay progranabilir akışkan simülasyonlarına olan ihtiyacın artması, araştırmacıları daha çabuk ve istikrarlı çözümler bulmaya itmiştir.Bu tez araştırmasında, Navier-Stokes denklemlerini Langrange ve örtük metodlar kullanarak çözen, kolay programlanabilir, gerçek zamanlı bir duman simülasyonu sunulmaktadır. Araştırma, çözümün kendisine odaklandığı gibi, Navier-Stokes denklemlerini ve bu denklemlerin nasıl derive edildiklerini anlatarak akışkan mekaniği ile ilgili de kapsamlı bir bilgi sunmaktadır. Çözüm duman simülasyonu için verilmesine rağmen, diğer akışkanlar için de uygulanabilir. Simülasyonun diğer bir özelliği ise kullanıcının isteğine göre 2 ve 3 boyutlu uzaya uyumlu olabilmesidir.","Realistic simulation of fluid-like behaviour is an important and challenging problem in computer graphics. Huge and increasing amount of animations has made this phenomena even more important. Although many scientists provided solutions regarding this issue, recently, the need of fast and easy implemented fluid simulations has directed researches to focus on quick and stable solutions.This thesis presents an unconditionally stable, easy implemented real-time smoke simulation, solving Navier-Stokes equations with Lagrangian and implicit methods. The study focuses on the comprehension of fluid dynamics as much as the solution, by providing background information about Navier-Stokes equations, how they are derived and used. While the proposed solution is applied only to create a simulation for smoke like behaviour, it is highly adaptive for other fluids as well. One important aspect of the simulation is being suitable for 2 and 3 dimensions, giving the flexibility to the animator to choose in between."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Üç boyutlu bir sanal ortamda gerçekleşen animasyonları görüntülemek üzere kamera yerleştirmek zor bir iştir. Uzayda bir nesnenin konumunu ve yönelimini doğru bir şekilde ayarlamak, ve dahası bu nesneyi sahnedeki olayları takip edecek şekilde hareket ettirmek uzmanlık gerektirir.Literatürde bu eylemi farklı derecelerde otomatik hale getirmeye yönelik öneriler getirilmiştir. Bazı yaklaşımlar kullanıcının sürekli kamera yerleşimiyle ilgilenmesini gerektirirken, bazıları da kullanıcının kamera yerleşimini yönlendirmesine hiç imkan tanımamaktadır.Bu çalışmada otomatik kamera yerleştirme işlevini gerçekleştirmek için ""Görev"" (Task) adını verdiğimiz yeni bir soyutlama öneriyoruz. Görevler kullanıcının geometrik detaylarla ilgilenmeden kolayca kamera yerleşimini yönlendirmelerini sağlayarak kullanım kolaylığı ve çıktı üzerinde kontrol imkanı arasında bir denge kurmaktadırlar. Kullanıcılar üç boyutlu animasyonların görselleştirildiği videolar hazırlarken görevleri kullanarak nesneler, nesneler arasındaki ilişkiler ve izleyici üzerinde bırakılan izlenimler gibi anlaşılır kavramlar üzerinden kamera yerleşimini yönetebilirler.Burada önerilen otomatik kamera yerleşimi altyapısı farklı görevlerden gelen talepleri bağdaştırır ve görevlerin ortak kullanabileceği geometrik hesaplama işlevleri sunar. Altyapının esnekliği ve genişletilebilirliği sistemin çok çeşitli üç boyutlu sahnelerde kullanılmasını ve çıktı olarak elde edilen videoların da görsel olarak çeşitlilik sunmasını mümkün kılar.","Placing cameras to view an animation that takes place in a virtual 3D environment is a difficult task. Correctly placing an object in space and orienting it, and furthermore, animating it to follow the action in the scene is an activity that requires considerable expertise.Approaches to automating this activity to various degrees have been proposed in the literature. Some of these approaches have constricted assumptions about the nature of the animation and the scene they visualize, therefore they can be used only under limited conditions. While some approaches require a lot of attention from the user, others fail to give the user sufficient means to affect the camera placement.We propose a novel abstraction called ""Task"" for implementing camera placement functionality. Tasks strike a balance between ease of use and ability to control the output by enabling users to easily guide camera placement without dealing with low-level geometric constructs. Users can utilize tasks to control camera placement in terms of high-level, understandable notions like objects, their relations, and impressions on viewers while designing video presentations of 3D animations.Our framework of camera placement automation reconciles the demands brought by different tasks, and provides tasks with common low-level geometric foundations. The flexibility and extensibility of the framework facilitates its use with diverse 3D scenes and visual variety in its output."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, yamaç aşağı kayan akışkan çığı ayrık öğeler yöntemini ve moleküler dinamik kurallarını uygulayarak bir modelleme ve benzetim yaklaşımı sunar. Benzetim için temel olarak parçacık sistemi, görsel oluşum için ?Marching Cubes? ve gölgelendirme betikleri kullanılmıştır. Parçacıklar arası ve parçacık ile yer yüzeyi arasında etkileşimleri hızlı bir şekilde modellemek için sabit kafes tabanlı komşu belirleme algoritması kullanılmıştır. Çakışma çözümlemesinde karın sıkışabilirliğini benzetmek için kitle-yay modellemesi ve parçacıklar arası çekim kuvveti uygulanmıştır. Çakışma çözümlemesinde yüksek performans elde etmek için genel amaçlı grafik ünitesi üzerine yazılan diller ve çok çekirdekli işlemcilerden yararlanılarak programlama yapılmıştır. Elde edilen sonuçlar, gerçekçi senaryoları hesaba katarak, farklı gösterim tekniklerinin bir birleşimi halinde görüntülenmiştir.","This thesis presents an approach for modeling and simulation of a flowing snow avalanche, which is formed of dry and liquefied snow that slides down a slope, by using molecular dynamics and discrete element method. A particle system is utilized as a base method for the simulation and marching cubes with real-time shaders are employed for rendering. A uniform grid based neighbor search algorithm is used for collision detection for inter-particle and particle-terrain interactions. A mass-spring model of collision resolution is employed to mimic compressibility of snow and particle attraction forces are put into use between particles and terrain surface. In order to achieve greater performance, general purpose GPU language and multi-threaded programming is utilized for collision detection and resolution. The results are displayed with different combinations of rendering methods for the realistic representation of the flowing avalanche."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hiperçizge bölümleme son zamanlarda dağıtık veri erişimi ve uzamsal veri tabanlarında iletişim ve disk erişim maliyetlerini doğru bir sekilde yakalamak için kullanılmıştır. Bu alanlardaki hiperçizge modellerinde, hiperçizge bölümleme kullanılarak elde edilen bölümlerin kalitesi hedeflenen problemin objektifi için çok önemli olabilir. Çoklama, dağıtık veri erişimi ve veri tabanı sistemlerinde çeşitli performans meselelerini ele almak için yaygın olarak kullanılan bir terminolojidir. Çoklamanın arkasındaki ana motivasyon, hedeflenen konunun performansını daha fazla alan kullanma pahasına geliştirmektir.Bu çalışmada, hiperçizge bölümlemenin kalitesini düğüm çoklamasıyla geliştiren hiperçizge bölümleme şemalarının üstüne odaklanıyoruz. Bu aşamada, çoklama ve bölümlemenin bir arada yapıldığı bir çoklamalı hiperçizge bölümleme şeması öneriyoruz. Yaklaşımımız, hiperçizge bölümlemesi için başarılı çok seviyeli ve özyinelemeli ikiye bölümleme yöntemlerini kullanmaktadır. Çoklama, çok seviyeli yöntemin açılma safhasında verimli Fiduccia-Mattheyses (FM) yinelemeli geliştirme sezgiselini genişleterek elde edilmektedir. Bu genişletilmiş versiyona çoklamalı FM (rFM) diyoruz. Önerilen rFM sezgiseli yeni algoritmalar ve köşe durumları öne sürerek taşıma, çoklama ve azlama işlemlerini desteklemektedir. Önerilen çoklama şemasını çok seviyeli hiperçizge bölümleme aracı PaToH'a entegre edip çeşitli gerçekçi veri takımları üstünde test ediyoruz.","Hypergraph partitioning is recently used in distributed information retrieval (IR)and spatial databases to correctly capture the communication and disk accesscosts. In the hypergraph models for these areas, the quality of the partitionsobtained using hypergraph partitioning can be crucial for the objective of thetargeted problem. Replication is a widely used terminology to address differentperformance issues in distributed IR and database systems. The main motivationbehind replication is to improve the performance of the targeted issue at the costof using more space.In this work, we focus on replicated hypergraph partitioning schemes that im-prove the quality of hypergraph partitioning by vertex replication. To this end,we propose a replicated partitioning scheme where replication and partitioningare performed in conjunction. Our approach utilizes successful multilevel andrecursive bipartitioning methodologies for hypergraph partitioning. The repli-cation is achieved in the uncoarsening phase of the multilevel methodology byextending the efficient Fiduccia-Mattheyses (FM) iterative improvement heuris-tic. We call this extended heuristic replicated FM (rFM). The proposed rFMheuristic supports move, replication and unreplication operations on the verticesby introducing new algorithms and vertex states. We show rFM has the samecomplexity as FM and integrate the proposed replication scheme into the mul-tilevel hypergraph partitioning tool PaToH. We test the proposed replicationscheme on realistic datasets and obtain promising results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz Sensör Ağlar, mekansal olarak dağıtılan, enerji kısıtlamaları olan ve bünyesindeki sensörleri kullanarak işbirliği içinde farklı konumlardaki sıcaklık, ses, titreşim veya çevre kirliliği gibi fiziksel ve çevresel koşulları gözlemleyen otonom cihazlardan oluşmaktadır. Bu sensör düğümlerinin kısıtlı enerji kaynaklarına sahip olması nedeniyle, sensör ağlarında enerji verimliliği hassas bir tasarım meselesidir. Bütün düğümlerin eşzamanlı olarak aktif modunda çalışması, ağdaki yüksek yoğunluk dolayısıyla, aşırı nerji tüketimi ve paket çarpışmaları ile sonuçlanmaktadır. Enerji tüketimini azaltmak ve ağ ömrünü uzatmak için, bu tez, sensör düğümleri aktif sensör düğümü setleri şeklinde düzenlemek için merkezi bir çizge bölümleme yaklaşımı sunmaktadır. Şöyle ki, algılama ve haberleşme görevlerini başarılı olarak gerçekleştirmek için, her bir aktif set, istenilen seviyede algılama kapsaması sağlamakta ve bağlı bir ağ oluşturmaktadır. Önerdiğimiz yöntemi, ağ ömrü ve çalışma zamanı açısından farklı ağ topolojileri ve parametreleri altında simülasyonlar aracılığıyla değerlendirdik ve farklı aktif düğüm setleri seçme mekanizmalarıyla karşılaştırıldığında elde edilen aktif düğüm setleri sayısında yaklaşık olarak 50% iyileşme gözlemledik.","Wireless Sensor Networks consist of spatially distributed and energy-constrained autonomous devices called sensors to cooperatively monitor physical or environmental conditions such as temperature, sound, vibration, pressure or pollutants at different locations. Because these sensor nodes have limited energy supply, energy efficiency is a critical design issue in wireless sensor networks. Having all the nodes simultaneously work in the active mode, results in an excessive energy consumption and packet collisions because of high node density in the network. In order to minimize energy consumption and extend network life-time, this thesis presents a centralized graph partitioning approach to organize the sensor nodes into a number of active sensor node sets such that each active set maintains the desired level of sensing coverage and forms a connected network to perform sensing and communication tasks successfully. We evaluate our proposed scheme via simulations under different network topologies and parameters in terms of network lifetime and run-time efficiency and observe approximately 50% improvement in the number of obtained active node sets when compared with different active node set selection mechanisms."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Grafik İşleme Birimi (GPU) birincil olarak gerçek zamanlı görüntü oluşturmak için tasarlanmıştır. Karmaşık komut kümesi ve sınırlı ardışık düzene sahip merkezi işlem biriminin aksine GPU daha basit bir komut kümesine ve vektör verilerini koşut olarak çalıştırabilecek çok sayıda yürütme ardışık düzenine sahiptir. Olağan görevlerine ek olarak, GPU komut kümesi başka tip genel amaçlı hesaplamalar için kullanılabilir. GPU'ların işlem gücünü genel amaçlı hesaplamalarda değerlendirebilmek için Brook+, ATI CAL, OpenCL ve Nvidia Cuda gibi değişik programlama çerçeve modelleri önerilmiştir. Bu durum pek çok uygulamanın hızlandırılması için fırsat doğurmuştur.Bu çalışmada metrik tabanlı benzerlik araması alanında grafik kartlarının sağladığı avantajların kullanılması incelenmektedir. Sıkça ""imleç takibi"" gerektiren ağaç temelli yapıların aksine, KVP yapısı basit organizasyonu nedeniyle kolayca koşut olarak işlenmeye uygundur. ATI platformunda değişik genel amaçlı GPU programlama çerçeve modelleri kullanılarak (Brook+, ATI CAL ve OpenCL) Brute Force Linear Scan ve KVP algoritmaları gerçekleştirilmiş, yapılan çalışma sunulmuştur. Bu gerçekleştirimlerin deneysel sonuçları GPU uygulamalarının CPU sürümlerinden çok daha hızlı olduğunu göstermektedir.","A Graphic Processing Unit (GPU) is primarily designed for real-time rendering. In contrast to a Central Processing Unit (CPU) that have complex instructions and a limited number of pipelines, a GPU has simpler instructions and many execution pipelines to process vector data in a massively parallel fashion. In addition to its regular tasks, GPU instruction set can be used for performing other types of general-purpose computations as well. Several frameworks like Brook+, ATI CAL, OpenCL, and Nvidia Cuda have been proposed to utilize computational power of the GPU in general computing. This has provided interest and opportunities for accelerating different types of applications.This thesis explores ways of taking advantage of the GPU in the field of metric space-based similarity searching. The KVP index structure has a simple organization that lends itself to be easily processed in parallel, in contrast to tree-based structures that requires frequent ""pointer chasing"" operations. Several implementations using the general purpose GPU programming frameworks (Brook+, ATI CAL and OpenCL) based on the ATI platform are provided. Experimental results of these implementations show that the GPU versions presented in this work are several times faster than the CPU versions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalabalık simülasyonu, biyolojik ve sosyal modelleme, askeri simülasyonlar, bilgisayar oyunları ve filmler gibi geniş uygulama alanlarına sahiptir. Canlandırılmış sanal kalabalıkların simülasyonu bilgisayar grafikleri camiası için zorlu bir görevdir. Fiziksel ve geometrik özelliklerinin yanısıra, gerçek kalabalıkların hareketlerinin anlamları, sanal kalabalıkların tasarım ve gerçekleştirilmesinde önemlidir. Psikoloji, bizim kalabalıkları oluşturan bireylerin motivasyonlarını anlamamıza yardımcı olur. Özerk etmenlerin simülasyonuna psikolojik modelleri dahil etmek üzerine yoğun araştırma yapılmıştır. Buna rağmen, biz, çalışmamızda bireysel bir etmenin kendisinden ziyade çeşitli psikolojik özelliklere sahip bireylerden oluşan bir kalabalığın genel davranışıyla ilgilenmekteyiz. Bu amaçla, duygulanımın üç temel bileşenini dahil ettik: kişilik, duygu ve mizaç. Bu etkenlerden her biri farklı davranış sekillerinin ortaya çıkmasına farklı derecelerde katkıda bulunur.Böylece, parametreleri degiştirerek, farklı özelliklere sahip grupların birbirleriyle nasıl etkileştiklerini, ve buna bağlı olarak genel kalabalık davranışının nasıl etkilendiğini inceliyoruz.Sosyal psikoloji literatüründe kalabalıklar, kitleler ve güruhlar olarak sınıflandırılmıştır. Kitleler pasif kalabalıklar, güruhlar ise, duygusal, mantıksız ve görünürde homojen davranışlarda bulunan aktif kalabalıklardır. Bu tezde kitlelerin güruhlara dönüşümünü ve güruhların kolektif olarak uygun olmayandavranışlarda bulunuşunu inceliyoruz. Mevcut kalabalık simülasyonu araştırmaları, tüm güruh çeşitleri iiçinde sadece panik davranışı gösteren güruhlara odaklanmıştır. Biz, en son gelişmeleri kalabalıkların sınıflandırılmasına göre değişik çeşit güruhların simülasyonunu yaparak genişletiyoruz. Farklı güruh tiplerinin davranışını gerçekleştiren çeşitli senaryolar gösteriyoruz.Modelimiz, mevcut bir kalabalık simülasyonu sistemi olan HiDAC (Yüksek Yoğunluklu Özerk Kalabalıklar) üzerine kurulmuştur. HiDAC, bize kalabalıkların fiziksel ve alt duzeydeki psikolojik özelliklerini sağlar. Biz çalışmamızda, kullanıcıyı meşakkatli olan alt düzey parametre ayarlama işinden kurtararak bütün bu davranışları farklı psikolojik faktorlerde birleştiriyoruz. Bir kişilik modelinin HiDAC sistemine dahil edilmesi işleminin niyetlendiğimiz şekilde algılanıp algılanmadığına dair yaptığımız deneylerin sonuçlarını sunuyoruz.","Crowd simulation has a wide range of application areas such as biological and social modeling, military simulations, computer games and movies. Simulating the behavior of animated virtual crowds has been a challenging task for the computer graphics community. As well as the physical and the geometrical aspects,the semantics underlying the motion of real crowds inspire the design and implementation of virtual crowds. Psychology helps us understand the motivations of the individuals constituting a crowd. There has been extensive research on incorporating psychological models into the simulation of autonomous agents.However, in our study, instead of the psychological state of an individual agent as such, we are interested in the overall behavior of the crowd that consists of virtual humans with various psychological states. For this purpose, we incorporate the three basic constituents of affect: personality, emotion and mood. Each of these elements contribute variably to the emergence of different aspects of behavior. We thus examine, by changing the parameters, how groups of people with different characteristics interact with each other, and accordingly, how the global crowd behavior is influenced.In the social psychology literature, crowds are classified as mobs and audiences. Audiences are passive crowds whereas mobs are active crowds with emotional, irrational and seemingly homogeneous behavior. In this thesis, we examine how audiences turn into mobs and simulate the common properties of mobs to create collective misbehavior. So far, crowd simulation research has focused on panicking crowds among all types of mobs. We extend the state of the art to simulate different types of mobs based on the taxonomy. We demonstrate various scenarios that realize the behavior of distinct mob types.Our model is built on top of an existing crowd simulation system, HiDAC (High-Density Autonomous Crowds). HiDAC provides us with the physical and low-level psychological features of crowds. The user normally sets these parameters to model the non-uniformity and diversity of the crowd. In our work, we free the user of the tedious task of low-level parameter tuning, and combine all these behaviors in distinct psychological factors. We present the results of our experiments on whether the incorporation of a personality model into HiDAC was perceived as intended."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağlarına olan ilgi yakın zamanda büyüdü ve bu düşük güçlü kablosuz teknolojilerin kamera ve mikrofonlarla birleşmesiyle, düşük hızlı çevresel ölçüm verisi dışında, görüntü ve ses algılayıcı ağlarda gönderilebilmeye başlandı. Bu algılayıcı ağlara, kablosuz çokluortam algılayıcı ağlar deniliyor ve pil, hafızave erişilebilir veri hız yönünden hala kısıtlılar. Bu yüzden böyle bir ortamda çokluortam içeriği gönderimi yeni bir araştırma konusu haline geldi. Uygulamaya bağlı olarak, içeriğin tek bir hedefe yada birden fazla hedeflere gönderilmesi gerekebilir. Bu çalışmada bizler, bir çokluortam veri akışını birden fazla hedefe verimli ve etkili bir şekilde gönderim problemini ele aldık, örnek olarak kablosuz algılayıcı ağlarındaki çokluortam çoğagönderim problemini verebiliriz. Bu ağlarda öne sürülmüş var olan çoğagönderim çözümleri az bant genişliği kullanan ve gecikmeyi idare edebilen veriler icin enerji verimliliği sağlayabiliyorlar. Bu çalışmanın amacı ise, nispeten daha yüksek hızlı ve uzun süreli çokluortam veri akışları için istenilen servis kalitesini karşılamaya çalışan bir çoğagönderim sistemi oluşturmaktır. Çalışmamızın bir parçası olarak, yeterli bant genişliğine sahip olan, enerji verimli çoğagönderim ağaç oluşturabilen, bir merkezi bir de dağıtık versiyonlu çoğagönderim şeması oluşturduk. Simülasyonlar aracılığıyla önerdiğimiz protokollerimizin performansını inceledik ve sonucunda istenilen çoğagönderim ağaçlarını etkili bir şekilde oluşturabildiğini gördük.","In recent years, the interest in wireless sensor networks has grown and resultedin the integration of low-power wireless technologies with cameras and microphonesenabling video and audio transport through a sensor network besidestransporting low-rate environmental measurement-data. These sensor networksare called wireless multimedia sensor networks (WMSN) and are still constrainedin terms of battery, memory and achievable data rate. Hence, delivering multimediacontent in such an environment has become a new research challenge.Depending on the application, content may need to be delivered to a single destination(unicast) or multiple destinations (multicast). In this work, we considerthe problem of eciently and eectively delivering a multimedia stream to multipledestinations, i.e. the multimedia multicasting problem, in wireless sensornetworks. Existing multicasting solutions for wireless sensor networks provideenergy eciency for low-bandwidth and delay-tolerant data. The aim of thiswork is to provide a framework that will enable multicasting of relatively highrateand long-durational multimedia streams while trying to meet the desiredquality-of-service requirements. To provide the desired bandwidth to a multicaststream, our framework tries to discover, select and use multicasting paths that gothrough uncongested nodes and in this way have enough bandwidth, while alsoconsidering energy eciency in the sensor network. As part of our framework,we propose a multicasting scheme, with both a centralized and distributed version,that can form energy-ecient multicast trees with enough bandwidth. Weevaluated the performance of our proposed scheme via simulations and observedthat our scheme can eectively construct such multicast trees."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklama, veri erişimi ve veritabanı sistemlerinde aksaklığa dayanıklılık ve paralelizasyon ve işleme yüklerinin azaltılması için sıkça kullanılan bir tekniktir. Veri erişimi ve veritabanı sistemlerinde hiperçizge bölümlemesine dayanan bir çok kombinasyonel model önerilmiştir. Bu çalışmada, düğüm çoklamaları kullanılarak hiperçizge bölümlemelerindeki kesit boyutunun azaltılması üzerinde durmaktayız. Bu amaçla, verilen bir maksimum çoklama kapasitesi ve K parçalı hiperçizge bölümlemesi ile Denge Korumalı Min-Kesit Çoklama Kümesi'nin (DKMKÇK) bulunması problemi üzerine yoğunlaşmaktayız. DKMKÇK probleminde amaç, her parça için bulunacak bir çoklama kümesi ile baştaki bölümlemenin dengesini koruyarak kesit boyutunu azaltmaktır. Bu amaçla, küçültme (coarsening) ve tamsayı doğrusal programlama (integer linear programming (ILP)) yöntemlerinin seçkin bileşiminden oluşan bir model öneriyoruz. Modelde kullanılan küçültme algoritması Dulmage-Mendelsohn ayrışımına dayanmaktadır. Yapılan deneylerde, Dulmage-Mendelsohn ayrışımına dayalı küçültme yöntemi ile birlikte kullanılan ILP formülasyonunun mantıklı çalışma zamanları içinde, verilen bir K parçalı hiperçizge bölümlemesinin kesit boyutunu düğüm çoklamaları ile oldukça yüksek seviyelerde azalttığı gözlemlenmiştir.","Replication is a widely used technique in information retrieval and database systems for providing fault-tolerance and reducing parallelization and processing costs. Combinatorial models based on hypergraph partitioning are proposed for various problems arising in information retrieval and database systems. We consider the possibility of using vertex replication to improve the quality of hypergraph partitioning. In this study, we focus on the Balance Preserving Min-Cut Replication Set (BPMCRS) problem, where we are initially given a maximum replication capacity and a K-way hypergraph partition with an initial imbalance ratio. The objective in the BPMCRS problem is nding optimal vertex replication sets for each part of the given partition such that the initial cutsize of the partition is improved as much as possible and the initial imbalance is either preserved or reduced under the given replication capacity constraint. In order to address the BPMCRS problem, we propose a model based on a unique blend of coarsening and integer linear programming (ILP) schemes. This coarsening algorithm is based on the Dulmage-Mendelsohn decomposition. Experiments show that the ILP formulation coupled with the Dulmage-Mendelsohn decomposition-based coarsening provides high quality results in feasible execution times for reducing the cost of a given K-way hypergraph partition."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hareket yakalama, kullanımı gittikçe artan animasyon tekniklerindendir; lakin hareket yakalama ile elde edilen veriler kolaylıkla cok büyük boyutlara ulaşabilir. Bu durum hareket yakalamayı, hareket düzenleme, hareket anlama, otomatik hareket özetleme, hareket önizlemesi oluşturma ya da hareket veritabanı sorgulama gibi ceşitli uygulamalarda kullanışsız hale getirmektedir. Bu kısıtlamayı aşmak amacıyla, hareket yakalama dizisinden otomatik olarak anahtar kareleri bulabilen bir yöntem önermekteyiz. Bu yöntemde, girdi olarak kullanılan diziyi eğriler olarak alıp, 'hareket belirginliği' adlı yeni bir metrik kullanılarak bu eğrilerin en belirgin bölümleri bulunmaktadır. Analiz edilecek eğriler ""Esas Bileşen Analizi"" isimli boyut indirgeme metodu kullanılarak seçilmektedir. Daha sonra, uygulanan kare indirgeme tekniği ile önemli kareler anahtar kareler olarak çıkartılmaktadır. Bu yöntem sayesinde, hareket yakalama verisinin yaklaşık %8'i anahtar kare olarak seçilmektedir. Son olarak bu sonuçlar matematiksel ve kullanıcı testleri sayesinde değerlendirilmektedir.","Motion capture is an increasingly popular animation technique; however data acquired by motion capture can become substantial. This makes it difficult to use motion capture data in a number of applications, such as motion editing, motion understanding, automatic motion summarization, motion thumbnail generation, or motion database search and retrieval. To overcome this limitation, we propose an automatic approach to extract keyframes from a motion capture sequence. We treat the input sequence as motion curves, and obtain the most salient parts of these curves using a new proposed metric, called 'motion saliency'. We select the curves to be analyzed by a dimension reduction technique, Principal Component Analysis. We then apply frame reduction techniques to extract the most important frames as keyframes of the motion. With this approach, around 8% of the frames are selected to be keyframes for motion capture sequences. We have quantized our results both mathematically and through user tests."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda akademide ve endüstride, XML veritabanları ve belge derlemlerinde anahtar sözcük aramak için çeşitli teknikler önerilmiştir. Bu tekniklerin pek çoğunda kullanılan veri yapısı, dünya çapında ağ (WWW) gibi büyük metin verileri üzerinde anahtar sözcük aramada en gelişmiş teknik olan ters indekstir. Bir tam eleman indeksi her bir XML elemanını, metni, kendisinin doğrudan içeriği ve torunlarının içeriklerinden oluşan ayrı bir belge olarak düşünür ve indeksler. Tam eleman indekse yöneltilen önemli bir eleştiri (XML belgelerinin iç içe yapısından dolayı) yüksek derecede fazlalık içermesidir. Bu durum tam eleman indeksin büyük ölçekli XML erişimi durumlarında kullanımını azaltır.Bu tezde XML anahtar sözcük arama için tam eleman indeksinin kullanımının verimlilik ve etkiliği araştırılmaktadır. Öncelikle, kayıpsız indeks sıkıştırma tekniklerinin tam eleman indeksinin büyüklüğünü önemli ölçüde azaltabileceği, böylece tipik bir arama motorundaki sorgu işleme stratejilerinin böyle bir indeks üzerinde verimli bir şekilde çalışabileceği öne sürülmektedir. Bir tam eleman indeksinin en önemli dezavantajı boyutunun büyüklüğüdür. Bu sorun çözüldüğü takdirde bu tip indeks kullanımının, sonuç kalitesi (etkililik) ve sorgu işleme performansını (verimlilik) son zamanlarda önerilen diğer tekniklere kıyasla geliştirebileceği gösterilmektedir. Ayrıca tam eleman indeksi kullanmak, birleşik bir taslakta sorgu sonuçlarını, sıralı belge listesi (bir arama motorunun kullanıcısının beklediği şekilde) ya da sorgu sözcüklerinin tümünü içeren eleman listesi (bir veritabanı sistemi kullanıcısının beklediği şekilde) gibi farklı formlarda oluşturmaya olanak sağlar.Bu tezin ikinci bir katkısı olarak, tam eleman indeksin büyüklüğünü daha da azaltmak için kayıplı bir yaklaşım olan statik budama tekniğinin kullanılması önerilmektedir. Bu şekilde, bir elemanın sözcüklerinin yukarı seviyelerdeki tekrarının, elemanın metinsel içeriği ve arama motorunun sıralama işlevi dikkate alınarak, uyarlanabilir bir şekilde azaltılması amaçlanmaktadır. Yani indeksteki tekrarlamaların, çıkarılmaları sonuç kalitesini azaltmadığı takdirde, ortadan kaldırılmasına çalışılmaktadır. Deneysel çalışmalarla, budanmış indeks dosyalarının çok yüksek budama seviyelerine kadar, erişim etkililiği açısından, tam eleman indeksiyle karşılaştırılabilir, hatta ondan daha iyi olduğu gösterilmektedir.Son olarak, indeks budama stratejilerinin, bir XML derleminin belge vektörlerinin büyüklüklerinin azaltılarak gruplama performansının geliştirilmesinde kullanılması önerilmektedir. Deneyler, belli durumlar için, koleksiyonun %70 kadarı budanarak, bir grup değerlendirme metriğine göre, orijinal koleksiyonla aynı kaliteyi sağlayan bir gruplama yapısı oluşturulabildiğini göstermektedir.","In the last decade, both the academia and industry proposed several techniques to allow keyword search on XML databases and document collections. A common data structure employed in most of these approaches is an inverted index, which is the state-of-the-art for conducting keyword search over large volumes of textual data, such as world wide web. In particular, a full element-index considers (and indexes) each XML element as a separate document, which is formed of the text directly contained in it and the textual content of all of its descendants. A major criticism for a full element-index is the high degree of redundancy in the index (due to the nested structure of XML documents), which diminishes its usage for large-scale XML retrieval scenarios.As the first contribution of this thesis, we investigate the efficiency and effectiveness of using a full element-index for XML keyword search. First, we suggest that lossless index compression methods can significantly reduce the size of a full element-index so that query processing strategies, such as those employed in a typical search engine, can efficiently operate on it. We show that once the most essential problem of a full element-index, i.e., its size, is remedied, using such an index can improve both the result quality (effectiveness) and query execution performance (efficiency) in comparison to other recently proposed techniques in the literature. Moreover, using a full element-index also allows generating query results in different forms, such as a ranked list of documents (as expected by a search engine user) or a complete list of elements that include all of the query terms (as expected by a DBMS user), in a unified framework.As a second contribution of this thesis, we propose to use a lossy approach, static index pruning, to further reduce the size of a full element-index. In this way, we aim to eliminate the repetition of an element's terms at upper levels in an adaptive manner considering the element's textual content and search system's ranking function. That is, we attempt to remove the repetitions in the index only when we expect that removal of them would not reduce the result quality. We conduct a well-crafted set of experiments and show that pruned index files are comparable or even superior to the full element-index up to very high pruning levels for various ad hoc tasks in terms of retrieval effectiveness.As a final contribution of this thesis, we propose to apply index pruning strategies to reduce the size of the document vectors in an XML collection to improve the clustering performance of the collection. Our experiments show that for certain cases, it is possible to prune up to 70% of the collection (or, more specifically, underlying document vectors) and still generate a clustering structure that yields the same quality with that of the original collection, in terms of a set of evaluation metrics."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gecikme dirençli ağlarda, ağ her ne kadar anlık olarak bağlı bulunmasa da düğümler arasında zaman içinde oluşan bağlantılar, ağı tüm zaman aralığında bağlı hale getirir. Böyle bir durumda, geleneksel yönlendirme yöntemleri kaynak ve hedef arasında hali hazırda uçtan uca bir yol bulunmadığı için başarılı olamaz. Bu çalışma, süreli bağlantılardan oluşan gecikme dirençli ağlarda yönlendirmeyi incelemektedir. Farklı ağ yapılarının gereksinimlerini karşılamak için farklı süreli bağlantı türleri çözümlenerek, süreli bağlantılar için çeşitli yönlendirme yöntemleri önerilmekte, önerilen yöntemler en erken ve en kısa yoldan teslimi garantilemektedir. Yönlendirme yöntemleri, ayrıntılı benzetim ve deneyler ile değerlendirilip gözde yönlendirme yöntemleriyle de karşılaştırılmaktadır. Değerlendirmeler, önerilen yöntemlerin gecikme dirençli ağlarda uygulanabilir ve etkin bir seçenek olduğunu göstermektedir.","In delay tolerant networks (DTNs), the network may not be fully connected at any instance of time, but connections occurring between nodes at different times make the network connected through the entire time continuum. In such a case, traditional routing methods fail to operate as there are no contemporaneous end-to-end paths between sources and destinations. This study examines the routing in DTNs where connections arise in a periodic nature. Various levels of periodicity are analyzed to meet requirements of different network models. We propose various routing algorithms for periodic connections. Our proposed methods can find routes that can guarantee earliest delivery and minimum hop count. We evaluate our routing schemes via extensive simulation experiments and also compare them to some other popular routing approaches proposed for delay tolerant networks. Our evaluations show the feasibility and effectiveness of our schemes as alternative routing methods for delay tolerant networks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda dijital kameraların hızlı gelişimi ve sosyal paylaşım sitelerinin kullanımının artması ile büyük oranda resim internet ortamında paylaşılır hale geldi. Tek bir paylaşım sitesinin bile 4 milyardan fazla resim sağladığı bir dönemde, internet ortamındaki bu resimlerin etkin bir şekilde düzenlenmesi ihtiyacı giderek artmaktadır.Büyük miktarda veriyi rahat bir şekilde yönetmek için, ortak etiketleme yöntemleri sistemler tarafından kullanılmaktadır. Bu yöntemler, verilere kullanıcılar tarafından atanmış, içerik bilgisi barındıran kelimeleri kullanarak düzenleme işlemlerini gerçekleştirmektedirler. Ancak çeşitli sebeplerden dolayı, kullanıcılar tarafından sağlanan bu etiketler, hem içerik hem de miktar olarak yetersiz kalmaktadırlar.Bu çalışmada, yetersiz ve yanlış etiketleme sorununu çözebilmek amacıyla kullanılacak iki etiket önerim sisteminin incelemeleri sunulmaktadır. Bu sistemlerin amacı, resim yükleme sırasında kullanıcılara, resmin içeriği ile ilgili etiketler önermek ve etiketleme işlemini kolaylaştırmaktır. Yöntemlerin amacı kullanıcılara tamamlanmış bir etiket listesi sunmak değil, onlara yardımcı olacak, resmin içeriği hakkında bilgi veren etiketler önermektir.","Due to the fast development of affordable digital cameras and the new trend ofsharing media through the web, large amounts of images have become availableon the Internet. Thus, at a time when a single site alone hosts over 4 billionphotos, the necessity of managing these massive numbers of photos for eff'cientand effective browsing/searching operations has increased.To properly organize large amounts of data, systems have been using collaborativetagging methods by assigning descriptive words, tags, to data andperforming text-based search and retrieval operations on these words. Unfortunately,due to various reasons, both the amount and quality of these tags assignedby users are low.In this work, we present and analyze two applications of tag expansion methodson photo-sharing websites. The purpose of these methods is to assist usersfor proper tagging at upload time. The goal of the approaches is not to give usersa complete set of tags that could be directly used but to give a list of, possibly,incomplete set of tags that would help or guide the users to tag in accordancewith the image content. With this assistance, problems such as incorrect taggingand insufficient tagging are expected to be solved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Uydu teknolojisindeki gelişmeler ve Dünya'nın geniş bir yüzeyini kapsayan detaylı görüntülerin mevcut olması, uydu görüntülerinde otomatik içerik çıkarma ve sınıflandırma yapabilen akıllı sistemlere duyulan ihtiyacı her geçen gün arttırmaktadır. Yeni nesil sensörlerden alınan çok yüksek uzamsal çözünürlüklü görüntülerdeki artan detaylar yeni uygulamaları mümkün kılmakla birlikte temel nesnelerin sezimini zorlaştırmaktadır. Görüntü yapıları hakkındaki bağlamsal bilgiler birbirinden bağımsız nesnelerin sezimini geliştirme potansiyeline sahiptir. Bu nedenle, özünde heterojen olan görüntü bölgelerinin tanımlanması, görüntü içeriğini anlamak için alternatif bir yoldur. Bileşik yapılar olarak da bilinen bu bölgeler birçok farklı türdeki temel nesnelerden oluşmaktadır. Kelimeler-torbası gibi popüler gösterimler, yerel operatörler kullanılarak çıkarılan temel nesne parçalarını kullanır fakat mekansal bilgi eksikliği nedeniyle onların yapısını tutamaz. Dolayısıyla, bileşik yapıların sezimi spektral, uzaysal ve yapısal bilgilerin ortak modellenmesini içeren yeni görüntü gösterimlerini zorunlu kılar.Biz, çizgelerin gösterim gücü ile kelimeler-torbası gösteriminin verimliliğini birleştiren bir görüntü gösterimi öneriyoruz. Önerilen yöntem üç bölümden oluşmaktadır. İlk bölümde, veri kümesindeki her bir görüntü yerel görüntü özellikleri ve onların uzamsal ilişkileri kullanılarak çizge yapısına dönüştürülür. Dönüştürme yöntemi ilk olarak gri seviye eşiklemesi ile elde edilen en kararlı uç bölgelerden, ilgili yerel yamaları tespit eder. Sonra, bu yamalar bir yerel bilgi çizelgesi oluşturmak için nicelendirilir, ve yamaları çizge düğümü gibi göstererek ve onları Voronoi mozaiğinden elde edilen kenarlarla birleştirerek her bir görüntü için bir çizge inşa edilir. Görüntülerin çizgelere dönüştürülmesi bir soyutlama düzeyi sağlar ve sınıflandırma için geriye kalan işlemler çizgeler üzerinde yapılır. Önerilen yöntemin ikinci bölümü görüntü çizgelerinin sınıflandırılması için en önemli altçizgelerin kümesini seçen bir çizge madenciliği algoritmasıdır. Önerdiğimiz çizge madenciliği algoritması ilk olarak her sınıf için sık görülen altçizgeleri bulur, sonra sınıf içinde görülme dağılımları açısından altçizgeler ve sınıflar arasındaki bağıntı miktarları ölçülerek en ayırt edici olanları seçer; ve son olarak altçizgeler arasındaki fazlalığı dikkate alarak en iyi temsil edenlerin seçmesiyle küme boyutunu küçültür. Altçizge kümesi madenciliğinden sonra her bir görüntü çizgesi, her bir bileşeninin bu kümenin belli bir altçizgesinin görüntüde görülme sayısını tuttuğu bir histogram vektörü ile gösterilir. Altçizge histogram gösterimi görüntü çizgelerinin istatistiksel sınıflandırıcılar kullanılarak sınıflandırılmasını mümkün kılar. Yöntemin son bölümü etiketli verilerinden model öğrenilmesini içerir. Görüntülerin anlamsal sahne türlerine sınıflandırılması için destek vektör makineleri (DVM) kullanıyoruz. Ek olarak, görüntüler üzerine dağılan temalar, aynı veriler üzerinde öğretilen gizli Dirichlet tahsisi (GDT) modeli kullanılarak keşfedilir. Bu sayede, farklı sahne türlerinden heterojen bir içeriğe sahip görüntüler bir tema dağılım vektörü olarak gösterilebilirler. Bu gösterim tema analizi ile görüntülerin daha ileri düzeyde sınıflandırılmasını sağlar.Antalya'nın bir Ikonos görüntüsü üzerindeki deneyler önerilen gösterimin karmaşık sahne türlerinin sınıflandırılmasındaki etkinliğini göstermektedir. DVM modeli Antalya görüntüsünden kesilen görüntülerde sekiz üst düzey anlamsal sınıf için umut verici sınıflandırma doğruluğu elde etti. Ayrıca, GDT modeli tüm uydu görüntüsünde ilginç temalar keşfetti.","The need for intelligent systems capable of automatic content extraction and classification in remote sensing image datasets, has been constantly increasing due to the advances in the satellite technology and the availability of detailed images with a wide coverage of the Earth. Increasing details in very high spatial resolution images obtained from new generation sensors have enabled new applications but also introduced new challenges for object recognition. Contextual information about the image structures has the potential of improving individual object detection. Therefore, identifying the image regions which are intrinsically heterogeneous is an alternative way for high-level understanding of the image content. These regions, also known as compound structures, are comprised of primitive objects of many diverse types. Popular representations such as the bag-of-words model use primitive object parts extracted using local operators but cannot capture their structure because of the lack of spatial information. Hence, the detection of compound structures necessitates new image representations that involve joint modeling of spectral, spatial and structural information.We propose an image representation that combines the representational power of graphs with the efficiency of the bag-of-words representation. The proposed method has three parts. In the first part, every image in the dataset is transformed into a graph structure using the local image features and their spatial relationships. The transformation method first detects the local patches of interest using maximally stable extremal regions obtained by gray level thresholding. Next, these patches are quantized to form a codebook of local information and a graph is constructed for each image by representing the patches as the graph nodes and connecting them with edges obtained using Voronoi tessellations. Transforming images to graphs provides an abstraction level and the remaining operations for the classification are made on graphs. The second part of the proposed method is a graph mining algorithm which finds a set of most important subgraphs for the classification of image graphs. The graph mining algorithm we propose first finds the frequent subgraphs for each class, then selects the most discriminative ones by quantifying the correlations between the subgraphs and the classes in terms of the within-class occurrence distributions of the subgraphs; and finally reduces the set size by selecting the most representative ones by considering the redundancy between the subgraphs. After mining the set of subgraphs, each image graph is represented by a histogram vector of this set where each component in the histogram stores the number of occurrences of a particular subgraph in the image. The subgraph histogram representation enables classifying the image graphs using statistical classifiers. The last part of the method involves model learning from labeled data. We use support vector machines (SVM) for classifying images into semantic scene types. In addition, the themes distributed among the images are discovered using the latent Dirichlet allocation (LDA) model trained on the same data. By this way, the images which have heterogeneous content from different scene types can be represented in terms of a theme distribution vector. This representation enables further classification of images by theme analysis.The experiments using an Ikonos image of Antalya show the effectiveness of the proposed representation in classification of complex scene types. The SVM model achieved a promising classification accuracy on the images cut from the Antalya image for the eight high-level semantic classes. Furthermore, the LDA model discovered interesting themes in the whole satellite image."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İsim tamlaması çıkarımı bir çok doğal dil işleme konusunda kullanılabilen yüzeysel çözümlemenin alt kategorisidir. Bu tezde, biz Türkçe metinler için bir isim tamlaması çıkarımı sistemi öneriyoruz. Cümle bileşkenleri arasındaki ilişkiyi göstermek ve isim tamlamalarını bulmak için ağırlıklı bir kısıtlayıcı bağımlı çözümleyici kullanıyoruz.Bağımlı çözümleyici kısıtlamaları belirlemek için, manual olarak oluşturulan ve biçimbirimsel ve anlamsal bilgileri birleştirebilen etkili kurallar kullanır. Kurallar esnek yapıları gereği karmaşık isim tamlamaları yapılarını çözmek için uygundur. Kural dizisi değiştirilerek bağımlı çözümleyici diğer tüm cümle parçacığı çeşitlerini içeren bir yüzeysel çözümleyici olarak da kolaylıkla kullanılabilir.Türkçe için insanlar tarafından oluşturulmuş güvenilir bir veri grubunun olmaması Türkçe ile ilgili olan doğal dil işleme çalışmalarını için önemli bir problemdir. Bu yüzden, Türkçe için ilk isim tamlaması veri gruplarını oluşturduk. Bu veri grupları üzerinde yaptığımız testlere göre, bizim önerdiğimiz isim tamlaması çıkarımı sistemimiz, umut verici sonuçlarvermektedir.Kelimelerin biçimbirimsel bilgisini bilmek bağımlı çözümleyicinin doğru çalışması için önemlidir. Bu yüzden, bu tezde, istatistiksel bilgi ile, elle oluşturulmuş gramer kuralları ile dönüşüm bazlı öğrenilmiş kuralları bir arada kullanan hibrit bir biçimbirimsel belirsizliği giderme tekniği önerdik. Ayrıca bizim belirsizlik giderici sistemimizin performansını ölçmek için bir veri grubu oluşturduk. Yaptığımız testlere göre, bizim önerdiğimiz sistem umut verici sonuçlar vermektedir.","Noun phrase chunking is a sub-category of shallow parsing that can be used formany natural language processing tasks. In this thesis, we propose a noun phrasechunker system for Turkish texts. We use a weighted constraint dependencyparser to represent the relationship between sentence components and to determine noun phrases.The dependency parser uses a set of hand-crafted rules which can combinemorphological and semantic information for constraints. The rules are suitablefor handling complex noun phrase structures because of their flexibility. Thedeveloped dependency parser can be easily used for shallow parsing of allphrase types by changing the employed rule set.The lack of reliable human tagged datasets is a significant problem fornatural language studies about Turkish. Therefore, we constructed the first nounphrase dataset for Turkish. According to our evaluation results, our noun phrasechunker gives promising results on this dataset.The correct morphological disambiguation of words is required for thecorrectness of the dependency parser. Therefore, in this thesis, we propose ahybrid morphological disambiguation technique which combines statisticalinformation, hand-crafted grammar rules, and transformation based learningrules. We have also constructed a dataset for testing the performance of ourdisambiguation system. According to tests, the disambiguation system is highlyeffective."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"3B görüntüleme ve ekran teknolojilerindeki hızlı gelişim, 3B görsel içeriğin uygun şekilde sunulması problemini de beraberinde getirmektedir. 3B bilgisayar görüntüleri üretilirken, kullanıcının nesnelerin uzamsal özelliklerini doğru olarak algılayabilmesini kolaylaştırmaya dikkat edilmelidir.Bu tezde onerilen çözüm, verilen bir 3B sahnede derinlik algısının otomatik olarak iyileştirilmesinde ya da 3B uygulama geliştiricilere derinlik algısının artırılması için uygun görüntüleme yöntemlerinin önerilmesinde bir araç olarak kullanılabilir.Sunulan çözümde, verilen 3B sahne için uygun olan derinlik ipuçları ve bu ipuçlarını sağlayan görüntüleme yöntemleri otomatik olarak belirlenmektedir. Öncelikle, bulanık mantık tabanlı bir algoritmayla, verilen sahne için her bir ipucunun önem derecesi, uygulamanın amaç ve sahnenin uzamsal özellikleri göz önünde bulundurularak hesaplanmaktadır. Daha sonra, önemli derinlik ipuçlarını sağlayan grafiksel görüntüleme yöntemlerinin maliyetlerini ve derinlik algısına katkılarnı dengelemeye yönelik olarak bir knapsack modelioluşturulmaktadır. Bu kar-zarar analizi sayesinde derinlik algısını artırma amaçlı grafiksel yöntemlerden verilen sahne için uygun olanlar belirlenmektedir.Bu çalışmada ayrıca, önerilen otomatik derinlik algısı iyileştirme sisteminin test edilen diğer yöntem seçim tekniklerine göre istatistiksel olarak (p<0.05 ) daha başarılı olduğunu gösteren birkaç nesnel ve öznel kullanıcı testine de yer verilmektedir.","Rapid progress in 3D rendering and display technologies brings the problem of better visualization of 3D content. Providing correct depth information and enabling the user to perceive the spatial relationship between the objects is one of the main concerns during the visualization of 3D content.In this thesis, we introduce a solution that can either be used for automatically enhancing the depth perception of a given scene, or as a component that suggests suitable rendering methods to application developers.In this novel solution, we propose a framework that decides on the suitable depth cues for a given 3D scene and the rendering methods which provide these cues. First, the system calculates the importance of each depth cue using a fuzzy logic based algorithm which considers the user's tasks in the application and the spatial layout of the scene. Then, a knapsack model is constructed to keep the balance between the rendering costs of the graphical methods that provide these cues and their contribution to depth perception. This cost-profit analysis step selects the proper rendering methods for the given scene.In this work, we also present several objective and subjective experiments which show that our automated depth perception enhancement system is statistically (p < 0.05 ) better than the other method selection techniques that are tested."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Video indeksleme ve erişimi sistemleri büyük çaptaki video verilerine hızlı, doğal ve kolay bir şekilde ulaşılabilmesini amaçlar. Son zamanlarda video arşivlerinin çok hızlı büyümesiyle bu sistemlerin önemi daha da artmıştır. Bu tez, video çözümleme, indeksleme ve erişimi konularında yeni yöntemler öneren BilVideo-7 sistemini sunmaktadır.BilVideo-7, karmaşık çok kipli video sorgularını aynı anda destekleyen, dağıtık mimariye sahip, MPEG-7 uyumlu bir video indeksleme ve erişimi sistemidir. Video veri modeli bir MPEG-7 pro ? li üzerine bina edilmiş olup, videolar bu pro ? le uygun olarak çekimlere, anahtar karelere, durağan ve hareketli bölgelere ayrılmaktadır. Videoların bu veri modeline uygun XML gösterimleri, BilVideo-7'nin MPEG-7 uyumlu video öznitelik çıkarma ve etiketleme yazılımı yardımıyla elde edilip XML veritabanında saklanmaktadır. Kullanıcılar, görsel sorgulama arayüzünü kullanarak metin, renk, doku, biçim, konum, hareket ve uzamsal-zamansal sorguları kolay bir şekilde yapabilmektedir. Kompozit sorgu arayüzü, kullanıcıların, istenilen sayıda video parçasını ve betimleyicisini bir araya getirip aralarındaki uzamsal-zamansal ilişkileri belirleyerek, oldukça karmaşık, çok kipli sorguları kolayca formüle edebilmesini sağlamaktadır. Sorgular, çok izlekli bir sorgu işleme sunucusu tarafından işlenmekte; istemcilerden gelen sorgular önce alt sorgulara ayrılmakta ve herbir sorgu, kendi sorgu tipine ait biz izlek tarafından işlenmektedir. Daha sonra, alt sorgu sonuçları birleştirilerek nihai sorgu sonucu elde edilip istemciye geri gönderilmektedir. Sistemin bir bütün olarak özgünlüğü, MPEG-7 uyumlu bir ortamda, detaylı bir video veri modeli, çok sayıda betimleyici ve çok kipli sorgu işleme özelliği ile güçlü bir video indeksleme ve sorgulama sistemi olmasıdır.","Video indexing and retrieval aims to provide fast, natural and intuitive access to large video collections. This is getting more and more important as the amount of video data increases at a stunning rate. This thesis introduces the BilVideo-7 system to address the issues related to video parsing, indexing and retrieval.BilVideo-7 is a distributed and MPEG-7 compatible video indexing and retrieval system that supports complex multimodal queries in a uni ? ed framework. The video data model is based on an MPEG-7 pro ? le which is designed to represent the videos by decomposing them into Shots, Keyframes, Still Regions and Moving Regions. The MPEG-7 compatible XML representations of videos according to this pro ? le are obtained by the MPEG-7 compatible video feature extraction and annotation tool of BilVideo-7, and stored in a native XML database. Users can formulate text, color, texture, shape, location, motion and spatio-temporal queries on an intuitive, easy-to-use visual query interface, whose composite query interface can be used to formulate very complex queries containing any type and number of video segments with their descriptors and specifying the spatio-temporal relations between them. The multi-threaded query processing server parses incoming queries into subqueries and executes each subquery in a separate thread. Then, it fuses subquery results in a bottom-up manner to obtain the ? nal query result and sends the result to the originating client. The whole system is unique in that it provides very powerful querying capabilities with a wide range of descriptors and multimodal query processing in an MPEG-7 compatible interoperable environment."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar destekli görüntü analizi araçları, otomatikleştirilmiş kanser tanı ve derecelendirmesi alanında giderek önemli hale gelmektedir. Bu araçların, kayda değer öznelliklere neden olabilen doku histopatolojik incelemesinde patologlara yardımcı olma potansiyelleri bulunmaktadır. Bu analiz araçları, dokular ile ilgili nicel bilgi sağlayarak öznelliğin azaltılmasına yardımcı olmaktadır. Literatürde, bu tip hesaplamasal araçların, dokuyu farklı imge öznitelikleri ile gösteren farklı yöntemler kullanarak geliştirilmesi önerilmiştir. En çok kullanılan yöntemlerden biri, doku bileşenleri arasındaki uzamsal ilişkiyi niceleyerek dokuyu temsil eden yapısal yöntemdir. Önceki yapısal yöntemler ile değişik doku türleri için umut verici sonuçlar elde edilmesine rağmen, bu yöntemler, bir dokunun nicelenmesi için yalnızca çekirdek bileşenlerini kullanmakta ve dokudaki diğer bileşenlerin varlığını dikkate almamaktadır. Öte yandan, bir dokunun değişik bileşenlerinden elde edilebilecek ek bilgi, bu dokunun daha iyi gösterilmesinde, ve dolayısıyla daha güvenilir kararlar alınmasında önemlidir.Bu tez, otomatik kanser tanı ve derecelendirmesi için histopatolojik imgelerin nicelenmesinde kullanılabilecek yeni bir yapısal yöntem sunmaktadır. Önceki yöntemlerin aksine, önerilen yöntem, farklı doku bileşenlerinin uzamsal dağılımlarını dikkate alarak dokuyu temsil etmeyi önermektedir. Bu amaçla, önerilen yöntem farklı doku bileşenlerin üzerinde bir çizge tanımlar ve bu çizgenin kenarlarını uç bileşenlerinin türlerine göre renklendirir. Ardından, elde edilen bu ?renkli çizgeler?den yeni bir yapısal öznitelik kümesi çıkarır ve bukümeyi dokuların sını ? andırılmasında kullanır. 258 farklı hastadan alınan 3236 kolon doku imgesi üzerinde yapılan deneyler, önerilen renkli çizge yönteminin, öğrenme seti için yüzde 94.89 ve test seti için yüzde 88.63 doğruluk verdiğini göstermektedir. Deneylerimiz ayrıca, farklı doku bileşenleri arasındaki uzamsal ilişkiyi gösteren renkli kenarların tanımlanmasının ve bunlar üzerinde çıkarılan çizge özniteliklerinin kullanımının, önceki yapısal yöntemlerin sını ? andırma başarısını kayda değer ölçüde artırdığını göstermektedir.","Computer aided image analysis tools are becoming increasingly important in automated cancer diagnosis and grading. They have the potential of assisting pathologists in histopathological examination of tissues, which may lead to a considerable amount of subjectivity. These analysis tools help reduce the subjectivity, providing quantitative information about tissues. In literature, it has been proposed to implement such computational tools using di ? erent methods that represent a tissue with di ? erent set of image features. One of the most commonly used methods is the structural method that represents a tissue quantifying the spatial relationship of its components. Although previous structural methods lead to promising results for di ? erent tissue types, they only use the spatial relations of nuclear tissue components without considering the existence of di ? erent components in a tissue. However, additional information that could be obtained from other components of the tissue has an importance in better representing the tissue, and thus, in making more reliable decisions.This thesis introduces a novel structural method to quantify histopathological images for automated cancer diagnosis and grading. Unlike the previous structural methods, it proposes to represent a tissue considering the spatial distribution of di ? erent tissue components. To this end, it constructs a graph on multiple tissue components and colors its edges depending on the component types of their end points. Subsequently, a new set of structural features is extracted from these ?color graphs? and used in the classi ? cation of tissues. Experiments conducted on 3236 photomicrographs of colon tissues that are taken from 258 di ? erent patients demonstrate that the color graph approach leads to 94.89 percent trainingaccuracy and 88.63 percent test accuracy. Our experiments also show that the introduction of color edges to represent the spatial relationship of di ? erent tissue components and the use of graph features de ? ned on these color edges signi ? cantly improve the classi ? cation accuracy of the previous structural methods."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalabalık simülasyonları, akademik ve endüstriyel çevrelerden gördüğü ilgigiderek artmakta olan nispeten yeni bir araştırma alanıdır. Bu tez, sanal insankalabalıkları içinde bulunan bireylerin davranışlarının modellenmesi ve kontrolü için özgün ve hibrid bir yaklaşım olan Uyarlanabilir Izgaralar yaklaşımınıönermektedir. Bu yaklaşımda kalabalığa dahil olan her bireyin hareketi yerelve genel yol planlaması stratejileri kullanılarak planlanır. Genel yol planlamasıiçin simülasyonun gerçekleştiği arazinin 2-boyutlu topoloji bilgisinden yararlanılarakdüzenli bir navigasyon haritası çıkartılır. Bu düzenli navigasyon haritasıdüzensiz, uyarlanabilir hücrelerden oluşacak şekilde parsellenir ve bu hücreleribarındıran uyarlanabilir bir ızgara elde edilir. Uyarlanabilir hücrelerin topolojibilgisi kullanılarak bir navigasyon grafiği hazırlanır. Bu grafik üzerinden kalabalıkiçerisindeki bireylerin olası her hedef noktası için bir potansiyel alan oluşturulur.Oluşturulan potansiyel alanlar sabit zamanlı erişim için kaydedilir ve bireyleringenel hareket planlamasında kullanılır. Yerel hareket planlaması için ise HelbingTrafik-Akış modelinden yararlanılmıştır. Yerel hareket planlaması sayesindebireyler komşuluklarındaki engellerden ve diğer bireylerden kaçınabilirler. Yerelve genel hareket modeli sonucu oluşan potansiyel kuvvetler toplanarak bireylerinhareketi sağlanır. Bu yaklaşım bireylerin herhangi bir sınırlamadan bağımsızolarak davranış sergilemesine imkan verir.","Crowd simulation is a relatively new research area, attracting increasing attentionfrom both academia and industry. This thesis proposes Adaptive Grids, anovel hybrid approach for controlling the behavior of agents in a virtual crowd. Inthis approach, the motion of each agent within the crowd is planned consideringboth global and local path planning strategies. For global path planning, a cellularadaptive grid is constructed from a regular navigation map that representsthe 2-D topology of the simulation terrain. A navigation graph with efficientsize is then pre-computed from the adaptive grid for each possible agent goal.Finally, the navigation graph is used to generate a potential field on the adaptivegrid by using the connectivity information of the irregular cells. Global pathplanning per agent has constant time complexity. For local path planning, HelbingTraffic-Flow model is used to avoid obstacles and agents. Potential forcesare then applied on each agent considering the local and global decisions of theagent, while providing each agent the freedom to act independently."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu hız oranlı IEEE 802.11 ağlarında, bilinen IEEE 802.11 Dağıtılmış Koordinasyon Fonksiyonu (DKF) Çoklu Erişim Kontrol (ÇEK) mekanizması en büyük ve en küçük iş/zaman oranının eşitliğini ve birden fazla kullanıcılı senaryolarda eşit kanal erişim sayısını, kanal yayın sürelerini eşitleyemeden sağlamaktadır. Buna bağlı olarak, diğerlerine göre daha kötü kanalları kullanan ya da gönderilecek paket uzunlukları daha fazla olan kullanıcılar, diğer kullanıcılara göre ciddi anlamda kanalı daha fazla işgal etmekte ve bunun sonucunda da daha iyi kanalı kullanan veya daha kısa paketlere sahip olan kullanıcıların iş/zaman oranı düşmektedir. Bu olay literatürde, performans anormalliği problemi olarak tanımlanmıştır. Bu tezde, dağıtılmış, kanal yayın sürelerini eşitleyerek performans anormalliği problemini çözen ve klasik IEEE 802.11 DKF ÇEKinde bir değişiklik yapılmasını gerektirmeyen yeni bir algoritma sunmaktayız. Sunulan algoritmada sistemdeki her kullanıcı, birden fazla klasik IEEE 802.11 DKF geri sayma algoritması ? kaç tane algoritma kullanılacağı her kullanıcı tarafından, diğer kullanıcılardan bağımsız bir şekilde, sadece paket uzunluğu ve kanal erişim hızı gibi yerel bilgiler ışığında hesaplanmaktadır ? çalıştırmaktadır. Bütün analitik ve simülasyon tabanlı sonuçlar, önerilen bu dağıtılmış kanal yayın sürelerini eşitleyen algoritmanın verimliliğinin doğruluğunu ispatlamaktadır.Anahtar Kelimeler: IEEE 802.11 Dağıtılmış Koordinasyon Fonksiyonu (DKF) Çoklu Erişim Kontrol (ÇEK), kanal yayın süresi eşitliği, çoklu hız oranlı kablosuz ağ, performans anormalliği","In a multi-rate IEEE 802.11 network, the conventional Distributed Coordination Function (DCF) Medium Access Control (MAC) aims to ensure max-min throughput fairness and equal channel access in scenarios with multiple nodes, while failing to satisfy air-time fairness. Consequently, nodes that have relatively poor channels or longer packets to transmit invade the channel substantially more than others, hence decreasing the throughput of nodes which have better channels or shorter packets. This phenomenon is known as the performance anomaly problem in the existing literature. In this thesis, we propose a novel distributed air-time fair algorithm to cope with the performance anomaly problem without having to change the conventional IEEE 802.11 DCF MAC. In the proposed algorithm, each node in the system runs multiple instances of the conventional IEEE 802.11 DCF back-off algorithm where the number of instances for the particular node is calculated independently from other nodes using only local information such as packet lengths and transmission rates. Both analytical and simulation-based results are provided to validate the effectiveness of the distributed air-time fair algorithm we propose.Keywords: IEEE 802.11 Distributed Coordination Function (DCF) MAC, air-time fairness, multi-rate wireless network, performance anomaly"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Modern işletim sistemlerinde bulunan birçok önlem ve koruma mekanizmasınarağmen, bellek bozma açıklarının istismarı yazılım sistemlerinin ve bilgisayarağlarının güvenliği için hala ciddi bir tehdit oluşturmaktadır. Yakın geçmişte ortayaatılan ?Dönüşe-Dayalı Programlama (DDP)? isimli istismar tekniği son zamanlardaakademik ortamda oldukça dikkat çekti.DDP saldırıları, serbest-dal yönergeleriyle, yani bir saldırganın yürütme akışınıkontrol etmesine olanak sağlayan yönergelerle sonlanan kısa kod dizilerinden faydalanır.İkili yürütülebilirlerde mevcut olan bu tür dizileri, diğer bir deyişleaygıtları, teşhis ederek ve bunları birbirlerine zincirleyerek keyfi hesaplamalar yapmakmümkündür. Geçmişte bu konu üzerinde yapılan araştırmalar çoğunlukla orijinalsaldırı tekniklerinin geliştirilmesi veya sadece belirli saldırı türevlerini hedef alankısmi çözümler önerilmesi üzerine odaklanmıştır.Bu çalışmada, DDP'nin mümkün olan her şekline yönelik ilk pratik çözümütemsil eden, derleyici tabanlı bir yaklaşım sunuyoruz. Çözümümüz, bir saldırganınkötüye kullanmasını engellemek amacıyla hizalı serbest-dal yönergelerini koruyabilmekteve ikili yürütülebilirlerin içerisindeki tüm hizasız serbest-dal yönergeleriniortadan kaldırabilmektedir. Yaklaşımımıza dayanarak x86 mimarisini hedefleyen birprototip geliştirdik; GNU libc ve birkaç gerçek uygulama derleyerek bu prototipi değerlendirdik.Deney sonuçlarına göre, sunduğumuz çözüm her türlü DDP saldırısınıengelleyebilmektedir.","Despite the numerous prevention and protection mechanisms that have been introducedinto modern operating systems, the exploitation of memory corruption vulnerabilitiesstill represents a serious threat to the security of software systems and networks. Arecent exploitation technique, called Return-Oriented Programming (ROP), has latelyattracted a considerable attention from academia.ROP attacks utilize short code sequences each ending with a free-branch instruction,i.e. an instruction that allows the attacker to control the execution flow. Identifyingsuch sequences, or gadgets, available in binary executables and chaining themtogether, it is possible to perform arbitrary computations. Past research on the topichas mostly focused on refining the original attack technique, or on proposing partialsolutions that target only particular variants of the attack.In this work, we present a compiler-based approach that represents the first practicalsolution against any possible form of ROP. Our solution is able to protect thealigned free-branch instructions to prevent them from being misused by an attacker,and to eliminate all unaligned free-branch instructions inside a binary executable. Wedeveloped a prototype based on our approach for the x86 architecture, and evaluatedit by compiling GNU libc and a number of real-world applications. The results ofthe experiments demonstrate that our solution is able to prevent any form of return-orientedprogramming attack."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Risk birçok farklı alanda mevcuttur; tıbbi tanı, finansal piyasalar, dolandırıcılık tespiti ve sigorta poliçeleri bunların birkaçıdır. Çeşitli risk ölçütleri ve risk tahmin sistemleri bugüne kadar önerildi ve bu tez yeni bir risk tahmini yöntemi sunmaktadır. Receiver Operating Characteristics (ROC) eğrisi altındaki alanı maksimize ederek risk tahmin yöntemi (REMARC), risk tahmini bir sıralama sorunu olarak tanımlar. ROC eğrisi altındaki alan (AUC) değeri sıralama kalitesini ölçme ile ilgili olduğundan, REMARC tek bir öznitelik üzerinde en yüksek AUC'yi elde ederek her öznitelik üzerinde mümkün olabilecek en iyi sıralamayı sağlamayı hedeflemetedir. Verilen bir kategorik öznitelik için, herhangi bir risk yordamının en yüksek AUC'yi elde etmek için sağlaması gereken şartın ne olduğunu ispatladık. Sayısal öznitelikler de ölçüt olarak AUC'yi kullanan bir yöntemle ayrıklaştırılmıştır. Sonra, sezgisel bir yaklaşımla AUC'nin maksimize eldilmesi tüm veriseti üzerine genişletilmiştir. REMARC eksik verileri, ikili sınıfları, sürekli ve kategorik öznitelikleri işleyebilir. REMARC yöntemi sadece risk değeri tahmin etmekle kalmaz aynı zamanda her bir öznitelik üzerinde analiz yapar ve karar verme esnasında alan uzmanlarına değerli bilgiler sağlar. REMARC'ın performansı, UCI veriseti deposundan elde edilen birçok veri seti ile support vector machine naïve Bayes, decision trees (karar ağaçları) ve boosting (arttırma) yöntemleri gibi modern algoritmalar kullanılarak değerlendirilmiştir. AUC ölçütüyle yapılan değerlendirmeler göstermektedir ki REMARC diğer birçok makina öğrenmesi yönteminden önemli derecede daha iyi tahmin performansına sahiptir ve diğer yöntemden çoğundan daha hızlı çalışmaktadır.Kardiyovasküler cerrahi alanı, REMARC yöntemi ile yeni risk tahmini çerçevesi oluşturmak amacıyla seçilmiştir. TurkoSCORE projesi, REMARC algoritmasının öğrenme aşaması için veri toplamak amacıyla kullanıldı. REMARC'ın tahmin performansı, en popüler kardiyovasküler cerrahi riski değerlendirme yöntemlerinden biri olan EuroSCORE ile karşılaştırıldı. EuroSCORE Türk hastalar üzerinde değerlendirildi ve EuroSCORE modelinin Türk nüfusü için yeterli olmadığı gösterildi. Sonra, EuroSCORE ve tahmin için REMARC kullanan TurkoSCORE'un tahmin performansı karşılaştırldı. Deneysel değerlendirmeler göstermektedir ki REMARC Türk hasta popülasyonunda EuroSCORE'a göre daha iyi tahmin performansı göstermektedir.","Risks exist in many different domains; medical diagnoses, financial markets, fraud detection and insurance policies are some examples. Various risk measures and risk estimation systems have hitherto been proposed and this thesis suggests a new risk estimation method. Risk estimation by maximizing the area under a Receiver Operating Characteristics (ROC) curve (REMARC) defines risk estimation as a ranking problem. Since the area under ROC curve (AUC) is related to measuring the quality of ranking, REMARC aims to maximize the AUC value on a single feature basis to obtain the best ranking possible on each feature. For a given categorical feature, we prove a sufficient condition that any function must satisfy to achieve the maximum AUC. Continuous features are also discretized by a method that uses AUC as a metric. Then, a heuristic is used to extend this maximization to all features of a dataset. REMARC can handle missing data, binary classes and continuous and nominal feature values. The REMARC method does not only estimate a single risk value, but also analyzes each feature and provides valuable information to domain experts for decision making. The performance of REMARC is evaluated with many datasets in the UCI repository by using different state-of-the-art algorithms such as Support Vector Machines, naïve Bayes, decision trees and boosting methods. Evaluations of the AUC metric show REMARC achieves predictive performance significantly better compared with other machine learning classification methods and is also faster than most of them.In order to develop new risk estimation framework by using the REMARC method cardiovascular surgery domain is selected. The TurkoSCORE project is used to collect data for training phase of the REMARC algorithm. The predictive performance of REMARC is compared with one of the most popular cardiovascular surgical risk evaluation method, called EuroSCORE. EuroSCORE is evaluated on Turkish patients and it is shown that EuroSCORE model is insufficient for Turkish population. Then, the predictive performances of EuroSCORE and TurkoSCORE that uses REMARC for prediction are compared. Empirical evaluations show that REMARC achieves better prediction than EuroSCORE on Turkish patient population."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Haber portalları okuyuculara bilgi erişimi, kişiselleştirilmiş bilgi filtreleme, özet çıkarma ve haber kümeleme gibi bir çok hizmet sunmaktadır. Bunlara ek olarak, pek çok haber portalı çok sayıda kaynaktan beslenerek kullanıcılarının gelişmeleri değişik açılardan değerlendirebilmelerını sağlamaktadır. Fakat artan haber kaynağı ve haber sayısı, haber okuyucularının kendi ilgi alanlarında olan haberleri bulabilmelerini zorlaştırmaktadır. Haberlerin kolay bir şekilde taranabilmesi için değişik düzenlemelerde bulunulmaktadır. Bu düzenlemelerden biri olan yeni olay bulma ve izleme (YOBİ) haberler bahsettikleri olaylara göre organize etmektedir. Çok sayıda kaynak kullanılmasından kaynaklanan bilgi tekrarlanmasından dolayı YOBİ uygulaması da bazen kendi başına yeterli olamamaktadır. Bu tezde, bir konuyu takip eden haberler üzerinde yenilik bulma (YB) uygulanması incelenmektedir. Bu amaçla ortalama 51 izleyen haber içeren 59 konudan oluşan bir Türkçe YB deney derlemi, BilNov, tarafımızdan hazırlanmıştır. YB için üç metot önermekteyiz; kosinüs benzerliğine dayalı YB yöntemi, dil modellemeye dayalı YB yöntemi ve kapsama katsayısına dayalı YB yöntemi. Ayrıca, literatürde ilk defa kategori temelli sınır değeri öğrenme üzerine de deneyler yapılmaktadır. Ek olarak Türkçe üzerinde YB yöntemleri için doküman vektör uzunlukları ve düzgünleştirme benzeri bazı deneysel parametrelerle ilgili gözlemler sunulmaktadır. Son olarak TREC Yenilik Bulma 2004 deney derlemiyle de deneyler yaptıyoruz. BilNov kullanılarak yapılan deneylerin sonuçlarına göre dil modellemeye dayalı YB yöntemi diğer iki yöntemi belirgin bir şekilde geçmektedir ve ayrıca kategoriye dayalı sınır değeri öğrenme yaklaşımı da genel sınır değeri öğrenmeyle karşılaştırıldığında umut verici sonuçlar vermektedir.","News portals provide many services to the news consumers such as information retrieval, personalized information filtering, summarization and news clustering. Additionally, many news portals using multiple sources enable their users to evaluate developments from different perspectives by richening the content. However, increasing number of sources and incoming news makes it difficult for news consumers to find news of their interest in news portals. Different types of organizational operations are applied to ease browsing over the news for this reason. New event detection and tracking (NEDT) is one of these operations which aims to organize news with respect to the events that they report. NEDT may not also be enough by itself to satisfy the news consumers' needs because of the repetitions of information that may occur in the tracking news of a topic due to usage of multiple sources. In this thesis, we investigate usage of novelty detection (ND) in tracking news of a topic. For this aim, we built a Turkish ND experimental collection, BilNov, consisting of 59 topics with an average of 51 tracking news. We propose usage of three methods; cosine similarity-based ND method, language model-based ND method and cover coefficient-based ND method. Additionally, we experiment on category-based threshold learning which has not been worked on previously in ND literature. We also provide some experimental pointers for ND in Turkish such as restriction of document vector lengths and smoothing methods. Finally, we experiment on TREC Novelty Track 2004 dataset. Experiments conducted by using BilNov show that language model-based ND method outperforms other two methods significantly and category-based threshold learning has promising results when compared to general threshold learning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tarihi dokümanların sayısal ortama aktarılması ile, bu doköumanlara hızlı erişim daha çok önem kazanmıştır. Sayısal ortamdaki tarihi dokümanların elle dizinlenmesi çok zaman almanın yanı sıra ancak sınırlı sayıda dokümanlar üzerinde yapılabilmektedir. Bu yüzden otomatik dizinleme önem kazanmaktadır. Optik karakter tanıma (OPT) sistemleri yıllardır çalısılan bir konu olmakla beraber, tarihi dokümanlar üzerinde çoğunlukla istenilen sonuçları vermemektedir. Buna neden olarak, tarihi dokümanların yıpranmış olması, ve yazım şekillerinin farklılıkları gösterilebilir. Daha da önemlisi, OPT sistemleri genellikle tek bir dile odaklı olarak çalışmaktadır, ve farklı diller için çalışan sistemler nadir olarak bulunmaktadır. Kelime tabanlı arama teknikleri, OPT çalışmalarına alternatif olarak sunulmuştur ve kelimelerin tek bir seferde okunduğu prensibine dayanır. Bu tip çalısmalarda, kelimenin harferini ayrı ayrı incelemek yerine kelimeninbütün olarak incelenmesi esasına dayanır. Yıpranmış ve lekeli dokümanlar, ve farklı yazım şekilleri gibi etkenlerden dolayı, tarihi dokümanlarda tanımlama ve arama, kelime tabanlı arama çalışmalarında da henüz tam olarak çözülememiştir. Bu çalışmada, bu problemlere çözüm olarak basit fakat etkili bir yöntem sunulmuştur; bu yöntemde kelimeler çizgi tabanlı bir niteleme yöntemiyle ifade edilmiştir. Buna ek olarak, iki farklı eşleme yöntemi sunulmuş, ve bu yöntemler kelime eşlemek ve redif bulmak için kullanılmıştır. Çizgi tabanlı niteleme yöntemini kullanan sunduğumuz yaklaşımlar, önceki çalışmaların aksine karmaşık ön işleme safhalarına ihtiyaç duymamaktadır. Kelime eşleme için yapılan deneylerin sonuçlarının, daha önceki çalişmalarda elde edilen sonuçlardan daha iyi olduğu gözlemlenmiştir. Redif bulma işlemi göz önünde bulundurulduğunda ise deneylerin sonuçları, daha detaylı çalışmalar için ümit vaat edicidir.","With the increase of the number of documents available in the digital environment, efficient access to the documents becomes crucial. Manual indexing of the documents is costly; however, and can be carried out only in limited amounts. Therefore, automatic analysis of documents is crucial. Although plenty of effort has been spent on optical character recognition (OCR), most of the existing OCR systems fail to address the challenge of recognizing characters in historical documents on account of the poor quality of old documents, the high level of noise factors, and the variety of scripts. More importantly, OCR systems are usually language dependent and not available for all languages. Word spotting techniques have been recently proposed to access the historical documents with the idea that humans read whole words at a time. In these studies the words rather than the characters are considered as the basic units. Due to the poor quality of historical documents, the representation and matching of words continue to be challenging problems for word spotting. In this study we address these challenges and propose a simple but effective method for the representation of word images by a set of line descriptors. Then, two different matching criteria making use of the line-based representation are proposed. We apply our methods on the word spotting and redif extraction tasks. The proposed line-based representation does not require any specific pre-processing steps, and is applicable to different languages and scripts. In word spotting task, our results provide higher scores than the existing word spotting studies in terms of retrieval and recognition performances. In the redif extraction task, we obtain promising results providing a motivation for further and advanced studies on Ottoman literary texts."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda İnternetteki hızlı gelişme, içeriğindeki bilgilerin sürekli artış göstermesi bubilgilerin düzenlenmesi ihtiyacını ortaya çıkarmıştır. Ayrıca Web ortamındaki haberkaynaklarının sayısında ve bu kaynaklar tarafından yayımlanan haberlerde aşırı artışgözlenmektedir. Bu artış sonrasında bu haberlerin düzenlenmesi içerisinden yeniolayların bulunması, yeni haberlerin izleyenlerinin tespiti önemli problem halinegelmiştir. Yeni olay bulma (YOB) ve izleme haber akışlarını takip ederek, bu sorunuçözmeyi amaçlamaktadır. Haberlerde genel olarak önemli konular haberin başlarındaverilmektedir. Bu gözlemden hareketle araştırmamızda YOB deneylerimizde en iyisonucu veren Okapi benzerlik formülünün terim ağırlıklandırması fonksiyonunudeğiştirerek, kelimelerin haber içindeki sırasını bu fonksiyona uyarlayarak bunu YOBsisteminde kullandık. Bu amaçla, Türkçe için hazırlanmış olan BilCol2005 derlemiylebirçok deney gerçekleştirdik. BilCol2005 deney derlemi TDT çalışmalarındanesinlenerek hazırlanmıştır. Derlem 209,305 dokümandan ve seksen tanesi insanlartarafından etiketlenmiş olaylardan oluşmaktadır. Bu çalışmada çeşitli kronolojik terimağırlıklandırması (KTA) fonksiyonlarının, başarımı %13 kadar arttırdığı gözlenmiştir.Ayrıca KTA kullanarak yapılan YOB sisteminin BilCol2005'ten N-geçişli bulmayöntemiyle elde edilen farklı deney derlemlerinde de başarılı sonuçlar verdiğigözlenmiştir. Yapılan test sonuçlarında iyileştirmeler istatistiksek olarak kayda değerolduğu gözlenmiştir.","News web pages are an important resource for news consumers since the Internetprovides the most up-to-date information. However, the abundance of this informationis overwhelming. In order to solve this problem, news articles should be organized invarious ways. For example, new event detection (NED) and tracking studies aim tosolve this problem by categorizing news stories according to events. Generally,important issues are presented at the beginning of news articles. Based on thisobservation, we modify the term weighting component of the Okapi similarity measurein several different ways and use them in NED. We perform numerous experiments inTurkish using the BilCol2005 test collection that contains 209,305 documents from theentire year of 2005 and involves several events in which eighty of them are annotated byhumans. In this study, we developed various chronological term ranking (CTR)functions using term positions with several parameters. Our experimental results showthat CTR in combination with Okapi improves the effectiveness of a baseline systemwith a desirable performance up to 13%. We demonstrate that NED using CTR has arobust performance in different versions of TDT collection generated by N-passdetection evaluation. The tests indicate that the improvements are statisticallysignificant."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Robotlar genel olarak algılayıcılardan veri toplama, algısal girdileri kararalmak için değerlendirme, ve komutları uygun eyleyiciler üzerinde uygulamaşeklinde bir dizi görevleri yerine getirerek çalışmaktadırlar. Bu döngü verilerinrobotun elektro-mekanik bileşenleri arasındaki iletimini gerektirdiğinden,yüksek kalitede bir iletişimi sağlama temel bir gereksinimdir. Kaliteli bir iletişimaltyapısı güvenilirlik, dayanıklılık, geliştirilebilirlik ve verimlilik gibi özelliklerinyanı sıra, kullanıldığı robotun bütün iletişim ihtiyaçlarını karşılayabiliyor olmalıdır.Örneğin, hızlı hareket eden, tepkin bir kontrol birimine sahip olan vekritik zamanlı işlerde kullanılan özerk bir robotun istenen tepki sürelerini garantieden gerçek zamanlı bir iletişim altyapısına sahip olması gereklidir.Evrensel Robot Veriyolu (URB), dağıtık kontrol sistemleri için tasarlanmışI2C tabanlı bir iletişim çatısıdır ve özellikle hızlı hareket eden özerk robotlardakullanılmak üzere geliştirilmiştir. Kullanıcılara gerçek zamanlı işlem olanağı tepkizamanlarında üst limitler tanımlanarak verilmiştir. URB, merkezi bir kontrolbirimi ile dağıtık bulunan algılayıcı ve eyleyici birimleri arasındaki veri iletimineolanak tanır. Merkezi bir topolojiyi benimseyerek dağıtık uç birimleri doğrudanmerkezi kontrol birimine bağlamak, merkezi birim etrafında bir darboğaz yaratmakta,özellikle ölçeklenebilirlik, gürültü ve kablolama hususlarında sorunlaryaratmaktadır. Bu problemi aşmak için, URB'nin fiziksel gerçekleştirimindemerkezi ve uç birimler arasına bir ağ geçidi (köprü) yerleştirilmiştir. Bu ağgeçidi merkezi birimin iş yükünü hafifletmekte ve bağlı bununduğu I2C veriyolunuyönetmektedir. Merkezi birim ve ağ geçidi arasındaki üst bağlantı kanalıolarak adlandırdığımız bağlantı, iletişim gereksinimlerini karşılayan yüksek bantgenişliğine sahip herhangi bir iletişim seçeneği ile gerçekleştirilebilinir.Bu tezin temel katkısı URB üst bağlantı kanalının Evrensel Seri Veriyolu(USB) kullanarak tasarım ve gerçekleştirimidir. USB'nin yoklama düzeneğindendolayı gerçek zamanlı işlem tam anlamıyla mümkün olmasa bile, 1ms'lik yoklamaperyotu uygulama alanımız için kabul edilebilir bir değerdir. Bu tezde, URBüst bağlantısının fiziksel gerçekleştiriminde kullanılan bazı donanım bileşenleriile geliştirilen yazılımların detaylı açıklamaları yer almaktadır. Bu detaylarınbaşlıcaları ağ geçidinde çalışan bellenim, merkezi birimde çalışan Linux tabanlıaygıt sürücü ve USB kütüphanesini kullanan bir istemci kontrol yazılımı, ve sonolarak uygulama-sürücü ve sürücü-bellenim arasında yer alan alt-protokollerdir.Bu tez ayrıca USB üst bağlantısının başarımını gidiş-geliş gecikmesi, bantgenişliği, ölçeklenebilirlik, dayanıklılık ve güvenilirlik gibi ölçütlere göre belirlemeyeçalışan deneylerin sonuçlarını da içermektedir. Son olarak, bu tez aygıtsürücü geliştirimi, Linux çekirdek programlama, iletişim protokolleri, USB vegerçek zamanlı uygulama alanları gibi konularda kaynak niteliğindedir.","A typical robot operates by carrying out a sequence of tasks, usually consistingof acquisition of sensory data, interpretation of sensory inputs for makingdecisions, and application of commands on appropriate actuators. Since this cycleinvolves transmission of data among electro-mechanical components of therobot, high quality communication is a fundamental requirement. Besides beingreliable, robust, extensible, and efficient, a high quality communication infrastructureshould satisfy all additional communication requirements that are specific tothe robot it is used within. To give an example, for a rapid moving autonomousrobot with a reactive controller which is intended to be used in time critical situations,a real-time communication infrastructure which guarantees demandedresponse times is required.The Universal Robot Bus (URB) is a distributed communication frameworkbased on the widely used I2C standard, intended to be used specifically withinrapid autonomous robots. Real-time operation guarantees are provided by definingupper bounds in response times. URB facilitates exchange of informationbetween a central controller and distributed sensory and actuator units. Adoptionof a centralized topology by connecting distributed units directly to a centralcontroller creates a bottleneck around the central unit, causing problems in scalability,noise and cabling. In order to overcome this problem, URB is physicallyrealized such that gateways (bridges) are incorporated between the central anddistributed units which offload the work of the central unit and master the underlyingI2C bus. Connection between the central unit and the gateway, the uplinkchannel, can be established using any high bandwidth communication alternativewhich successfully satisfies communication requirements of the system.The main contribution of this thesis is the design and implementation of the URB uplink channel using the well known Universal Serial Bus (USB) protocol.Although true real-time operation is not feasible with USB due to its pollingmechanism, USB frame scheduling of 1ms is acceptable for our application domain.In this thesis, hardware components used in the USB uplink implementationas well as our software implementation are covered in detail. These detailsinclude the firmware running on the gateway, a Linux based device driver and aclient control software that uses a USB library running on central controller, andfinally sub-protocols between the application-driver and driver-firmware layers.The thesis also includes our experiments to estimate the performance of the USBuplink in terms of its roundtrip latency, bandwidth, scalability, robustness, andreliability. Finally, this thesis also serves as a reference on distributed systems,device driver development, Linux kernel programming, communication protocols,USB and its usage in real-time applications."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"İyi bilinen örnek bir uzamsal ağ olan yol ağları, bölge bazlı servisler, akıllı yolculuk sistemleri, araç telematikleri, ve bölgeye duyarlı reklamcılık gibi coğrafi bilgi sistemi uygulamalarının temel bir parçasını oluşturmaktadır. Uygulamada, yol ağ verileri geçici belleğe sığamayacak kadar büyüktür. Hem çeşitli uzamsal ve uzamsal olmayan özellikler, hem de ilgi çekici noktalar kavşak ve yollarla ilişkilendirildiğinden, verinin oldukça büyük bir parçası ikincil bellekte saklanmalıdır. Ağ sorgusu işlemede, veri erişimi sırasındaki uzamsal tutarlılık zamansal tutarlılığa neden olmaktadır; böylece, bağlı kavşaklara neredeyse eşzamanlı erişilmektedirler. Bu gerçeği gözönünde bulundurarak, bağlı kavşakların verilerini aynı disk bölümleri içerine yerleştirilmesi mantıklı görünmektedir ki veriler daha az sayıda disk erişimi ile getirilebilsin. Şu andaki verileri disk bölümlerine kümeleyen çizge modelinin ardıl getirme operasyonlarının disk erişim maliyetini doğru yakalamadığını gösteriyoruz. Topak sorgu işleme sırasındaki disk erişimini en aza indirmek için sorgu işlemede uzamsal ve zamansal tutarlılığı sorgu kayıtlarını kullanarak doğru kapsayan hiperçizge bölümlemeye dayalı kümeleme modelleri öneriyoruz. Yol ağları için yola dayalı depolama planınını yaygın kullanılan kavşağa dayalı depolama planına alternatif olarak tanıtıyoruz. Ağ sorgularında getirilecek aday ardılları sorgu işleme sırasında azaltacak yeni bir ardıl getirme işlemi olarak Değerlendirilmemiş-Ardılları-Getir'i sunuyoruz. İki değişik Değerlendirilmemiş-Ardılları-Getir işlem örneğini inceliyoruz: İşlenmemiş-Ardılları-Getir işlemi tipik olarak Dijkstra'nın tek başlangıç kısayol çözüm yolunda ve Görülmemiş-Ardılları-Getir işlemi tipik olarak atrımlı ağ genişleme taslağında ortaya çıkıyor. Benzetim sonuçları kümeleyen hiperçizge modellerini kullanan depolama ve erişim planlarımızın topak sorgu işleme sırasındaki disk erişim sayısını azaltmakta oldukça etkili olduğunu göstermektedir.","A well-known example of spatial networks is road networks, which form an integral part of many geographic information system applications, such as location-based services, intelligent traveling systems, vehicle telematics, and location-aware advertising. In practice, road network data is too large to fit into the volatile memory. A considerable portion of the data must be stored on the secondary storage since several spatial and non-spatial attributes as well as points-of-interest data are associated with junctions and links. In network query processing, the spatial coherency that exists in accessing data leads to a temporal coherency; in this way, connected junctions are accessed almost concurrently. Taking this fact into consideration, it seems reasonable to place the data associated with connected junctions in the same disk pages so that the data can be fetched to the memory with fewer disk accesses. We show that the state-of-the-art clustering graph model for allocation of data to disk pages is not able to correctly capture the disk access cost of successor retrieval operations. We propose clustering models based on hypergraph partitioning, which correctly encapsulate the spatial and temporal coherency in query processing via the utilization of query logs in order to minimize the number of disk accesses during aggregate query processing. We introduce the link-based storage scheme for road networks as an alternative to the widely used junction-based storage scheme. We present Get-Unevaluated-Successors (GUS) as a new successor retrieval operation for network queries, where the candidate successors to be retrieved are pruned during processing a query. We investigate two different instances of GUS operation: the Get-unProcessed-Successors operation typically arises in Dijkstra's single source shortest path algorithm, and the Get-unVisited-Successors operation typically arises in the incremental network expansion framework. The simulation results show that our storage and access schemes utilizing the proposed clustering hypergraph models are quite effective in reducing the number of disk accesses during aggregate query processing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Eşler Arası (Peer-to-peer) mimarisi, klasik istemci/sunucu mimarisinin yerinialarak çeşitli uygulamalarda giderek daha çok kullanılmaya başlamıştır. Mobilcihazların gelişen özellikleriyle beraber, kablosuz ağlar eşler arası modelini kullanıp, bu ağların altyapı gerektirmeme, ölçeklenebilirlik ve dengeli yük dağılımı gibi özelliklerinden faydalanmaya başlamıştır. Mobil eşler arası (Mobile Peer-to-peer) ağlar terimi, eşler arası mimari ile kablosuz ağları birleşiminden ortayaçıkmıştır.Sağlık bilgi sistemleri 1950'li yıllardan beri sağlık görevlilerinin hasta vehastalık bilgilerini tutmasını ve birbirleriyle iletişimini sağlamıştır. Günümüzdeistemci/sunucu mimarisiyle geliştirilmiş olarak yaygın bir şekilde hastanelerdekullanılmaktadır. Kablosuz teknolojiler de tıp alannda özellikle hasta takipamacıyla kullanılmaktadır.Bu tezde, tıbbi bir ortam icin tasarlanmış olan bir mobil eşler arası ağ üzerindeçalışan çeşitli veri dağılım stratejileri sunulmakta ve değerlendirilmektedir. İlkolarak, tasarlanan ağ sistemi, ağ topolojisiyle birlikte tanıtılmaktadır. Dahasonra, önerilen veri dağıtım stratejileri tanımlanmaktadır. En son olarak da, bustratejiler, tıbbi bir sistemin nitelik ve ihtiyaçlarına göre değerlendirilmektedir.","Peer-to-peer (P2P) architecture is becoming increasingly popular for variousapplications, replacing the classical Client-Server architecture. With theenhanced capabilities of mobile devices (PDAs, mobile phones, etc.) wirelessnetworks started to take advantage of P2P paradigm along with its propertieslike infrastructure-free operation, scalability, balanced and distributed workload.Mobile peer-to-peer (MP2P) networks refer to the application of P2P architectureover wireless networks. Problems about dissemination of data in both P2P andMP2P networks are widely studied, and there are many proposed solutions.Healthcare information systems are helping clinicians to hold the informationbelonging to patients and diseases, and to communicate with each othersince early 1950s. Today, they are widely used in hospitals, being constructedusing Client-Server network architecture. Wireless technologies are also appliedto medical domain, especially for monitoring purposes.In this thesis, we present and evaluate various data dissemination strategies towork on a mobile peer-to-peer (MP2P) network designed for a medical healthcareenvironment. First, the designed network system is presented along with the networktopology. Then, the proposed data dissemination strategies are described.And nally, these strategies are evaluated according to the properties and needsof a medical system."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Aranan bilgiyi Web'de etkili bir şekilde bulmak, son on yıldaki en zorlu problemlerden biri olmuştur. Aranan bilgiyi içeren belgelerin bulunması için Web Arama Motorları kullanılmaktadır. Ancak, bir çok durumda kullanıcı bir belge kümesinden çok belirli bir bilgiye ihtiyaç duyar. Soru Yanıtlama sistemleri bu problemi adreslemektedir. Soru yanıtlama sistemleri bir sorunun yanıtı olarak bir belge kümesi yerine açık yanıtlar döndürürler. Soru yanıtlama sistemlerinin yanıtladığı sorular beş sınıfa ayrılabilir: tekil yanıtlı, liste, tanım, karmaşık, ve kurgusal sorular. Tekil yanıtlı bir sorunun tam olarak tek bir yanıtı vardır ve bu yanıt genellikle kişi, tarih ve yer gibi bir varlık ismidir. Bu tez kapsamında, Türkçe Tekil Yanıtlı Soru Yanıtlama için örüntü eşleştirme yaklaşımı geliştirdik. TREC-10 Soru Yanıtlama kulvarında yarışan soru yanıtlama sistemlerinden birçoğu gelişmiş dilbilimsel araçlar kullanmıştır. Ancak, bu kulvardaki en başarılı soru yanıtlama sistemi sadece çok miktarda yüzeysel örüntü kullanmıştır. Bu nedenle, biz de Türkçe Tekil Yanıtlı Soru Yanıtlama için yanıt örüntüsü eşleştirme yaklaşımının potansiyelini araştırmaya karar verdik. Yanıt örüntüsü çıkarmak için gövdeleme ve varlık isimleri işaretleme içeren yöntemler denedik. Yanıt örüntülerini sorgu genişletme için de kullandık. Sistemin performansını değerlendirmek için bir çok deney yaptık. Diğer tekil yanıtlı soru yanıtlama sistemlerinin performansları ile karşılaştırıldğında, yöntemlerimiz iyi sonuçlar vermektedir. Yapılan deneyler, varlık isimleri işaretleme yönteminin sistemin performansını artırdığını göstermektedir.","Efficiently locating information on the Web has become one of the most important challenges in the last decade. The Web Search Engines have been used to locate the documents containing the required information. However, in many situations a user wants a particular piece of information rather than a document set. Question Answering (QA) systems have addressed this problem and they return explicit answers to questions rather than set of documents. Questions addressed by QA systems can be categorized into five categories: factoid, list, definition, complex, and speculative questions. A factoid question has exactly one correct answer, and the answer is mostly a named entity like person, date, or location. In this thesis, we develop a pattern matching approach for a Turkish Factoid QA system. In TREC-10 QA track, most of the question answering systems used sophisticated linguistic tools. However, the best performing system at the track used only an extensive list of surface patterns; therefore, we decided to investigate the potential of answer pattern matching approach for our Turkish Factoid QA system. We try different methods for answer pattern extraction such as stemming and named entity tagging. We also investigate query expansion by using answer patterns. Several experiments have been performed to evaluate the performance of the system. Compared with the results of the other factoid QA systems, our methods have achieved good results. The results of the experiments show that named entity tagging improves the performance of the system."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Video gözetim son yıllarda en çok ilgilenilen ve üzerinde çalışılan video işleme uygulama alanlarından biridir. Olağandışı durum yakalamak için anlamsal içeriğe otomatik erişim en temel görevlerdendir; ancak büyük girdi boyutu ve görsel özelliklerdeki değişkenlik nedeniyle problem zorluğunu korumaktadır. Literatürde gözetim videolarına otomatik erişim alanında yeterli çalışma yapılmış olmasına rağmen olay modelleri ve içerik erişimi alanlarında anlambilimsel eksiklikler bulunmaktadır. Bu tez kapsamında özellikle iç mekan videoları için senaryo tabanlı sorgulama çatısı önerilmektedir. Senaryo olay yüklemlerinden oluşan bir dizi olarak belirlenmekte ve nesne tabanlı düşük-düzey özellikleri ve yönsel yüklemler ile zenginleştirilebilmektedir. Ayrıca, anahtar çerçeve etiketleme tekniği önerilmektedir. Bu teknik anahtar çerçeve yakalama algoritmasından gelen anahtar çerçevelere etiket atayarak videonun olay dizisi olarak ifade edilmesini sağlamaktadır. Anahtar çerçeve yakalama, içeriğin bakış tabanlı gösterilmesini sağlayan ters izleme yöntemine dayanmaktadır. Bu olay dizisi gösterimi kullanılarak olağandışı durum yakalamak amacıyla sonlu durum makinesi tabanlı mekanizmalar geliştirilmiştir. Bu mekanizmalar ayrıca yardımcı veri çıkarma işleminde de kullanılmaktadır. Senaryo tabanlı sorgulama çatısı ters izleme yöntemini sayesinde olay sonu analizi için ters sorgulama ve bakış tabanlı sorgulamayı desteklemektedir. Desteklenen sorgu tiplerini ifade etmek amacıyla özel olarak tasarlanmış bir gözetim sorgu dili önerilmektedir. Sorgu dilinde ifadeyi kolaylaştırmak için ayrıca görsel sorgu belirleme arayüzü geliştirilmiştir. Başarım deneylerinde gösterildigi gibi anahtar çerçeve etiketleme algoritması bellek gereksinimini önemli ölçüde düşürmekte ve yeterli düzeyde olağandışı durum yakalama başarımı göstermektedir. Ayrıca sorgulama işlemi başarımının video gözetim alanında etkili ifade gücü ve yeterli erişim doğruluğuna sahip olduğunu göstermek için deneyler yapılmıştır.","Video surveillance has become one of the most interesting and challenging application areas in video processing domain. Automated access to the semantic content of surveillance videos to detect anomalies is among the basic tasks; however due to the high variability of the visual features and large size of the video input, it still remains a challenging issue. A considerable amount of research dealing with automated access to video surveillance has appeared in the literature; however, significant semantic gaps in event models and content-based access still remain. In this thesis, we propose a scenario-based query processing framework for video surveillance archives, especially for indoor environments. A scenario is specified as a sequence of event predicates that can be enriched with object-based low-level features and directional predicates. We also propose a keyframe labeling technique, which assigns labels to keyframes extracted based on keyframe detection algorithm, hence transforms the input video to an event sequence based representation. The keyframe detection scheme relies on an inverted tracking scheme, which is a view-based representation of the actual content by an inverted index. We also devise mechanisms based on finite state automata using this event sequence representation to detect a typical set of anomalous events in the scene, which are also used for meta-data extraction. Our query processing framework also supports inverse querying and view-based querying, for after-the-fact activity analysis, since the inverted tracking scheme effectively tracks the moving objects and enables view-based addressing of the scene. We propose a specific surveillance query language to express the supported query types in a scenario-based manner. We also present a visual query specification interface devised to enhance the query-specification process. It has been shown through performance experiments that the keyframe labeling algorithm significantly reduces the storage requirements and yields a reasonable anomaly detection performance. We have also conducted performance experiments to show that our query processing technique has a high expressive power and satisfactory retrieval accuracy in video surveillance."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"SAR (Sentetik Açıklık Radarı) imgelerinde insan yapımı nesnelerin tanımı ve sınıflandırması aktif bir araştırma alanı oluşturmuştur, çünkü kızılötesi ve optik algılayıcıların aksine SAR algılayıcıları günün her saatinde ve her türlü hava şartlarında imge üretebilmektedir [1, 2]. Bu tezde SAR imgelerinden çeşitli öznitelik parametre çıkarma yöntemleri sunulmaktadır. Bu yeni yaklaşım bir ilgi bölgesinin (ROI) ortak değişinti matrisinin hesaplanmasını içeren bölge ortak değişinti (RC) yöntemine dayanmaktadır. Ortak değişinti matrisinin elemanları hedef tespitinde kullanılmıştır. Ayrıca, SAR imgelerinde hedef tespiti için hesaplama yükü açısından daha verimli olan bölge ortak fark matrisi de tanıtılmıştır. MSTAR (Hareketli ve Durağan Hedef Tanıma) veritabanı üzerinde hedef tespit benzetim sonuçları verilmiştir. Bölge ortak değişinti ve fark yöntemleri yüksek tespit doğrulukları ve düşük yanlış kabul yüzdeleri ortaya koymaktadır. Bu yöntemlerin performansları çeşitli uzaklık mertrikleri ve Destekçi Vektör Makine (SVM) sınıflandırıcıları kullanılarak incelenmiştir. Aynı zamanda, benzetim sonuçlarına bakıldığında, bölge ortak fark yönteminin yaygın olarak uygulanan ve burada SVM ile kullanılan Temel Bileşenler Analizi (PCA) yöntemine göre daha iyi sonuçlar verdiği gözlemlenmiştir.Tezin ikinci bölümünde, önerilen yöntemlerin hesaplama yükünü azaltmak için yeni teknikler öne sürülmektedir. Bu yaklaşımda, bir önişleme kademesi olarak imge bölgeleri yön süzgeçlerinden (DFs) geçirilmiştir. Önerilen ortak değişinti ve fark yöntemleri bu süzgeçler aracılığıyla belirlenen kategoriler içerisinde uygulanmıştır. MSTAR veritabanında l1 düzge uzaklık metriği ve SVM kullanılarak alınan kararlar doğrultusunda hedef tespit benzetim sonuçları ortaya konulmuştur. Eğitim imgelerinin kategorilere ayrılmasından dolayı, l1 düzge uzaklık ölçütü kullanıldığında bu imgelerle yapılan karşılaştırma sayısı azalmaktadır. Bu sayede, önceki algoritmanın hesaplama yükü büyük ölçüde düşer. Uzaklık ölçütü olarak l1 düzgeye dayalı SAR imge sınıflandırması sonuçları SVM kullanılarak elde edilen sonuçlara göre daha iyidir. Ayrıca bu sonuçlar, özellikle ortak fark öznitelikleri kullanıldığında, iki-kademeli sistemin hedef tespit performansının önerilen önceki yönteme göre fazla düşmediğini göstermiştir.","Automatic recognition and classification of man-made objects in SAR (Synthetic Aperture Radar) images have been an active research area because SAR sensors can produce images of scenes in all weather conditions at any time of the day which is not possible with infrared and optical sensors [1, 2]. In this thesis, different feature parameter extraction methods from SAR images are proposed. The new approach is based on region covariance (RC) method which involves the computation of a covariance matrix of a ROI (region of interest). Entries of the covariance matrix are used in target detection. In addition, the use of computationally more efficient region codifference matrix for target detection in SAR images is also introduced. Simulation results of target detection in MSTAR (Moving and Stationary Target Recognition) database are presented. The RC and region codifference methods deliver high detection accuracies and low false alarm rates. The performance of these methods is investigated with various distance metrics and Support Vector Machine (SVM) classifiers. It is also observed that the region codifference method produces better results than the commonly used Principle Component Analysis (PCA) method which is used together with SVM.The second part of the thesis offers some techniques to decrease the computational cost of the proposed methods. In this approach, ROIs are filtered by directional filters (DFs) at first as a pre-processing stage. Images are categorized according to the filter outputs. The proposed RC and codifference methods are applied within the categories determined by these filters. Simulation results of target detection in MSTAR database are presented through decisions made with l1 norm distance metric and SVM. The number of comparisons made with the training images using l1 norm distance measure decreases as these images are distributed into categories. Therefore, the computational cost of the previous algorithm is significantly reduced. SAR image classification results based on l1 norm distance metric are better than the results obtained using SVM and they show that the two-stage approach does not reduce the performance rate of the previously proposed method much, especially when codifference features are used."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Eşik kriptografisi, kriptografik bir işlemin gerçekleştirilebilmesi için gerekli olan yetkinin birden çok kullanıcı arasında paylaştırılması gereken durumlarla ilgilenir. Böyle durumlarda genellikle, bir bankanın gizli kriptografik anahtarı gibi çok gizli bir bilgi, bir anahtar paylaşım yöntemi kullanarak, belli sayıda katılımcının gizli bilgiye ulaşabileceği; ancak daha az sayıdaki grupların ulaşamayacağı şekilde bir grup insan arasında paylaştırılır. Anahtar paylaşım problemi ve ilk çözümleri 1979 yılında birbirlerinden bağımsız biçimde Shamir ve Blakley tarafından sunulmuştur. Birbirinden farklı olan bu iki anahtar paylaşım yöntemi de lineer bir anahtar paylaşım yöntemidir. Lİteratürde anahtar paylaşım yöntemlerine birçok eklenti yapılmış ve bu yöntemlere dayanan birçok çözüm yer almıştır. Lİteratürdeki pek çok eklenti temel olarak Shamir anahtar paylaşım yöntemini kullanmıştır. Bu çalışmada Shamir anahtar paylaşım yöntemi için önerilmiş olan bazı eklentilerin Blakley anahtar paylaşım yöntemine nasıl uygulanabilecekleri gösterilmiştir.Standart tek kullanıcılı pek çok kriptografik işlemin eşik kriptografisinde karşılığı vardır. Fonksiyon paylaştırılması problemi, kriptografik bir operasyonun (örneğin şifre çözme veya nitelikli imza atma) hesaplanmasının farklı katılımcılar arasında paylaştırılması ile ilgilidir. Hesaplama için gerekli değerler, uygun bir anahtar paylaşım yöntemi kullanarak taraflara dağıtılır. Daha önce literatürde, pek çoğu Shamir'in anahtar paylaşımını kullanan bir çok fonksiyon paylaşım yöntemi yer almıştır. Bu çalışmada, lineer anahtar paylaşım yöntemleri kullanarak fonksiyon paylaşımının nasıl yapılabileceği incelenmiş ve RSA imzası oluşturma, Paillier şifre çözme ve Sayısal İmza Standardı (DSS) imzası oluşturma için çözümler sunulmuştur. Bu çalışmada önerilen eşik RSA yöntemi Shoup'un Shamir anahtar paylaşımı temelli yönteminin bir genellemesidir. Bu yöntem, benzer bir şekilde sağlam ve sabit düşman modelinde kanıtlanabilir şekilde güvenlidir.Eşik kriptografisinde grupların yetkilendirilmesi basitçe sadece grubun büyüklüğü göz önüne alınarak yapılır. Bundan başka, istenen herhangi bir grubun yetkilendirilebildiği genel erişim yapıları vardır. Kullanıcıların gruplara ayrıldığı ve grup içindeki kullanıcıların birbirlerinin dengi olduğu çok kısımlı erişim yapıları genel erişim yapılarının bir örneğini oluştururlar. Bu erişim yapısı herhangi bir erişim yapısını göstermek için kullanılabilir, çünkü bütün erişim yapıları çok bölümlüdür. Bu erişim yapılarını kullanarak anahtar paylaşımı problemini incelemek için Çin kalan teoremine dayanan Migmotte ve Asmuth-Bloom anahtar paylaşım yömtemleri kullanıldı. Cevaplamaya çalıştığımız soru herhangi bir erişim yapısı için Mignotte veya Asmuth-Bloom dizilerinin bulunup bulunamayacağıdır. Bu amaç için literatürde yer alan bir yöntem uyarlanarak bu diziler oluşturulmuştur. Buna ek olarak, bahsedilen problemi birden çok dizi oluşturarak çözen yeni bir anahtar paylaşım yöntemi önerilmiştir.","Threshold cryptography deals with situations where the authority to initiate orperform cryptographic operations is distributed amongst a group of individuals.Usually in these situations a secret sharing scheme is used to distribute sharesof a highly sensitive secret, such as the private key of a bank, to the involvedindividuals so that only when a su ? cient number of them can reconstruct thesecret but smaller coalitions cannot. The secret sharing problem was introducedindependently by Blakley and Shamir in 1979. They proposed two di ? erent so-lutions. Both secret sharing schemes (SSS) are examples of linear secret sharing.Many extensions and solutions based on these secret sharing schemes have ap-peared in the literature, most of them using Shamir SSS. In this thesis, we applythese ideas to Blakley secret sharing scheme.Many of the standard operations of single-user cryptography have counter-parts in threshold cryptography. Function sharing deals with the problem ofdistribution of the computation of a function (such as decryption or signature)among several parties. The necessary values for the computation are distributedto the participants using a secret sharing scheme. Several function sharingschemes have been proposed in the literature with most of them using Shamirsecret sharing as the underlying SSS. In this work, we investigate how functionsharing can be achieved using linear secret sharing schemes in general and givesolutions of threshold RSA signature, threshold Paillier decryption and thresholdDSS signature operations. The threshold RSA scheme we propose is a generaliza-tion of Shoup?s Shamir-based scheme. It is similarly robust and provably secureunder the static adversary model.In threshold cryptography the authorization of groups of people are decided simply according to their size. There are also general access structures in whichany group can be designed as authorized. Multipartite access structures consti-tute an example of general access structures in which members of a subset areequivalent to each other and can be interchanged. Multipartite access structurescan be used to represent any access structure since all access structures are mul-tipartite. To investigate secret sharing schemes using these access structures,we used Mignotte and Asmuth-Bloom secret sharing schemes which are basedon the Chinese remainder theorem (CRT). The question we tried to asnwer waswhether one can ? nd a Mignotte or Asmuth-Bloom sequence for an arbitraryaccess structure. For this purpose, we adapted an algorithm that appeared in theliterature to generate these sequences. We also proposed a new SSS which solvesthe mentioned problem by generating more than one sequence."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Market sepet verisinden ilişkisel kural öğrenme gibi tipik bir uygulamada, sabit bir zaman dilimi için toplanan işlemler kümesi kural öğrenme algoritmalarına girdi olarak kullanılır. Örneğin, yaygın olarak bilinen Apriori algoritması böyle bir işlem kümesinden ilişkisel kural kümesi öğrenmek üzere uygulanabilir. Ancak, işlemler kümesinden ilişkisel kurallar öğrenme işlemi bir kerelik bir işlem değildir. Örneğin, herhangi bir market yöneticisi her ay bir kez, son bir ay süresince toplanan işlemler kümesi üzerinde ilişkisel kural öğrenme işlemini gerçekleştirebilir. Bu nedenden dolayı, işlem kümelerinin sisteme akan paketler şeklinde girdi olduğu bir problemi ele alacağız. İşlemler kümeleri değişiklik gösteren büyüklükte ve zaman dilimlerinde sisteme gelebilir. Herhangi bir işlemler kümesi sisteme vardığında, ilişkisel kural öğrenme algoritması bu son işlemler kümesi üzerinde çalıştırılarak yeni ilişkisel kurallar öğrenilir. Bu yüzden, öğrenilen ilişkisel kurallar kümesi zaman içinde gitgide büyümekte ve bunların içinden ilginç olanlarının elde edilmesi uzmanlar için pratik bir işlem olmaktan çıkmaktadır. Bu kurallar dizisinden ""ilişkisel kural kümesi akımı"" veya ""akan ilişkisel kurallar"" olarak bahsedebiliriz ve bu araştırmamızın ardındaki ana motivasyon, ilginç kural seçme probleminin üstesinden gelebilecek bir teknik geliştirmektir. Başarılı bir ilişkisel kural madenciliği sistemi ilginç kuralları seçerek konunun uzmanlarına sunabilmelidir. Ancak, belli bir alanda ilişkisel kuralların ilginçliğinin tanımı uzmandan uzmana ve hatta aynı uzman için zaman içinde farklılık gösterebilir. Bu tezde, akan ilişkisel kuralların ilginçlik konsepti tanımı için kişisel bir model öğrenmek üzere sonradan-işlemli bir metod önermekteyiz. Önerilen metodun eşsizliği ilişkisel kuralların ilginçlik kavramını fayda maksimizasyonu tabanlı bir sınıflandırma problemi olarak formüle edebilme ve her bir kullanıcı için farklı bir ilginçlik modeli elde edebilme yeteneğidir. Bu yeni sınıflandırma planında, belirleyici öznitelikler ilişkisel kuralların ilginçliği ile alakalı seçici nesnel ilginçlik faktörleridir ve hedef öznitelik adı geçen kuralların ilginçlik etiketinden oluşmaktadır. Önerilen metod artımlı bir şekilde çalışarak belli bir seviyede kullanıcı etkileşimi içermektedir. Metod gerçek bir market veri kümesi üzerinde değerlendirilmekte ve sonuçlar modelin ilginç kuralları başarılı bir biçimde seçebildiğini göstermektedir.","In a typical application of association rule learning from market basket data, a set of transactions for a fixed period of time is used as input to rule learning algorithms. For example, the well-known Apriori algorithm can be applied to learn a set of association rules from such a transaction set. However, learning association rules from a set of transactions is not a one-time only process. For example, a market manager may perform the association rule learning process once every month over the set of transactions collected through the previous month. For this reason, we will consider the problem where transaction sets are input to the system as a stream of packages. The sets of transactions may come in varying sizes and in varying periods. Once a set of transactions arrive, the association rule learning algorithm is run on the last set of transactions, resulting in a new set of association rules. Therefore, the set of association rules learned will accumulate and increase in number over time, making the mining of interesting ones out of this enlarging set of association rules impractical for human experts. We refer to this sequence of rules as ""association rule set stream"" or ""streaming association rules"" and the main motivation behind this research is to develop a technique to overcome the interesting rule selection problem. A successful association rule mining system should select and present only the interesting rules to the domain experts. However, definition of interestingness of association rules on a given domain usually differs from one expert to the other and also over time for a given expert. In this thesis, we propose a post-processing method to learn a subjective model for the interestingness concept description of the streaming association rules. The uniqueness of the proposed method is its ability to formulate the interestingness issue of association rules as a benefit-maximizing classification problem and obtain a different interestingness model for each user. In this new classification scheme, the determining features are the selective objective interestingness factors, including the rule's content itself, related to the interestingness of the association rules; and the target feature is the interestingness label of those rules. The proposed method works incrementally and employs user interactivity at a certain level. It is evaluated on a real market dataset. The results show that the model can successfully select the interesting ones."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Resim, görüntü, metin dökümanları gibi karmaşık ve düzensiz yapılarda, benzerlik taraması önemli bir işlemdir. Sıkça kullanılan bir yöntem, bu karmaşık verileri öznitelik vektörleriyle temsil etmektir. Bir başka çözüm ise, sadece bir mesafe fonksiyonuna dayanan metrik uzaylar yaklaşımını kullanmaktır. Objelerin iç yapıları hakkında herhangi bir bilgiye bağlı olunmadığından, daha genel bir iskelet oluşturulmaktadır. Metrik uzay yapısını kullanan yöntemlerin, özellikle yüksek boyutlarda daha iyi performans sergiledikleri gösterilmiştir.Benzerlik taramasında kullanılan yaygın bir sorgu şekli, sorgu objesinin, verilen belirli bir alan içindeki komşularının bulunduğu, alan sorgusudur. Bir başka önemli sorgu ise, en yakın k komşu sorgusudur. İstenilen en uzak komşunun mesafesi değişkenlik gösterdiği için, bu sorguları işlemesi daha zordur. Bu nedenle, tam olarak k tane objeyi kapsayacak bir alan tahmini ile işlem bir alan sorgusuna indirgenebilir. Bu tekniği kullanan yöntemlerle ilgili genel bir sorun, alan tahminin düşük çıktığı durumlarda, algoritma az sayıda obje döndürür ve kalan komşuları bulmak için dizin verisi üzerinde birden çok tarama gerekir.Bu tezde, en yakın k komşu taraması için, alan tahminine dayalı yeni bir sistem sunulmaktadır. Bu sistemde, sadece bir sıralı dizin taraması uygulanmaktadır. Bu, eksik komşu bulunduğu durumlar için, uygun aday olabilecek objelerin kısa bir listede tutulması ile sağlanmaktadır. Ayrıca, daha önce savunulmuş yöntemlerden daha iyi bir alan tahmini içeren yeni algoritmalar önerilmiştir. Bu tahminlerin, bahsedilen aday listesinin boyutunu düşük seviyede tutabilecek kadar gerçeğe yakın olduğu gösterilmektedir.","Similarity searching is an important problem for complex and unstructured data such as images, video, and text documents. One common solution is approximating complex objects into feature vectors. Metric spaces approach, on the other hand, relies solely on a distance function between objects. No informationis assumed about the internal structure of the objects, therefore a more general framework is provided. Methods that use the metric spaces have also been shown to perform better especially on high dimensional data.A common query type used in similarity searching is the range query, where all the neighbors in a certain area dened by a query object and a radius are retrieved. Another important type, k-nearest neighbor queries return k closest objects to a given query center. They are more dicult to process since the distance of the kth nearest neighbor varies highly. For that reason, some techniques are proposed to estimate a radius that will return exactly k objects, reducing the computation into a range query. A major problem with these methods is that multiple passes over the index data is required if the estimation is low.In this thesis we propose a new framework for k-nearest neighbor search based on radius estimation where only one sequential pass over the index data is required. We accomplish this by caching a short-list of promising candidates. We also propose several algorithms to estimate the query radius which outperform previously proposed methods. We show that our estimations are accurate enough to keep the size of the promising objects at acceptable levels."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Web dokümanlarının sayısı on yıldan fazla bir süredir katlanarak artmaktadır. Benzer şekilde, Web ortamında kısmen veya tamamen eşlenik dokümanlar sıklıkla görülmektedir. İnternet teknolojisindeki ilerlemeler beraberinde haber ajanslarının sayısını artırmıştır. İnsanlar haberleri farklı kaynaklardaki dokümanları bir araya toplayan haber portalları üzerinden okuma eğilimindedirler. Bu portallarda eşlenik veya yaklaşık aynı haberlerin bulunması yaygın bir problemdir. Eşlenik haberler fazlalık oluşturur ve çok az kullanıcı aynı bilgileri içeren haberleri okumak isteyebilir. Eşlenik dokümanlar arama motorlarının etkinliğini ve verimliliğini düşürmektedir. Bu tezde yeni bir yaklaşık aynı haberleri saptama algoritması olan Tweezer'ı önerip, değerlendirdik. Bu algoritmada adlandırılmış nesnelere karşılık gelen kelimeler ile bu kelimelerin öncesinde gelen ve onları izleyen kelimeler dokümanın imzasının oluşturulmasında kullanılmaktadır. Aynı imzayı paylaşan dokümanlar yaklaşık-aynı olarak kabul edilmektedir. Adlandırılmış nesnelerin saptanması için Türkçe Adlandırılmış Nesne Tanıyıcı, TuNER, yöntemi önerilmiştir. Tweezer'ın değerlendirmesi için Bilkent Haber Portalı'ndan sağlanan haberler kullanılarak hazırlanan doküman seti kullanılmıştır. Deneylerde Tweezer en gelişkin eşlenik saptama algoritmalarından birisi olan ve kelimelerin Ters Doküman Frekansı, IDF, değerlerini kullanarak doküman imzalarını çıkaran I-Match ile karşılaştırılmıştır. Yanlış ikaz ve kaçırma oranı olasılıklarını birleştiren bir maliyet fonksiyonu, ve anma ve duyarlılıgı birleştiren F-ölçütü kullanılarak Tweezer'ın I-Match'ten istatiksel olarak önemli ölçüde daha iyi olduğu deneysel şekilde gösterilmiştir. Bunun yanında Tweezer, I-Match'ten en az %7 daha hızlıdır.","The number of web documents has been increasing in an exponential manner for more than a decade. In a similar way, partially or completely duplicate documents appear frequently on the Web. Advances in the Internet technologies have increased the number of news agencies. People tend to read news from news portals that aggregate documents from different sources. The existence of duplicate or near-duplicate news in these portals is a common problem. Duplicate documents create redundancy and only a few users may want to read news containing identical information. Duplicate documents decrease the efficiency and effectiveness of search engines. In this thesis, we propose and evaluate a new near-duplicate news detection algorithm: Tweezer. In this algorithm, named entities and the words that appear before and after them are used to create document signatures. Documents sharing the same signatures are considered as a near-duplicate. For named entity detection, we introduce a method called Turkish Named Entity Recognizer, TuNER. For the evaluation of Tweezer, a document collection is created using news articles obtained from Bilkent News Portal. In the experiments, Tweezer is compared with I-Match, which is a state-of-the-art near-duplicate detection algorithm that creates document signatures using Inverse Document Frequency, IDF, values of terms. It is experimentally shown that the effectiveness of Tweezer is statistically significantly better than that of I-Match by using a cost function that combines false alarm and miss rate probabilities, and the F-measure that combines precision and recall. Furthermore, Tweezer is at least 7% faster than I-Match."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hakem değerlendirmelerinin ordinal sıralama yöntemiyle yapılanlarında, belirli bir grup öneri, maksimum sayıda ikili kıyaslama elde etmek için sabit sayıda hakeme, belirli hakem kapasitesi ve öneri konu kısıtları altında atanır. Aşağıda belirtilen iki ilgili problem üzerinde çalışıldı: (1) Her bir hakemin n tane öneri içinden en fazla k tane okuyabilmesi varsayımı altında, öyle ki 2 <= k <= n, tüm öneri çiftlerinin en az bir hakem tarafından okunmasını garanti edebilmek için en az sayıda gereken hakem sayısının hesaplanması, (2) (1)'de hesaplanan en az sayıda hakem sayısına denk gelen atama yapısının bulunması. k = n durumunda 1 hakemin hem gerekli hem de yeterli olduğu ve k = 2 durumunda n(n-1)/2 hakemin hem gerekli hem de yeterli olduğu kolaylıkla görülmektedir. k = n/2 durumunda 6 tane hakemin hem gerekli hem de yeterli olduğu gösterilmiştir. Ayrıca k = n/3 durumunda 11 hakemin gerekli ve 12 hakemin yeterli olduğu ve k = n/4 durumunda 18 hakemin gerekli ve 20 hakemin yeterli olduğu gösterilmiştir. Herhangi bir k, 2 <= k <= n değeri için daha genel bir alt sınır hakem sayısı n(n-1)/k(k-1) olarak sunulmuştur ve bu alt sınıra en fazla iki katına kadar asimtotik olarak denk gelen bir atama yapısı sunulmuştur. Bu sonuçlar sadece teorik olarak ilgi çekici olmakla kalmayıp, önerilerin hakemlere etkin bir şekilde atanmaları için pratik yöntemler de sağlamaktadır.","In ordinal evaluations of proposals in peer review systems, a set of proposals is assigned to a fixed set of referees so as to maximize the number of pairwise comparisons of proposals under certain referee capacity and proposal subject constraints. The following two related problems are considered: (1) Assuming that each referee has a capacity to review k out of n proposals, 2 <= k <= n, determine the minimum number of referees needed to ensure that each pair of proposals is reviewed by at least one referee, (2) Find an assignment that meets the lower bound determined in (1). It is easy to see that one referee is both necessary and sufficient when k = n, and n(n-1)/2 referees are both necessary and sufficient when k = 2. It is shown that 6 referees are both necessary and sufficient when k = n/2. Furthermore it is shown that 11 referees are necessary and 12 are sufficient when k = n/3, and 18 referees are necessary and 20 referees are sufficient when k = n/4. A more general lower bound of n(n-1)/k(k-1) referees is also given for any k, 2 <=k <= n, and an assignment asymptotically matching this lower bound within a factor of 2 is presented. These results are not only theoretically interesting but they also provide practical methods for efficient assignments of proposals toreferees."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek ve artan oranlarda videonun çeşitli ağlarda serbestçe yayımlanması, telif haklarının korunması için otomatik video kopya sezimi ihtiyacını beraberinde getirmiştir. Çoklu ortam teknolojilerindeki gelişmeler, filigram yaklaşımına alternatif olarak içerik tabanlı video kopya sezimi yöntemini ortaya sürmüştür.Bu tez çalışmasında video kliplerini eşleştirmeyi sağlayan çok kipli bir sistem önerilmektedir. İlk olarak bir yüz detektörü kullanılarak yüz içeren video bölümleri belirlenir. Yüz ve bedenin yüze yakın belirli bir kısmını eşleştirmek, aynı kişiyi (örneğin, sunucu veya politik lider) farklı olaylarda veya sahnelerde ayırma esnekliği sağlar. İkinci olarak, harekete bağlı benzerliği olan video bölümlerini eşleştirmek için uzaysal ve zamansal dizileri eşleyen bir teknik kullanılır. Son olarak, yüz içermeyen video bölümleri düşük seviyeli görsel öznitelikler ile eşleştirilmektedir. Bunlara ek olarak, videoları bölütlemek için kullanılan renk diklemlerinde bulanık mantıktan yararlanılmaktadır. Gürültüleri, silinen film karelerini, iç içe geçmiş çerçeveleri tespit etmek ve durağan bölgeler için maske oluşturmak için de yöntemler önerilmiştir.Tanıtılan sistem, TRECVID 2008 yarışmasında İçerik Tabanlı Kopya Sezimi görevi için hazırlanan sorgu ve referans videoları üzerinde test edilmiş, sonuçlarımız bu göreve katılan en iyi 8 teknikle karşılaştırılmıştır. Bu deneylerde sistemimizin diğer birçok modern teknikten daha verimli ve etkili çalıştığı gösterilmiştir.","Huge and increasing amount of videos broadcast through networks has raised the need of automatic video copy detection for copyright protection. Recent developments in multimedia technology introduced content-based copy detection (CBCD) as a new research field alternative to the watermarking approach for identification of video sequences.This thesis presents a multimodal framework for matching video sequences using a three-step approach: First, a high-level face detector identifies facial frames/shots in a video clip. Matching faces with extended body regions gives the flexibility to discriminate the same person (e.g., an anchor man or a political leader) in different events or scenes. In the second step, a spatiotemporal sequence matching technique is employed to match video clips/segments that are similar in terms of activity. Finally the non-facial shots are matched using low-level visual features. In addition, we utilize fuzzy logic approach for extracting color histogram to detect shot boundaries of heavily manipulated video clips. Methods for detecting noise, frame-droppings, picture-in-picture transformation windows, and extracting mask for still regions are also proposed and evaluated.The proposed method was tested on the query and reference dataset of CBCD task of TRECVID 2008. Our results were compared with the results of top-8 most successful techniques submitted to this task. Experimental results show that the proposed method performs better than most of the state-of-the-art techniques, in terms of both effectiveness and efficiency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Internet'deki bilgi patlaması sonucu gezinme, erişim ve süzme gibi haber portalı servisleri önemli araştırma ve uygulama alanları haline gelmiştir. Bu çalışmada, Internet'deki haber tüketicilerinin ihtiyaçlarına yönelik, kişiselleştirmeden yeni olay belirleme ve izlemeye kadar geniş bir yelpazede çeşitli özgün yetenekleri olan Bilkent Haber Portalı'nın tasarım ve geliştirme detayları verilmektedir. Tez, bu sistemin mimari tasarımını, veri ve dosya yapılarını ve deneysel temellerini sunmaktadır. Portalın yeni olay belirleme ve izleme bileşeninin geliştirilmesi ve değerlendirilmesi için bir deney derlemi oluşturulmuştur: BilCol2005. Bu deney derlemi 2005 yılına ait 209,305 haber ve seksen adedi kullanıcı tarafından değerlendirilmiş birçok olay içermektedir. Bu derlem, Türkçede yeni olay berlirleme ve izleme algoritmalarının deneysel olarak ölçülebilmesine olanak sağlamaktadır. Deney derleminin hazırlanabilmesi için TDT araştırma programının yönergeleri takip edilerek bir web uygulaması, ETracker, geliştirilmiştir. Ayrıca, bilgi erişimi ve süzme yetenekleri olan bu haber portalının gerçekleştirilmesinde kullanılacak çeşitli parametrelerin, bilgi erişimi üzerine etkileri deneysel olarak ölçülmüştür. Bu amaçla, kök bulma yöntemlerinin, doküman uzunluğunun, sorgu uzunluğunun etkileri ve ölçeklenebilirlik konuları incelenmiştir.","News portal services such as browsing, retrieving, and filtering have become an important research and application area as a result of information explosion on the Internet. In this work, we give implementation details of Bilkent News Portal that contains various novel features ranging from personalization to new event detection and tracking capabilities aiming at addressing the needs of news-consumers. The thesis presents the architecture, data and file structures, and experimental foundations of the news portal. For the implementation and evaluation of the new event detection and tracking component, we developed a test collection: BilCol2005. The collection contains 209,305 documents from the entire year of 2005 and involves several events in which eighty of them are annotated by humans. It enables empirical assessment of new event detection and tracking algorithms on Turkish. For the construction of our test collection, a web application, ETracker, is developed by following the guidelines of the TDT research initiative. Furthermore, we experimentally evaluated the impact of various parameters in information retrieval (IR) that has to be decided during the implementation of a news portal that provides filtering and retrieval capabilities. For this purpose, we investigated the effects of stemming, document length, query length, and scalability issues."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgi güvenliği, elektronik iletişimin hayatımızın her alanına girmesi ile birlikte giderek daha çok önemli hale gelmektedir. Bilgi güvenliği kavramının içeriği kullanıldığı uygulamanın çeşidine ve gereksinimlerine göre değişebilmektedir. Fakat kullanılan alan ya da uygulama ne olursa olsun, güvenlik için hangi algoritmalar kullanılırsa kullanılsın, güvenlik ilk önce gerekli kişilerin bilmesi gereken bir anahtarın gizli kalmasına dayanmaktadır.Güvenliğin en önemli unsuru olan anahtarların gizli kalması ve kaybolmaması gereksinimleri değişik problemleri de beraberinde getirmektedir. Anahtarın sadece bir kişide, sunucuda ya da veritabanında saklanması, sistemin güvenliğini o kişinin güvenliğine ve güvenilirliğine indirgemektedir. Bunun yanında şifrenin başka bir kopyasının olmaması da yazılım/donanım arızaları gibi durumlarda anahtarın tamamen kaybedilmesi gibi sakıncalar içermektedir. Anahtarın birden fazla kişide bulunması durumunda ise anahtarı ele geçirmeye çalışan biri için artık bir değil birden fazla hedef vardır ve dolayısıyla, anahtarın güvenliği bu kişilerinin en az güvenliğe sahip olanının güvenliğine indirgenmektedir.Anahtar paylaştırma yöntemleri ilk olarak yukarıda bahsedilen problemleri çözmek için önerilmiştir. Bu yöntemlerdeki anafikir anahtarın belli bir grup içinde öyle paylaştırılmasıdır ki, sadece önceden belirlenen koalisyonlar bir araya geldiğinde anahtarı elde edebilmeli daha küçük koalisyonlar ise anahtar hakkında hiçbir bilgi elde edememelidir. Bu sayede, şirketlerin karar mekanizması uygulamaları, büyük ölçekli finans uygulamaları, nükleer sistemlerin komuta-kontrol uygulamaları gibi alanlarda gizli kalması gereken anahtarlar anahtar paylaştırma yöntemleri kullanılarak saklanabilir.Eşik kriptografisi anahtar paylaştırma yöntemlerinin özel bir hali ile ilgilenir. Eşik kriptografisine dayanan anahtar paylaştırma yöntemlerinde bir koalisyonun içindeki kişi sayısı, büyüklüğü, belli bir eşiği, kısaca t, geçiyorsa, o koalisyon anahtarı elde edebilir. Daha küçük koalisyonlar ise anahtar hakkında hiç bir bilgi elde edemezler. Literatürde ilk önerilen anahtar paylaştıma yöntemlerinden biri Shamir'in eşik kriptografisine dayanan yöntemidir. Shamir bu yöntemde anahtarı t-1 dereceli bir polinomun sabit terimi olarak düşünmüş ve polinomun geçtiği noktaları grup içinde dağıtmıştır. Bu sayede, gerekli olduğunda t büyüklüğündeki bir koalisyon, polinomu yaratarak anahtarı elde edebilir. Bu yöntem sonraları güvenlik üzerine araştırma yapan bilim insanları tarafından kabul görmüş ve değişik uygulamalarda kullanılmıştır. Bu yöntem ile yaklaşık aynı zamanlarda önerilen Blakley'in düzlem geometrisine dayalı anahtar paylaştırma yöntemi ve Asmuth ve Bloom'un önerdiği Çin Kalan Teoremi'ne dayalı yöntem güvenlik açısından gerekli ve yeterli şartları sağladıkları halde araştırmacılar tarafından rağbet görmemişlerdir.Anahtar paylaştırma yöntemleri yukarıda bahsedilen uygulamalar dışında da değişik güvenlik uygulamaları için temel yapı parçacığı görevini görmüşlerdir. Bu uygulamalar, genelde fonksiyon paylaştırma yöntemi olarak bilinen, herhangi bir fonksiyonun çıktısının, herbiri gizli bir fonksiyon girdisine sahip bir grup tarafından, fonksiyon girdileri gizli kalmak şartı ile hesaplanması problemini içerir. Literatürde, anahtar paylaştırma yöntemleri temel alınarak paylaştırılan bu fonksiyonlara RSA, ElGamal ve Paillier gibi açık anahtar algoritmalarının imza yada şifreleme fonksiyonları örnek gösterilebilir. Elektronik seçim gibi yeni nesil uygulamalar fonksiyon paylaştırma yöntemlerini yoğun bir şekilde kullanmaktadır.Daha önce de bahsedildiği gibi, Shamir'in anahtar paylaştırma yöntemi literatürde sıklıkla kullanılan bir yöntem olup diğer anahtar paylaştırma sistemleri pek rağbet görmemektedir. Fakat, bu tezin gösterdiği gibi Çin Kalan Teoremi'ne dayalı anahtar paylaştırma yöntemleri de pratik olarak bu tür uygulamalarda kullanılabilir. Her uygulama değişik güvenlik gereksinimlerine sahip olduğu için, Shamir'in yöntemi değişik eklentiler tasarlanarak çeşitli uygulamalarda kullanılmıştır. Bu tez temel olarak farklı anahtar paylaştırma yöntemlerinin çeşitli uygulamalarda nasıl kullanabileceği üzerine yoğunlaşacaktır. Tezde Çin Kalan Teoremi'ne dayalı bir anahtar paylaştırma yöntemi olan Asmuth-Bloom yöntemi için bazı değişiklikler önerilecektir. Sonra da bu yeni yöntemler kullanılarak kanıtlanabilir güvenliğe sahip fonksiyon paylaştırma yöntemleri ve halihazırda varolan uygulamalarda gereken değişik güvenlik eklentileri tasarlanacaktır.","Information security has become much more important since electronic communication is started to be used in our daily life. The content of the term information security varies according to the type and the requirements of the area. However, no matter which algorithms are used, security depends on the secrecy of a key which is supposed to be only known by the agents in the first place.The requirement of the key being secret brings several problems. Storing a secret key on only one person, server or database reduces the security of the system to the security and credibility of that agent. Besides, not having a backup of the key introduces the problem of losing the key if a software/hardware failure occurs. On the other hand, if the key is held by more than one agent an adversary with a desire for the key has more flexibility of choosing the target. Hence the security is reduced to the security of the least secure or least credible of these agents.Secret sharing schemes are introduced to solve the problems above. The main idea of these schemes is to share the secret among the agents such that only predefined coalitions can come together and reveal the secret, while no other coalition can obtain any information about the secret. Thus, the keys used in theareas requiring vital secrecy like large-scale finance applications and commandcontrol mechanisms of nuclear systems, can be stored by using secret sharing schemes.Threshold cryptography deals with a particular type of secret sharing schemes. In threshold cryptography related secret sharing schemes, if the size of a coalition exceeds a bound t, it can reveal the key. And, smaller coalitions can reveal no information about the key. Actually, the first secret sharing scheme in the literature is the threshold scheme of Shamir where he considered the secret as the constantof a polynomial of degree t - 1, and distributed the points on the polynomial to the group of users. Thus, a coalition of size t can recover the polynomial and reveal the key but a smaller coalition can not. This scheme is widely accepted by the researchers and used in several applications. Shamir's secret sharing scheme is not the only one in the literature. For example, almost concurrently, Blakley proposed another secret sharing scheme depending on planar geometry and Asmuth and Bloom proposed a scheme depending on the Chinese Remainder Theorem. Although these schemes satisfy the necessary and sufficient conditions for the security, they have not been considered for the applications requiring asecret sharing scheme.Secret sharing schemes constituted a building block in several other applications other than the ones mentioned above. These applications simply contain a standard problem in the literature, the function sharing problem. In a function sharing scheme, each user has its own secret as an input to a function and the scheme computes the outcome of the function without revealing the secrets. In the literature, encryption or signature functions of the public key algorithms like RSA, ElGamal and Paillier can be given as an example to the functions shared by using a secret sharing scheme. Even new generation applications like electronic voting require a function sharing scheme.As mentioned before, Shamir's secret sharing scheme has attracted much of the attention in the literature and other schemes are not considered much. However, as this thesis shows, secret sharing schemes depending on the Chinese Remainder Theorem can be practically used in these applications. Since each application has different needs, Shamir's secret sharing scheme is used in applications with severalextensions. Basically, this thesis investigates how to adapt Chinese Remainder Theorem based secret sharing schemes to the applications in the literature. We first propose some modifications on the Asmuth-Bloom secret sharing scheme and then by using this modified scheme we designed provably secure function sharing schemes and security extensions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hiperçizge Bölümleme (HB) ve Düğüm Ayıracı ile Çizge Bölümleme (DACB) problemleriliteratürde gayet bilinen, koşut ve bilimsel hesaplamalarda etkin biçimde kullanılan problemlerdir. Koşut hesaplamalarda tipik bir problem, verilerin ve görevlerin değişik sayıda işlemciye öyle şekilde dağıtılmasıdır ki hesaplamanın çalışma performansı zaman ve yer gereksinimleri açısından hızlı ve verimli olsun. Bunun yanında, DACB problemi seyrek doğrusal sistemlerin verimli çözülebilmesi için doluluk azaltan sıralama yapmak için etkin biçimde kullanılmaktadır. Bu da bilimsel hesaplamanın konu sahası içine girmektedir. Bu tez çalışmasında, HB ve DACB problemleri arasındaki ilişki incelenmektedir. Bu bağlamda, iki kombinatoriyal dönüştürüm açığa çıkarılmıştır. Birinci dönüştürme, HB probleminden DACB problemine, ikincisi ise DACB probleminden HB probleminedir. HB probleminden DACB problemine dönüştürmede girdi dönüşümü kolay olmamaktadır. Girdi dönüşümünden kasıt, verilen bir çizgeyi, problem dönüştürümü uygun olacak şekilde bir hiğerçizgeye dönüştürmektir. DACB probleminden HB problemine dönüştürümde ise çıktı dönüşümü kolay olmamaktadır. Burada da kasıt, verilen bir çizge bölümlemeyi asıl problemimiz olan hiperçizge bölümlemeye dönüştürmektir. Bu kısımda önemli ve faydalı imkansizlık sonuçları türetilmiştir. Bu kolay olmayan kısımlar derinliğince incelenmiş, etkili ve verimli çözümler ve yöntemler önerilmiştir.Bu çalışma çerçevesinde, bir HB tabanlı doluluk azaltan sıralama aracı olan oPaToH, gelişmiş girdi dönüşümleri ile genişletilmiştir. Bunun yanında, başka bir doluluk azaltan sıralama aracı olan onmetıs kullanılarak DACB tabanlı bir HB aracı olan ?hpmetis? üretilmiştir. Ayrıca, yine onmetis kullanılarak bir Dantzig-Wolfe ayrıştırma aracı olan ?dwmetis? üretilmiştir. Dantzig-Wolfe ayrıştırma ise doğrusal problemlerin etkin koşutlanmasında kullanılmaktadır. Deneysel sonuçlarımız gösterdi ki, genişletilen oPaToH, seyrek doğrusal sistem çözümünde hali hazırdaki iyi yöntemlere göre %20'lere varan iyileşme göstermiştir. Üretilen Dantzig-Wolfe ayrıştırma aracı dwmetis ise hali hazırda kullanılan HB tabanlı yöntemlere göre makul kalıte farklılıklarında 5 kata kadar hızlı sonuç üretebilmiştir. Bu sonuç da gayet değerlidir, çünkü toplam performans ölçülürken önçalışma zamanı da değer taşımaktadır. Sonuç olarak, bu çalışmada gösterildi ki koşut ve bilimsel hesaplama işlemlerinin, HB ve DACB problemlerini birbirlerine dönüştürümü kullanılarak daha hızlı çalışmaları sağlanabilmektedir.","Hypergraph Partitioning (HP) and Graph Partitioning by Vertex Separator (GPVS)problems are very well known problems which are used in scienti ? c and parallel com-puting effectively. A typical problem in parallel computing is to partition the data/tasksinto several processors such that the overall performance of the computation gets morequali ? ed in terms of time and/or memory. Besides, GPVS is generally used for ? ll-reducing ordering of sparse matrices for solving sparse linear systems ef ? ciently whichlies in the area of scienti ? c computing. In this thesis, the relation between these twoproblems, HP and GPVS problems, are investigated. Two combinatorial reductions,from HP Problem to GPVS Problem and from GPVS Problem to HP Problem are dis-closed along with their theoretical bases. In practice, the nontrivial part of HP Problemto GPVS Problem reduction is the input transformation, that is, converting a graph toa hypergraph such that the reduction holds. The nontrivial part of the reduction fromGPVS Problem to HP Problem is the output transformation, that is, decoding a vertexseparator of the corresponding graph to a partition for the hypergraph. In this part,some useful impossibility results are derived. These nontrivial parts are investigateddeeply and effective and ef ? cient algorithms and methods are proposed.Furthermore, ?oPaToH?, an HP-based ? ll-reducing ordering tool based on PaToH,is enhanced along with implementation of input transformations. Besides, based on? ll-reducing ordering tool onmetis, a GPVS-based HP tool ?hpmetis? is derived anda Dantzig-Wolfe decomposition tool for ef ? cient parallelizm of linear programmingproblem solutions is constructed, which is called as ?dwmetis?. The ? ll-reducing or-dering results obtained with enhanced oPaToH produced more quali ? ed ordering re-sults such as up to %20 improvements for operation count compared to state-of-theart ordering tools such as onmetis. Note that decreasing operation count relates toperforming sparse linear equation solutions faster. The Dantzig-Wolfe decompositionresults with dwmetis produced results around 5 times faster than the state-of-the arthypergraph partitioning tool PaToH with comparable quality for net balancing. Thisis also valuable because the preprocessing overhead is also considered inside the totalexecution time, generally. As a result, in this work it is showed that parallel and sci-enti ? c computing applications can be performed faster by exploiting the combinatorialreductions between HP problem and GPVS problem."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Literatürdeki nesnesel görüntü analizi tekniklerinin ortak amacı görüntü türdeş bölgelere bölütlemek ve bunları sınıflandırmaktır. Fakat bu türdeş bölgeler, yeni nesil algılayıcılardan elde edilen yüksek uzamsal çözünürlüklü görüntülerde çok küçük detaylara karşılık gelmektedir. Görüntü içeriğini üst düzeyde anlamamızı sağlayan dikkate değer bir yöntem içsel olarak heterojen bölgelerin tanımlanmasıdır. Farklı tip temel nesnelerin birleşmesinden oluşan bu tür imge bölgeleri bileşik yapılar olarak da adlandırılır. Bileşik yapıların saptanması, pikseller yerine temel nesneler kullanan genellenmiş bölütleme veya doku analizi problemi olarak görülebilir. Geleneksel bölütleme yöntemleri benzer spektral içerikli bölgeleri bulurken, doku bulma teknikleri ise belirli bir ölçek ve yönelim gerektirir. Bundan dolayı bu iki teknik de değişik spektral içerik ve gelişigüzel ölçek ve yönelimli bileşik yapıların karmaşıklığıyla başa çıkamamaktadır.Bu tez çalışmasında temel nesnelerden oluşan bileşik görüntü yapılarının bulunmasını sağlayan öğreticisiz bir yöntem önerilmektedir. İlk bölütleme adımı homojen spektral içerikli görüntü bölgeleri üretir. Sonrasında bölütleme sonuçları, düğümleri bölgeler ve kenarları bölgeler arasındaki ilişkiler olan bir ilişkisel çizgeye aktarılır. Birlikte sıkça görülen bölgeler çok ilgili olarak değerlendirilir. Bu ilişki komşu bölgelerdeki geçişlerin sıklığına bağlı olarak modellenir ve önemli ilişkiler, geçişlerin öznitelikleri kullanılarak oluşturulan olasılık dağılımındaki yerel enbüyük olarak bulunur. Ayrıca çok ilgili bölgeler içeren altçizgeler de bileşik yapılara karşılık gelmektedir. Bu yüzden kurulan çizgedeki altçizgeleri ortaya çıkarmak için iki farklı yöntem kullanılmaktadır. İlk yöntemde çizge ayrıklaştırılır ve tekrar eden altçizgeler çizge bazlı bilgi çıkarma algoritmasıyla bulunur. Tek başına bir altçizge belirli bir bileşik yapıya karşılık gelmese bile farklı altçizgeler bir bileşik yapının parçaları olabilir. Bundan dolayı bileşik yapılar, altçizgeler histogramlarının kayar imge pencereleri ile gruplandırılmaları sayesinde bulunur. İkinci yöntem düzgelenmiş kesitler algoritmasıyla çizge bölütlemesi içerir. Önemli ilişkilerin altçizgelerdeki dağılımı bize bileşik yapılar hakkında bir fikir vereceğinden, altçizgeler en önemli ilişkiler histogramı ile tekrar gruplandırılır.Önerilen yöntem Ikonos görüntülerinde test edilmiştir. Deneyler sonucunda bulunan bölgelerin yüksel yoğunluklu yerleşim alanı, düşük yoğunluklu yerleşim alanı ve arazi gibi heterojen içerikli farklı üst düzey yapılara karşılık geldiği görülmüştür.","The common goal of object-based image analysis techniques in the literature is to partition the images into homogeneous regions and classify these regions. However, such homogeneous regions often correspond to very small details in very high spatial resolution images obtained from the new generation sensors. One interesting way of enabling the high-level understanding of the image content is to identify the image regions that are intrinsically heterogeneous. These image regions are comprised of primitive objects of many diverse types, and can also be referred to as compound structures. The detection of compound structures can be posed as a generalized segmentation or generalized texture detection problem, where the elements of interest are primitive objects instead of traditional case of pixels. Traditional segmentation methods extract regions with similar spectral content and texture models assume specific scale and orientation. Hence, they cannot handle the complexity of compound structures that consist of multiple regions with different spectral content and arbitrary scale and orientation.In this thesis, we present an unsupervised method for discovering compound image structures that are comprised of simpler primitive objects. An initial segmentation step produces image regions with homogeneous spectral content. Then, the segmentation is translated into a relational graph structure whose nodes correspond to the regions and the edges represent the relationships between these regions. We assume that the region objects that appear together frequently can be considered as strongly related. This relation is modeled using the transition frequencies between neighboring regions, and the significant relations are found as the modes of a probability distribution estimated using the features of these transitions. Furthermore, we expect that subgraphs that consist of groups of strongly related regions correspond to compound structures. Therefore, we employ two different procedures to discover the subgraphs in the constructed graph. During the first procedure the graph is discretized and a graph-based knowledge discovery algorithm is applied to find the repeating subgraphs. Even though a single subgraph does not exclusively correspond to a particular compound structure, different subgraphs constitute parts of different compound structures. Hence, we discover compound structures by clustering the histograms of the subgraph instances with sliding image windows. The second procedure involves graph segmentation by using normalized cuts. Since the distribution of significant relations within resulting subgraphs gives an idea about the nature of corresponding compound structure, the subgraphs are further grouped by clustering the histograms of the most significant relations.The proposed method was tested using an Ikonos image. Experiments show that the discovered image areas correspond to different high-level structures with heterogeneous content such as dense residential areas with high buildings, dense and sparse residential areas with low height buildings and fields."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz duyarga ağları kullanılarak doğal ortamların izlenmesi üzerine bir çok uygulama alanı geliştirilmiştir. Bu tez çalışmamızda, bizler de orman yangınlarının erken tespitinde ve yangının izlenmesi sürecinde kablosuz duyarga ağlarını kullanarak bir sistem tasarladık. Orman yangınları dünyada çevresel tahribata neden olan başlıca sebeplerden biridir. şu anki yangın gözetleme ve takip sistemleri ormanları anlık olarak bütünüyle izleme ve olası bir yangın tehlikesini önceden tespit etme konusunda başarısız olmaktadır. öte yandan, kablosuz duyarga ağlarını kullanarak geliştirilen çözümler sıcaklık ve nem değerlerini, anlık olarak, sahanın farklı noktalarından, gece ve gündüz farketmeksizin sürekli olarak alabilmekte ve de merkezi birimlere taze ve güvenilir bilgi sunabilmektedir. Fakat, duyarga ağlarında kullanılan duyarga düğümleri kısıtlı enerji kaynaklarına sahiptir ve zorlu dış koşullara karşı dayanıklı değillerdir. Geliştirilen uygulamalarda bu engellerin dikkatli bir şekilde ele alınması gereklidir.Tez çalışmamızda kablosuz duyarga ağlarını kullanarak orman yangınlarını erken tespit etmek ve izleyebilmek amacıyla geniş kapsamlı bir sistem geliştirdik. Sunduğumuz sistem kablosuz duyarga ağlarıyla ilgili bir ağ altyapısı, düğümlerin ormana yerleştirilmesi ile ilgili özel bir mekanizma ve düğümlerin küme içi ve kümeler arası iletişim protokollerini içermektedir. Sistemimiz orman yangınlarını mümkün olan en kısa sürede tespit etmeyi hedeflerken, düğümlerin enerji harcama oranlarını da dikkatlice gözetmektedir. Ayrıca sistemin çalışmasını engelleyebilecek zorlu çevresel koşullar için de önlemler hazırlanmıştır. Sunduğumuz sistemi geliştirebilmek, test edebilmek ve farklı yapılarla kıyaslayabilmek adına bir de simülator gelistirdik. Bununla birlikte yangının başlaması ve ilerlemesi ile ilgili olarak 3. parti bir yangın simülatörünü kullandık. Simülatör üzerinde çok çeşitli testler yaparak sunduğumuz sistemin potensiyel yangınları tespit etmekte daha hızlı tepki verdiğini ve daha az enerji tükettiğini gözlemledik.","Wireless sensor networks have a broad range of applications in the category of environmental monitoring. In this thesis, we consider the problem of forest fire detection and monitoring as a possible application area of wireless sensor networks. Forest fires are one of the main causes of environmental degradation nowadays. The current surveillance systems for forest fires lack in supporting real-time monitoring of every point of the region at all time and early detection of the fire threats. Solutions using wireless sensor networks, on the other hand, can gather temperature and humidity values from all points of field continuously, day and night, and, provide fresh and accurate data to the fire fighter center quickly. However, sensor networks and nodes face serious obstacles like limited energy resources and high vulnerability to harsh environmental conditions, that have to be considered carefully.In our study, we propose a comprehensive framework for the use of wireless sensor networks for forest fire detection and monitoring. Our framework includes proposals for the wireless sensor network architecture, clustering and communication protocols, and environment/season-aware activity-rate selection schemes to detect the fire threat as early as possible and yet consider the energy consumption of the sensor nodes and the physical conditions that may hinder the activity of the network. We also implemented a simulator to validate and evaluate our proposed framework, which is using an external fire simulator library. We did extensive simulation experiments and observed that our framework can provide fast reaction to forest fires while also consuming energy efficiently."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bazı kablosuz sensör ağları uygulamalarında sensör aygıtlarının algıladıkları veriler ilintilidir. Bu gibi kablosuz sensör ağı uygulamalaının tamamen çalışır durumda olması için bütün sensör aygıtlarının aktif (çalışıyor durumda) olmalarına gerek yoktur. Buna karşılık, aktif olarak seçilen sensör aygıtlarının kendi aralarıda haberleşmelerini sağlayacak kablosuz bir ağ kurarak topladıkları ilintili verileri sorumlu merkeze göndermeleri gerekmektedir. Sensörler arasında ilintili veri bulunan kablosuz sensör ağları uygulamalarında hangi sensör aygıtlarının aktif durumda olacağının belirlenmesi, haberleşebilen ilinti-bazlı küme (connected correlation-dominating set) problemi olarak ifade edilebilir. Bu tez çalışmasının katkısı çift yönlüdür: İlk olarak aktif sensör aygıtlarının belirlenebilmesi için etkin ve hızlı çalışan tekrarlamalı iyileştirme gerçekleştiren buluşsal bir algoritma (iterative improvement heuristic) önerilmektedir. İkinci olarak ise aktif sensör aygıtı kümesine seçilen sensör aygıtı sayısı azaltılırken, bu kümeye seçilen sensör aygıtlarının yüksek enerjiye sahip olabilmelerine imkan veren bir yarar fonksiyonu önerilmektedir. Detaylı simülasyonlarla ileri sürdüğümüz bu yaklaşımın hem kablosuz sensör ağının işleme süresi bakımından, hem de algoritma çalışma zamanı bakımından iyi sonuçlar ortaya koyduğu görülmektedir.","In wireless sensor network applications where data gathered by different sensor nodes is correlated, not all sensor nodes need to be active for the wireless sensor network to be functional. However, the sensor nodes that are selected as active should form a connected wireless network in order to transmit the collected correlated data to the data gathering node. The problem of determining a set of active sensor nodes in a correlated data environment for a fully operational wireless sensor network can be formulated as an instance of the connected correlation-dominating set problem. In this work, our contribution is twofold; we propose an effective and runtime efficient iterative improvement heuristic to solve the active sensor node determination problem and a benefit function that aims to minimize the number of active sensor nodes while maximizing the residual energy levels of the selected active sensor nodes. Extensive simulations we performed show that the proposed approach can achieve a good performance in terms of both network lifetime and runtime efficiency."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son on yılda internet üzerindeki bilgi kaynakları sürekli artmaktadır ve buna bağlı olarak Web üzerinde bilginin büyüklüğü de hızla çoğalmaktadır. Bu, çevrimiçi haber ve haber kaynakları için de geçerlidir. Haber okuyucuları, daha çok ilgi duydukları konular hakkındaki haberleri okumayı tercih ederler. Yeni olay belirlemeve izleme (YOBİ) uygulamaları zaman sırasıyla gelmekte olan bir haber dizisinde yeni olaylara karşılık gelen ilk haberleri bulmayı ve bu ilk haberlerin tanımlamış olduğu konular ile ilgili haberleri kümelemeyi hedefler. Literatürde YOBİ daha çok konu tanıma ve izleme (KTİ) adıyla bilinmektedir. Bu tezde, çok sayıda Türkçe Web haber kaynağından sağlanan haberler kullanılarak anında yeni olay belirleme ve izleme problemleri Türkçe icin geliştirilmiş bu konuyla ilgili ilk büyük deney derlemi (BilCol2005) kullanılarak araştırılmıştır. Bu deney derlemi 2005 yılı 209,305 adet haberden oluşmuş olup 80 adet tekil konu ve ilgili hikayeleri ile birlikte insanlar tarafından etiketlenmiştir. Çalışmada Türkçe YOBİ işlemlerinde bazı benzerlik hesaplama katsayıları için her bir kelimenin ilk 5-6 harfin o kelimenin kökü olarak kullanmanın dilin morfolojik yapısını kullanan karmaşık bir kök bulma yaklaşım ile yarışabildiği gösterilmiştir. Aynı zamanda kelime durma listesi kullanımının sistem başarımını kayda değer düzeyde etkilediği ve iki farklı benzerlik hesaplama yönteminin sağladığı güvenlik skorlarının sade bir yaklaşımla birleştirerek sistem etkinliğinin yükseltilebileceği gösterilmektedir.","The amount of information and the number of information resources on the Internet have been growing rapidly for over a decade. This is also true for on-line news and news providers. To overcome information overload news consumers prefer to track the topics that they are interested in. Topic detection and tracking (TDT) applications aim to organize the temporally ordered stories of a news stream according to the events. Two major problems in TDT are new event detection (NED) and topic tracking (TT). These problems respectively focus on finding the first stories of previously unseen new events and all subsequent stories on a certain topic defined by a small number of initial stories. In this thesis, the NED and TT problems are investigated in detail using the first large-scale test collection (BilCol2005) developed by Bilkent Information Retrieval Group. The collection contains 209,305 documents from the entire year of 2005 and involves several events in which eighty of them are annotated by humans. The experimental results show that a simple word truncation stemming method can statistically compete with a sophisticated stemming approach that pays attention to the morphological structure of the language. Our statistical findings illustrate that word stopping and the contents of the associated stopword list are important and removing the stopwords from content can significantly improve the system performance. We demonstrate that the confidence scores of two different similarity measures can be combined in a straightforward manner for improving the effectiveness."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Arama motorları, Ağ üzerinde bol miktarda bulunan metin verilerini getirmenin birincil aracıdırlar. Standart bir arama motoru üç temel görevi yerine getirir: Ağ tarama, indirilen içeriği indeksleme ve bu indeks üzerinde sorgu işleme. Bu işler için verimli yöntemler geliştirmek önemli bir araştırma konusudur. Bu tezde, bir arama motorunun yaptığı bu üç temel işe ilişkin verimli stratejiler önerilmektedir. Önerilen yöntemlerin çoğu, en geniş anlamıyla belge gruplarının (ki bunlar otomatik olarak elde edilmiş belge demetleri/sınıfları ya da elle düzenlenmiş kategorizasyonlar olabilir) halihazırda bulunduğu veya etkin bir şekilde elde edilebileceği durumlarda uygulanabilir. Ek olarak, sorgu görünümlerini kullanan bir statik indeks budama stratejisi de önerilmektedir.Ağ tarama işi için, bir konu sınıflandırmasındaki belge sınıfları arasındaki kuralları kullanan kural-tabanlı bir odaklanmış tarama stratejisi önerilmiştir. Bu kurallar, iki sınıf arasındaki birbirlerine Ağ bağlantısı verme olasılığını temsil ederler. Önerilen kural-tabanlı tarayıcı, bir yol üzerindeki aranan konuya ilişkisiz sayfaları takip ederek konuyla ilişkili bir sayfaya ulaşabilmekte (yani tünelleme yapabilmekte) ve böylece aranan konuda daha yüksek oranda sayfa bulabilmektedir.İndeksleme ve sorgu işleme kapsamındaysa belge gruplarını (demetler veya kategoriler) kullanarak arama yapma işine yoğunlaşılmıştır. Geleneksel demet-tabanlı getirme (DTG) senaryosunda, öncelikle verilen bir serbest metin sorgusuna en benzer belge demetleri belirlenir, sonra da bu demetlerdeki belgeler arasından sorgu yanıtı olanlar seçilip sıralanarak sunulur. Verimli DTG için, ilk olarak bazı alternatif sorgu işleme yöntemleri belirlenmiş ve değerlendirilmiştir. Sonra, yeni bir indeks organizasyonu olarak demet-atlayan ters indeks yapısı (DA-TİY) tanıtılmıştır. Bu yeni yapıyı kullanan DTG'nin klasik indeks kullanan önceki stratejilere göre daha başarılı olduğu çeşitli veri kümeleri ve arama parametreleri kullanılarak gösterilmiştir. Bu tezde DA-TİY'in sorgu-demet benzerliğini hesaplamakta kullanılacak tüm bilgileri içeren daha geliştirilmiş bir hali de önerilmektedir. Bahsedilen indeks yapısı üzerinde çalışan artırımlı-DTG yaklaşımı tanıtılmakta ve farklı senaryolar için arama verimliliği gösterilmektedir.Son olarak, arama motoru sorgu kütüklerinden elde edilen sorgu görünümleri kullanılarak daha başarılı statik indeks budama yöntemleri geliştirilmiştir. Bu da yine arama motorlarındaki indeksleme işiyle ilgilidir. Sorgu görünümü yaklaşımı literatürde bulunan çeşitli budama algoritmalarına ve bunların bizim tarafımızdan önerilen bazı başka biçimlerine yerleştirilmiştir. Sorgu görünümü tabanlı stratejilerin, mevcut diğer teknikleri hem ""ve"" hem de ""veya"" cinsi sorgu işleme durumlarında sorgu cevap kalitesi bakımından önemli ölçüde geçtiği gösterilmiştir.","Search engines are the primary means of retrieval for text data that is abundantly available on the Web. A standard search engine should carry out three fundamental tasks, namely; crawling the Web, indexing the crawled content, and finally processing the queries using the index. Devising efficient methods for these tasks is an important research topic. In this thesis, we introduce efficient strategies related to all three tasks involved in a search engine. Most of the proposed strategies are essentially applicable when a grouping of documents in its broadest sense (i.e., in terms of automatically obtained classes/clusters, or manually edited categories) is readily available or can be constructed in a feasible manner. Additionally, we also introduce static index pruning strategies that are based on the query views.For the crawling task, we propose a rule-based focused crawling strategy that exploits interclass rules among the document classes in a topic taxonomy. These rules capture the probability of having hyperlinks between two classes. The rule-based crawler can tunnel toward the on-topic pages by following a path of off-topic pages, and thus yields higher harvest rate for crawling on-topic pages.In the context of indexing and query processing tasks, we concentrate on conducting efficient search, again, using document groups; i.e., clusters or categories. In typical cluster-based retrieval (CBR), first, clusters that are most similar to a given free-text query are determined, and then documents from these clusters are selected to form the final ranked output. For efficient CBR, we first identify and evaluate some alternative query processing strategies. Next, we introduce a new index organization, so-called cluster-skipping inverted index structure (CS-IIS). It is shown that typical-CBR with CS-IIS outperforms previous CBR strategies (with an ordinary index) for a number of datasets and under varying search parameters. In this thesis, an enhanced version of CS-IIS is further proposed, in which all information to compute query-cluster similarities during query evaluation is stored. We introduce an incremental-CBR strategy that operates on top of this latter index structure, and demonstrate its search efficiency for different scenarios.Finally, we exploit query views that are obtained from the search engine query logs to tailor more effective static pruning techniques. This is also related to the indexing task involved in a search engine. In particular, query view approach is incorporated into a set of existing pruning strategies, as well as some new variants proposed by us. We show that query view based strategies significantly outperform the existing approaches in terms of the query output quality, for both disjunctive and conjunctive evaluation of queries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Biyolojik yolaklar, canlı bir hücre içersinde moleküller arasında gerçekleşen biyolojik tepkimeleri temsil ederler. Günümüzde genel ağdan erişilebilen biyolojik yolak verisi içeren veri tabanlarının sayısı yüzler mertebesindedir. Bu verilerin değişimi, ele alınması ve saklanması, gerek anlaşılabilirlik gerekse de toplananverilerin arttırılması açısından oldukça önemlidir. Bu gereksinimlerin bir sonucu olarak, toplanan verilerin anlamlı ve mantıklı bir düzende gruplanabilmesi için birçok biyolojik model geliştirilmiştir.Verilerin miktarı ve karmaşıklığı arttıça yolakların görsellenmesi kaçınılmaz bir ihtiyaç olmuştur. Çizgeler, yolakların modellenmesinde doğal olarak uygundurlar. Yolakların dinamik olarak görsel temsillerinin oluşturulması için çizge görselleme alanından yöntemler gerekmektedir. Sonuç olarak yolak verisini çizge biçiminde yorumlayarak görselleyen yazılım araçları ortaya çıkmıştır. Ne var ki, bu araçların birçoğu biyolojik modelin karmaşıklığının düzgün ele alınamaması, görselleme standardizasyonu eksikliği veya uzun yükleme adımlarının bulunması gibi nedenlerden ötürü yetersiz bulunmaktadır.Bu tez çalışmasında, açık kaynak kodlu olan ve BioPAX formatında saklanmış yolak modellerinin web tabanlı olarak görsellenmesi servisi veren VISIBIOweb geliştirilmiştir. Java prgoramlama dilinde Eclipse GEF kütüphanesi üzerine geliştirilen VISIBIOweb Apache Tomcat sunucusunda çalışmaktadır. Kullanıcı tarafında, sunucuda oluşturulan modelin görsellenmesi için Google Maps API kullanılmaktadır.VISIBIOweb yakınlaştırma, kaydırma ve çizge nesnelerini seçme gibi temel çizge görüntüleme özelliklerini desteklemektedir. Kullanıcılara, seçilen çizge özelliklerini listeleyen inceleme penceresi sağlanmıştır. Yüklenilen biyolojik model için oluşturulan görüntü sabit resim olarak kaydedilebilmektedir. Biyoljik modellerin görüntüleri tıpkı Google haritaları gibi saklanılarak başka web sayfalarının içerilerine konulabilmektedir. Oluşturulan çiizgenin mizanpaj bilgisi kullanıcılara XML formatında bir dosya ile sunulmaktadır. Böyle bir formatın geliştirilmesi, BioPAX formatı için resmi bir mizanpaj ilavesi geliştirme açısından iyi bir başlangıç noktasıdır.","A biological pathway is a representation of biological reactions between molecules in a living cell. At present, there are hundreds of Internet-accessible databases storing biological pathway data. Exchanging, handling, and storing this data are crucial in terms of both providing understandability and allowing further enhancements on the gathered data. As a result of this necessity, many biological models were developed to cluster the data in a meaningful manner under a semantically reasonable hierarchy.As the amount and complexity of the data increases, visualization of pathways becomes inevitable. Graphs are inherently suitable for modeling pathways. The task of creating a visual representation for pathways dynamically requires methods from the area of graph visualization. As a result, many software systems, which can interpret the pathway data with a graph structure and visualize the constructed graph, emerged. However, many of these software systems are insufficient due to poor complexity handling of the underlying model, lack of visual standardization or long installation steps.In this thesis, we introduce VISIBIOweb, a new open-source and web-based visualization service for biological pathway models stored in BioPAX (Biological Pathways Exchange Language) format. VISIBIOweb runs on Apache Tomcat server and is implemented in Java based on Eclipse GEF (Graphical Editing Framework). Google Maps API is used on the client side as the core component to visualize the representation constructed on the server.VISIBIOweb supports basic graph viewing functionalities such as zooming, scrolling, and selection of graph objects. The inspector window is provided to view the properties of the selected graph object. Once the view for the uploaded biological model is created, it can be stored as a static image. The biological models can also be persisted and embedded within other web sites just like Google Maps. The layout information of the constructed graph is also provided in an XML-based format. The introduction of such a format is a good starting point to develop an official layout extension for BioPAX format."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz örgüsel ağlar, radyoların örgüsel topoloji oluşturdukları bir ceşit haberleşme ağıdır. Kablosuz örgüsel ağların amaca özel tasarısız ağlar, kablosuz yerel ağlar, kablosuz kişisel ağlar ve kablosuz kentsel ağlar gibi bir çok ağ çeşidinde gözlenen kısıtlayıcı etkenlere çözümler sunması ve bu ağların performanslarını önemli ölçüde arttırması beklenmektedir. Bu sebeplerden ötürü, son dönemlerde kablosuz örgüsel ağlar alanında hızlı ilerlemeler kaydedilmiş ve bir çok yeni konuşlandırmalar yer almıştır.Kablosuz örgüsel ağlar için tasarlanmış bir kanal erişimi zamanlama algoritması basit, ölçeklenebilir ve işletim yükü az olan bir algoritma olmalıdır. Son dönemlerdeki araştırmalarda katmanlı ağ gerçekleştirme yapısındaki katmanların harmanlanmasıyla elde edilebilecek olası performans arttırımları da incelenmektedir. Kablosuz örgüsel ağlarda yüksek trafik taleplerini karşılamaya uygun STDMA-tabanlı kanal erişim yöntemleri sıkça kullanılmaktadır. Bu tezde, ağda üretilen toplam iş miktarını arttırmayı amaçlayan OLSR'a özgü bilgilerdenfaydalanarak işleyen, biri dağıtık, biri merkezi kontrollü olmak üzere iki farklı STDMA-tabanlı kanal erişimi zamanlama algoritması sunulmaktadır. Merkezi kontrollü algoritma klasik grafik renklendirme algoritmasının bir uyarlaması olup dağıtık algoritma her radyonun tahmin edilebilir bir rastgelelik ile kararaldığı bir algoritmadır. Önerilen algoritmalar birbirleriyle ve OLSR'a özgü bilgileri kullanmayan uyarlamaları ile ns-2 simülasyon ortamında ki yaslanmışlardır. Simülasyon sonuçlarımız OLSR bilgisinin MAC katmanınca ekstra bir yük olmaksızın elde edilebileceğini ve OLSR'a özgü bilgilerin kullanılması halinde elde edileceği öngörülen performans artışını doğrulamaktadır.","A wireless mesh network (WMN) is a communications network in which the nodes are organized to form a mesh topology. WMNs are expected to resolve the limitations and signifıcantly improve the performance of wireless ad-hoc, local area, personal area, and metropolitan area networks, which is the reason thatthey are experiencing fast-breaking progress and deployments. WMNs typically employ spatial TDMA (STDMA) based channel access schemes which are suitable for the high traffıc demands of WMNs. Currentresearch trends focus on using loosening the strict layered network implementation in order to look for possible ways of performance improvements. In this thesis, we propose two STDMA-based cross-layer OLSR-Aware channel access scheduling schemes (one distributed, one centralized) that aim better utilizingthe network capacity and increasing the overall application throughput by using OLSR-specifıc routing layer information in link layer scheduling. The proposed centralized algorithm provides a modifıcation of the traditional vertex coloring algorithm while the distributed algorithm is a fully distributed pseudo-randomalgorithm in which each node makes decisions using local information. Proposed schemes are compared against one another and against their Non-OLSR-Aware versions via extensive ns-2 simulations. Our simulation results indicate that MAC layer can obtain OLSR-specifıc information with no extra control overhead and utilizing OLSR-specifıc information signifıcantly improves the overall network performance both in distributed and centralized schemes. We further show that link layer algorithms that target the maximization of concurrent slot allocations do not necessarily increase the application throughput."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmanın hedef kullanım alanı, görüntülemelerin değişik parametreler ile aynı veri kümesi üzerinde defalarca tekrarlandığı hacim görüntüleme uygulamalarıdır. Görevler arası etkileşimin tek sebebi birden fazla görev için girdi olan veri primitiflerinin bulunmasıdır. Hem hesapsal yapı hem de görevlerin tahmini bitiş süreleri ardışık görüntüleme aşamalarında değişebilirler.Hesapsal yapıdaki değişim, görevlerin veri girdi gereksinimlerindeki değişimi ifade eder. Paralel sistemdeki her bir işlemcinin sınırlı bellek kapasitesi olduğundan, sınırlı miktarda saklama alanını kopyaların saklanması için ayırabiliriz. Eldeki görüntüleme aşamasının paralelleştirilmesi için, dağıtım modelinin daha önceki görüntüleme aşamalarının kopya dağıtım yapısından, eldeki görüntüleme aşamasının gerektirdiği haberleşme gereksinimlerini azaltmak için, yararlanması gereklidir.Bu problemin çözümü için iki aşamalı bir model öneriyoruz. İlk aşama için önerilen hiperçizge parçalama temelli modelin amacı girdi verilerin kopyalanması/taşınmasından kaynaklanan toplam haberleşme hacmini asgariye indirirken işlemcilerin hesapsal dengelerini ve girdi alış yükleri arası dengelerini korumaktır. İkinci aşama için önerilen ağ akışı temelli model, işlemcilerin ele aldığı en büyük mesaj hacmini, işlemcileri gönderi görevleri ile ilgili görevlendirme konusunda veri kopyalama sonucu oluşmuş esnekliği kullanarak, asgariye indirmeyi amaçlar. Önerilen modelin geçerliliği doğrudan hacim görüntüleme algoritmasının görüntü-uzayı paralelleştirilmesi aracılığı ile doğrulanmıştır.","The focus of this work is on parallel volume rendering applications in which renderings with different parameters are successively repeated over the same dataset. The only reason for inter-task interaction is the existence of data primitives that are inputs to several tasks. Both computational structure and expected task execution times may change during successive rendering instances. Change in computational structure means change in the data primitive requirements of tasks.Since the individual processors of a parallel system have a limited storage capacity, we can reserve a limited amount of storage for holding replicas at each processor. For the parallelization of a particular rendering instance, the remapping model should utilize the replication pattern of the previous rendering instance(s) for reducing the communication overhead due to the data replication requirement of the current rendering instance.We propose a two-phase model for solving this problem. The hypergraph-partitioning-based model proposed for the first phase aims to minimize the total message volume that will be incurred due to the replication/migration of input data while maintaining balance on computational and receive-volume loads of processors. The network-flow-based model proposed for the second phase aims to minimize the maximum message volume handled by processors via utilizing the flexibility in assigning send-communication tasks to processors, which is introduced by data replication. The validity of our proposed model is verified on image-space parallelization of a direct volume rendering algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kötücül yazılım bit ve baytlar dünyasının en büyük problemlerindenbirisidir. Genellikle kötücül yazılımlar bir kullanıcının normaldeyapmayacağı, sanal bir orduya üye olmak, ya da kullanıcının kendi özelbilgilerini kötücül yazılımı yazanlara göndermek gibi eylemlerigerçekleştirirler. Bilinmeyen programlar için halka açık analizservisleri bulunmaktadır. Kum bahçesi analizi güvenilmeyenprogramların izole ortamlarda çalıştırılması ile yapılır. Bu yöntemkötücül yazılım analizinde sık olarak kullanılan bir yöntemdir. Kumbahçesi analizi kötücül yazılım analizinde önemli bir tekniktir. Halkaaçık kum bahçesi analizi platformları, kullanıcıların kendibilgisayarlarına zarar vermeden programın çalışması sırasında oluşanizleri görmelerine yardım eder. Halka açık kum bahçesi analiziortamlarının ana problemlerinden biri de kötücül yazılım sahiplerininde servisleri kullanabilmesidir. Eğer yazarlar kum bahçesisistemlerini tanıyabilirlerse kötücül davranışları saklayabilirler. Butez halka açık olan Anubis sistemi için, bilinen tespitsistemlerini, bilinmeyen olası tespit sistemlerini ve bizim Anubis'inparmak izlerini kötücül yazılımlardan saklama ve kötücül yazılımtespitindeki hatalı negatif oranlarını düşürme çabalarımızısunmaktadır.","Malware is one of the biggest problems of the world of bits and bytes. Generallymalware does activities a user normally does not do, such as becoming part of avirtual army or submitting confidential data of the user to the malware author.There are publicly available analysis services for unknown binaries. Sandbox analysisis performed by execution of an untrusted binary in an isolated environment.It is a very common technique for malware research. Publicly available sandboxanalysis platforms help users to see traces of the execution without harming theirsystem. Also it helps owners of the sandbox to collect malware and makes thejob of analysts easier. One major problem of the public sandbox testing is thatmalware authors can also benefit from analysis of sandboxes. If they can identifysandbox systems they can hide malicious behavior. This thesis presents the publiclyused Anubis sandbox, detection mechanisms used against Anubis[3], furtherpossible detection mechanisms and our efforts for hiding fingerprint of Anubisfrom malware and decreasing the resulting false negative rates for the malwaredetection."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matrix-vektör çarpımı doğrusal denklem sistemi çözen yazılımlarda çok önemli bir çekirdek işlemdir.Aynı seyrek matris, seyrek olmayan bir vektörle çok defa çarpılır.Şu anki teknolojinin sunduğu çok seviyeli önbellekler etkin kullanılırsa, bu çarpma işlemi sırasında önemli performans kazançları olabilmektedir.Lakin düzensiz veri erişimine neden olan matrisler önbellekteki veri yerelliğinin kullanımını olumsuz etkilemektedir.Bu problemi çözmek için önbellek yerelliğini kullanan pek çok yöntem şu zamana kadar sunulmuştur.Bu çalışmada, biz de iki farklı çerçeve sunuyoruz: tek matris-vektor çarpımı ve çoklu matris-vektor çarpımı.Tek matris-vektör çarpımı çerçevesinde, önbelleğin boyutunu dikkate alarak matrisin satır ve sütunlarını yeniden sıralayan ve bu sıralam işlemini hiperçizge bölümleme ile yapan bir yöntem sunuyoruz.Ve bu yöntemlere ek olarak önbelleğin boyutunu dikkate almadan yerelliği sağlayacak bir yöntem öneriyoruz.Bir de sutunları sıkıştırıp alansal yerelliği sağlayan önişleme yöntemi sunuyoruz.Çoklu matris-vektör çarpımı çerçevesinde, matrisi alt matrislere ayırarak veri yerelliğini sağlamaya çalışmayı hedefliyoruz.Yine bu ayırma işleminde de hiperçizge kullanılıyor.Alt matrislerin çarpma sırası da önem taşıdığından veri yerelliğini arttıran bir sıralamayı bulma problemini de seyyar satıcı problemi olarak çözülebileceğini açıklıyoruz.Deneysel sonuçlar bu önerilen çerçeve ve yöntemlerin şu anda kullanılan yöntemlerden daha hızlı çalıştığını göstermektedir.","The sparse matrix-vector multiplication (SpMxV) is an important kernel operationwidely used in linear solvers. The same sparse matrix is multiplied by a dense vec-tor repeatedly in these solvers to solve a system of linear equations. High performancegains can be obtained if we can take the advantage of today?s deep cache hierarchyin SpMxV operations. Matrices with irregular sparsity patterns make it difficult toutilize data locality effectively in SpMxV computations. Different techniques are pro-posed in the literature to utilize cache hierarchy effectively via exploiting data local-ity during SpMxV. In this work, we investigate two distinct frameworks for cache-aware/oblivious SpMxV: single matrix-vector multiply and multiple submatrix-vectormultiplies. For the single matrix-vector multiply framework, we propose a cache-sizeaware top-down row/column-reordering approach based on 1D sparse matrix parti-tioning by utilizing the recently proposed appropriate hypergraph models of sparsematrices, and a cache oblivious bottom-up approach based on hierarchical clusteringof rows/columns with similar sparsity patterns. We also propose a column compres-sion scheme as a preprocessing step which makes these two approaches cache-line-sizeaware. The multiple submatrix-vector multiplies framework depends on the partition-ing the matrix into multiple nonzero-disjoint submatrices. For an effective matrix-to-submatrix partitioning required in this framework, we propose a cache-size awaretop-down approach based on 2D sparse matrix partitioning by utilizing the recentlyproposed fine-grain hypergraph model. For this framework, we also propose a trav-eling salesman formulation for an effective ordering of individual submatrix-vectormultiply operations. We evaluate the validity of our models and methods on a widerange of sparse matrices. Experimental results show that proposed methods and mod-els outperforms state-of-the-art schemes."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar bilimindeki hızlı gelişmeler çoklu ortam, bilgisayar oyunları, sanal gerçeklik gibi çeşitli uygulama alanlarının doğmasını sağlamıştır. Bütün bu uygulama alanları 3 boyutlu geometrik modellerin biçimlendirilip canlandırılması prensibiyle çalışmaktadır. Bu nedenle, bilgisayar modellemesini ve canlandırmasını sağlayan çeşitli araçlar geliştirilmiştir. Fakat, bu araçların çoğu modelleme ve canlandırma konularıyla ilgili belli bir seviyede deneyim gerektirmektedir. Bu durum, deneyimsiz kullanıcılar açısından bir engel oluşturmaktadır. Bu çalışmada, bu probleme çözüm oluşturmak amacıyla, özellikle amatör kullanıcılar için tasarlanmışs bir canlandırma sistemi sunulmaktadır. Benimsenen ana yaklaşım model biçmlendirme uygulamalarında oldukça önemli kabul edilen Laplace yöntemini kullanmaktır. Sunulan çözüm, bir modelin göze çarpan daha küçük parçalardan oluştuğunu kabul ederek bu parçaların Laplace yöntemiyle aynı anda biçimlendirilmesini sağlamaktadır. Böylece, modele hareket ediyor hissi kazandırılmaktadır. Hem yerel düzenlemeleri hem de modelin toplu hareketini sağlayacak etkileşiim teknikleri geliştirilerek daha memnun edici sonuçlar elde edilmektedir. Son olarak, belirtilen yaklaşım, çoklu dokunmatik ekran teknolojisini ve direkt manipulasyon tekniklerini kullanarak sistemin kullanılabilirliğini arttırmayı amaçlamaktadır. Belirtilen metodlar, önerilen çözümün faydalarını göstermek amacıyla farklı modeller için basit canladırmalar yaratılarak testedilmiştir.","Fast developments in computer technology have given rise to different application areas such as multimedia, computer games, and Virtual Reality. All these application areas are based on animation of 3D models of real world objects. For this purpose, many tools have been developed to enable computer modeling and animation. Yet, most of these tools require a certain amount of experience about geometric modeling and animation principles, which creates a handicap for inexperienced users. This thesis introduces a solution to this problem by presenting a mesh animation system targeted specially for novice users. The main approach is based on one of the fundamental model representation concepts, Laplacianframework, which is successfully used in model editing applications. The solution presented perceives a model as a combination of smaller salient parts and uses the Laplacian framework to allow these parts to be manipulated simultaneously to produce a sense of movement. The interaction techniques developed enable users to carry manipulation and global transformation actions at the same time to create more pleasing results. Furthermore, the approach utilizes the multi-touch screen technology and direct manipulation principles to increase the usability of the system. The methods described are experimented by creating simple animations with several 3D models; which demonstrates the advantages of the proposed solution."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Nesne tanıma, bilgisayarlı görme alanının en temel problemlerinden biridir. Bilgisayarlar gördüklerini insanlar gibi anlayabilsin diye teknikler geliştirmek nesne tanıma araştırmalarındaki ana uğraştır. Bir sahnedeki nesneleri bulmak ve tanımlayabilmek için en çok kullanılan yöntemlerde, alt-düzey görüntü öznitelikleri (renk, doku, vb.), ilgi noktaları/bölgeleri, süzgeç tepkileri, vb. özelliklerden yararlanılmaktadır. Bunlar belirli nesne sınıfları için düzgün çalışsa da, genel bir çözüm olmaktan uzaktırlar. Bu yüzden, sahne bağlamını kullanmak güncel bir eğilim halini almıştır. Bağlam nesneler arası ve nesne - sahne arası ilişkilerin kurallarını belirlemektedir. Nesne tanıyıcıların ortaya çıkardığı sahne düzenleşimleri bazı durumlarda sahne bağlamıyla örtüşmemektedir. Örneğin, bir mutfak ortamında araba görülmesi mutfak bağlamı açısından pek olası değildir. Bu durumda, mekanın bir mutfak olduğunu bilmek bu tür çelişkili tanımlamaları engellemekte kullanılabilir.Bağlamsal bilginin getirdiği faydaları hesaba katarak, bu tezde, nesne tanıma başarımını arttırmak için tek tek sezilmiş nesneler arasındaki bağlamsal etkileşimlerden yararlanan bir nesne tanıma çerçevesi anlatılmaktadır. İlk katkımız nesne sezicilerin tasarımında görülmektedir. Çerçevemizde üç farklı nesne sezim yöntemi tanımlanmıştır. Bunlardan ikisi, şekil bazlı ve piksel sınıflandırması bazlı nesne sezicilerdir ve tasarımlarında genel olarak varolan yöntemlerden yararlanılmaktadır. Bunlardan başka, yüzey doğrultusu bazlı nesne sezici isimli üçüncü bir yöntem geliştirilmiştir. Bu yeni nesne sezim yöntemindeki ana amaç, şekil, renk ve doku özellikleri ayırt edici olmasa da yüzey doğrultuları (diklik ya da yataylık durumları) tutarlı olan nesnelerin sezilebilmesini sağlamaktır. Duvar, masa üstü, yol, vb. nesneler bu gruba dahil edilmektedir. İkinci katkımız, nesneler arasındaki uzamsal ilişkilere dayanan bağlamsal etkileşim modelidir. Nesneler arasındaki uzamsal ilişkileri göstermek için göreli konum, ölçek ve doğrultu bilgilerini içeren üç tane öznitelik tanımlanmıştır. Bu öznitelikleri ve nesne etkileşim olurluğu modelini kullanarak sahnenin anlamsal, uzamsal ve duruş bağlamları aynı anda ifade edilebilmektedir. Üçüncü ana katkımız, bireysel nesne etiketlerine ve nesne ikilileri arasındaki etkileşimlere bağlı olan sahne olasılık fonksiyonunun enbüyütülerek, nesnelerin en son etiketlerinin atanmasıdır. En tutarlı sahne düzenleşimini bulmak için bu enbüyütme problemi, doğrusal eniyileme kullanılarak çözülmüştür.LabelMe ve Bilkent veri kümelerinde, hem sahne türünü (iç mekan ya da dış mekan) hesaba katarak hem de katmayarak deneyler gerçekleştirilmiştir. LabelMe veri kümesinde sahne türü bilgisi kullanılmadığında F2 başarı ölçütü 0.09'dan 0.20'ye yükselmiştir. Sahne türü bilgisinden yararlanıldığında F2 ölçütü 0.17'den 0.25 değerine ulaşmıştır. Benzer başarım artışları Bilkent veri kümesinde gerçekleştirilen deneylerde de görülmüştür. Sahne türü hesaba katılmadığında F2 ölçütü 0.16'dan 0.36'ya yükselirken, sahne türü dikkate alındığında ölçüt, 0.31 değerinden 0.44 değerine yükselmiştir. Bu deneyler sonucunda, bağlamsal etkileşimlerin nesne tanıma başarımına olumlu bir etkisi olduğu gösterilmiştir.","Object recognition is one of the fundamental tasks in computer vision. The main endeavor in object recognition research is to devise techniques that make computers understand what they see as precise as human beings. The state of the art recognition methods utilize low-level image features (color, texture, etc.), interest points/regions, filter responses, etc. to find and identify objects in the scene. Although these work well for specific object classes, the results are not satisfactory enough to accept these techniques as universal solutions. Thus, the current trend is to make use of the context embedded in the scene. Context defines the rules for object - object and object - scene interactions. A scene configuration generated by some object recognizers can sometimes be inconsistent with the scene context. For example, observing a car in a kitchen is not likely in terms of the kitchen context. In this case, knowledge of kitchen can be used to correct this inconsistent recognition.Motivated by the benefits of contextual information, we introduce an object recognition framework that utilizes contextual interactions between individually detected objects to improve the overall recognition performance. Our first contribution arises in the object detector design. We define three methods for object detection. Two of these methods, shape based and pixel classification based object detection, mainly use the techniques presented in the literature. However, we also describe another method called surface orientation based object detection. The goal of this novel detection technique is to find objects whose shape, color and texture features are not discriminative while their surface orientations (horizontality or verticality) are consistent across different instances. Wall, table top, and road are typical examples for such objects. The second contribution is a probabilistic contextual interaction model for objects based on their spatial relationships. In order to represent the spatial relationships between objects, we propose three features that encode the relative position/location, scale and orientation of a given object pair. Using these features and our object interaction likelihood model, we achieve to encode the semantic, spatial, and pose context of a scene concurrently. Our third main contribution is a contextual agreement maximization framework that assigns final labels to the detected objects by maximizing a scene probability function that is defined jointly using both the individual object labels and their pairwise contextual interactions. The most consistent scene configuration is obtained by solving the maximization problem using linear optimization.We performed experiments on the LabelMe and Bilkent data sets by both utilizing and not utilizing the scene type (indoor or outdoor) information. While the average F2 score increased from 0.09 to 0.20 without the scene type assumption, it increased from 0.17 to 0.25 when the scene type is known on the LabelMe dataset. The results are similar for the experiments performed on the Bilkent data set. F2 score increased from 0.16 to 0.36 when the scene type information is not available and it increased from 0.31 to 0.44 when this additional information is used. It is clear that the incorporation of the contextual interactions improves the overall recognition performance."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgi görselleme çeşitli çalışma alanlarından elde edilen verilerin anlaşılması ve analizi açısndan oldukça önemlidir. Çizge yerleşimi ise bilgi görsellemede önemli bir problemdir ve çizge tabanlı bilgilerin görsellenmesinde önemli rol oynar.Bilginin türüne bağlı olarak çizgeyi çizmenin pek çok tarz ve yöntemi vardır. Kümelenmiş bilgi görselleme, çizge yerleşim probleminin popüler bir alanıdır ve konu üzerinde pek çok çalışmalar olmuştur. Fakat bu çalışmalardan çok azı kümeleri ifade etmek için dairesel yerleşim üzerine yoğunlaşmışdır. Bu çalışmada, kümelenmiş çizgelerin dairesel tarzda yerleşimi için yeni bır algoritma sunulmaktadır. Algoritma, geleneksel güce-dayalı yerlesim şablonunu esas almakta ve her bir kümeyi çizmek için daireler kullanmaktadır. Ayrıca değişebilir düğüm büyüklüklerini desteklemektedir. Kümeler arası ve aynı zamanda da küme içi kenar kesişimlerini göz önünde tutarak bölüm çizgesinin (küme düğümlerinin oluşturduğu çizge) yerleşimini ele alan ilk algoritmadır. Deneysel sonuçlar, hesaplama zamanı ve genelde kabul edilen yerleşim niteliği açısından algoritmanın son derece başarılı olduğunu ortaya koymaktadır. Algoritma Chisio'nun (sürüm 1.1) bir parçası olarak başarıyla uygulanmıştır. Chisio, Bilkent Üniversitesi i-Vis (bilgi görselleme) Araştırma Gurubu tarafından geliştirilmiş açık kaynak kodlu ve genel amaçlı bir çizge düzenleyicidir.","Visualization of information is essential for comprehension and analysis of the acquired data in any field of study. Graph layout is an important problem in information visualization and plays a crucial role in the drawing of graph-based data. There are many styles and ways to draw a graph depending on the type of the data. Clustered graph visualization is one popular aspect of the graph layout problem and there have been many studies on it. However, only a few of them focus on using circular layout to represent clusters. We present a new, elegant algorithm for layout of clustered graphs using a circular style. The algorithm is based on traditional force-directed layout scheme and uses circles to draw each cluster in the graph. In addition it can handle non-uniform node dimensions. It is the first algorithm to properly address layout of the quotient graph while considering inter-cluster relations as well as intra-cluster edge crossings. Experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory. The algorithm has been successfully implemented as part of Chisio, version 1.1. Chisio is an open source general purpose graph editor developed by i-Vis (information visualization) Research Group of Bilkent University."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüzyılımızda artan kanser vakaları, bilgisayar destekli araçların kullanımını kaçınılmaz kılmıştır; bunlar patologların kanserli dokulara daha kesin tanı koymalarına ve sınıflandırmalarına yardımcı olmayı amaçlamaktadır. Bu matematiksel araçlar, daha tutarlı ve nesnel yapılar sunarak gözlemci-içi ve gözlemciler-arası değişkenliği azaltmaya olanak sağlar. Günümüzde, özellikle dokusal ve/veya yapısal doku analizi temelli otomatik kanser tanı ve sınıflandırması üzerine çok miktarda çalışma bulunmaktadır. Önceki yapısal yaklaşımların farklı tipte dokular için umut verici sonuçlar göstermelerine rağmen, bu yaklaşımlar hücre çekirdeği dışındaki doku bileşenlerinden sağlanabilecek potansiyel bilgiyi kullanabilmekten yoksundurlar. Halbuki bu ek bilgi, farklılaşmış bileşenlerden oluşan doku tipleri için ana bilgi kaynaklarından birisini oluşturmaktadır; örneğin lümen bölgeleri, kolon dokusu içindeki bezleri tanımlamaya yardımcı olmaktadır.Bu tez çalışması, hücre çekirdeği dışındaki doku bileşenlerinin kullanımı için yeni bir yapısal yaklaşımı, yeni bir çeşit kısıtlı Delaunay üçgenlemesini, ortaya koymaktadır. Bu yapısal yaklaşım öncelikle hücre çekirdekleri ve lümen bölgeleri üzerinde iki düğüm kümesi tanımlar. Daha sonra, lümen düğümleri kısıtları oluşturacak şekilde, çekirdek düğümleri üzerinde bir kısıtlı Delaunay üçgenlemesi oluşturur. Son olarak, bu yeni tanımlanan kısıtlı Delaunay üçgenlemesinden çıkarılacak öznitelikleri kullanarak doku örneklerini sınıflandırır.Elli sekiz farklı hastadan alınan 213 kolon doku örneği üzerinde gerçekleştirdiğimiz deneyler, kısıtlı Delaunay üçgenlemesi yaklaşımı ile eğitim kümesi için yüzde 87:83, test kümesi içinse yüzde 85:71 gibi yüksek doğruluk değerleri elde edildiğini ortaya koymuştur. Ayrıca deneylerimiz, yeni özniteliklerin tanımlanmasına izin veren bu yeni yapısal gösterimin, kanserli dokuların incelenmesi için daha gürbüz bir çizge-tabanlı yöntem olduğunu ve önceki yöntemlere göre daha yüksek başarı sağladığını göstermektedir.","In our century, the increasing rate of cancer incidents makes it inevitable to employ computerized tools that aim to help pathologists more accurately diagnose and grade cancerous tissues. These mathematical tools offer more stable and objective frameworks, which cause a reduced rate of intra- and inter-observer variability. There has been a large set of studies on the subject of automated cancer diagnosis/grading, especially based on textural and/or structural tissue analysis. Although the previous structural approaches show promising results for different types of tissues, they are still unable to make use of the potential information that is provided by tissue components rather than cell nuclei. However, this additional information is one of the major information sources for the tissue types with differentiated components including luminal regions being useful to describe glands in a colon tissue.This thesis introduces a novel structural approach, a new type of constrained Delaunay triangulation, for the utilization of non-nuclei tissue components. This structural approach first defines two sets of nodes on cell nuclei and luminal regions. It then constructs a constrained Delaunay triangulation on the nucleus nodes with the lumen nodes forming its constraints. Finally, it classifies the tissue samples using the features extracted from this newly introduced constrained Delaunay triangulation.Working with 213 colon tissues taken from 58 patients, our experiments demonstrate that the constrained Delaunay triangulation approach leads to higher accuracies of 87:83 percent and 85:71 percent for the training and test sets, respectively. The experiments also show that the introduction of this new structural representation, which allows definition of new features, provides a more robust graph-based methodology for the examination of cancerous tissues and better performance than its predecessors."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Günümüz modern ağ arama motorları, büyük döküman kolleksiyonlarında hızlımetin erişimi yapabilmek için ters dizin yapısını kullanırlar. Erişim işlemininparalalleştirilmesi için ters dizinin, dizin sunucular arasında dağıtılması gerekmektedir.Ters dizinin dağıtımı genellikle terim-bazlı ya da döküman-bazlı olarakyapılır. Her iki dağıtım şeklinin de performansı sistemdeki toplam disk erişimisayısına ve toplam iletişim hacmine bağlıdır.Paralel metin erişiminde klasik yöntem her iki dağıtım yöntemi için deMerkezi Simsar Sorgu İşleme Yöntemi'ni kullanmaktır. Bu yöntemde merkezisimsarın birleştirme işlemlerinden dolayı çok yüklenerek işlem hızını belirleyendarboğaz konumuna geldiği bilinmektedir. Yakın geçmişte birleştirme işleminindizin sunucularda gerçekleştirilmesine dayalı, Boru Hattı Sorgu İşleme Yöntemialternatif bir metod olarak önerilmiştir. Bu çalışmada Merkezi Simsar ve BoruHattı Sorgu İşleme Yöntemleri'nin ölçeklenebilirlik ve göreceli performanslarınıçözümleyip, değişken sorgu ağırlıklarında lehte ve alehte özelliklerini ortayaçıkaracağız.","Today?s state-of-the-art search engines utilize the inverted index data structurefor fast text retrieval on large document collections. To parallelize the retrievalprocess, the inverted index should be distributed among multiple index servers.Generally the distribution of the inverted index is done in either a term-based or adocument-based fashion. The performances of both schemes depend on the totalnumber of disk accesses and the total volume of communication in the system.The classical approach for both distributions is to use the Central BrokerQuery Evaluation Scheme (CB) for parallel text retrieval. It is known that in thisapproach the central broker is heavily loaded and becomes a bottleneck. Recently,an alternative query evaluation technique, named Pipelined Query EvaluationScheme (PPL), has been proposed to alleviate this problem by performing themerge operation on the index servers. In this study, we analyze the scalabilityand relative performances of the CB and PPL under various query loads to reportthe benefits and drawbacks of each method."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda, İnternet erişimi giderek kolaylaştıkça ve ucuzladıkça, İnternetkullanıcılarına sunulan verinin miktarı ve değişim hızı şaşırtıcı boyutlaraulaşmaktadır. İnternet'in sürekli değişen yapısı, yeni verilerin kimi zamanönemini kaybetmemiş eski verilerin yerini aldığı, sürekli değişen ve güncellenen birbilgi kolleksiyonunu doğurur. Bu önemli zamansal verileri korumayı amaçlayançok sayıda yeni çalışma literatürde mevcuttur ve bu çalışmaların sayıları kadararşiv boyutları da gün geçtikçe artmaktadır. İnanıyoruz ki, yakın gelecekte,geniş kapsamlı zamansal ağ veri kolleksiyonlarına erişebilme hedefi doğrultusunda,makul bir süre zarfında zaman aralığı sorgularına cevap verebilen metin erişimisistemleri ortaya çıkacaktır. Zamansal verilerin devasa boyutları ve birim zamanadüşen aşırı miktardaki sorgu sayısı, zamansal bilgi erişimi sistemlerini mümkünolduğunca paralel uygulamaları kullanmaya itecektir. Paralel sistemlerde, verikolleksiyonlarını ters dizin endekslerini kullanarak endekslemek için, ters dizinendekslerinin dağıtımı üzerine bir strateji izlenmelidir. Bu çalışmada, zamanagöre ve terimlere göre bölümlendirilmiş zamansal ağ ters dizin endekslerininyapılabilirliği incelenmiş ve birim zamanda cevaplanan sorgu sayısı göz önündebulundurularak, zamansal ağ sorgularını cevaplayabilen yeni bir paralel metinerişimi sistemi uygulaması sunulmuştur. Ayrıca, atlama listelerini ve rasgeleseçim algoritmalarını kullanarak sorgu sonuçlarını sıralayan yöntemlerin zamanagöre bölümleme şeması üzerindeki performansları karşılaştırılmıştır. Küçük veorta sayıdaki işlemciler üzerinde yapılan deneyler, orta ve uzun sorguların zamanagöre bölümlenmiş ters dizinlerde daha iyi sonuç verdiğini ortaya koymuştur.","In recent years, as the access to the Internet is getting easier andcheaper, the amount and the rate of change of the online datapresented to the Internet users are increasing at an astonishingrate. This ever-changing nature of the Internet causes anever-decaying and replenishing information collection where newlypresented data generally replaces old and sometimes valuable data.There are many recent studies aiming to preserve this valuabletemporal data and size and number of temporal Web data collectionsare increasing. We believe that soon, information retrieval systemsresponding to time-range queries in a reasonable amount of time willemerge as a means of accessing vast temporal Web data collections.Due to tremendous size of temporal data and excessive number ofquery submissions per unit time, temporal information retrievalsystems will have to utilize parallelism as much as possible.In parallel systems, in order to index collections using invertedindices, a strategy on distribution of the inverted indices has tobe followed. In this study, the feasibility of time-basedpartitioned versus term-based partitioned temporal-webinverted-indices is analyzed and a novel parallel text retrievalsystem for answering temporal web queries is implemented consideringthe number of queries processed in unit time. Moreover, weinvestigate the performance of skip-list based and randomized-selectbased ranking schemes on time-based and term-based partitionedinverted indexes. Finally, we compare time-balanced andsize-balanced time-based partitioning schemes. The experimentalresults at small to medium number of processors reveal that formedium to long length queries time-based partitioning works better."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışmasında, Osmanlıca bitişik metinlerdeki bağlı karakterler için içeriğe dayalı tümleşik bir bölütleme ve tanıma yöntemi önerilmiştir. Bu yöntem, öncelikle dökümanlardaki bağlı bileşenlerden sistematik olarak bir takım olası parçalar çıkartır ve bu parçalara en çok benzeyen aday karakterleri belirler. Sonrasında ise belirlenen çok sayıdaki aday karakterlerin arasından sözdizim olarak en olası olanlarını seçmek için bir skor fonksiyonu tanımlanmıştır. Bu skor fonksiyonunu maksimize eden karakterler, döngüsüz bir çizge oluşturulup bu çizge üzerindeki en uzun yolu hesaplayarak bulunmaktadır. Deney sonuçları göstermektedir ki, önerilen yöntem yüksek duyarlık ve tanıma oranları sağlamaktadır. Yapılan bilgi erişim deneyleri ise, önerilen yöntemin Osmanlıca matbaa metinler için tasarlanmış bir bilgi erişim sisteminin bir parçası olarak kullanılabileceğini göstermektedir.","In this thesis, a novel context-sensitive segmentation and recognition method for connected letters in Ottoman script is proposed. This method first extracts a set of possible segments from a connected script and determines the candidate letters to which extracted segments are most similar. Next, a function is defined for scoring each different syntactically correct sequence of these candidate letters. To find the candidate letter sequence that maximizes the score function, a directed acyclic graph is constructed. The letters are finally recognized by computing the longest path in this graph. Experiments using a collection of printed Ottoman documents reveal that the proposed method provides very high precision and recall figures in terms of character recognition. In a further set of experiments we also demonstrate that the framework can be used as a building block for an information retrieval system for digital Ottoman archives."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağları küçük, akıllı ve pille çalışan cihazlardır ve bu cihazlar, geniş alanlara yayılabilir olup, ortamları fiziksel olçümler alarak gözleyebilirler. Kablosuz algılayıcılar rastgele bir şekilde bir alana yerleştirilirler. Bu algılayıcılar aralarında çok-sekmeli bir ağ oluştururlar ve bütün algılayıcılar baz istasyonuna doğru rota oluştururlar. Bu tezde MICA2 algılayıcılarından oluşan, ofis ve ev gibi kapalı alanları gözleme amaçlı kullanılan kablosuz algılayıcı ağı test alanı E-Sense sistemi önerilmektedir. Bu test alanı İnternetten ulaşılabilir olup algılayıcı ağına web tabanlı arabirim sağlar. Web tabanlı arabirim aracılığıyla kullanıcılar, algılayıcı ağına istek gönderebilir ve bu isteklerin cevaplarını fiziksel ölçümler de dahil olmak üzere görebilir.E-Sense sistemi ayrıca bizim tasarlayıp uygulamasını yazdığımız dağıtık ve enerjiyi dikkate alan rota protokolü içerir. Protokolün amacı enerjinin algılayıcılarda verimli ve dengeli kullanılması ve ağın ömrünü uzatmaktır. Rota protokolü çok düğümden bir düğüme doğrudur ve ağaç tabanlıdır. Her düğüm bağımsız olarak sinyal gücü değerlerine göre ağaçtaki ata düğümünü belirler. Protokol ayrıca harcanan enerjiyi daha da azaltmak için algılayıcılardaki iletim gücünü ayarlayabilir. Test alanı, uygulama ve ağ seviyelerindeki deneysel çalışmalar için de faydalı olacaktır.","Wireless sensor networks consist of small, smart and battery-powered devicessuitable for widespread deployment to monitor an environment by taking physicalmeasurements. Wireless sensor nodes are deployed over an area in a randommanner. They need to self-establish a wireless multi-hop network and routingpaths from all sensor nodes to a central base station. In this thesis, we presentour E-Sense system, a wireless sensor network testbed consisting of MICA2 sensornodes which can be used to monitor an indoor environment like oce buildingsand homes. The testbed can be accessed through the Internet and provides a webbasedinterface to the sensor network. The users of the network can be locatedat any point in the Internet. Via the web based interface, the users can submitvarious types of queries to the sensor network and get the replies including thephysical measurement results.The E-Sense system also includes a distributed and energy-aware routing protocolthat we designed and implemented. The protocol aims ecient and balancedusage of energy in the sensor nodes to prolong the lifetime of the network. Therouting protocol is based on a many-to-one routing tree where each node independentlydetermines its next parent depending on the values of RSSI (ReceivedSignal Strength Indicator). The protocol can also adjust the transmit power tofurther decrease the energy spent in each sensor node. The testbed will be usefulfor experimental studies at both application and network levels."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gezgin robot uygulamalarının çoğu robotun bilinmeyen bir çevrede önceden bilgi sahibiolmadan kendini konumlandırarak gezinimi sağlamasını ve görevleri gerçekleştirmesinigerektirmektedir. Robot, bilinmeyen ortamın haritasını çıkarıp bu haritada kendisinikonumlandırabilmelidir. Eşanlı Konumlandırma ve Haritalama (EKVH) olarak bilinenbu problem son yirmi yıl içerisinde robotbilim araştırmaları arasında oldukçayoğun ilgi görmüştür. Bu çalışma, görü algılayıcıları ile donatılmış tek ve birden fazlarobot için EKVH problemi üzerinde durmaktadır. Sabit yükseklikte uçtuğu varsayılantek ve birden fazla İnsansız Hava Aracı (İHA) için görü-tabanlı bir Eşanlı Konumlandırmave Haritalama (EKVH) algoritması geliştirilmiştir. Farklı yer işaretleri eldeetmek için araç üzerindeki kameradan elde edilen görüntülerin öznitelikleri kullanılarak,Genişletilmiş Kalman Süzgeci (GKS), Bilgi Süzgeci (BS) ve de Parçacık Süzgeci EKVHproblemine uygulanmıştır. Bazı benzetim sonuçları sunularak bu yöntemler arasındabir karşılaştırma yapılmıştır. Parçacık Süzgecinin daha iyi kestirim başarımı olduğuama GKS ve BS'nin daha tutarlı sonuçlar verdiği gösterilmiştir.Anahtar Kelimeler: İHA, EKVH, Genişletilmiş Kalman Süzgeci, Bilgi Süzgeci,Parçacık Süzgeci, çok-etmenli sistemler.","Most mobile robot applications require the robot to be able to localize itself in anunknown environment without prior information so that the robot can navigate andaccomplish tasks. The robot must be able to build a map of the unknown environmentwhile simultaneously localizing itself in this environment. The Simultaneous Localizationand Mapping (SLAM) is the formulation of this problem which has drawn aconsiderable amount of interest in robotics research for the past two decades. Thiswork focuses on the SLAM problem for single and multiple agents equipped with visionsensors. We develop a vision-based 2-D SLAM algorithm for single and multipleUnmanned Aerial Vehicles (UAV) flying at constant altitude. Using the features ofimages obtained from an on-board camera to identify different landmarks, we applydifferent approaches based on the Extended Kalman Filter (EKF), the InformationFilter (IF) and the Particle Filter (PF) to the SLAM problem. We present some simulationresults and provide a comparison between the different implementations. Wefind Particle Filter implementations to perform better in estimations when comparedto EKF and IF, however EKF and IF present more consistent results.Keywords: UAV, SLAM, Extended Kalman Filter, Information Filter, Particle Filter,FastSLAM, SIFT, multi-agent systems."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sürekli olarak büyüyen video arşivlerinde insan hareketleri ve aktiviteleriyle ilgili çok geniş miktarda ilginç bilgi bulunmaktadır. Bu tezde, bu bilgileri elde etme ve insan hareketini anlama konusuna bilgisayarlı görü açısından yaklaşıyoruz. Bu amaçla, kolaydan zora doğru sıralanan iki ayrı senaryo için çözümler öneriyoruz. İlk senaryoda, nispeten kolay sayılabilecek durumlardaki teksel aksiyon tanıma problemini ele almaktayız. Bu senaryo için, insan duruşunun varolan aktiviteyi tanımlamak için pekçokfaydalı ipucu içerdigine inanıyoruz ve iki boyutlu aksiyonlar için karmaşık modellemeye gitmeden, bu şekil bilgisini çok kompakt biçimlerde gösterebiliriz. Bu kapsamda, yüksek doğruluk oranlı insan aksiyonu tanımanının mümkün olduğunu 1) videolardansiluet bilgisi çıkarmanın mümkün olduğu durumlarda dikdörtgensel alanların uzamsal yönelimli histogramlarını kullanarak, 2) siluet bilgisi bulunmadığı durumlarda sınırlardan çıkarılmış çizgilerin dağılımlarını kullanarak gösteriyoruz. Buna ekolarak, videolarda, tanıma doğruluğunu yerel ve genel hareket bilgisi eklemek suretiyle geliştirebileceğimizi kanıtlıyoruz. Şekil bilgisinin ayrıştırıcı bir çerçeve dahilinde, durağan resimlerdeki insan hareketlerini tanıma probleminde bile oldukça faydalıolduğunu gösteriyoruz.İkinci senaryo karmaşık insan aktivitelerinin, değişen arka plan ve görüş açıları gibi komplike durumlarda tanınması ve erişimi konularını içermektedir. Böyle durumlarda üç boyutlu insan aktiviteleri betimlemek ve bir hareket derlemesini görselörneğe ihtiyaç olmaksızın sorgulamak için bir yöntem tanımlıyoruz. Yaklaşımımız, vücut bölümleri üzerinde oluşturulan ve zamansal ve uzamsal olarak düzenlenebilecek aktivite birimlerine dayanmaktadır. Arama birimlerinin varlığı, önce insan vücudununtakibi, bu takip izlerinin üçüncü boyuta taşınması ve hareket algılama verisi üzerinde öğrenilmiş modellerle karşılaştırmak yolu ile otomatik olarak sağlanmaktadır. Kısa zamanlı uzuv davranış modellerimiz etiketlenmiş hareket algılama veri kümesi kullanılarak oluşturulmaktadır. Video sorgu dilimiz sonlu durumlu özdevinirlerden faydalanmaktadır ve sadece basit metin kodlamasıyla tanımlanabilir olup görsel örneğe ihtiyaç duymamaktadır. Çalışmamızda karmaşık hareket ve aktivite derlemesineuyguladığımız geniş aralıktaki sorguların sonuçlarını sunuyoruz. Kendi yöntemimizi izleme verisi üzerine uygulanmış ayrıştırıcı yöntemlerle karşılaştırıyoruz; ve yöntemimizin belirgin derecede gelişmiş performans sergilediğini gösteriyoruz. Deneysel kanıtlarımız, yöntemimizin görüş yönü farklılıklarına dayanıklı olduğunu ve kıyafetlerdeki önemli değişikliklerinden etkilenmediğini ispatlamaktadır.","Within the ever-growing video archives is a vast amount of interesting informationregarding human action/activities. In this thesis, we approach the problem of extractingthis information and understanding human motion from a computer vision perspective.We propose solutions for two distinct scenarios, ordered from simple to complex. Inthe first scenario, we deal with the problem of single action recognition in relativelysimple settings. We believe that human pose encapsulates many useful clues for recognizingthe ongoing action, and we can represent this shape information for 2D singleactions in very compact forms, before going into details of complex modeling. Weshow that high-accuracy single human action recognition is possible 1) using spatialoriented histograms of rectangular regions when the silhouette is extractable, 2) usingthe distribution of boundary-fitted lines when the silhouette information is missing.We demonstrate that, inside videos, we can further improve recognition accuracy bymeans of adding local and global motion information. We also show that within a discriminativeframework, shape information is quite useful even in the case of humanaction recognition in still images.Our second scenario involves recognition and retrieval of complex human activitieswithin more complicated settings, like the presence of changing background andviewpoints. We describe a method of representing human activities in 3D that allowsa collection of motions to be queried without examples, using a simple and effectivequery language. Our approach is based on units of activity at segments of the body,that can be composed across time and across the body to produce complex queries.The presence of search units is inferred automatically by tracking the body, lifting thetracks to 3D and comparing to models trained using motion capture data. Our modelsof short time scale limb behaviour are built using labelled motion capture set. Our query language makes use of finite state automata and requires simple text encodingand no visual examples. We show results for a large range of queries applied to acollection of complex motion and activity. We compare with discriminative methodsapplied to tracker data; our method offers significantly improved performance. Weshow experimental evidence that our method is robust to view direction and is unaffectedby some important changes of clothing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mültimedya, iletişim ve depolama teknolojilerindeki gelişmeler sayesinde depolananişitsel-görsel içerik miktarı önemli ölçüde artmıştır. Bu büyeyenmültimedya içeriğini düzenlemek ve içeriği erişilebilir hale getirmek ihtiyacıaraştırmacıları mültimedya veri tabanı sistemleri geliştirmeye yöneltmiştir.Fakat, geliştirilen sistemlerin mültimedya içeriğini tanımlama yollarının farklıolması sistemlerin birlikte işlerliğine olanak vermiyordu. Bu sorunu ¸cözmekve işitsel-görsel içeriğin tanımlanmasını standartlaştırmak için MPEG grubutarafından MPEG-7 standardı geliştirilmiştir.Bu tezde, MPEG-7 uyumlu bir video veri tabanı için sorgu dili ve sorguişlemcisi önerilmektedir. Sorgu işlemcisi üç ana parçadan oluşmaktadır: sorguayrıştırıcı, sorgu yürütücü, ve sonuç birleştirici. Sorgu ayrıştırıcı, XML tabanlısorguyu ayrıştırır ve alt sorgulara böler. Her bir alt sorgu ilgili sorgu yürütücütarafından çalıştırılır ve kullanıcı tarafından belirlenen ağırlıklara göre alt sorgucevapları birleştirilerek asıl sorgu sonucu oluşturulur. Bu tez çalışması sonucundaoluşan BilVideo v2.0 video veritabanı sistemi anahtar kelime tabanlı, yerleşimsel,zamansal, hareket izdüşüm ve alt seviyedeki (renk, ¸sekil ve desen) video sorgularınaolanak tanımaktadır. BilVideo v2.0 MPEG-7 uyumluluğu, alt seviyedekisorguları desteklemesi ve ağırlıklı sorgu birleştirmesiyle atası olan BilVideo sistemindenayrılmaktadır.","Based on the recent advancements in multimedia, communication, and storagetechnologies, the amount of audio-visual content stored is increased dramatically.The need to organize and access the growing multimedia content led researchersto develop multimedia database management systems. However, each system hasits own way of describing the multimedia content that disables interoperabilityamong other systems. To overcome this problem and to be able to standardize thedescription of audio-visual content stored in those databases, MPEG-7 standardhas been developed by MPEG (Moving Picture Experts Group).In this thesis, a query language and a query processor for an MPEG-7 compliantvideo database system is proposed. The query processor consists of threemain modules: query parsing module, query execution module, and result fusionmodule. The query parsing module parses the XML based query and divides itinto sub-queries. Each sub-query is then executed with related query executionmodule and the final result is obtained by fusing the results of the sub-queries accordingto user defined weights. The prototype video database system BilVideov2.0, which is formed as a result of this thesis work, supports spatio-temporaland low level feature queries that contain any weighted combination of keyword,temporal, spatial, trajectory, and low level visual feature (color, shape and texture)queries. Compatibility with MPEG-7, low-level visual query support, andweighted result fusion feature are the major factors that highly differentiate betweenBilVideo v2.0 and its predecessor, BilVideo."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ters indekslerin sıkıştırılması konusuna son yıllarda oldukça ilgi duyulmuştur. Ters indeks yapısında, her terim için bir döküman listesi tutulur. Ters indeksin sıkıştırılması, indeksin boyutunu azaltır ve bu da disk ulaşım süresini azaltacağından dolayı sorgu süresinin azalmasını sağlar.Son çalışmalarda, döküman numaralarının yeniden atanmasının, ters indeks sıkıştırılmasında oldukça fazla etkili olabileceği gösterilmiştir. Bu çalışmamızda, ters indekslerdeki terim ve döküman numaralarını, indeksin matris gösterimini köşegensel blok formuna dönüştürerek yeniden atamaya yarayan ve böylelikle sıkıştırma oranında oldukça fazla artış sağlayan bir yöntem öneriyoruz. Bu dönüşüm için sıkıştırma oranını %50'lere kadar artıran bir ""row-net"" hipergraf parçalama modeli kullanıyoruz. Bildiğimiz kadarıyla, bu yöntem bundan önce önerilen bütün yöntemlerden daha etkili sıkıştırma oranları sağlamaktadır.","Compression of inverted indexes received great attention in recent years. An inverted index consists of lists of document identifiers, also referred as posting lists, for each term. Compressing an inverted index reduces the size of the index, which also improves the query performance due to the reduction on disk access times.In recent studies, it is shown that reassigning document identifiers has great effect in compression of an inverted index. In this work, we propose a noveltechnique that reassigns both term and document identifiers of an inverted index by transforming the matrix representation of the index into a block-diagonal form, which improves the compression ratio dramatically. We adapted row-net hypergraph-partitioning model for the transformation into block-diagonal form, which improves the compression ratio by as much as 50%. To the best of our knowledge, this method performs more effectively than previous inverted index compression techniques."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birçok uygulama alanı, özellikle de gerçek zamanlı sistemler ve robotik uygulamalar, çoğunlukla hem kesikli hem de sürekli özelliklerini içerirler. Robotik uygulamalarda, kesikli planlama ya da kontrol teorisi problemlerini çözmek için birçok farklı yaklaşım ortaya atılmıştır. Ayrıca, bunların kombinasyonunu barındıran problemleri çözmek için farklı yöntemler mevcuttur. Fakat, bu yöntemler uygulama alanının hem kesikli hem sürekli sorunlarını çözmek için bütün bir sistem oluşturamazlar. Bu yüzden, bu tür problemleri betimeleyebileceğimiz ve çözebileceğimiz bütün bir sisteme önemli ölçüde ihtiyaç vardır. Yeni bir biçim olan Kısıtlı Sezgisel Doğrusal Mantık(KSDM), sürekli kısıtlama çözücülerle doğrusal mantığı birleştirir. Doğrusal mantık, varsayımları kaynak olarak kullanarak durum geçiş problemlerini rahatlıkla çözebilecek çok önemli bir özelliğe sahiptir. Başka bir taraftan, kısıtlama çözücü kısıtlama olarak tanımlanmış sürekli problemleri çözecektir. KSDM'nin bu iki özelliği robotik alan uygulamalarının tanımlanmasını ve çözümlenmesini güçlü bir şekilde yapacaktır. Bu tezde, KSDM'nin hem Twelf Mantıksal Çatısı hem de Prolog kullanarak gerçekleştirilmesinin üzerine odaklanılmıştır. Bu tezi okuyan okuyucu, klasik görüşün robotik alan uygulamaları için hangi eksiklikleri içerdiği, sezgiselcilikten ve doğrusalcılıktan nasıl kazançlar elde edileceği, bir mantıksal biçimin içinde basit bir robotik alan uygulamasının nasıl ifade edileceği, mantıksal sistemdeki bir kanıtın nasıl robotik alanda bir plana karşılık geleceği, Mantıksal Çatı ve Prologun hangi artıları ve eksileri olduğu ile KSDM'nin gerçekleştiriminin hem Twelf Mantıksal Çatı hem de Prolog çerçevesinde nasıl olacağı gibi önemli sorulara yanıt bulacaktır.","The underlying domain of various application areas, especially real-time systems and robotic applications, generally includes a combination of both discrete and continuous properties. In robotic applications, a large amount of different approaches are introduced to solve either a discrete planning or control theoretic problem. Only a few methods exist to solve the combination of them. Moreover, these methods fail to ensure a uniform treatment of both aspects of the domain. Therefore, there is need for a uniform framework to represent and solve such problems. A new formalism, the Constrained Intuitionistic Linear Logic (CILL), combines continuous constraint solvers with linear logic. Linear logic has a great property to handle hypotheses as resources, easily solving state transition problems. On the other hand, constraint solvers deal well with continuous problems defined as constraints. Both properties of CILL gives us powerful ways to express and reason about the robotics domain. In this thesis, we focus on the implementation of CILL in both the Twelf Logical Framework and Prolog. The reader of this thesis can find answers of why classical aspects are not proper for the robotics domain, what advantages one can gain from intuitionism and linearity, how one can define a simple robotic domain in a logical formalism, how a proof in logical system corresponds to a plan in the robotic domain, what the advantages and disadvantages of logical frameworks and Prolog have and how the implementation of CILL can or cannot be done using both Twelf Logical Framework and Prolog."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Küçük özerk robotların tasarımı ve yapımı birçok algılayıcının seçilmesini,programlanmasını ve ortak bir arayüz geliştirilmesini de kapsayan zorlu biriştir. Bu kapsamlı işlemi kolaylaştırmak, hem donanım hem de yazılım üzerindemodülerliği ve genişletilebilirliği sağlamayı gerektirir. Bu tez, I 2C tabanlı vegerçek zamanlı altyapısal bir mimari olan Evrensel Robot Veriyolu (URB)'nungeliştirilmesiyle ilgilidir. URB'nin asıl amacı hareket eden bir robot platformuüzerinde dağınık halde bulunan, küçük algılayıcıları ve hareket unsurlarını kon-trol eden yerel düğümlerin hızlı ve gerçek zamanlı olarak sisteme dahil edilme-sidir. URB, merkezi bir bilgisayara gerçek zamanlı RS232 ve USB bağlantı desteğisağlayan etkili ve zahmetsiz bir altyapı olarak tasarlanmıştır.URB altyapısı, benzer şekilde hareketli robotlarda dahili haberleşmeyi sağla-yan RiSEBus'tan esinlenilerek geliştirilmiş olmakla birlikte, bizim ihtiyaçlarımızagöre iyileştirilmiştir. URB protokolu hızlı ve sık platform tasarım değişiklikleriiçin modüler ve genişletilebilir bir mimari önermektedir. Ayrıca, hareketli robot-lar yapısal olarak duzensiz ortamlarda çalışabilmek için kesin algısal işlemlereve tahminlere ihtiyaç duyarlar. Bu nedenle, URB guvenilir donanım ve yazılımbileşenleriyle gerçek zamanlı işlemleri desteklemektedir.Bu tezin ilk onemli katkısı, çoklu düğümler üzerinden veri edinmenin otomatikolarak senkronize edilmesinin tasarımı ve uygulamasıdır. Senkronizasyon algorit-mamız, veri okuma işlemlerinden once her düğümün veri edinme işlemlerinin aynıanda bitirilmesini garanti etmektedir. Ayrıca deneylerimiz de her düğümün küçükfarklarla aynı anda veri edindiklerini kanıtlamaktadır. Bu tezin ikinci katkısı ise,çoklu düğümler üzerinden otomatikleştirilmiş ve denetlenmemiş veri edinimidir.Özerk veri edinimi, periyodik olarak ve sıkça ihtiyaç duyulan verilerin düğümlerüzerinden edinilmesini sağlayan bir işlemdir. Bu geliştirme merkezi işlemci bir-imi üzerindeki hesaplama yükünü azaltırken, haberleşme ortamı üzerindeki bantgenişliği masrafını da azaltır. Bu tez URB'nin uygulama detaylarının yanındaayrıca ağ mimarileri ve protokolleri, ağ uygulamalarının robot sistemlerindekiyeri, senkronizasyon algoritmaları ve robot sistemlerinde senkronizasyon uygula-maları uzerine bir araştırma olarak da hizmet etmektedir.","Design and construction of small autonomous mobile robots is a challenging taskthat involves the selection, interfacing and programming of a large number ofsensor and actuator components. Facilitating this tedious process requires modularityand extensibility in both hardware and software components. This thesisconcerns the development of a real-time infrastructural architecture called theUniversal Robot Bus (URB), based on the popular Inter-Integrated Circuit (I2C)bus standard. The main purpose of the URB is the rapid development and realtimeinterfacing of local nodes controlling small sensor and actuator componentsdistributed on a mobile robot platform. It is designed to be very lightweightand efficient, with real-time support for RS232 or USB connections to a centralcomputer.The URB infrastructure is inspired from the RiSEBus architecture, which isalso an internal communication protocol for mobile robots, and developed to fitour requirements. URB offers a modular and extensible architecture for rapidand frequent changes to the platform design. Mobile robots also need to performaccurate sensory processing and estimation in order to operate in unstructuredenvironments. Hence, the URB also supports real-time operations with reliablehardware and software components.The first novel contribution of this thesis is the design and implementationof automatic synchronization of data acquisition across multiple nodes. Our synchronizationalgorithm ensures that each node completes data acquisition taskssimultaneously well before read operations. Our experiments also prove thateach individual node acquires data at approximately the same time instant. Thesecond major contribution of this thesis is the incorporation of automated and unsupervised data acquisition across multiple nodes into the URB protocol. Autonomousdata acquisition helps acquire periodic and frequently needed dataover nodes. This enhancement reduces the computational load on the centralprocessing unit and reduces bandwidth costs over the communication medium.This thesis also servers a survey on network architectures and protocols, networkapplications in robotics, synchronization algorithms, and applications of synchronizationin robotics besides implementation details of the URB."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kalp damar cerrahisi kapsamında yapılan ameliyatlarda ölüum riskinin belirlenip hasta ve hasta yakınlarına ameliyat öncesinden bildirilmesi büyük önem arz etmektedir. Bu amaçla Avrupalı araştırmacılar tarafından EuroSCORE (The European System for Cardiac Operative Risk Evaluation) adında bir sistem geliştirilmiştir. Bu sistem ameliyat öncesi ölçülen bazı parametreleri kullanarakameliyat sırasında veya ameliyattan sonraki ilk 30 gün içerisinde hastanın ölüm (mortality) riskini vermektedir. Bu model Avrupa'da yapılan çok sayıdaki ameliyatta kaydedilen bilgilerin istatistiksel olarak değerlendirilmesiyle oluşturulmuştur.Günümüzde cerrahi tekniklerinde gelişmeler ve ameliyatlardaki ölüm risklerinde düşüş görülmesine rağmen, hasta ve hasta yakınları için ölüm riskinin bilinmesi hala daha büyük önem taşımaktadır. Ayrıca, hastanın ölüm riskinin bilinmesi devlet ve özel sağlık sigorta şirketleri için gerekmektedir.Bu araştırmada Türkiye'deki hastanelerde yapılan kalp-damar ameliyatlarında ölçülen parametrelerin kaydedilebileceği bir veri tabanı ve bu kayıtlar üzerinde makine öğrenmesi çalışmaları ile EuroSCORE'a benzer bir risk belirleme modelinin öğrenileceği bir sistem geliştirilmiştir. Bu araştırmada, özniteliklerin ROC alanı risk hesaplanmasında özniteliklerin ağırlığı olarak kullanılmaktadır.Bu şekilde, tüm ROC alanını maksimum hale getirerek daha iyi bir öznitelik tabanlı makine öğrenmesi ve risk tahmin modeli geliştirilmiştir.Hastaların genetik özellikleri ve yaşam tarzları göz önüne alındığında, Türk hastaların kalp-damar ameliyatlarındaki ölüm risklerinin Avrupalı hastalardan farklı olması kuvvetle muhtemeldir. Bu çalışmada, bu farklılık araştırılmıştır.","It is very important to inform the patients and their relatives about the risk of mortality before a cardiovascular operation. For this respect, a model called EuroSCORE (The European System for Cardiac Operative Risk Evaluation) has been developed by European cardiovascular surgeons. This system gives the risk of mortality during or 30 days after the operation, based on the values of some parameters measured before the operation. The model used by EuroSCORE has been developed by statistical data gathered from large number of operations performed in Europe.Even though due to the surgical techniques that have been developed recently and the risk of mortality has been reduced in a large extent, predicting that risk as accurately as possible is still primary concern for the patients and their relatives in cardiovascular operations. The risk of operation also essentially tells the surgeon how a patient with similar comorbidity would be expected to fare based on a standard care. The risk of patient is also important for the health insurance companies, both public and private. In the context of this project, a model that can be used for mortality is developed.In this research project, a database system for storing data about cardiovascular operations performed in Turkish hospitals, a web application for gathering data, and a machine learning system on this database to learn a risk model, similar to EuroSCORE, are developed. This thesis proposes a risk estimation system for predicting the risk of mortality in patients undergoing cardiovascular operations by maximizing the Area under the Receiver Operating Characteristic (ROC) Curve (AUC).When the genetic characteristics and life styles of Turkish patients are taken into consideration, it is highly probable that the mortality risks of Turkish patients may be different than European patients. This thesis also intends to investigate this issue."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, yordamsal modellemeye dayalı bir otomatik bina üretim sistemi ve sanal şehirler içinde kalabalık simülasyonu sağlayan bir simülasyon sistemi sunulacaktır. Binalar modellenirken, detaylı ve tutarlı geometrilerin üretilebilmesi için şekil gramerleri kullanılmıştır. Model üretme süreci belli noktalarda olasılıksal olarak işleyerek, gerekli çeşitlilikte bina modellerinin üretilmesine olanak sağlamaktadır. Bina modellerinin üretim süreci sonunda sahip olacağı geometri, türetme kuralları tarafından belirlenmektedir. İnsan kalabalıklarının şehir içi davranışları, simülasyon senaryosuna bağlıdır. Bu tezde, şehir içindeki insan kalabalıklarının, özellikle yangın, patlama ya da terörist saldırı sonucunda meydana gelebilecek acil durumlardaki davranışlarının simülasyonu amaçlanmaktadır. Kaçışan insan kalabalıklarının simülasyonunda kullanılan akışkan mekaniğine dayalı yaklaşım diğer yaklaşımlardan daha verimli olmaktadır. Simülasyon, sadece acil duruma sebebiyet veren olaya yakın bölgelerde yapılmaktadır. Elde edilen simülasyonların canlandırılmasını ve görüntülenmesini hızlandırmak amacıyla, kapatılan alanlar önceden hesaplanarak depolanmaktadır. Simülasyonun gorüntülenmesi sırasında, kullanıcı tarafından gorülemeyecek insan modellerinin canladırılması ve görüntülenmesi engellenmektedir. Önişlem aşamasında, şehir modeli içinde dolaşılabilecek alanlar, birbirine eş hücrelere bölünüp, bu hücrelerin bölgeden görülme bilgileri grafik işlemci ünitesinin kapatılan bölge sorguları yardımıyla hesaplanmaktadır.","In this thesis, we present an automatic building generation method based on procedural modeling approach, and a crowd animation system that simulates a crowd of pedestrians inside a city. While modeling the buildings, to achieve complex and consistent geometries we use shape grammars. The derivation process incorporates randomness so the produced models have the desired variation. The end shapes of the building models could be defined in a certain extent by the derivation rules. The behavior of human crowds inside a city is affected by the simulation scenario. In this thesis, we specifically intend to simulate the virtual crowds in emergency situations caused by an incident, such as a fire, an explosion, or a terrorist attack. We prefer to use a continuum dynamics-based approach to simulate the escaping crowd, which produces more efficient simulations than the agent-based approaches. Only the close proximity of the incident region, which includes the crowd affected by the incident, is simulated. In order to speed up the animation and visualization of the resulting simulation, we employ an offline occlusion culling technique. During runtime, we animate and render a pedestrian model only if it is visible to the user. In the pre-processing stage, the navigable area of the scene is decomposed into a grid of cells and the from-region visibility of these cells is computed with the help of hardware occlusion queries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Algılayıcı ağlarında gözlem yapan bir algılayıcının yeri, doğru veri analiziyapılması için hayati önem arz eder. Küresel Konumlama Sistemi (GPS) hassasyer tayinine imkan sağlayan oturmuş bir sistemdir. Buna karşın, kaynaksınırlamaları ve boyut gereksinimleri söz konusu teknolojinin maliyet etkin olaraktasarlanan küçük algılayıcılarda kullanılmasına olanak sağlamamaktadır. Bununyerine, çoğu konumlar birtakım algoritmalar kullanılarak tahmin edilmektedir.Bu gibi tahminler sahadan toplanan veride kaçınılmaz olarak hata bulunmasınaneden olmaktadır. Hatalı veri analizine yol açabilecekleri durumlarda bu türhataların saplanması büyük önem arz etmektedir. Bununla birlikte, uygulamalarınçoğu bileşenleri karar verme süreçlerinde bildirilen konum verisine itimatetmektedirler. Bu yüzden konumlama hatalarının etkilerini uygulama bakışaçısıyla anlamak önemlidir. Bu güne kadar, her algılayıcının ağdan izole edilmişbireysel konumu, konum belirlemede odak noktası olmuştur. Biz bu tezde,karşılaştırmalarda bütün ağ topolojisini ve göreli algılayıcı konumlarını dikkate almayanyaklaşımlarda bulunan problemlere dikkat çektik. Daha sonra literatürdekullanılan metrikleri tarif edip, bu araştırma alanında kullanılabilecek birkaç yenimetrik önerdik. Ayrıca, mevcut ve önerdiğimiz metriklerin davranışlarını anlamakamacıyla simülasyonlar koşturduk. Simülasyon sonuçlarını tartıştıktansonra kablosuz algılayıcı ağ uygulamaları için kullanılabilecek bir metrik seçimmetodolojisi önerdik.","In ad-hoc and sensor networks, the location of a sensor node making an observationis a vital piece of information to allow accurate data analysis. GPS is anestablished technology to enable precise position information. Yet, resource constraintsand size issues prohibit its use in small sensor nodes that are designed tobe cost efficient. Instead, most positions are estimated by a number of algorithms.Such estimates, inevitably introduce errors in the information collected from thefield, and it is very important to determine the error in cases where they leadto inaccurate data analysis. After all, many components of the application relyon the reported locations including decision making processes. It is, therefore,vital to understand the impact of errors from the applications? point of view. Todate, the focus on location estimation was on individual accuracy of each sensor?sposition in isolation to the complete network. In this thesis, we point out theproblems with such an approach that does not consider the complete networktopology and the relative positions of nodes in comparison to each other. Wethen describe the existing metrics, which are used in the literature, and also proposesome novel metrics that can be used in this area of research. Furthermore,we run simulations to understand the behavior of the existing and proposed metrics.After having discussed the simulation results, we suggest a metric selectionmethodology that can be used for wireless sensor network applications."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Eşler arası bilgisayar ağları yaklaşımı kaynak paylaşımı ve içerik dağıtımındageleneksel istemci-sunumcu yaklaşımına karşı yaygın ve başarılı bir seçenek olarakoldukça dikkat çekmektedir. Ancak, araştırmacılar eşler arası bilgisayar ağlarınınetkin ve verimli çalışmasını, dolayısıyla, bu yaklaşımın geleceğini ciddi olaraktehdit eden önemli miktarda ?katkısız katılımı? bu ağlarda gözlemlemişlerdir. Bunedenle, katkısız katılımın eşler arası bilgisayar ağları üzerindeki olumsuz etkisiniazaltmak veya kaldırmak önemli bir araştırma konusu haline gelmiş ve bu alandabir çok çalışma yapılmıştır.Bu tezde, katkısız katılımın eşler arası bilgisayar ağları üzerindeki olumsuz etkisininazaltılması ve kullanıcıların katkı yapmaya teşvik edilmesi maksadıylaiki yeni yaklaşım önerilmiştir. Bu ana yaklaşımlar, katkıda bulunan kullanıcılarınbaşarımını artırırken katkısız kullanıcıları cezalandırmayı sağlayacakşekilde tasarlanmıştır. Birinci ana yaklaşımda, katkısız kullanıcıların tespiti vecezandırılmasına dayanan dağıtık ve yerselleştirilmiş bir çözüm önerilmiştir. Buyaklaşım, Bul ve Cezalandır Yöntemi olarak adlandırılmıştır. Eşler Arası BağlantıYönetim Protokolü adı verilen ikinci ana yaklaşımda ise, kullanıcılar arasındakibağlantıları kullanıcıların katkısına göre yönetmeyi esas alan bağlantı tabanlı birçözüm önerilmiştir.Önerilen ana yaklaşımları değerlendirmek için yeni bir simülatör geliştirilmişve bir çok deney yapılmıştır. Simülasyon sonuçları göstermiştir ki önerilenana yaklaşımların kullanılması, katkısız katılımın eşler arası bilgisayar ağlarıüzerindeki olumsuz etkisini azaltmış ve genelde başarımı artırmıştır. Bunlara ekolarak, önerilen ana yaklaşımları kullanan ağlar daha güçlü ve daha ölçeklenebilirhale gelmişlerdir.","The peer-to-peer (P2P) network paradigm has attracted a significant amount ofinterest as a popular and successful alternative to traditional client-server modelfor resource sharing and content distribution. However, researchers have observedthe existence of high degrees of free riding in P2P networks which poses a seriousthreat to effectiveness and efficient operation of these networks, and hence totheir future. Therefore, eliminating or reducing the impact of free riding on P2Pnetworks has become an important issue to investigate and a considerable amountof research has been conducted on it.In this thesis, we propose two novel solutions to reduce the adverse effects of freeriding on P2P networks and to motivate peers to contribute to P2P networks.These solutions are also intended to lead to performance gains for contributingpeers and to penalize free riders. As the first solution, we propose a distributedand localized scheme, called Detect and Punish Method (DPM), which dependson detection and punishment of free riders. Our second solution to the free ridingproblem is a connection-time protocol, called P2P Connection Management Protocol(PCMP), which is based on controlling and managing link establishmentsamong peers according to their contributions.To evaluate the proposed solutions and compare them with other alternatives,we developed a new P2P network simulator and conducted extensive simulationexperiments. Our simulation results show that employing our solutions in a P2Pnetwork considerably reduces the adverse effects of free riding and improves theoverall performance of the network. Furthermore, we observed that P2P networksutilizing the proposed solutions become more robust and scalable."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son yıllarda çok geniş veri tabanlarının kullanımıyla birlikte içerik tabanlıgörüntü indekslemesi ve erişimi önemli bir araştırma konusu halini almıştır.Görütü indekslenmesinde kullanılan alt düzey öznitelikler görüntülerin karmaşıkiçeriklerini yeterli olarak ifade edememektedirler. Bu çalışmada, görüntü indekslemesiiçin sahne sınıflandırmasını baz alan bir görüntü erişim sistemitanımlanmıştır. İlk olarak renk ve doğrusal çizgi yapı özellikleri kullanılarakgörüntüler bölütlenmiştir. Çizgi yapı özellikleri kullanılarak, insan yapısı gibibirörnek renklerden oluşmayan yapıların görüntülerden bölütlenmesi hedeflenmektedir.Bölütleme sonucunda elde edilen tüm bölütler k-means öbekleme algoritmasıkullanılarak öbeklendikten sonra, her görüntü içermiş olduğu bölüttürlerinin histogramıyla ifade edilmiştir. Elde edilen histogramlar üzerinde çoksınıflı ve tek sınıflı sınıflandırıcılar eğitilmiş ve her görüntü için o görüntününfarklı sınıflara ait olma olasılıkları bulunmuştur. Bir görüntü aynı anda birdenfazla sınıfa ait olabileceğinden, görüntüleri en yüksek olasılık değerini verensınıfla etiketlemek yeterli olmayabilir. Bu nedenle, görüntüler tüm sınıflara aitolma olasılıkları ile indekslenmiş ve içerik tabanlı görüntü erişimi bu indekslerkullanılarak gerçekleştirilmiştir. Görüntü erişim sistemini insan algısıyla desteklemekve anlambilimsel uçurumu en aza indirgemek için erişim senaryosuna tek sınıfsınıflandırıcı bazlı ilgililik geri beslemesi eklenmiştir. Bunun için, ilgili görüntüleriçok iyi modelleyen, ilgili olmayan görüntülerden de bir o kadar uzak duranbir hiperküre oluşturan destek vektör veri tanımlaması kullanılmıştır. Önerilenyöntemler TRECVID ve Corel veri kümelerinde denenmiş ve başarılı sonuçlarelde edilmiştir.","Content-based image indexing and retrieval have become important researchproblems with the use of large databases in a wide range of areas. Because of theconstantly increasing complexity of the image content, low-level features are nolonger sufficient for image content representation. In this study, a content-basedimage retrieval framework that is based on scene classification for image indexingis proposed. First, the images are segmented into regions by using their color andline structure information. By using the line structures of the images the regionsthat do not consist of uniform colors such as man made structures are captured.After all regions are clustered, each image is represented with the histogram ofthe region types it contains. Both multi-class and one-class classification modelsare used with these histograms to obtain the probability of observing differentsemantic classes in each image. Since a single class with the highest probabilityis not sufficient to model image content in an unconstrained data set with a largenumber of semantically overlapping classes, the obtained probability values areused as a new representation of the images and retrieval is performed on thesenew representations. In order to minimize the semantic gap, a relevance feedbackapproach that is based on the support vector data description is also incorporated.Experiments are performed on both Corel and TRECVID datasets and successfulresults are obtained."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Telsiz algılayıcı ağları önceden belirlenmiş bir amacı gerçekleştirmek için tasarsız bir biçimde örgütlenen yüzlerce veya binlerce algılayıcı düğümden oluşur. Telsiz algılayıcı ağlarda bellek ve işlemcide sınırlamalar olsa da, onları geleneksel ağlardan ayıran en önemli kısıt pil problemidir. Algılayıcı düğümler zor çevresel koşulların olduğu alanlara yayıldığından, biten pilleri yenileriyle değiştirmek pratik olarak mümkün olmamaktadır. Bu durum her bir algılayıcı düğümünün enerjisinin hem düğüm hem de ağ seviyesinde dikkatlice kullanılmasını gerektirmektedir. Literatürde, ağ ömrünü geliştirmek için veri toplaşımı, enerji etkin yönlendirme düzenleri ve ortama erişim protokolleri gibi bir çok yaklaşım önerilmiştir. Bu yaklaşımların temel motivasyonu servis kalitesinden ödün vermeyerek ağ ömrünü geliştirmektir. Çıkış düğümü (veri toplanan düğüm) yer değişimi literatürde ağ ömrünü geliştirmek için sunulan etkin çözümlerden biridir.Bu tezde, kontrol edilebilir çıkış düğümü yer değişimine odaklanıp, bu problemin çıkış düğümü yerleri belirleme, hareket kararı parametreleri gibi değişik kısımlarının çözümü için bir algoritma kümesi sunulmuştur. Ayrıca, ağ ömrü uzatma çözümünün farklı bir bileşeni olarak yük dengeli topoloji yapılandırması konusunda da bir algoritma verilmiştir. Yer değişimi düzeninin farklı bileşenlerini literatürdeki diğer ilgili yaklaşımlarla, bütün çıkış düğümü yer değişimi düzenini ise, rasgele yer değişimi ve hareketsiz çıkış düğümü durumlarıyla karşılaştıran deney sonuçları sunulmuştur. Sonuç olarak, algoritmalarımızın rasgele yer değişimi ve hareketsiz durumlara göre daha iyi sonuçlar verdiği görülmüştür.","A wireless sensor network (WSN) consists of hundreds or thousands of sensor nodes organized in an ad-hoc manner to achieve a predefined goal. Although WSNs have limitations in terms of memory and processor, the main constraint that makes WSNs different from traditional networks is the battery problem. Since sensor nodes are generally deployed to areas with harsh environmental conditions, replacing the exhausted batteries become practically impossible. This requires to use the energy very carefully in both node and network level. Different approaches are proposed in the literature for improving network lifetime, including data aggregation, energy efficient routing schemes and MAC protocols, etc. Main motivation for these approaches is to prolong the network lifetime without sacrificing service quality. Sink (data collection node) mobility is also one of the effective solutions in the literature for network lifetime improvement.In this thesis, we focus on the controlled sink mobility and present a set of algorithms for different parts of the problem, like sink sites determination, and movement decision parameters. Moreover, a load balanced topology construction algorithm is given as another component of network lifetime improvement. Experiment results are presented which compare the performance of different components of the mobility scheme with other approaches in the literature, and the whole sink mobility scheme with random movement and static sink cases. As a result, it is observed that our algorithms perform better than random movement and static cases for different scenarios."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Histopatolojik inceleme, kanseri de içeren büyük bir hastalık grubunun tanısı içinen sık kullanılan klinik tanı tekniklerinden birisidir. İnsan gözüyle uygulananbu yöntemin içerdiği gözlemci tutarsızlığını ve manüel harcanan eforu azaltmakiçin birçok sayısal yöntem önerilmiştir. Bu yöntemler, dokuyu bir matematikselözellikler kümesi olarak tanımlar. Tanımlanan özellikleri sonraki biyopsi analizleri için kullanır.Bez yapıları içeren doku tiplerinde, bezlerde meydana gelendeğişikliklerin incelenmesi bu analizlerden bir tanesidir. Böyle analizlerde ilkadım, dokudaki bezsel alanları bolümlemektir.Bu tezde, bağırsak bezlerinin bölümlenmesi için nesne tabanlı bir yöntemönerilmektedir. Bu yöntemde, görüntüden bir basit nesneler kümesi çıkartılmasıve bu nesnelerin uzamsal dağılımları kullanılarak bezlerin konumlarının tespitedilmesi önerilmiştir. Önerilen bu yöntemde, pikseller öncelikle çeşitli histolojikyapılara karşılık gelecek şekilde, renk yoğunluklarına göre gruplanır. Ardından,kümelenmiş görüntüden bir basit dairesel nesneler kümesi (lümen bölgeleri içinbeyaz nesneler ve hücresel bölgeler için siyah nesneler) elde edilir ve bu nesnelerdenuzamsal dağılımlarını nicelemek amacıyla bir çizge oluşturulur. Sonrasında,bu çizgeden bir dizi özellik çıkartılır ve çıkartılan bu özellikler bez adaylarınınbaslangıç çekirdek noktalarını belirlemek için kullanılır. Bu çekirdek noktalarındanbaşlayarak ve siyah nesnelerin konumları gözetilerek, bezlerin iç bölgelerialan büyütme yöntemiyle tespit edilir. Son olarak, bezlere ait olmayan alanlar,tespit edilen iç bölgelerden çıkartılmış bir diğer özellik kümesi sayesinde elenirve geriye kalan gerçek bezlerin sınırları, bu bezlerin yakınındaki siyah nesnelerinkonumları gözetilerek belirlenir.Kalın bağırsak biyopsi görüntüleri üzerinde yaptığımız deneyler, önerdiğimizbu yöntemin yüksek hassasiyet, belirlilik ve doğruluk oranları sağlayabildiğini göstermiştir.Ayrıca, deneylerimizden elde ettiğimiz sonuçlar bu yöntemin,daha önceki çalışmalarda önerilmiş piksel tabanlı bez bölümleme algoritmalarınınperformanslarını istatistiksel anlamlı bir şekilde iyileştirdiğini ortayakoymuştur. Yapılan deneyler, yöntemin nesne tabanlı yapısının, boyama vekesme yöntemindeki farklılıkların yan etkilerine karşı tolerans sağladığını dagöstermiştir. Önerilen bu yöntem, otomatik kanser tanısı ve derecelendirmesiamacıyla, bezlerin daha ileri düzey analizi için bir altyapı da sunmaktadır.","Histopathological examination is the most frequently used technique for clinical diagnosis of a large group of diseases including cancer. In order to reduce the observer variability and the manual effort involving in this visual examination, many computational methods have been proposed. These methods represent a tissue with a set of mathematical features and use these features in further analysis of the biopsy. For the tissue types that contain glandular structures, one of these analyses is to examine the changes in these glandular structures. For such analyses, the very first step is to segment the tissue into its glands.In this thesis, we present an object-based method for the segmentation of colon glands. In this method, we propose to decompose the image into a set of primitive objects and use the spatial distribution of these objects to determine the locations of glands. In the proposed method, pixels are first clustered into different histological structures with respect to their color intensities. Then, the clustered image is decomposed into a set of circular primitive objects (white objects for luminal regions and black objects for nuclear regions) and a graph is constructed on these primitive objects to quantify their spatial distribution. Next, the features are extracted from this graph and these features are used to determine the seed points of gland candidates. Starting from these seed points, the inner glandular regions are grown considering the locations of black objects. Finally, false glands are eliminated based on another set of features extracted from the identified inner regions and exact boundaries of the remaining true glands are determined considering the black objects that are located near the inner glandular regions.Our experiments on the images of colon biopsies have demonstrated that our proposed method leads to high sensitivity, specificity, and accuracy rates and that it greatly improves the performance of the previous pixel-based gland segmentation algorithms. Our experiments have also shown that the object-based structure of the method provides tolerance to artifacts resulting from variances in biopsy staining and sectioning procedures. This proposed method offers an infrastructure for further analysis of glands for the purpose of automated cancer diagnosis and grading."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Saç, 15 yıldan fazladır bilgisayar grafiği toplumu için aktif bir araştırma alanı olmuştur. Geçmişte, saçın modelleme, simulasyon, animasyon ve görüntüleme bakış açılarından birçok farklı yaklaşımlar araştırılmış ve önerilmiştir. Bu tezde, saç modelleri üretmek ve bu üretilen modelleri fiziksel şartlar altında gerçek zamanlı olarak simüle etmek için geliştirdiğimiz, çizim bazlı doğrudan etkileşim arayüzü kullanılan aracı sunuyoruz. Geliştirilen araç tez içerisinde, problemin saç modelleme, saç simülasyonu, saç çizimi ve saç görüntülenmesi gibi yönleriyleincelenecektir. Son olarak, araç için uygulanılan kullanım testinin sonuçları sunulmaktadır.","Hair has been an active research area in computer graphics society for over a decade. Different approaches have been proposed for different aspects of hair research such as modeling, simulating, animating and rendering. In this thesis, we introduce a sketch-based tool making use of direct manipulation interfaces to create hair models and furthermore simulate the created hair models underphysically based constraints in real-time. Throughout the thesis, the created tool will be analyzed with respect to dierent aspects of the problem such as hair modeling, hair simulation, hair sketching and hair rendering. Finally, the results of a usability test of the tool are provided."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar tabanlı animasyonlar (insanlar, diğer canlılar ve her türlü nesneler) uzun seneler boyu üzerinde araştırma yapılmış konulardır. Bu animasyonların başarıya ulaşmasında en önemli rol, nesnelerin hareketlerinin doğru şekilde taklit edilebilmesi olmuştur. Bilgisayar teknolojisinde yaşanan hızlı ilerlemeler sonucu, son derece gerçekçi animasyonlar üretilmeye başlanmıştır. Animasyonlarda en çok dikkat çeken öğelerse, tahmin edilebileceği gibi en hareketli bölgelerdir. Bu hareketli öğeler insan ve yüz animasyonlarında genellikle saçlardır.Yüksek kaliteli animasyonların yapılmasında, sadece fiziksel gerçekçi harekete ve görüntüye sahip animasyonlar yapmaktan ziyade, bu animasyonları gerçek zamanlı olarak kontrol edebilme gereksinimi de oluşmuştur. Bu tezde tartışılacak sistem, yüksek kalitede bir animasyonun sıfırdan oluşturulup, anahtar kareler kullanılarak hareketlendirme problemine bir çözüm önermektedir. Saçın gerçek zamanlı olarak tasarlanıp, anahtar kareler aracılığıyla animasyonu amaçlanmaktadır.Bu çalışmada daha önce saç animasyonu ile ilgili yapılmış diğer sistemlerden bahsedilecek ve söz konusu sistemler dışında nasıl bir şekilde bu animasyonların oluşturabileceğine dair çözümler getirilecektir.","Computer generated animations of humans, animals and all other kinds of objects have been studied extensively during the last two decades. The key for creating good animations has been to correctly imitate the behaviors of real objects and reflect these into computer generated images. With the rapid development of computer technology, creating realistic simulations has become possible, and the most striking components of these realistic animations happen to be the most dynamic (moving) parts; hair, in the case of human animations.With the development of high quality hair animations, the concern is not only creating physically correct animations, but also controlling these animations. An implementation of a key frame hair animation creation system, supported by a hair design tool, helping to model and animate hair easily, and provide these functionalities in real time is the aim of the proposed system.This work reviews several hair animation and sketching techniques, and proposes a system that provides a complete level of control (capable of controlling even the individual hair strands) of key frame animation and hair design in real time."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, video bilgisinden insan hareketlerini tanımaya yönelik bir çalışma sunulmaktadır. Literatüre ana katkılarımız, videoları temsil ederken poz bilgisinin verimli kullanılması ve en önemlisi insan hareketlerinin poz cümleleri olarak değerlendirilmesi ve bu cümlelerin dizgi eşleme yöntemlerinden yararlanarak sınıfandırılmasıdır. Tek bir aktörün tek bir hareket gerçekleştirdiği videolar üzerine odaklanıyoruz. Her bir sözcük bir poza denk gelecek biçimde, aktiviteleri sözcükler içeren dökümanlar olarak temsil ediyoruz. Poz bilgisinin hareketi tanımlarken çok güçlü bir kaynak olduğunu düşünmekteyiz. Pozları en iyi şekilde betimlemek için literatürde başarılı sonuçlar vermiş dört farklı yöntemi kullanıyoruz; gradyan yön histogramları, k-bitişik kesim, şekil konteksti ve optik akım histogramları. Hareketleri temsil etmek amacıyla, ilk önce aktivite veritabanımız için sözlük niteliği taşıyacak bir kod rehberi oluşturuyoruz. Hareketleri poz sözcüğü dizileri, poz cümleleri olarak temsil ediyoruz. Aktiviteler arasıbenzerlikeri bulurken, dizgi eşleştirme yöntemlerinden yararlanıyoruz. Ayrıcapoz kümeleri yöntemini de karşılaştırma amaçlı uygulamaktayız ve poz cümleleritekniğimizin üstün olduğu göstermekteyiz. Metodumuzun verimini ölçmek içinsıklıkla kullanılan iki video veritabanı, Weizmann ve KTH, üzerinde testlerimizigerçekleştirdik. Pozun hareket tanımında çok açıklayıcı olduğunu ve hareketlerinkarışık dinamiklerini inceleyen yöntemler yerine, daha basit tekniklerle de başarılısonuçlar alınabileceğini göstermekteyiz.","In this thesis we address the problem of human action recognition from video sequences. Our main contribution to the literature is the compact use of poses while representing videos and most importantly considering actions as pose-sentences and exploit string matching approaches for classification. We focus on single actions, where the actor performs one simple action through the video sequence. We represent actions as documents consisting of words, where a word refers to a pose in a frame. We think pose information is a powerful source for describing actions. In search of a robust pose descriptor, we make use of four well-known techniques to extract pose information, Histogram of Oriented Gradients, k-Adjacent Segments, Shape Context and Optical Flow histograms. To represent actions, first we generate a codebook which will act as a dictionary for our action dataset. Action sequences are then represented using a sequence of pose-words, as pose-sentences. The similarity between two actions are obtained using string matching techniques. We also apply a bag-of-poses approach for comparison purposes andshow the superiority of pose-sentences. We test the efficiency of our method with two widely used benchmark datasets, Weizmann and KTH. We show that pose is indeed very descriptive while representing actions, and without having to examine complex dynamic characteristics of actions, one can apply simple techniqueswith equally successful results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Karar verme bir çok seçeneğin arasından en iyiyi seçme işidir. Gerçek uygulamalarda, karar vericinin en iyi karara varabilmesi için gerekli olan bilginin bir maliyeti vardır. Bu tezde, karar verme aşamasında ortaya çıkan hatalı kararın maliyeti ile en iyi kararı vermek için kullanılan bilginin maliyetini beraber ele alan yeni bir öğrenme yöntemi önerilmiştir. önerilen bu yeni yöntem, maliyete duyarlı öğrenmeye nitelliği ve tutarlılığı iki yeni kavram olarak sunmuştur.Bu çalışmayla ilk olarak, nitel maliyet kavramı makine öğrenmesi sürecine dahil edilmiştir.Verilen kararın hatalı olmasından kaynaklanan maliyet ile bu kararı verebilmek için kullanılacak bilginin maliyeti arasındaki ilişkinin bir çok problemde nicel olarak tanımlanamamasından dolayı nitel maliyet kavramı önemlidir. örneğin kanser teşhisinde, yanlış teşhis yapmanın maliyetinin teşhis için kullanılan testlerin maliyetinden daha büyük olduğu söylenebilir. Fakat, bu iki kavram arasındaki ilişkinin nicel olarak tanımlanması zordur. Daha önce maliyete duyarlı öğrenmeyle ilgili yapılan çalışmalar bu iki maliyetin birbiriyle olan ilişkisinin nicel olarak tanımlanmasını şart koşmuşlardır. Bu yüzden, önerilen nitel maliyet ilişkisi kavramı bu konu hakkındaki çalışmalara yeni bir boyut kazandırmıştır.İkinci olarak ise, bu tezde yapılan çalışma yeni elde etmeyi beklediğimiz bilgi ile şimdi sahip olduğumuz bilgi arasındaki tutarlığı göz önüne almıştır. Eğer yeni elde edilecek bilgi şimdiki bilgimize yeni bir şey eklemiyor ya da bir başka deyişle yeni elde edilecek bilgi şimdiki bilgimizle tutarlı ise önerilen yöntem yeni elde edilecek bilgi için gerekli olan maliyetin karşılanmasını reddetmektedir. Böylece önerdiğimiz yöntemle, karar verme aşamasında karar verme sürecini etkilemeyen bilgi için maliyet yapılmamış olmaktadır. Bu kavram daha önceki çalışmalarda hiç kullanılmamıştır.Üç farklı medikal veri kümesi üzerindeki deneylerimiz, önerdiğimiz yöntemin teşhisteki doğruluk oranını etkilemeden, kullanılanmedikal testlerin maliyetini büyük ölçeklerde azaltmayı başardığını göstermiştir.","Decision making is a procedure for selecting the best action among severalalternatives. In many real-world problems, decision has to be taken under thecircumstances in which one has to pay to acquire information. In this thesis, wepropose a new framework for test-cost sensitive classification that considers themisclassification cost together with the cost of feature extraction, which arisesfrom the effort of acquiring features. This proposed framework introduces twonew concepts to test-cost sensitive learning for better modeling the real-worldproblems: qualitativeness and consistency.First, this framework introduces the incorporation of qualitative costs intothe problem formulation. This incorporation becomes important for many realworld problems, from finance to medical diagnosis, since the relation betweenthe misclassification cost and the cost of feature extraction could be expressedonly roughly and typically in terms of ordinal relations for these problems. Forexample, in cancer diagnosis, it could be expressed that the cost of misdiagnosisis larger than the cost of a medical test. However, in the test-cost sensitive classificationliterature, the misclassification cost and the cost of feature extractionare combined quantitatively to obtain a single loss/utility value, which requiresexpressing the relation between these costs as a precise quantitative number.Second, the proposed framework considers the consistency between the currentinformation and the information after feature extraction to decide which featuresto extract. For example, it does not extract a new feature if it brings no newinformation but just confirms the current one; in other words, if the new featureis totally consistent with the current information. By doing so, the proposedframework could significantly decrease the cost of feature extraction, and hence,the overall cost without decreasing the classification accuracy. Such consistencybehavior has not been considered in the previous test-cost sensitive literature.We conduct our experiments on three medical data sets and the results demonstratethat the proposed framework significantly decreases the feature extractioncost without decreasing the classification accuracy."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilinmeyen bir ortamda keşif yapan seyyar bir robot, almaçları vazıtasıyla konumunutakip etmek durumunda kalabilir. Mesafe taramalarının eşlenmesi, robotun geçtiği seyir yoluüzerindeki iki farklı mevkide kaydedilen mesafe almacı kayıtlarında ortak olan özniteliklerinbulunmasıyla, bu mevkiler arasındaki konum farkının hesaplanmasıdır. Bu tezde, bilinmeyenve farklı konumlarda kaydedilmiş lazer mesafe taramalarından çıkartılan, ortak doğruparçalarını eşleyerek konum farkını hesaplayan, doğru parçası tabanlı bir mesafe taramasıeşleme algoritması sunulmaktadır. Bu algoritmada eşleme işlemi, doğru parçaları arasındaki,geometrik ilişkiler olarak adlandırdığımız, değişmez geometrik öznitelikler kullanılarak gerçekleştirilmektedir.Mesafe noktaları yerine bu noktalara oturtulan doğru parçalarının kullanılmasıiki farklı tarama arasındaki konum farkını kestirmek için yapılan hesaplamalarınkarmaşıklığını azaltmaktadır. Diğer mesafe taraması eşleme algoritmalarıyla kıyaslandığında,bizim metodumuzun küresel tarama eşleme, harita oluşturma, yer tanıma, döngü kapatmave çoklu robot ile haritalama problemlerinin gerçek zamanlı çözümleri için etkili bir altyapısunduğu görülmektedir.","A mobile robot exploring an unknown environment often needs to keep track of its posethrough its sensors. Range scan matching is a way of computing the pose difference of arobot at two different locations on the navigation path by ? nding common features observedin range sensor readings recorded at these locations. In this thesis, we introduce a newalgorithm which computes this pose difference by matching common line segments extractedfrom two laser range scans taken from two different but unknown poses. In this algorithm,matching is performed by exploiting invariant geometric relations among line segments. Theuse of line segments instead of range points also reduces the computational complexity of determiningthe pose difference between two distinct scans. Compared to other scan matchingalgorithms, our method presents a powerful means for global scan matching, map building,place recognition, loop closing and multirobot mapping, all in real-time."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz algılayıcı ağları için geliştirilmiş pek çok uygulama gerçek zamanlı iletişime gerek duymaktadır ve gerçek zamanlı uygulamalar paketlerin varmaları gereken noktaya zamanında ulaşmalarını gerektirmektedir. Ancak bu uygulamalar değişik öncelikte paketler gönderebilir ve paketlerin gecikme toleransları birbirinden farklı olabilir. Bu yüzden gönderilen paketleri önceliğine göre ayırdetmek hedefe zamanında ulaşmaları açısından büyük önem taşır. Bu bağlamda, önerdiğimiz yönlendirme protokolü ile radyonun iletim gücünü ayarlayarak acil paketleri zamanında yerlerine ulaştırmak ve mümkün olduğunda iletim gücünü azaltarak enerji tüketimini azaltmak istiyoruz ve bu şekilde gerçek zamanlı iletişimi desteklemeyi amaçlıyoruz. Önerdiğimiz protokol ayrıca paketleri önceliğine göre zamanlayarak acil paketlere öncelik verilmesini sağlıyor. Radyo iletim gücünün gecikme üzerindeki etkisini gözlemleyebilmek için algılayıcı ağları test ortamımızda çeşitli deneyler yaptık. Tahmin edildiği üzere, iletim gücünü artırmak ulaşım menzilini ve bağlantı kalitesini artırarak hedefe ulaşmak için gereken zıplayış sayısını azaltıyor. Bu nedenle radyo gücünü ayarlamak paketlerin varış zamanlarını büyük ölçüde etkiliyor ve aradaki gecikmeyi azaltabiliyor. Paket Önceliğine Göre Zamanlama ve Güç Yönetimi Destekli Gerçek Zamanlı Yönlendirme protokolümüz farklı öncelikte paketler için değişik seviyelerde iletim gücü kullanıyor. Acil olan paketleri aradaki gecikmeyi azaltmak için daha yüksek güçler kullanarak gönderiyor. Ayrıca enerji kaybını azaltmak ve algılayıcı birimlerine yükü orantılı dağıtmak için düşük öncelikteki paketleri düşük seviyede güç kullanarak gönderiyor. Simülasyon sonuçları önerdiğimiz protokolün sabit güç kullanan protokollerle karşılaştırıldığında daha çok paketi süresi bitmeden varması gereken yere ulaştırdığını ve radyoda harcanan enerjiyi azalttığını gösteriyor. Ayrıca sonuçlar, yöntemimizin algılayıcılarda diğer radyoların sinyallerinden meydana gelen karışmayı azaltıp, paket yükünü ağ içinde dengeli dağıtmaya yardımcı olduğunu ortaya koyuyor.","Many wireless sensor network applications require real-time communication, and real-time applications require packets to reach destination on time. However, applications may send packets with different priorities and hence delay bounds for packets may vary significantly. Therefore packet differentiation in the network is essential for meeting the deadline requirements. We propose a routing protocol that supports real-time communication by utilizing transmit power adjustment in order to meet the deadline of urgent packets and use energy efficiently. Our protocol also provides packet scheduling and gives precedence to urgent packets. We have conducted experiments on our sensor network testbed to observe the effects of transmit power on end-to-end delay. As expected, increasing transmit power increases the range and link quality, and reduces the number of hops to reach destination. Therefore adjusting transmit power has a great effect on delivery time and can reduce the end-to-end delay. Our protocol, Real-time Routing with Priority Scheduling and Power Adjustment, uses different levels of transmit power for packets with different priorities. It sends urgent packets with maximum power to minimize end-to-end delay and lower priority packets with reduced power to save energy and balance the load on nodes. Simulation results show that our routing protocol increases the deadline meet ratio of packets and reduces the transmit energy spent per packet when compared to routing protocols that use fixed transmit power. Additionally, results indicate that our approach lessens the interference on sensor nodes that are caused by other transmissions and helps balancing the load on the nodes."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Otomobiller için GPS cihazları ve havayolu şirketlerinin ağ siteleri gibi bir çok sistem, mekanlar arasında rota bilgileri sunmaktadır. Bu tip sistemler yaygın olarak kullanılmalarına ve rota bilgilerini doğru bir şekilde sunmalarına rağmen, kullanıcıların veri girişine izin vermemektedir. Bu tip sistemlerde tüm veri, sistem yöneticileri tarafından girilmekte ve sistemler kullanıcılarının rota tecrübelerinden faydalanamamaktadır.Bu çalışmada, kullanıcı sorguları karşılığında mekanlar arasında rotalar sunan katılımcı bir sistem sunulmuştur. Sistemdeki veriler kullanıcılar tarafından girilmektedir. Verilerin saklanması için, mekanların, mekanlar arasındaki bağlantıların ve mekanlar arasındaki ilişkilerin (kapsama, komşuluk, kesişme) tanımlandığı bir model sunulmuştur. Rotaları bulabilmek için, A* arama algoritmasının özelleştirilmiş bir uyarlaması sunulmuştur. A*CD (A* for Collaborative Data) olarak adlandırdığımız bu uyarlama, arama esnasında mekanları işlerken, hedef mekana kalan tahmini bedeli hesaplamak için buluşsal yöntemler kullanmaktadır. Ayrıca alternatif rotalar sunmak, belli bağlantı tiplerini hariç tutmak ve çok sayıda durağa sahip taşım araçları ile ilgili sorunlara çözüm getirmek için A* arama algoritması üzerinden yapılmış değişiklikler sunulmuştur. Bedel modeli olarak seyahat süresi ve seyahat maliyeti (finansal) kullanılmaktadır.Çalışmamızda, sezgisel bağlantılar kavramı da sunulmuştur. Seçilen mekanlar arasında bir rota bulunamaması durumunda bile, sistem eksik bağlantılara sahip bir rota dönebilmektedir. Eksik bağlantılar, mekanlar arasındaki ilişkiler yardımıyla doldurulabilmektedir.A*CD algoritmasının performansını değerlendirmek amacıyla otomatik testler sunulmuştur. Bu testler A*CD algoritması ile bulunan rotaların bedellerinin en düşük bedelli rotaya çok yakın olduğunu göstermektedir. Sezgisel bağlantılar kavramını örneklemek için otomatik olmayan testler sunulmuştur.","Many systems, such as in-car GPS devices and airline company web sites, provide route information between locations. Although such systems are used widely and can provide route information successfully, users of these systems cannot contribute to the data entry process. In these systems, data is entered by the administrators and these systems cannot take advantage of the route expertise of their users.In this work, we present a collaborative system, which provides routes between locations upon user queries. The data in the system is entered by the users of the system. We present a model which is containing locations, links between locations and relationships between locations (containment, neighborhood and intersection) in order to store the data. For the route finding purpose, we present a customized version of the A* search algorithm. This customized version, named A*CD (A* for Collaborative Data), uses heuristics for estimating the cost remaining to the target location while processing the nodes. A*CD can also provide alternative routes, exclude certain link types in the searches according to user preferences and handle the problems associated with multiple stop transportation lines. As the cost models, we use duration and financial cost.We also present the intuitive connections concept. Even if a route does not exist between the selected locations, the system can provide a route with missing links. The gap(s) between the disconnected locations are filled by the help of the relationships between locations.In order to evaluate the performance of the A*CD algorithm, we present automated tests. These tests show that the costs of the routes that are provided by the A*CD algorithm are close to the actual shortest routes. In order to demonstrate the intuitive connections concept, we also present manual test queries."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Okuma eylemi esnasında, her kelimenin anlamı, ondan önce gelen kelimelerin anlamları bağlamında işlenir. Geleneksel bilgi erişim sistemleri belgeleri tasnif etmek ve onlara erişmek için genellikle dizin terimleri kullanırlar. Fakat, metnin sıradan bir kelimeler kümesine dönüşmesi, belge ve sorgudaki anlamsal özellikleri de yok etmektedir. Bu durum, bilgi erişim işlemlerinde dilbilimsel teorileri uyarlamayı ve dil işleme tekniklerini uygulamayı mecbur kılmaktadır. Bir belgede dizin terimlerinin birlikte görülmesi tesadüfi değildir. Sıklıkla, bir belgede, bir kelimenin varlığı bir diğerinin varlığını çeker. Bu, tamlamalar gibi kısa mesafe (yakınlık) ya da sözcüksel bağdaşıklık olarak da adlandırılan uzun mesafe (geçişkenlik) ilişkisi şeklinde ortaya çıkabilir. Bağlam tespiti konusundayapılan çoğu çalışma ya kısa ya da uzun mesafe sözcüksel ilişkileri tahmin etmeye dayanmaktadır. Bu çalışmada, belgeler için bir çizge gösterimi ve bu gösterime dayalı yeni bir sıralama sistemi önerilmektedir. Çizgeler yardımı ile, hem kısa hem de uzun mesafe sözcüksel ilişkileri tek bir yapıda tutup, belgeler için bir bağlam puanı hesaplamak mümkün olmaktadır. Üç TREC belge kolleksiyonunda yapılan deneyler, Okapi BM25 erişim modeline kıyasla önemli başarım artışı göstermiştir. Ayrıca, belgelerde bulunan sorgu terimleri arasındaki bağdaşıklığın doğası ve eğilimi hakkında dilbilimsel sonuçlar elde edilmiştir.","During the course of reading, the meaning of each word is processed in the context of the meaning of the preceding words in text. Traditional IR systems usually adopt index terms to index and retrieve documents. Unfortunately, a lot of the semantics in a document or query is lost when the text is replaced with just a set of words (bag-of-words). This makes it mandatory to adapt linguistic theories and incorporate language processing techniques into IR tasks. The occurrences of index terms in a document are motivated. Frequently, in a document, the appearance of one word attracts the appearance of another. This can occur in forms of short-distance relationships (proximity) like common noun phrases as well as long-distance relationships (transitivity) defined as lexical cohesion in text. Much of the work done on determining context is based on estimating either long-distance or short-distance word relationships in a document. This work proposes a graph representation for documents and a new matchingfunction based on this representation. By the use of graphs, it is possible to capture both short- and long-distance relationships in a single entity to calculate an overall context score. Experiments made on three TREC document collections showed significant performance improvements over the benchmark, Okapi BM25, retrieval model. Additionally, linguistic implications about the nature and trend of cohesion between query terms were achieved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kimlik tabanlı kriptografi, geleneksel açık anahtar altayapılarındaki anahtar yönetim süreçlerinin basitleştirilmesinde kullanılır. Genele açık her türlü bilgi, örneğin e-posta adresi, ad-soyad açık anahtar olarak kullanılabilir. Bu sayede, bir grubun açık anahtarını elde etme ve bu anahtarın sertifikasının geçerli olup olmadığını kontrol etme süreçleri problem olmaktan çıkar. İkidoğrusal eşlemelerin kriptografide kullanılmaya başlamasından sonra, kimlik tabanlı kriptografi çok aktif bir araştırma alanı olmuştur. Son zamanlarda çok sayıda kimlik tabanlı elektronik imza önerileri sunulmuştur. Bu tezde, genelleştirilmiş kimlik tabanlı ElGamal imza yöntemlerini sunuyoruz ve genelleştirilmiş yöntemin literatürdeki kimlik tabanlı imza yöntemlerinin bir çoğunu kapsadığını gösteriyoruz. Bunun yanında, ekstra özellikler sunan kimlik tabanlı imzaları da inceliyoruz. İmzadan mesajı yeniden inşa etmeye olanak sağlayan imza yöntemlerinde mesajın imzadan ayrı olarak gönderilmesine gerek yoktur. Görmeden imza yöntemi bir kullanıcının imzalatmak istediği mesajı, imzalayacak kişiye vermeden o mesajı imzalatmasına olanak sağlar. İmzala-şifrele yöntemi, elektronik imza ve şifrelemenin tek bir adımda daha az işlem gücü kullanılarak yapılmasını sağlar.Bu tezde, ekstra özellikler sunan kimlik tabanlı imzalama yöntemlerini genelleştirdik ve birçok daha önce önerilmemiş imza yöntemi elde ettik. Önerdiğimiz genelleştirilmiş kimlik tabanlı imzalar, kimlik tabanlı ElGamal imzaları ve eklentileri için genel bir taslak oluşturuyor. Ek olarak, bazı görmeden imzalama yöntemlerimiz daha önce önerilmiş yöntemlerden daha hızlı çalışıyor.","ID-based cryptography helps us to simplify key management process in traditional public key infrastructures. Any public information such as the e-mail address, name, etc., can be used as a public key and this solves the problem of obtaining the public key of a party and checking that its certificate is valid. ID-based cryptography has been a very active area of research in cryptography since bilinear pairings were introduced as a cryptographic tool. There have been many proposals for ID-based signatures recently. In this thesis, we introduce the concept of generalized ID-based ElGamal signatures and show that most of the proposed ID-based signature schemes in the literature are special instances of this generalized scheme. We also investigate ID-based signatures providing additional properties. Signature schemes with message recovery provide the feature that the message is recoverable from the signature and hence does not need to be transmitted separately. Blind signatures provide the feature that a user is able to get a signature without giving the actual message to the signer. Finally, signcryption schemes fulfill the job of a digital signature and encryption in a single step with a lower computational cost.We generalize the ID-based signatures providing these properties and obtain numerous new signatures which have not been explored before. The generalized ID-based signatures we described provide a unified framework for ID-based El-Gamal signatures and extensions."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Üç boyutlu bir poligonsal modeldeki görsel açıdan önemli bölgeler dikkat çekici bölge kavramı ile tanımlanır. Bu kavramı kullanan poligonsal model basitleştirme algoritmaları sabit bir üç boyutlu model üzerindeki dikkat çekici detayları koruyarak çalışır. Bu çalışmada hareketli üç boyutlu modeller üzerinde basitleştirme için kullanılacak olan bir dikkat çekici bölge metriği ortaya koyulmaktadır. Bu metrikte durağan modeller için geliştirilmiş olan metriğe ek olarak dinamik bir modeldeki hızlanma ve yavaşlama bilgisi kullanılmaktadır. Bu, animasyondaki görsel açıdan önemli ve dikkat çekici özelliklerin korunmasını sağlar. Harmonik ve periyodik salınım türü hareketler bir canlandırma sürecinde önemli ölçüde göze çarpmaktadır. Bu tez çalışmasında hareket eden poligonsal modellerde bu tür hareketleri algılayabilen ve çarpıcı bölge tespitinde kullanabilen bir model basitleştirme yöntemi geliştirilmiştir. Geliştirilen model basitleştirme yöntemi harmonik ve periyodik salınım türü hareket yapan modeller üzerinde kullanılmış ve görsel kalite açısından başarılı sonuçlar elde edilmiştir.","Mesh saliency identifies the visually important parts of a mesh. Mesh simplification algorithms using mesh saliency as simplification criterion preserve the salient features of a static 3D model. In this thesis, we propose a saliency measure that will be used to simplify animated 3D models. This saliency measure uses the acceleration and deceleration information about a dynamic 3D mesh in addition to the saliency information for static meshes. This provides the preservation of sharp features and visually important cues during animation. Since oscillating motions are also important in determining saliency, we propose a technique to detect oscillating motions and incorporate it into the saliency based animated model simplification algorithm. The proposed technique is experimented on animated models making oscillating motions and promising visual results are obtained."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kablosuz Örgü Ağlar yönlendiriciler arası çoklu atlamalı haberleşme yaparak kablosuz kapsama alanını genişlettikleri için geleceği parlak olan bir teknolojidir. Ama, çoklu atlamalı ağlarda atlama ve düğüm sayısı artarken ağın toplam iş gücü düşmektedir. Toplam iş gücünü artırmak için, bazı örgü yönlendiricileri birden fazla radyoyla donatılarak çok sayıdaki örtüşmeyen kanalların kullanılabilir bant genişliğinden faydalanabilmektedir. Fakat, bu bant genişliğini verimli kullanabilmek için de kanal tahsisi dikkatli bir şekilde yapılmalıdır. üstelik, en iyi kanal tahsisi algoritması da NP-Zordur. Bu tezde, topolojisi ve trafik profili verilen bir çok radyolu kablosuz örgü ağının kullanılabilir bant genişliğinin verimli şekilde kullanması için birleşik bir kanal tahsisi ve yönlendirme çözümü öneriyoruz. İlk başta, akışların son yollarını öngörüyor ve verilen trafik profili ve öngörülen yolları kullanarak bağların yükünü tahmin ediyoruz. Daha sonra üç değişik buluşsal ile bağların tahsis sırasını belirliyoruz. Daha sonra, sırası gelen bağa kullanılabilir kanallar arasında en az meşgul olanını tahsis ediyoruz. Son olarak, yönlendirme algoritmamız akışları alternatifler arasında en az yoğun olan yoldan yönlendiriyor. Çok radyolu ve çok kanallı kablosuz ağları destekleyen bir Ns-2 simülatörünü kullanarak kanal tahsis ve yönlendirme algoritmamızı değerlendirdik ve sonuçlarımızı tek kanallı ve çok radyolu çok kanallı kablosuz örgü ağlarında kanal tahsisi için kullanılan çeşitli algoritmaların sonuçlarıyla karşılaştırdık. Sonuçlar gösterdi ki, birleşik kanal tahsisi ve yönlendirme algoritmamız sadece 2 radyo ve 3 kanal kullanarak tek kanallı ağdan yaklaşık 5 kat fazla iş gücü yarattı. Ayrıca algoritmamız diğer çok radyolu çok kanallı algoritmalardan daha başarılı sonuçlar elde etti.","Wireless Mesh Network is a promising technology since it extends the range of wireless coverage by multi-hop transmission between routers. However, in multihop networks the total throughput decreases with increasing number of nodes and hops. To increase the total throughput, some mesh routers are equipped with multiple radios to use the available bandwidth of multiple non-overlapping channels. However, channel assignment should be done carefully to effectively use this available bandwidth. Moreover, the optimal channel assignment algorithm is NP-hard. In this thesis, we propose a joint channel assignment and routingsolution to effectively use the available bandwidth for multi-radio wireless mesh networks with given network topology and traffic profile. Initially, we predict the final routes of the flows and estimate the loads on the links using these path predictions and given traffic profile. Then three different heuristics determine the assignment order of the links. Then the least busy channel among the available channels is assigned to the link. Finally, our routing algorithm routes the flows such that the selected path is the least busy path among the alternatives. We evaluated our channel assignment and routing algorithm using ns-2 simulator which supports multiple channels and multiple radios per node and we compared our results with single channel WMNs, and different algorithms for multi-radio multi-channel WMNs. The results show that our joint algorithm successfully achieves up to 5 times more throughput than single channel WMN with using just 2 radios and 3 channels. Our algorithms also out-performs other comparedchannel assignment algorithms for multi-radio multi-channel WMNs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gerçekçi insan vücut animasyonu yapımında özel donanım yardımıyla yakalanmış hareketlerin tekrar kullanımı ve karıştırılması animasyon ve bilgisayar grafiğinin en zor problemlerinden biridir. Yürüme, hızlı ve yavaş koşma günlük insan eylemleri arasında en sık kullanılanlardandır. Bu tezde, bu tür hareketlerin kullanıcı tarafından tanımlanmış açısal ve doğrusal hız gibi parametreler doğrultusunda üretilmesi için çoklu karıştırmaya dayalı iki aşamalı bir yöntem önerilmiştir. Birinci aşamada, geniş bir veri tabanından başlayarak benzer ve kısa hareketlerden oluşan bir çizge oluşturulmaktadır. Bu aşama, bahsi geçen veri tabanından hareket ayıtlanması, ön ayarlamalar, hata düzeltilmesi, hareketlerin senkronize edilmesi, ve hareket geçişlerine göre kısımlara ayrılmasından oluşur. İkinci aşamada, oluşturulan çizge üzerinde bir yol takip edilerek verilen parametrelere göre animasyon gerçek zamanda üretilir. Bu aşamada, karıştırmada kullanılacak hareket sayısına göre iki ayrı yaklaşım kullanılır: saçılmış veri aradeğerlemesine dayalı karıştırma ve doğrusal aradeğerlemeye dayalı karıştırma. Tanımlanan sistem genişletilebilir ve etkili bir sistemdir, ve gerçek zamanlı uygulamalarda kullanılabilir.","Reuse and blending of captured motions for creating realistic motions of human body is considered as one of the challenging problems in animation and computer graphics. Locomotion (walking, running and jogging) is one of the most common types of daily human motion. Based on blending of multiple motions, we propose a two-stage approach for generating locomotion according to user-specified parameters, such as linear and angular velocities. Starting from a large dataset of various motions, we construct a motion graph of similar short motion segments. This process includes the selection of motions according to a set of predefined criteria, the correction of errors on foot positioning, pre-adjustments, motion synchronization, and transition partitioning. In the second stage, we generate an animation according to the specified parameters by following a path on the graph during run-time, which can be performed in real-time. Two different blending techniques are used at this step depending on the number of the input motions: blending based on scattered data interpolation and blending based on linear interpolation. Our approach provides an expandable and efficient motion generation system, which can be used for real time applications."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Botnet'ler botmaster adı verilen saldırganlar tarafından uzaktan kontrol edilip yönetilebilen, ele geçirilmiş makinalardan oluşan ağlardır. Botnetler genelde dağıtık hizmet engelleme saldırıları uygulamakö reklam içerikli e-posta göndermek ya da kimlik hırsızlığı yapmak için kullanılırlar. Son yıllarda, kötü niyetli faliyetlerdeki temel amaç, haker topluluğundaki özenti çocukların saygınlık kazanma isteklerinden daha çok organize saldırılarla finansal kazanç sağlamaktır. Bu değişim, daha sofistike kötü niyetli faliyetler yapabilme özelliği olan botnetlerin sayısındaki artışın nedenini de açıklar. Son zamanlarda, araştırmacılar botnetleri yakalamak için yoğun çalışmalar yapmaktalar. Şimdiye kadar geliştirilen sistemler, bazı bot özelliklerine, çoğalma yöntemlerine ya da saldırı şekillerine odaklandıkları için ne yazık ki çok sınırlıdırlar. Biz, ağ trafiğini izleyerek, yerel ağdaki bot tarafından ele geçirilmiş makinaları tespit eden bir sistem sunuyoruz. Bizim amacımız, bot yayılma vektöründen bağımsız bir şekilde ele geçirilmis makinaları tespit eden daha genel bir yakalama yönetmi geliştirmektir. Bunun için, botların en belirgin karakteristiği olan komut alma ve komuta itaat etmek özelliğinden yararlanıyoruz. Bot tarafından üretilmiş ağ trafiğini inceleyip, komutları ve cevaplarını tespit ediyoruz. Ardından, belirli bot davranışlarını tetikleyen benzer komutlardan, komut ve kontrol protokolü hakkında bir ön bilgiye sahip olmadan bot yakalama imzaları üretiyoruz. Ürettiğimiz imzalar, bir üniversitenin trafiğini izleyen ve denetleyen bir IDS'e uygulanmıştır. Yaptığımız deneylerin sonunda, bizim sistemimizin bot tarafından ele geçirilmiş makinaları çok düşük orandaki yanlış alarmlar ile yakaladığı ortaya çıkmıştır.","A botnet is a network of compromised machines that are remotely controlled andcommanded by an attacker, who is often called the botmaster. Such botnets areoften abused as platforms to launch distributed denial of service attacks, sendspam mails or perform identity theft. In recent years, the basic motivationsfor malicious activity have shifted from script kiddie vandalism in the hackercommunity, to more organized attacks and intrusions for ¯nancial gain. This shiftexplains the reason for the rise of botnets that have capabilities to perform moresophisticated malicious activities. Recently, researchers have tried to developbotnet detection mechanisms. The botnet detection mechanisms proposed to datehave serious limitations, since they either can handle only certain types of botnetsor focus on only speci¯c botnet attributes, such as the spreading mechanism, theattack mechanism, etc., in order to constitute their detection models.We present a system that monitors network tra±c to identify bot-infectedhosts. Our goal is to develop a more general detection model that identi¯essingle infected machines without relying on the bot propagation vector. To thisend, we leverage the insight that all of the bots get a command and perform anaction as a response, since the command and response behavior is the uniquecharacteristic that distinguishes the bots from other malware. Thus, we examinethe network tra±c generated by bots to locate command and response behaviors.Afterwards, we generate signatures from the similar commands that are followedby similar bot responses without any explicit knowledge about the commandand control protocol. The signatures are deployed to an IDS that monitors thenetwork tra±c of a university. Finally, the experiments showed that our systemis capable of detecting bot-infected machines with a low false positive rate."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Su ve diğer akışkanların benzetimi, bilgisayar grafiğinde hep popüler bir konu olmuştur ve bu konuda akışkan simülasyonun değişik yönlerini içeren pek çok çözüm sunulmuştur. Fakat bu çözümlerin çoğu, akışkanlar dinamiğinin karmaşık doğasından dolayı, gerçek zamanlı konulara, özellikle de bilgisayar oyunları gibi kullanıcı etkileşimi olan uygulamalara uygulanabilir nitelikte değildir. Gerçekçi bir davranışın yanında gerçek zamanlı koşum kısıtlarına da uymayı hedefleyen çözümler genellikle Navier-Stokes denklemlerini Yumuşatılmış Parçacık Hidrodinamiği (Smoothed Particle Hydrodynamics - SPH) tekniğini kullanarak çözme eğilimindedirler. Biz bu çalışmada, su dinamiğini modellemek için birden çok SPH katmanı kullanarak yeni bir yaklaşım ortaya koyuyoruz. Bu yaklaşım, toplam hesaplama zamanını azaltırken, su yüzeyindeki detay seviyesini de arttırmaktadır. Bunun için, su yüzeyini daha detaylı göstermek üzere küçük parçacıklar kullanan SPH katmanları kullanılırken, suyun geriye kalan büyük kısmına genel akışkan davranışını koruyacak şekilde daha az sayıda ancak daha büyük parçacıklar kullanan katmanlar uygulanmıştır. Bu şekilde, su miktarı arttıkça katlanan performans kazanımları sağlanırken, görsel açıdan benzer veya daha gerçekçi sonuçların alınması başarılmıştır.","Simulation of water and other fluid phenomena have always been a popular topic in the computer graphics research area and many solutions provided in this topic covers many fluid simulation aspects. However, with the complex nature of physics of fluid dynamics, usually these solutions are not applicable to the real-time domain, especially interactive applications like computer games. The solutions that both target a realistic behavior and real-time CPU boundaries tend to solve the problem by utilizing Smoothed Particle Hydrodynamics (SPH) technique in the solution of Navier-Stokes equations. In this study, we introduce a novel approach for modeling of the water dynamics with multiple layers of SPH. This approach increases the level of detail in the constructed water surfaces while decreasing the required overall computation time. To achieve this, an extra SPH layer is introduced to use larger particles to fill most of the fluid volume which helps to simulate general fluid behavior in less numbers while utilizing other extra SPH layers with small particles to fill up in-betweens for finer detail in water surfaces. The performance gain can be up to several magnitudes with the increase of the water size while maintaining visually similar or more appealing results."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Gelişen teknoloji ile birlikte geniş ve şok-modelli veri kümeleri yaygın hales s c ugelmiştir. Bu veri kümelerinin etkin ve hızlı bir şekilde erişimi, düzenlenmesis u s s uve analizi büyük bir ilgi alanı oluşturmaktadır. Hem internet uzerindeki haberuu s üresimleri ve hem de televizyondaki haber gürüntüleri kendi işinde bir şok bilgiyiou u c cbarındıran ünemli veri kaynaklarıdır. Kişiler genellikle haberin ana konusu olup,o sbu kişileri sorgulama ünemli ve şoğu zaman istenen bir işlemdir.s o cg sBu şalışmada, haber fotoğraï¬arı ve gürüntülerinden oluşan geniş verics g ou u s skümelerinde kişilerin sorgulanmasını sağlayan şizgeye dayalı bir yüntemu s g c osunulmuştur. Yüntem isim ve yüzlerin ilişkilendirilmesine dayanmaktadır. Habers o u sbaşlığında kişinin ismi geşiyor ise fotoğrafta da o kişinin yüzünün bulunacağısg s c g s uuu gvarsayımıyla, ilk olarak sorgulanan isim ile ilişkilendirilmiş, fotoğraï¬ardaki tüms s g uyüzler seşilir. Bu yüzler arasında sorgu kişisine ait farklı koşul, poz ve zamanlardau c u s sşekilmiş, pek şok resmin yanında, haberde ismi geşen başka kişilere ait yüzler yac s c c s s uda kullanılan yüz bulma yünteminin hatasından kaynaklanan yüz olmayan resim-u o uler de bulunabilir. Yine de, şoğu zaman, sorgu kişisine ait resimler daha şok olup,cg s cbu resimler birbirine diğerlerine olduğundan daha şok benzeyeceklerdir. Bu ne-g g cdenle, yüzler arasındaki benzerlikler şizgesel olarak betimlendiğinde, birbirine enu c gşok benzeyen yüzler bu şizgede en yoğun bileşen olacaktır. Bu şalışmada, sorguc u c g s csismiyle ilişkilendirilmiş, yüzler arasında birbirine en şok benzeyen alt kümeyi bu-s su c ulan, şizgeye dayalı bir yüntem sunulmaktadır. şizgeye bağlı yaklaşımla bulunanc o c g sbu sonuş, daha sonra yeni karşılaşılan yüzlerin tanınmasında da model olarakc ss ukullanılabilmektedir. Aynı zamanda, şalışmada sunulan şizgeye dayalı yaklaşımcs c shaber gürüntülerindeki spikerlerin otomatik olarak bulunması ve elenmesinde deou ukullanılmıştır.siiiDeneyler iki ayrı veri kümesi kullanılarak gerşekleştirilmiştir: haberu c s sË veri kümesi Yahoo! Haber kanalı uzerindenfotoğraï¬arı ve haber gürüntüleri. Ilkg ou u u ütoplanmiş binlerce resimden oluşmaktadır. Ikinci küme ise NIST tarafındans s uTRECVID 2004 yarışması işin sağlanan 229 haber gürüntüsünden oluşmaktadir.s c g ou uu sResimler gercek hayattan alınmış olduğundan yüzler poz, ışıklandırma ve ifades g u solarak şok fazla şeşitlilik güstermektedir. Deneylerde elde edilen sonuşlar, sadecec cs o cisim bazlı sonuşlara güre daha iyi olup, büyük şapta yüz tanıma işin ï¬kir ver-c o uu c u cmektedir.Keywords: Face recognition, face retrieval, SIFT features.","Along with the recent advances in technology, large quantities of multi-modaldata has arisen and became prevalent. Hence, eï¬ective and eï¬cient retrieval,organization and analysis of such data constitutes a big challenge. Both newsphotographs on the web and news videos on television form this kind of data bycovering rich sources of information. People are mostly the main subject of thenews; therefore, queries related to a speciï¬c person are often desired.In this study, we propose a graph based method to improve the performanceof person queries in large news video and photograph collections. We exploitthe multi-modal structure of the data by associating text and face information.On the assumption that a person?s face is likely to appear when his/her name ismentioned in the news, only the faces associated with the query name are selectedï¬rst to limit the search space for a query name. Then, we construct a similaritygraph of the faces in this limited search space, where nodes correspond to thefaces and edges correspond to the similarity between the faces. Among thesefaces, there could be many faces corresponding to the queried person in diï¬erentconditions, poses and times. There could also be other faces corresponding toother people in the news or some non-face images due to the errors in the facedetection method used. However, in most cases, the number of correspondingfaces of the queried person will be large, and these faces will be more similar toeach other than to others. To this end, the problem is transformed into a graphproblem, in which we seek to ï¬nd the densest component of the graph. Thismost similar subset (densest component) is likely to correspond to the faces ofthe query name. Finally, the result of the graph algorithm is used as a model forfurther recognition when new faces are encountered. In the paper, it has beeniiishown that the graph approach can also be used for detecting the faces of theanchorpersons without any supervision.The experiments are performed on two diï¬erent data sets: news photographsand news videos. The ï¬rst set consists of thousands of news photographs fromYahoo! news web site. The second set includes 229 broadcast news videos pro-vided by NIST for TRECVID 2004. Images from the both sets are taken in reallife conditions and, therefore, have a large variety of poses, illuminations andexpressions. The results show that proposed method outperforms the text onlybased methods and provides cues for recognition of faces on the large scale.Keywords: Face recognition, face retrieval, SIFT features."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Insan saşının modellenmesi ve ışıklandırılması bilgisayar graï¬ği alanında daimac s gilgi ceken konulardan biri olmuştur.ş s Aşık geometrik modeller ve kapalıchacim yoğunluğu modelleri, insan saşının modellenmesi ve ışıklandırılması işing g c s cgeliştirilen tekniklerdendir. Bu tekniklerle birlikte, karma tutam modelleri desson zamanlarda bu konuda başarılı olmuştur. Bu calışmada, 3 Boyutlu Karmaşıks s şs sDokular ismini verdiğimiz yeni bir uc boyutlu doku modeli ve bunun uretilmesig üş üanlatılmaktadır. Daha sonra, bu doku modeli kullanılarak kıvırcık ve dalgalısaş modeli gibi karmaşık saş şekilleri karma tutam modeliyle birleştirilerek kul-c s cs slanılmaktadır. Geliştirdiğimiz model, kıvırcık ve dalgalı saş modelleme işins g c cgereken kullanıcı eforunu azaltmaktadır. Bu calışma ile, hacim yoğunluğu mod-şs g gelinin ve karma tutam modelinin yetersizliklerini gidermeyi hedeï¬emekteyiz.Dokuların ürtüştürülmesi amacıyla uc boyutlu bir silindirik doku ürtüştürmeo us u u üş o us ufonksiyonu sunulmaktadır. Işıklandırma sisteminin tasarımında, yeni nesil ekranskartlarının sağladığı imkanlardan yararlanılarak yüksek performanslı ışıklandırmag g u ssağlanmıştır.g sAnahtar süzcükler : Saş modelleme, saş ışıklandırma, kıvırcık saş, dalgalı saş,c cs c coukarma tutam saş modeli, hacim yoğunluğu saş modeli, uc boyutlu doku, ucc g g c üş üşboyutlu doku ürtüştürme.o us uiv","Human hair modeling and rendering have always been a challenging topic incomputer graphics. The techniques for human hair modeling consist of explicitgeometric models as well as volume density models. Recently, hybrid clustermodels have also been successful in this subject. In this study, we present anovel three dimensional texture model called 3D Fuzzy Textures and algorithmsto generate them. Then, we use the developed model along with a cluster modelto give human hair complex hairstyles such as curly and wavy styles. Our modelrequires little user eï¬ort to model curly and wavy hair styles. With this study,we aim at eliminating the drawbacks of the volume density model and the clusterhair model with 3D fuzzy textures. A three dimensional cylindrical texture map-ping function is introduced for mapping purposes. Current generation graphicshardware is utilized in the design of rendering system enabling high performancerendering.Keywords: Hair modeling, hair rendering, curly hair, wavy hair, cluster hairmodel, volume density model, 3D texture, 3D texture mapping.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez, literatürde bulunan Cox hizmet dağılımlı ve değişik büyüklükteu g gs uu ubekleme yerleri olan kapalı kuyruk ağları işin ayrıştırmaya dayalı iki yaklaşıkg c s ssabit nokta üteleme yüntemini, faz-tipli servis dağılımlarını kapsayacak şekildeo o g sgenişletmektedir. Ayrıştırmadan ortaya şıkan altağların her birine karşı gelens s c g sindirgenemeyen Markov zincirinin, Kronecker şarpımlar kullanılarak hiyerarşikc süolarak nasıl ifade edilebileceğini güstermektedir. Onerilen yüntemler her birg o oaltağın uzun vadeli olasılık vektürünü her sabit nokta ütelemesinde şok seviyelig ou u o cbir yüntemle hesap edebilen bir yazılım paketinde kodlanmıştır. Yüntemler,o s ocsşeşitli ürnekler uzerinde, biri ayrıştırılmamış kapalı kuyruk ağı işin şok seviyelio ü s s gccyüntem olmak uzere, yazılım paketi kullanılarak başkalarıyla doğruluk ve etkin-o ü s glik bakımından karşılaştırılmış ve yakınsama üzellikleri tartışılmıştır. Sayısalss s o s ssonuşlar, iki yaklaşık sabit üteleme yünteminin dikkate alınan problemler arasındac s o odoldurduğu bir boşluk olduğunu güstermiştir.g s g o sAnahtar süzcükler : Kapalı kuyruk ağları Â· Faz-tipli hizmet dağılımları Â· Kroneckerou g ggüsterimleri Â· Ağ ayrıştırması Â· Sabit nokta ütelemesi Â· Cok seviyeli yüntemler.o g s o ş oiv","This thesis extends two approximative ï¬xed-point iterative methods basedon decomposition for closed queueing networks (QNs) with Coxian service dis-tributions and arbitrary buï¬er sizes from the literature to include phase-typeservice distributions. It shows how the irreducible Markov chain associated witheach subnetwork in the decomposition can be represented hierarchically usingKronecker products. The proposed methods are implemented in a software tool,which is capable of computing the steady-state probability vector of each subnet-work by a multilevel method at each ï¬xed-point iteration. The two methods arecompared with others, one being the multilevel method for the closed QN itself,for accuracy and eï¬ciency on a number of examples using the tool, and theirconvergence properties are discussed. Numerical results indicate that there is aniche among the problems considered which is ï¬lled by the two approximativeï¬xed-point iterative methods.Keywords: Closed queueing networks Â· Phase-type service distributions Â· Kro-necker representations Â· Network decomposition Â· Fixed-point iteration Â· Multi-level methods.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË Ë ËşË Ë ËMETRIK UZAYLARDA INDEKSLEME ICIN DINAMIKË ËË üVE ADAPTIF YENI BIR YONTEMUmut TOSUNBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Dr. Cengiz Celiko şüuTez Yüneticisi: Prof. Dr. Ozgür UlusoyoAğustos, 2007gBilgisayar Bilimi uygulamaları, genellikle verinin etkin bir bişimde depolan-cması ve getirilmesi ile ilgilenirler. Geleneksel veritabanlarının iyi tanımlanmışsË skisel Veritabanı paradigmasını kullanarakyapısı, gereken sorgu nesnelerine Ilişetkin bir şekilde erişmeyi sağlar. Fakat günümüzde gürüntü, video, ses klibi ves s g uu u ou umetin dükümanı gibi yapısal olmayan ve karmaşık veri ile uğraşmanın zorluk-ou s gslarıyla karşılaşılmaktadır. Multimedya Veri Edinme, Veri Madenciliği, Gürüntüss g ou uü grenmesi, Bilgisayar Gürüsü, Biyomedikal VeritabanlarıTanıma, Makina Oğ ouukarmaşık verinin etkin bir bişimde yünetilmesini gerektiren alanlardır. Karmaşıks c o sve yapısal olmayan veri şoğu zaman iyi tanımlanmış parşalara bülünememekte vecg s c outam bir eşleme sorguları tanımlamak işin uygulanamamaktadır. Bunun yerine,s ckullanılacak bir sorgu nesnesi yada prototip nesne sağlayarak benzer nesnelerigveritabanının getirmesini sağlayan benzerlik araştırması kullanılmaktadır.g sBenzerlik Araştırması işin bir popüler yaklaşım da veritabanı nesneleris c u sarasındaki ilişkiyi vektür uzayında ifade ederek bu ilişkiye yaklaşmaya şalışmaktır.s o s s csLiteratürde vektür uzaylarındaki benzerlik sorgusunu destekleyen iyi bilinen in-u odeksleme yüntemleri bulunmaktadır. Fakat bu yüntemlerin yüksek boyutlu verio o uişin etkili olmadığı güsterilmiştir. Diğer bir yaklaşım ise indeksleme işin Metrikc go s g s cUzaylar modelini kullanmaktır. Metrik Uzaylar uşgensel eşitsizlik üzelliği taşıyanüc s o gsbir uzaklık fonksyonu ile tanımlanırlar. Verinin iş yapısı ile ilgili varsayımlar ol-cmadığı işin yüksek seviyeli bir soyutlama sağlarlar ve daha fazla uygulanabilirliğegc u g gsahiptirler. Yüksek boyutlarda daha iyi performans sağladıkları da güsterilmiştir.u g o sDaha ünceki bir şok şalışma indeks yapısı oluşturulduktan sonra yeni nesneo c cs sviviieklenmesine izin vermeyen statik metotlara konsantre olmuştur. M-Ağaş, Slims gcAğaş, DF-Ağaş, Omni bazı popüler dinamik yapılardır. Bu metotlar taşangc gc u sdügumleri ayırarak ve ağaca B-Ağaş şeşitleri gibi yeni seviyeler ekleyerek ar-uğü g gccstarak büyüyebilirler. Maalesef bu yüntemler AESA, LAESA, Spaghettis ve Kvpuu ogibi sabit global pivot seti taşıyan düz yapılara güre şok daha kütü performanss u oc ougüstermektedirler. Sorgu nesnesi ve pivotlar arasındaki uzaklıklar hesaplanarak,overitabanının bir kısmı ünemli olmaktan şıkarılır. Pivot sayısı daha fazla seşiciliko c csağlamak işin kolaylıkla arttırılabilir ve daha iyi performans elde edilir. Fakat bellig cbir sorgu yarışapı işin optimum sayıda pivot bulunmaktadır ve şok fazla pivotc c ckullanımı sorgu ve indeks oluşturma maliyetlerini arttırır. Yakın zamanda yenisveritabanı nesneleri eklenebilen ve dinamik bir şekilde yeni nesnelerin bazılarınıspivot olarak seşerek ilerleyen LAESA varyasyonu Sparse Spatial Selection(SSS)ctakdim edilmiştir.sBu tezde SSS yünteminin kümelenmiş ve bir uca toplanmış dağılımlar işino u s sg cyol aştığı temel problemlere değinilecektir. Gerşek veri gruplarında bu türcg g c uüzellikler sıklıkla güzlemlenmiştir. SSS'in simetrik ve dengeli dağılımlar ayrıcao o s güzel sorgu yarışapları işin optimize edildiği güsterilecektir. Bu tezin ilk ana katkısıo c c gokümelenmiş yada bir uca toplanmış veride uygulanabilecek yeni bir pivot seşimu s s cËyüntemi sunmaktır. Ikinci katkı ise değişik sorgu yarışapları işin doğru pivoto gs c c gseşim sayısını bulmak olacaktır. Ayrıca sunulacak yeni indeksleme yüntemininc onesne ekleme maliyetine yük getirmezken ağaş tabanlı uygulamalara güre şoku gc ocdaha iyi performans sağladığı güsterilmektedir. Bunun yanı sıra bu yeni yapıg goveritabanındaki populasyon artışlarına da ustün şekilde adapte olabilmektedir.s üusAnahtar süzcükler : Metrik Uzay, Metrik Erişim Metotları, Kvp, Hkvp, EcKvp,ou sM-Ağaş, Slim-Ağaş, DF-Ağaş, Pivot, Uzaklık Hesaplaması.gc gc gc","ABSTRACTA NEW DYNAMIC AND ADAPTIVE SCHEME FORINDEXING IN METRIC SPACESUmut TOSUNM.S. in Computer EngineeringSupervisor: Dr. Cengiz Celikşü ur UlusoyCo-Supervisor: Prof. Dr. OzgüAugust, 2007Computer Science applications are often concerned with eï¬cient storage andretrieval of data. Well deï¬ned structure of traditional databases help to accessrequired query objects eï¬ectively using the Relational Database paradigm. How-ever, in recent times, we are faced with the challenges of dealing with unstruc-tured and complex data such as images, video, sound clips and text documents.Multimedia Information Retrieval, Data Mining, Pattern Recognition, MachineLearning, Computer Vision and Biomedical Databases are examples of the ï¬eldsthat require eï¬cient management of complex data. Complex, unstructured typeof data often cannot be broken down into well-deï¬ned components, and exactmatching cannot be applied for deï¬ning queries. Instead, the notion of similaritysearch is used where a query or prototype object is provided by the user and thedatabase retrieves the objects that are similar.One popular approach for similarity searching is to approximate the relation-ship between database objects by mapping them into a vector space. There arewell-known indexing methods in literature that support similarity queries in vec-tor spaces, however, it has been shown that these methods are ineï¬ective for highdimensional data. Another approach is to use Metric Spaces model for indexing.Metric spaces are deï¬ned by a distance function that has the triangular inequalityproperty. Since there are no assumptions about the structure of the data itself,they constitute a higher level abstraction and thus have more applicability. Theyhave also been shown to perform better in higher dimensions.A lot of the previous work in metric spaces have concentrated on static meth-ods that do not allow new insertions once the index structure has been initialized.ivvM-Tree, Slim-Tree, DF-Tree, Omni are some of the popular dynamic structures.These methods can grow incrementally by splitting overï¬owed nodes and addingnew levels to the tree very much like the B-tree variants. Unfortunately, they havebeen shown to perform very poorly compared to ï¬at structures such as AESA,LAESA, Spaghettis and Kvp that use a ï¬xed set of global pivots. The distancesbetween the query object and the pivots are computed to eliminate some portionof the database from consideration. The number of pivots can be easily increasedto provide more selectivity, thus better query performance. However, there is anoptimum number of pivots for a given query radius, and using too many pivotsincreases the costs of queries and the initialization of the index. Recently, SparseSpatial Selection(SSS) was introduced as a LAESA variant that allows insertionsof new database objects and dynamically promotes some of the new objects aspivots.In this thesis, we argue that SSS has fundamental problems that results inpoor query performance for clustered or otherwise skewed distributions. Realdatasets have often been observed to show such characteristics. We show thatSSS has been optimized to work for a symmetrical, balanced distribution andfor a speciï¬c radius value. Our ï¬rst main contribution is oï¬ering a new pivotpromotion scheme that can perform robustly for clustered or skewed distributions.Our second contribution is proposing new methods that solve the problem ofdetermining the right number of pivots for diï¬erent query radius values. Weshow that our new indexing scheme performs signiï¬cantly better than tree-baseddynamic structures while having lower insertion costs. We also show that ourstructure adapts to changes in the database population in a superior way.Keywords: Metric Space, Metric Access Methods, Kvp, Hkvp, EcKvp, M-Tree,Slim-Tree, DF-Tree, Pivot, Distance Computation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË ü Ë ËşË ËËKELIME ESLEME YONTEMI ICIN YENI BIRşËNITELEMEEsra AtaerBilgisayar Mü hendisliği, Yü ksek Lisansu g uTez Yüneticisi: Assist. Prof. Pınar Duygulu Sahino şTemmuz, 2007Tarihi arşivler dü nyanın pek cok yerinden araştırmacının ilgi alanına girmekte-s u sdir. Fakat, belgelerin elle cevirisi ve dizinlemesi zor bir iş olduğu işin bu arşivlerş s gc skullanılamaz durumdadır. Ayrıca elektronik imgeleme araşları ve imge işlemec steknikleri kü tü phane ve arşivlerin dijital ortama aktarılmasıyla gü n geştikşeuu s u cconem kazanmaktadır. Bu tezde erişim ve dizinlemede kullanılmak uzere ke-ü s ülime imgelerini nitelemek işin dilden bağımsız bir cozü m getirilmektedir. Karak-c g şü uter tanıma teknikleri aşırı onişleme ve ogrenme yünü nden eksiklikler işerirken,s üs üğ ou conerilen yüntem belgeleri kelimelere bülü tleyerek ayırt edici bülgeleri kullanarakü o ou obu kelimeleri nitelemektedir. Nesne ve manzara tasniï¬nde başarı güsteren gürsel-s o oogeler-kü mesi yüntemi kelime eşlemeye uyarlandı. Kıvrım, bağlantı bülgeleriüğ u o s g ove noktalar kelimeyi ayırt etmek işin oenmli gürsel oznitelikler olduğu işin bucü o ü gcbülgeleri tanımlamada başarılı olan ve imge eşlemede sıkşa kullanılan taş nokta-o s s c clar kullanıldı. Bu bülgelerin tespit edilmesinde Gauss Farkı ve Harris-Aï¬ne sezi-ocilerinden yararlanıldı ve tespit edilen bülgeler Scale Invariant Feature Transformo(SIFT) oznitelikleriyle tanımlandı. Her kelime SIFT tanımlayıcılarının vektürü onicemlenmesiyle oluşturulan gürsel ogelerin değişik dağılımlarına güre nitelendis o üğ gs g ove bu niteleme belge erişim ve dizinlemesi işin kullanıldı.s cDeneyler farklı yazı tipi işeren ve ceşitli yazarlarca yazılmış Arapşa, Lat-c şs s cince ve Osmanlıca belgelerde gerşekleştirildi. Veri kü melerinin farklı yazı tipleric s uişermesine ve ceşitli yazarlarca oluşturulmuş olmasına rağmen, sonuşlar onerilenc şs s s g cüüsistemin belge erişimi ve dizinlemede başarılı olduğunu güstermektedir. Onerilens s g oyüntem dilden bağımsız olduğu işin kolayca başka dillere de uyarlanabilir. Sistemo g gc sbelge erişiminde bu alandaki en iyi yüntemlere yakın bir başarım sergilemektedir.s o sBunun yanında onerilen yüntemin anlamsal benzerlikleri bulmada başrılı olmasıü o sbelge dizinleme işin etkili bişimde kulanılabileceğini güstermektedir.c c g oiiiAnahtar süzcükler : kelime eşleme, belge erişimi, gürsel ogeler kü mesi.s s o üğ uou","ABSTRACTA NEW REPRESENTATION FOR MATCHING WORDSEsra AtaerM.S. in Computer EngineeringSupervisor: Assist. Prof. Pınar Duygulu SahinşJuly, 2007Large archives of historical documents are challenging to many researchers allover the world. However, these archives remain inaccessible since manual index-ing and transcription of such a huge volume is diï¬cult. In addition, electronicimaging tools and image processing techniques gain importance with the rapidincrease in digitalization of materials in libraries and archives. In this thesis,a language independent method is proposed for representation of word images,which leads to retrieval and indexing of documents. While character recogni-tion methods suï¬er from preprocessing and overtraining, we make use of anothermethod, which is based on extracting words from documents and representingeach word image with the features of invariant regions. The bag-of-words ap-proach, which is shown to be successful to classify objects and scenes, is adaptedfor matching words. Since the curvature or connection points, or the dots areimportant visual features to distinct two words from each other, we make use ofthe salient points which are shown to be successful in representing such distinc-tive areas and heavily used for matching. Diï¬erence of Gaussian (DoG) detector,which is able to ï¬nd scale invariant regions, and Harris Aï¬ne detector, whichdetects aï¬ne invariant regions, are used for detection of such areas and detectedkeypoints are described with Scale Invariant Feature Transform (SIFT) features.Then, each word image is represented by a set of visual terms which are obtainedby vector quantization of SIFT descriptors and similar words are matched basedon the similarity of these representations by using diï¬erent distance measures.These representations are used both for document retrieval and word spotting.The experiments are carried out on Arabic, Latin and Ottoman datasets,which included diï¬erent writing styles and diï¬erent writers. The results show thatthe proposed method is successful on retrieval and indexing of documents even ifwith diï¬erent scripts and diï¬erent writers and since it is language independent,iiiit can be easily adapted to other languages as well. Retrieval performance of thesystem is comparable to the state of the art methods in this ï¬eld. In addition,the system is succesfull on capturing semantic similarities, which is useful forindexing, and it does not include any supervising step.Keywords: word matching, document retrieval, bag-of-features."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETğ ËşËKABLOSUZ ALGILAYICI AGLAR ICIN KONUM BAZLIü Ë ËCOKLU GONDERIM YOL BULMA ALGORITMALARIşğHakkı BAGCIBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gAğustos, 2007gKablosuz algılayıcı ağlarda bir mesajı birden fazla hedef algılayıcı dügumeg uğügündermek işin şoklu günderim yol bulma protokollerine ihtiyaş vardır. Algılayıcıo cc o cağlarında coğu zaman algılayıcı dügumlere birbirinden farklı belirleyiciler tahsisg şg uğüetmek etkili bir yol olmadığından, konum bilgisini ayırt edici ozellik olarak kul-g ülanmak ve mesajları hedef konumlara gündermek daha iyi bir yaklaşım olarako sgürünüyor. Bu tezde kablosuz algılayıcı ağlar işin algılayıcı dügumlerin konum bil-ou u g c uğügisini kullanarak calışan, iki yeni dağıtık coklu günderim algoritması oneriyoruz.şs g ş o üË algoritma coklu günderim ağacındaki toplam dal sayısını ve buna bağlı olarakIlk ş o g gtoplam günderme sayısını azaltmak işin hedef dügumleri aşısal konumlarına güreo c uğü c oËgruplayarak, her gruba bir mesaj günderir. Ikinci algoritma ise kaynak dügumdeo uğüühedef düğumlerin konum bilgisini kullanarak Oklit minimum kaplama ağacıug ü ghesaplar. Coklu günderim mesajları, oluşturulan bu ağaca güre hedef dügumlereş o s g o uğügünderilir. Bu yaklaşım toplam günderim sayısını azaltmayı amaşladığındano s o c gmesajları hedef dügumlere gündermek işin kullanılan toplam enerji miktarınınuğü o cdüşurülmesini sağlar. Bu iki algoritmayı birbiriyle ve başka bir konum bazlı cokluus ü u g s şgünderim protokolü olan PBM ile iletim başarısı, toplam günderim sayısı, uştano u s o cuca gecikme süresi ve günderilen toplam data miktarı aşısından karşılaştırdık.u o c ssSonuşlar güsterdi ki, onerdiğimiz algoritmalar daha olşeklenebilir ve enerji kul-c o ü g üclanımı bakımından daha etkindir. Bu sebeple kablosuz algılayıcı ağlarda coklug şgünderimde kullanılmak işin iyi birer adaydırlar.o cAnahtar süzcükler : Kablosuz Algılayıcı Ağlar, Konum Bazlı Coklu Günderim,ou g ş oCoğraï¬ Yol Bulma.giv","ABSTRACTLOCATION BASED MULTICAST ROUTINGALGORITHMS FOR WIRELESS SENSOR NETWORKSğHakkı BAGCIM.S. in Computer EngineeringËSupervisor: Asst. Prof. Dr. Ibrahim Kürpeoğluo gAugust, 2007Multicast routing protocols in wireless sensor networks are required for sendingthe same message to multiple diï¬erent destination nodes. Since most of thetime it is not convenient to identify the sensors in a network by a unique id,using the location information to identify the nodes and sending messages to thetarget locations seems to be a better approach. In this thesis we propose twodiï¬erent distributed algorithms for multicast routing in wireless sensor networkswhich make use of location information of sensor nodes. Our ï¬rst algorithmgroups the destination nodes according to their angular positions and sends amessage toward each group in order to reduce the number of total branchesin multicast tree which also reduces the number of messages transmitted. Oursecond algorithm calculates an Euclidean minimum spanning tree at the sourcenode by using the positions of the target nodes. According to the calculatedMST, multicast message is forwarded to destination nodes. This helps reducingthe total energy consumed for delivering the message to all target nodes since ittries to minimize the number of transmissions. We compare these two algorithmswith each other and also against another location based multicast routing protocolcalled PBM according to success ratio in delivery, number of total transmissions,traï¬c overhead and average end to end delay metrics. The results show thatalgorithms we propose are more scalable and energy eï¬cient, so they are goodcandidates to be used for multicasting in wireless sensor networks.Keywords: Wireless Sensor Networks, Location Based Multicasting, GeographicRouting.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ücUş boyutlu modellerin imge sıkıştırma yüntemleri kullanılarak sıkıştırılması işin birs o s cüyüntem ünerilmektedir.o o Onerilen yüntem, literatürdeki birşok algoritmanın ter-o u csine, modelleri 3-Boyutlu veriler yerine 2-Boyutlu veriler olarak ele almaktadır. 3-Boyutlu modeller ilk olarak düzenli ızgara yapıları uzerinde 2-Boyutlu imgelereu üüdünüştürülmektedir.o us u u Onerilen yüntem diËer yüntemlerde kullanılan parametriza-o g osyon tekniËine güre, hesaplama aşısından daha basittir.g o c Elde edilen imge ben-zeri temsilin sıradan imgelerden tek farkı, pikseller arası ilintinin yanyanalık iledeËil, 3-boyutlu modelin baËlanılırlık verisi kullanılarak saËlanmasıdır.g g g Bu ne-denle yaygın kullanılan dalgacık dünüşumü teknikleri bu temsil uzerinde şok iyio us ü u ü csonuşlar vermemektedir.c Burada ünerilen BaËlanılırlık Bazlı Uyarlamalı Dal-o ggacık Dünüşumü sayesinde dalgacık dünüşumü sıradüzensel yapısının detay katman-o us ü u o usü u ularında bulunan piksel deËerleri, alşak frekans katmanlarında bulunan komşularındang c süngürülebilmektedir.o ou Büylece oluşturulan dalgacık dünüşumü verileri Sıradüzenselo s o us ü u uAËaş Yapılarının Kümelere Bülüntülenmesi (Set Partitioning In Hierarchical Treesgc u ou u- SPIHT) ya da JPEG200 tekniklerinden biri kullanılarak kodlanmaktadı. SPIHTtekniËi sayesinde elde edilen veri dizgisi aşamalı güsterime uygundur; şunkü, dizgining s o cü ufarklı uzunluktaki bülümlerinden farklı şüzünürlüklerde modeller geri şatılabilmektedir.ou co u u u cvJPEG2000 yünteminin burada ünerilen şekli tek şüzünürlüklü gerişatıma olanako o s co u u u u cüsaËlamaktadır. Onerilen yüntemde dalgacık dünüşumü katsayılarının nicemlenme şeklig o o us ü u sgeri şatılan modelin şüzünürlüËunü belirlemektedir. Farklı dalgacık dünüşumü ta-c co u u ugü u o usü uban vektürleri kullanılarak yapılan deneyler sonucunda lazy dalgacık dünüşumününo o us ü u uen iyi sonuşları verdiËi güzlemlenmiştir.c go s BaËlanırlık bazlı uyarlamalı dalgacıkgdünüşumü kullanılarak yapılan deneylerin sonuşlarında bir ünceki yünteme güreo us ü u c o o ogelişme güzlemlenmiştir. Dalgacık dünüşumü verilerinin SPIHT ile kodlanmasıyla eldes o s o us ü uedilen sonuş, JPEG2000 ile yapılan kodlamanın sonucundan ve 3B modellerin MPEG-c3DGC ile kodlamasından daha başarılı olmuştur.s sAnahtar kelimeler: 3 Boyutlu modellerin Sıkıştırılması, 3B modellerin imge ben-szeri temsili, BaËlanılırlık Bazlı Uyarlamalı Dalgacık Dünüşumüg o us ü uvi","A Connectivity-Guided Adaptive Wavelet Transform (CGAWT) based mesh compres-sion algorithm is proposed. On the contrary to previous work, the proposed methoduses 2D image processing tools for compressing the mesh models. The 3D models areï¬rst transformed to 2D images on a regular grid structure by performing orthogonalprojections onto the image plane. This operation is computationally simpler than pa-rameterization. The neighborhood concept in projection images is diï¬erent from 2Dimages because two connected vertex can be projected to isolated pixels. Connectiv-ity data of the 3D model deï¬nes the interpixel correlations in the projection image.Thus the wavelet transforms used in image processing do not give good results onthis representation. CGAWT is deï¬ned to take advantage of interpixel correlations inthe image-like representation. Using the proposed transform the pixels in the detailsubbands are predicted from their connected neighbors in the low-pass subbands ofthe wavelet transform. The resulting wavelet data is encoded using either ?Set Parti-tioning In Hierarchical Trees? (SPIHT) or JPEG2000. SPIHT approach is progressivebecause diï¬erent resolutions of the mesh can be reconstructed from diï¬erent partitionsof SPIHT bitstream. On the other hand, JPEG2000 approach is a single rate coder.The quantization of the wavelet coeï¬cients determines the quality of the reconstructediiimodel in JPEG2000 approach. Simulations using diï¬erent basis functions show thatlazy wavelet basis gives better results. The results are improved using the CGAWTwith lazy wavelet ï¬lterbanks. SPIHT based algorithm is observed to be superior toJPEG2000 based mesh coder and MPEG-3DGC in rate-distortion.Keywords: 3D Model Compression, Image-like mesh representation, Connectivity-Guided Adaptive Wavelet Transformiv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Mantıksal ilişkilerin birincil sıra gerşekler olarak ifade edilmiş ürneklerdens c soşıkarılması Tü mevarımsal Mantık Programlama araştırmalarınca derinleme-c u ssine şalışılmış bir konudur. Sadece pozitif ürneklerden yola cıkılarak yapılancs s o şügrenmeler aşırı genellemelere neden olup tutarsız hipotezlerin sonuşlanmasınaoğ s cneden olabilir. Tek eşlemeli dizilere dayalı ozgü l genellemeler cıkaran birs üu şoğügrenme yünteminin dizi argümanlı onerileri ogrenebildiği güsterilmiştir. Buo u ü üğ go stez, dizilerin üzgü l genellemeleri ï¬krine dayalı, en az genel genelleme şemasınıou skullanma yoluyla geri plan bilgisini de dikkate alarak onerme genelleyenübir tü mevarımsal ügrenicinin gerşekleştirilebilmesi işin yapılan calışmayıu oğ c s c şsüzetlemektedir. Gerşekleştirilen sistem, ayrıca sayısal argü manlı onermeleri deo c s u ügenelleyebilecek şekilde genişletilmiş ve akrabalık ilişkileri, dilbilgisi ügrenme ves s s s oğsayısal veri işlemesi gereken mutagenesis tahmini gibi orneklerde başarılı sonuşlars ü s cverdiği güsterilmiştir.go sAnahtar süzcükler : tü mevarımsal mantık programlama, makine ogrenmesi, diziou u üğgenellemesi, hipotez, ürnek, geri plan bilgisi.oiv","Learning logical relations from examples expressed as ï¬rst order facts has beenstudied extensively by the Inductive Logic Programming research. Learning withpositive-only data may cause overgeneralization of examples leading to inconsis-tent resulting hypotheses. A learning heuristic inferring speciï¬c generalizationof strings based on unique match sequences is shown to be capable of learningpredicates with string arguments. This thesis outlines the eï¬ort showed to buildan inductive learner based on the idea of speciï¬c generalization of strings thatgeneralizes given clauses considering the background knowledge using least gen-eral generalization schema. The system is also extended to generalize predicateshaving numeric arguments and shown to be capable of learning concepts such asfamily relations, grammar learning and predicting mutagenecity using numericdata.Keywords: indective logic programming, machine learning, string generalization,hypotheses, example, background knowledge.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Ornek tabanlı otomatik ceviri sistemleri makine üğrenmesine dayananş ogyüntemlerden yararlanarak ceviri yaparlar. Bu ceviri sü reci şüyle üzetlenebilir:o ş ş u so oBir insan birinci dilde basit ve kısa cü mleleri ikinci dildeki karşılıkları ile bir-u slikte ezberledikten sonra muhakeme ile yeni verilen cü mleleri daha üncedenu oüğrendikleri aracılığıyla cevirebilir. Bizim sistemimizde ceviri ürnekleri ceviriog g ş ş o şkalıpları olarak tutulmaktadır. Ceviri kalıpları, iki ceviri ürneğinden, ürneklerinş ş og ofarklı kısımlarınının yerine değişkenler koyularak ügrenilmektedir. Değişikgs oğ gskısımların yerine geşen değişkenler, ceviri ürneklerinin herbirinden gelen değişikc gs ş o gskısımları genelleştirmektedir.s Bu sistemde değişkenlerin genelleştirdiklerigs skısımların tip bilgilerini de ceviri kalıplarının yapısına ekleyerek yanlış ceviriş sşsonuşlarının sistemce uretilmesinin engellenmesi amaşlanmaktadır.c ü cüAnahtar süzcükler : Ornek Tabanlı Otomatik Ceviri, Tip Destekli Ceviri Kalıpları,ou ş şü grenmesi.Makine Oğiv","Example based machine translation is a translation technique that leans onmachine learning paradigm. This technique had been modeled by the learningprocess as: a man is given short and simple sentences in language A with theircorrespondences in language B; he memorizes these pairs and then becomes ableto translate new sentences via these pairs in the memory. In our system thetranslation pairs are kept as translation templates. A translation template isinduced from given two translation examples by replacing diï¬ering parts in theseexamples by variables. A variable replacing a diï¬erence that consists of twodiï¬ering parts (one from the ï¬rst example, and the other one from the secondexample) is a generalization of those two diï¬ering parts and these variables aresupported with part-of-speech tag information in order to deteriorate incorrecttranslations. After the learning phase, translation is achieved by ï¬nding theappropriate template(s) and replacing the variables.Keywords: Example Based Machine Translation, Type Associated TranslationTemplate Induction, Machine Learning.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Moleküler biyolojideki son gelişmelerle ortaya cıkan büyük olşekli verilerden enu s ş u u ücyüksek oranda yararlanabilmek işin bunlarla organize şekilde ilgilenmek; veri-u c stabanları, arayüzler, veri gürüntüleme ve yorumlama araşları geliştirmek gerek-u ou u c smektedir. Protein hücre işi yerleşimi ve mikrodizi gen anlatım ifadesi deneyselu c sbiyolojici işin bir yol haritası olmadan once yoğun hesaplamalar gerektiren ikic ü galandır. Protein hücre işi yerleşimi protein işlevini aşıklamak aşışından ünemlidir.u c s s c cs oBu şalışmada, MEP2SL (model organisms proteome subcellular localizationcsdatabase) adında, model organizmaların tüm proteinleri işin kendini güncelleyenu c uaranabilir ve verileri bilgisayara aktarılabilir bir veritabanı yapılmıştır. Bu verita-sbanı, dokuz cokhücreli organizma işin bilinen deneysel yerleşim bilgisinin yanısıraş u c stahmine dayalı yerleşim bilgilerini barındırmaktadır. MEP2SL tahmine dayalısyerleşim sonuşları yüksek verimli deneysel maya yerleşim bilgileriyle uyumlu-s c u sluk güstermektedir. Ayrıca iki farklı veri kümesinde dürt farklı yerleşim tahmino u o saracı doğruluk oranlarına güre daha iyi sonuşlar vermektedir. Bu bulgular güzg o c oouününe alındığında MEP2SL sistemi pek cok arama, verileri bilgisayara aktarmag şseşeneği yanısıra daha fazla bilgiye yünelik araşları ve bağlantılarıyla berabercg o c gprotein hücre işi yerleşim bilgisi işin bir referans kaynağı olabilecek niteliktedir.u c s c gMikrodizi teknolojisi tüm genomun aynı anda incelenmesi işin uygun bir ortamu chazırlamaktadir. Bu calışmada Aï¬ymetrix HG-U133 Plus 2.0 dizileri işin DEGşs c(diï¬erentially expressed genes) adında, analiz ve veri geri aktarımı arayüzlerine usahip, ürün uzerinde kurulabilen ve aşık kaynak kodlu ayrımsal gen ifadeleri veri-ou ü ctabanı kurulmuştur. DEG, veritabanı ile tamamlanması sonucu sürekli veri depo-s ulamaya imkan sağlar. Ayrıca orün uzerine kurulabilme ozelliğiyle verilerini ortakg üu ü ü gerişime aşık sunuculara gündermek istemeyen kullanıcılar işin yararlı bir araştır.s c o c cAnahtar süzcükler : protein hücre işi yerleşimi üngürüsü, mikrodizi gen ifadesi,ou u c s o ouuşok hücreli model organizmalar, orün arayüzü ve veritabanı, proteom.c u üu uuv","In order to beneï¬t maximally from large scale molecular biology data gener-ated by recent developments, it is important to proceed in an organized mannerby developing databases, interfaces, data visualization and data interpretationtools. Protein subcellular localization and microarray gene expression are twoof such ï¬elds that require immense computational eï¬ort before being used asa roadmap for the experimental biologist. Protein subcellular localization is im-portant for elucidating protein function. We developed an automatically updatedsearchable and downloadable system called model organisms proteome subcellu-lar localization database (MEP2SL) that hosts predicted localizations and knownexperimental localizations for nine eukaryotes. MEP2SL localizations highly cor-related with high throughput localization experiments in yeast and were shownto have superior accuracies when compared with four other localization predic-tion tools based on two diï¬erent datasets. Hence, MEP2SL system may serve asa reference source for protein subcellular localization information with its inter-face that provides various search and download options together with links andutilities for further annotations. Microarray gene expression technology enablesmonitoring of whole genome simultaneously. We developed an online installablesearchable open source system called diï¬erentially expressed genes (DEG) thatincludes analysis and retrieval interfaces for Aï¬ymetrix HG-U133 Plus 2.0 ar-rays. DEG provides permanent data storage capabilities with its integration intoa database and being an installable online tool and is valuable for groups whoare not willing to submit their data on public servers.Keywords: protein subcellular localization prediction, microarray gene expression,eukaryotic model organisms, web interface and database, proteome.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Yüksek şüzünürlükteki uzaktan algılamalı uydu gürüntülerinde bülütlemeu co u u u ou u oukent uygulamalarında ünemli bir problemdir şunkü elde edilen bülütlemelerleo cü u ousınıï¬andırma işin piksel tabanlı spektral bilginin yanında uzamsal ve yapısal bil-cgiler elde edilebilir. Bu tezde, bişimbilimsel işlemlerle şıkarılan yapısal bilgi vec s cana bileşenler analizi (ABA) ile üzetlenen spektral bilgi kullanılarak gürültüdens o uu uetkilenmeyen bülütler elde eden bir yüntem sunduk. Yapılan deneyler yünteminou o ogürüntü uzerinde komşuluk bilgisini ve spektral bilgiyi beraber kullanmayan başkao u uü s sbir yünteme güre daha düzgün ve anlamlı yapılar buldugunu güstermiştir.o o uu o sDaha sonra, birden fazla ABA bandında ortaya şıkan sıradüzenselc ubülütlemeler arasından anlamlı yapılara denk gelen bağlı bileşenleri otomatikou g solarak seşmek işin ise ügreticisiz bir yüntem sunulmuştur. Bu problem, verilen bir-c c o o sden şok nesne/yapı işin farklı sıradüzensel bülütlemelerden gelen şok sayıda adayc c u ou cbülgeden oluşan uzayda bir gruplama problemi olarak gürülebilir. Bu amaşla,o s ou cgruplama problemini şüzmek işin olasılıksal Gizli Degişken Analizi (OGDA) kul-co c slanmaktayız. Yapılan deneyler yüntemin aynı nesne sınıfına ait bülütleri otomatiko ouolarak belirleyebildiğini güstermektedir.g oSon olarak, birden fazla seviyede bülütleme sonucunda elde edilen bülgeleriou okullanarak bir sınıï¬andırma yüntemi sunmaktayız. Farklı yapılardakı farklıoayrıntıları yakalamak işin farklı yapısal üğe boyut aralıkları kullanılarak birc oggürüntünün birden fazla ülşekte temsil edilmesi amaşlanmaktadır. Her birou u u oc cülşekte bülütleme yapılmakta ve ortaya şıkan her bir bülüt işerisindeki pik-oc ou c ou csellerin spektral üzelliklerinin bir üzeti ile temsil edilmektedir. Bu temsillero okullanılarak bülütler ünerilen gruplama yüntemi ile gruplanmakta ve bülütlerinou o o oufarklı ülşeklerdeki grup etiketleri piksellerin sınıï¬andırılmasında kullanılmaktadır.ocSon sınıï¬andırma karar ağacı sınıï¬andırıcısı ile yapılmaktadır. Yapılan deneylergiiiyüntemin uzamsal bilgiyi etkili bir şekilde kullanmayan klasik yünteme güreo s o oustünlüğunü güstermektedir.ü u ugü u oAnahtar süzcükler : Uydu gürüntüleri, sıradüzensel bülütleme, üğreticisiz nesneou ou u u ou ogsezimi, şok ülşekli sınıï¬andırma, uzamsal bilgi.c oc",
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,"Many search engines use a two-step process to retrieve from the web pages related to a user?s query. In the first step, traditional text processing is performed to find all pages matching the given query terms. Due to the massive size of the web, this step can result in thousands of retrieved pages. In the second step, many search engines sort the list of retrieved pages according to some ranking criterion to make it manageable for the user. One popular way to create this ranking is to exploit additional information inherent in the web due to its hyperlink structure. One successful and well publicized link-based ranking system is PageRank, the ranking system used by the Google search engine. The dynamically changing matrices reflecting the hyperlink structure of the web and used by Google in ranking pages are not only very large, but they are also sparse, reducible, stochastic matrices with some zero rows. Ranking pages amounts to solving for the steady-state vectors of linear combinations of these matrices with appropriately chosen rank-1 matrices. The most suitable method of choice for this task appears to be the power method. Certain improvements have been obtained using techniques such as quadratic extrapolation and iterative aggregation. In this thesis, we propose iterative methods based on various block partitionings, including those with triangular diagonal blocks obtained using cutsets, for the computation of the steady-state vector of such stochastic matrices. The proposed iterative methods together with power and quadratically extrapolated power methods are coded into a software tool. Experimental results on benchmark matrices show that it is possible to recommend Gauss-Seidel for easier web problems and block Gauss-Seidel with partitionings based on a block upper triangular form in the remaining problems, although it takes about twice as much memory as quadratically extrapolated power method. Keywords: Google, PageRank, stochastic matrices, power method, quadratic extrapolation, block iterative methods, aggregation, partitionings, cutsets, triangular blocks."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË ü üUZAKTAN ALGILANAN RESIMLERDE UST DUZEYËşË ËYAPILARI BULMAK ICIN GENEL DOKU MODELLERIEmel DoğrusüzgoBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Yard. Doş. Dr. Selim Aksoyo cHaziran, 2007Kentsel alan kullanımının güzetimi ve yünetimi, GIS (coğraï¬ bilgi sistem-o o gleri), kartograï¬, şevresel, zirai ve ekolojik şalısmalar işin ünemli bir kaynakc c coolan uydu gürüntülerinden otomatik olarak bilgi şıkarımı ve değerlendirilmesi,ou u c gbu gürüntülerin miktarının her geşen gün hızla artması ile ürüntü tanımaou u c u ou ualanı işin popüler bir problem haline gelmiştir. Bu tez, uzaktan algılananc u sgürüntülerden karmaşık yapıların sezimlemesini sağlamak amacıyla istatistik-ou u s gsel ve yapısal doku modelleri ünermektedir. Uydu gürüntülerindeki karmaşıko ou u syapılar, bulunduğu bülge işerisinde karakteristik uzamsal düzene sahip kompleksgo c ucoğraï¬ nesnelere denk gelmektedir. Literatürde, gürüntüleri piksel düzeyindekig u ou u uyüntemlerle sınıï¬andıran yaklaşımların tersine, biz bu şalışmada, basit coğraï¬o s cs gnesneleri dokunun temel birimi olarak kullanmakta ve bu temel üğelerin uzam-ogsal ürüntülerini modellemekteyiz. Bu yaklaşım, ilgilenilen gürüntü elemanlarınınou u s ou ugeleneksel yaklaşımdaki pikseller yerine kentsel temel üğelere dünüştüğu ?genel-s og o us ugüËlenmiş doku? ülşutü olarak gürülebilir. Ilgilendiğimiz uzamsal ürüntüler, bu temels o cü u ou g ou uüğelerin komşuluklar işerisindeki düzenli veya düzensiz şeklindeki dağılımlarıdır.og s c u u s gËBu tezde sunduğumuz yüntem iki basamaktan oluşmaktadır. Ilk basamakta,g o stemel üğeler spektral, dokusal ve morfolojik üzniteliklerin tek-sınıï¬ı sınıï¬andırıcı-og oËlarda kullanılmasıyla bulunmaktadır. Ikinci basamakta ise bu temel üğelerin oguzamsal ürüntüleri modellenmektedir. Bu basamakta, istatistiksel ya da yapısalou uËbir yaklaşım izlenebilir. Istatistiksel yaklaşımda, temel üğelerin uzamsal dağılımıs s og ganalizi eş oluşuma dayalı uzamsal bülge üznitelikleri ve Fourier spektrum ta-s s o obanlı frekans bülgesi üznitelikleri ile yapılmaktadır. Bahsedilen üznitelikler, odak-o o olanılan nesnenin, analizi yapılan gürüntü bülgesinde var olma olasılığını nicelemekou u o gişin kullanılmaktadır. Yapısal yaklaşımda ise, herbir düğumün bir temel üğeyec s ugü u ogkarşılık geldiği, ve komşuluk bilgisinin gürüntünün Voronoi diyagramı sayesindes g s ou u ubulunduğu şizge tabanlı bir güsterim sunulmuştur. Bir şizge, minimum kapsayangc o s cvviağacının belli bir eşik değere güre parşalanması yolu ile kümelere bülünmekteg s g o c u ouve kümeler, işlerindeki düğumlerin komşularıyla yaptığı aşı dağılımları dikkateu c ug ü s gc galınarak düzenli/düzensiz şeklinde sınıï¬andırılmaktadır.u u sBu tezde sunulan algoritmalar, yerleşim alanları ve liman olmak uzere ikis üşeşit kompleks coğraï¬ nesnenin bulunması problemine uygulanmıştır. Bu nes-cs g snelerin modellenmesindeki ilk adım, yerleşim alanları işin bina, liman işins c cËise gemi ve su gibi temel üğelerin bulunmasıdır. Ikinci adımda ise, bu nes-ognelerin uzamsal ürüntülerini modellemek uzere istatistiksel ve yapısal yaklaşımlarou u ü sco u u u u Ëoürneklendirilmiştir. Yüksek şüzünürlüklü Ikonos ve DOQQ gürüntüleri uzerindes u ou u üyapılan deney sonuşları, ünerilen tekniklerin büyük uydu gürüntüsü veric o uu ou uukümelerinde coğraï¬ nesnelerin varlığını sorgulamak amacıyla kullanılabileceğiniu g g ggüstermiştir.o süu uAnahtar süzcükler : Orüntü algılama, tek sınıï¬ı sınıï¬andırma, coğraï¬ nesne sez-ou gimi, eş oluşuma dayalı doku analizi, Fourier tabanlı doku analizi, şizge tabanlıs s cdoku analizi.","ABSTRACTGENERALIZED TEXTURE MODELS FORDETECTING HIGH-LEVEL STRUCTURES INREMOTELY SENSED IMAGESEmel DoğrusüzgoM.S. in Computer EngineeringSupervisor: Asst. Prof. Dr. Selim AksoyJune, 2007With the rapid increase in the amount and resolution of remotely sensed imagedata, automatic extraction and classiï¬cation of information obtained from suchimages have been an important problem in the ï¬eld of pattern recognition sinceremotely sensed imagery is a critical resource for diverse ï¬elds such as urban landuse monitoring and management, GIS and mapping, environmental change andagricultural and ecological studies. This thesis proposes statistical and structuraltexture models for detecting high-level structures in remotely sensed images. Thehigh-level structures correspond to complex geospatial objects with characteris-tic spatial layouts in a region. As opposed to the existing approaches that arebased on classifying images using pixel level methods, we propose to use simplegeospatial objects as textural primitives and exploit their spatial patterns. Thisrepresentation can be viewed as a ?generalized texture? measure where the imageelements of interest are urban primitives instead of the traditional case of pixels.The spatial patterns we are interested in correspond to the regular and irregulararrangements of these primitives within neighborhoods.The methodology we propose in this thesis has two steps. First, the primitivesof interest are detected using spectral, textural and morphological features withone-class classiï¬ers. Then, the spatial patterns of these primitives are modeled.At this step, either a statistical or a structural approach can be followed. Inthe statistical approach, analysis of the spatial arrangement of the primitives isdone by co-occurrence-based spatial domain features and Fourier spectrum-basedfrequency domain features. These features are used to quantify the likelihood ofpresence of the focused object in the image region being analyzed. In the struc-tural approach, a graph-theoretic representation is proposed where the primitivesform the nodes of a graph and the neighborhood information is obtained throughiiiivVoronoi tessellation of the image scene. Next, the graph is clustered by threshold-ing its minimum spanning tree and the resulting clusters are classiï¬ed as regularor irregular by examining the distributions of the angles between neighboringnodes.The algorithms proposed in this thesis are illustrated with the detection of twogeospatial objects: settlement areas and harbors. The ï¬rst step in the modelingof these objects is the detection of primitives such as buildings for settlementareas, and boats and water for harbors. In the second step, both statisticaland structural approaches are illustrated for the modeling of the spatial patternsof these objects. Results of the experiments on high-resolution Ikonos satelliteimagery and DOQQ aerial imagery show that the proposed techniques can beused for detecting the presence of geospatial objects in large remote sensing imagedatasets.Keywords: Pattern recognition, one-class classiï¬cation, geospatial object detec-tion, co-occurrence texture analysis, Fourier texture analysis, graph-based textureanalysis."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Tez Yüneticisi: Assist. Prof. Dr. Ibrahim Kürpeoğluo o gAğustos, 2007gGeliştirildiklerinden ve kullanıma sunulduklarından beri, kablosuz tasarsız ağlars gbayağı rağbet gürmektedir ve bir cok kablosuz ağ arayüz kartı tasarsız ağlarag g o ş g u gdestek sağladığı işin, bu tip ağ yapıları gerşek hayatta ozel amaşlara yünelik ol-g gc g c ü c omayan bir cok kullanım alanı bulmuştur. Su anda gerşek hayattaki en yaygınş s ş ctasarsız ağ tipi ?kablosuz mobil tasarsız ağlar (KMTA)?dır ve bu ağlar, ozellikleg g g ühareketlilik destekleri ve tasarsız doğalarından kaynaklanan konuşlandırma ko-g slaylıklarından dolayı büyük itibar gürmektedir. Web, dosya aktarımı, e-postauu ove hızlı mesajlaşma gibi yaygın ağ uygulamaları veri odaklı olduğundan ve sıkıs g gzaman kısıtlamaları altında calışmadığından, KTMA'lar geşmişte bu tip gerşekşs g cs czamanlı olmayan uygulamaları olanaklı kılacak şekilde kullanılmıştır. Fakat,s stelekonferans, IP uzerinden ses aktarımı, güvenlik ve takip uygulamaları gibiü uvakitliliğin onemli olduğu uygulamaların kullanımı yaygınlaştıkşa, coklu sek-gü g scşmeli KTMA'larda gerşek zamanlı traï¬k desteği ünemli bir konu olarak ortayac gocıkmaktadır.şCoklu sekmeli KTMA'larda gerşek zamanlı traï¬k desteği veren, olaya ve isteğeş c g gdayalı, bağ durumu tabanlı bir yol atama protokolü üneriyoruz. Elessar adınıg uoverdiğimiz protokol, bağ durumlu topoloji dağıtımına dayanmaktadır, ama dahag g gyaygın olan periyodik bağ durumu mesajlaşması yerine, Elessar'da, ilgilendiğimizg s golayların topoloji değişimleri olduğu olay tabanlı bağ durumu mesajları kul-gs g glanıyoruz. Büyle bir yaklaşımla protokolümüzün getirdiği ek yükü azaltmayıo s u uu g uuhedeï¬iyoruz. Protokol ek yükünü üzellikle diz ustü ve avuş işi bilgisayarlarıuuuo üu ccgibi insanlarla doğrudan etkileşim işerisinde bulunan araşlardan oluşan, düşukg s c c s usüseviyeli hareketlilik barındıran tasarsız ağlarda azaltmayı amaşlıyoruz. Bağ du-g c grumu tabanlı doğasından dolayı, protokolümüz gerşek zamanlı olmayan traï¬ğig uu c gvviherhangi bir ek işlem gerektirmeden desteklemektedir. Gerşek zamanlı traï¬ğis c gdesteklemek işin ise sadece ağda bir veya birden fazla gerşek zamanlı traï¬kc g cakışı olan durumlarda, talebe bağlı olarak calışan bir doğrudan tutar dağıtıms g şs g gmekanizması oneriyor ve kullanıyoruz. Kaynak rezervasyonu yapmadan, akıllıüyol seşimleri sayesinde gerşek zamanlı traï¬k akışlarına gevşek hizmet kalitesic c s sgüvencesi veriyoruz. Hizmet kalitesi güvencelerini, ağ düğumlerinde oluşabileceku u g ug ü saksaklıklara rağmen ve hareketlilik durumlarında bile, dinamik yol ayarlamalarıgsayesinde traï¬k akışının ümrü boyunca verebiliyoruz. Elessar, gerşek zamanlıs ou colan ve olmayan traï¬k akışlarına eş zamanlı destek verebildiği gibi, gecikme du-s s gyarlı ve kayıp duyarlı traï¬k gibi birden fazla gerşek zamanlı traï¬k akış tipini de eşc s szamanlı olarak destekleyebilmektedir. Yani protokolümüz sadece tek tip gerşekuu czamanlı traï¬k akışına destek vermek yerine bir cok traï¬k tipini desteklemektedir.s şElessar tamamen dinamik, kendinden ayarlamalı ve dağıtımlı olup, aşağıda yerg sgalan katmanların hizmet kalitesi sağlandığının farkında olmasını gerektirmemek-g gtedir.OMNeT++ kesikli olay simülasyon platformu ve bu platform işin geliştirilmişu c s sINET iskelet yapısı uzerinde gerşekleştirdiğimiz gerşekşi simülasyon deneyleriyle,ü c s g cc ualdığımız tasarım kararlarını ve protokolümüzün performansını değerlendirdik.g u uu gBu deneylerimizde IEEE 802.11b ortam erişim protokolünü ve hareketliliği simüles uu g uedebilmek işin rastlantısal yol noktası hareketlilik modelini kullandık. Deneycsonuşlarımız güstermektedir ki Elessar, hareketlilik durumlarında bile değişikc o gsgerşek zamanlı traï¬k tiplerini etkili bir bişimde desteklemektedir. Deneyleri-c cmiz sonucunda gürülüyor ki onerdiğimiz protokol en iyi performansını kücuko uu ü g uş üve orta olşekli ağlarda, düşuk ve orta hareketlilik seviyeleri işin güstermektedir.üc g usü c oHareketlilik seviyesi belirli bir eşiği aştıktan sonra akıllı yol seşimleri ortamdakisg s cyüksek dinamizmle tatminkar bir şekilde baş edememekte ve Elessar'ın getirdiğiu s s gek yük kabul edilebilir seviyeleri geşmektedir.u cAnahtar süzcükler : Kablosuz tasarsız ağlar, yol atama protokolü, gerşek zamanlıou g u ctraï¬k desteği, hizmet kalitesi.g","Wireless ad hoc networks have gained a lot of popularity since their intro-duction and as many wireless network interface cards provide support for ad hocnetworking, such networks have also seen real-life deployment for non-specializedpurposes. Wireless mobile ad hoc networks (MANETs) are currently the mostcommon type of ad hoc networks, and such networks are especially esteemed fortheir mobility support and ease of deployment due to their ad hoc nature. Asmost common network applications, such as the Web, FTP, email, and instantmessaging, are data-centric and do not operate under strict time constraints,MANETs have been deployed to enable such non-real-time applications in thepast. However, with the increasing use of real-time applications over ad hocnetworks, such as teleconferencing, VoIP, and security and tracking applicationswhere timeliness is of importance, real-time traï¬c support in multi-hop wirelessmobile ad hoc networks has become an issue.We propose an event-driven, link-state based, on-demand routing protocol toenable real-time traï¬c support in such multi-hop wireless mobile ad hoc net-works. Our protocol, which is named Elessar, is based on link-state topologydissemination, but instead of the more common periodic link-state messagingscheme, we employ event-driven link-state messages in Elessar, where topologychanges are the events of interest. Through such an approach, we aim to lower theoverhead of our protocol, especially for low-mobility cases, which is currently themost commonly encountered case with ad hoc networks deployed with machinesdirectly interacting with humans, such as PDAs and laptops. Due to its link-statenature, our protocol is able to support non-real-time traï¬c without any furtheraction. In order to support real-time traï¬c, however, we employ a direct costiiiivdissemination mechanism, which only operates on-demand when there are one ormore real-time ï¬ows in the network. We aim to provide soft quality-of-service(QoS) guarantees to real-time ï¬ows through intelligent path selection, withoutany resource reservation. We also aim to provide such QoS guarantees through-out the lifetime of a real-time ï¬ow, even in the face of node failures and mobility,by dynamic path adaptation during the lifetime of the ï¬ow. Elessar is able tosupport real-time and non-real-time traï¬c concurrently, as well as various diï¬er-ent types of concurrent real-time traï¬c, such as delay- and loss-sensitive traï¬c.Our protocol, therefore, does not aim to support a single type of real-time traï¬c,but rather a plethora of diï¬erent types of real-time traï¬c. Elessar is completelydistributed, dynamic and adaptive, and does not require the underlying MACprotocol to be QoS-aware.We analyse our design choices and the performance of our protocol throughrealistic simulation experiments conducted on the OMNeT++ discrete event sim-ulation platform, using the INET framework. We have used the IEEE 802.11bMAC protocol during our simulations and have employed the random waypointmobility model to simulate mobility. Our experimental results show that Elessaris able to eï¬ciently provide real-time traï¬c support for diï¬erent types of traf-ï¬c ï¬ows, even in the face of mobility. Our protocol operates best for small-to-medium-sized networks where mobility rates are low-to-medium. Once themobility rate exceeds a certain threshold, intelligent path selection cannot copesatisfactorily with the high dynamism of the environment and the overhead ofElessar exceeds acceptable levels due to its event-driven link-state nature.Keywords: Wireless ad hoc networks, routing protocol, real-time traï¬c support,quality-of-service (QoS)."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez calsmasnda, bir video parcasnn daha buyuk videolarda aranmas icin yeni bir yontem sunuyoruz. Sunulan yontem srallarn farkl zamanlarda yada farkl kameralarla cekilmesinden dolay ortaya ckabilecek olan gorus acs ve aydnlanma degisimlerine ve bunlara ek olarak srallardaki lm karelerinin sra ve say degisimlerine kars gurbuzdur. Bizim algoritmamz icin sorgu gerekmemekte ve verilen medyadaki butun tekrarl tamamiyle otomatik olarak bulmaktad r. _Ilk olarak medyadaki lm kareleri icn renk bilgilerine ve anahtar noktalarn daglmna dayanarak benzer lm kareleri bulunmaktadr. Bunu ardndan, medyadaki tekrarlar bir agac yaps kullanlarak aranmaktadr. Son olarak da bu tekrar eden srallar daha dogru sonuclar elde edilmesi icin sadelestirilmektedir. Bu calsmann deneyleri iki adet lmde ""Run Lola Run"" ve ""Groundhog Day"", TRECVID 2004 verisindeki reklamlarda ve CIVR'in Kopye Takibi icin verdigi veritabannda yaplmstr. Bu deneylerde kopya tanmada %90'nn uzerinde, diger datasetlerinde ise %80 uzerinde dogruluk degerleri elde edilmistir. Anahtar sozcukler : hikaye takibi, kopya yakalama, medya takibi.","In this thesis, we propose a new method to search dierent instances of a video sequence inside a long video. The proposed method is robust to view point and illumination changes which may occur since the sequences are captured in dierent times with dierent cameras, and to the dierences in the order and the number of frames in the sequences which may occur due to editing. The algorithm does not require any query to be given for searching, and nds all repeating video sequences inside a long video in a fully automatic way. First, the frames in a video are ranked according to their similarity on the distribution of salient points and colour values. Then, a tree based approach is used to seek for the repetitions of a video sequence if there is any. These repeating sequences are pruned for more accurate results in the last step. Results are provided on two full length feature movies, Run Lola Run and Groundhog Day, on commercials of TRECVID 2004 news video corpus and on dataset created for CIVR Copy Detection Showcase 2007. In these experiments, we obtain %93 precision values for CIVR2007 Copy Detection Showcase dataset and exceed %80 precision values for other sets. Keywords: copy detection, media tracking, story tracking."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilimsel merak, kalıtım-ülşekli bilginin sistem seviyesindeki araştırmalarınaoc syünelirken, molekül düzeyindeki hücresel süreşler hakkında uretilen verilero uu u uc ühızlanan bir oranla artmaktadır. Cizge tabanlı yolak ontolojiler ve veri tabanlarışbu tarz bilgiler işin geniş bir kullanım alanına sahiptir. Bu güsterim, hücreselc s o uağların programlı bir şekilde bütünleştirilmesinin yanı sıra yapısal ve dinamikg s uu süzelliklerini tahmin etmeye yünelik olarak şizge teorinin iyi anlaşılmış kavramlarıo o c s skullanılarak araştırılmasını mümkün kılmaktadır. Bu kapsamda, büyle bütünleşiks uu o uu sgeniş ağların, ilgili alt-ağların etken algoritmalar ve yazılım araşlarının yardımıylasg g cseşilerek şıkarılması amacıyla, etkili olarak sorgulanması zaruridir.c cBu amaşla, protein-protein etkileşiminden, metabolik yolaklara hatta sinyalc syolaklarına her türlü şizge tabanlı yolak veri tabanlarına uygulanabilir olmaku ucuzere, basit komşuluk sorgularından en kısa yol yolaklarına ve geri beslemeü sdüngülerine pek şok şizge teorik algoritmalar beraberinde bir sorgulama şerşevesiou cc ccgeliştirdik. Bu algoritmalar ayrıca yolak veritabanı işinde mevcut bileşik veyas c sbirbirinin işine yerleştirilmiş yapıları da oluşturulabilir ve Patika (Entegrasyonc s s sve Bilgi Kazanma işin Yolak Analiz Araşları) yazılımlarının sorgulama unsurlarıc cişerisinde uygulanmıştır. Ayrıca, süzkonusu algoritmaların geniş bir şizge tabanlıc s o s cyolak veritabanı işin biyolojik olarak ünem arz eden pek şok sorgunun cevap-c o clandırılması işin kullanışlı olduğu gürülmüştür.c s g o u us uAnahtar süzcükler : Cizge Algoritmaları, Cizge Sorgulama, Biyolojik Yolaklar,ou ş şYolak Veri Tabanları.iv","As the scientiï¬c curiosity shifts toward system-level investigation of genomic-scale information, data produced about cellular processes at molecular level hasbeen accumulating with an accelerating rate. Graph-based pathway ontologiesand databases have been in wide use for such data. This representation has madeit possible to programmatically integrate cellular networks as well as investigatingthem using the well-understood concepts of graph theory to predict their struc-tural and dynamic properties. In this regard, it is essential to eï¬ectively querysuch integrated large networks to extract the sub-networks of interest with thehelp of eï¬cient algorithms and software tools.Towards this goal, we have developed a querying framework along with a num-ber of graph-theoretic algorithms from simple neighborhood queries to shortestpaths to feedback loops, applicable to all sorts of graph-based pathway databasesfrom PPIs to metabolic pathways to signaling pathways. These algorithms canalso account for compound or nested structures present in the pathway data, andhave been implemented within the querying components of Patika (PathwayAnalysis Tools for Integration and Knowledge Acquisition) tools and have provento be useful for answering a number of biologically signiï¬cant queries for a largegraph-based pathway database.Keywords: Graph Algorithms, Graph Querying, Biological Pathways, PathwayDatabases.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Cizgeler, ağ oluşturmaktan, biyolojiye, bilgisayar bilimlerine, birşok alanda sıkşaş g s c ckullanılan veri modelleridir. Gürselleştirme, etkileşimli düzenleme kabiliyetio s s uve şizgelerin yerleştirilmesi, temeldeki ilişkisel bilgilerin şozümlenmesinde kritikc s s cü ukonulardır.Birşok ticari ve ticari olmayan şizge gürselleştirme araşları mevcuttur. Ancakc c o s cbileşik veya sıradüzensel olarak düzenlenmiş şizge güsterimini sağlayan araş sayısıs u u sc o g cşok sınırlı sayıdadır.cBiz adı Chisio olan yeni bir aşık-kaynak düzenleme ve yerleştirme ya-c u span şerşeve tanıtıyoruz. Chisio bedava, kullanımı kolay ve etkili, otomatikccyerleşim algoritmalarını destekleyen bir akademik şizge gürselleştirme aracı olaraks c o sgeliştirildi. Java'da yazıldı ve Eclipse'in Graï¬ksel Düzenleme Cerşevesi'ni (GEF)s u şctemel olarak aldı.Chisio yakınlaştırma, kaydırma, şizge nesneleri ekleme ya da cıkarma, harekets c şettirme, boyutunu değiştirme gibi standart şizge düzenleyici ozellikleri ile tamam-gs c u ülanmış üzelleşmemiş bileşik cizge düzenleyicisi olarak kullanılabilir. Var olan şizgeso s s sş u cnesnelerinin ozelliklerini ve yerleşim seşeneklerini değiştirmek işin nesne ozellikleriü s c gs c üve yerleşim seşenekleri pencereleri sunulmaktadır. Ayrıca, kullanılmakta olans cşizgeyi yazdırma ya da resim olarak ve kalıcı bellekte saklamak da mümkündür.c uuuSaklanan şizgeler ya da diğer araşlar tarafından yaratılmış GraphML bişimindekic g c s cdosyalar Chisio'ya yüklenebilir. Bunlardan başka, vurgulama mekanizması ileu skullanıcıların ilgilendiği altşizgelerin vurgulanması da mümkündür.g c uuuCerşeve, son kullanıcıların üzel ihtiyaşları işin kolayca üzelleştirilebilmeyeşc o c c o solanak sağlayacak bir mimariye de sahiptir. Ayrıca Chisio yay gümmeli'deng osıradüzensel yerleşime, bileşik yay gümmeliye, dairesel yerleşime ceşitli yerleşimu s s o s şs svvistilleri sunmaktadır. Bundan başka, yerleşim algoritması geliştirenler işin Chi-s s s csio'yu ideal bir sınama ortamı yapacak şekilde yeni algoritmalar doğruca ek-s glenebilir.Anahtar süzcükler : bilgi gürselleştirme, cizge yerleşimi, şizge düzenleme, yazılımou o s ş s c usistemi, şizge düzenleyici, bileşik şizgeler.c u sc","Graphs are data models, widely used in many areas from networking to biology tocomputer science. Visualization, interactive editing ability and layout of graphsare critical issues when analyzing the underlying relational information.There are many commercial and non-commercial graph visualization tools.However, overall support for compound or hierarchically organized graph repre-sentations is very limited.We introduce a new open-source editing and layout framework named Chisiofor compound graphs. Chisio is developed as a free, easy-to-use and powerful aca-demic graph visualization tool, supporting various automatic layout algorithms.It is written in Java and based on Eclipse?s Graphical Editing Framework (GEF).Chisio can be used as a ï¬nished generic compound graph editor with standardgraph editing facilities such as zoom, scroll, add or remove graph objects, move,and resize. Object property and layout options dialogs are provided to modifyexisting graph object properties and layout options, respectively. In addition,printing or saving the current drawing as a static image and persistent storagefacilities are supported. Saved graphs or GraphML formatted ï¬les created byother tools can be loaded into Chisio. Furthermore, a highlight mechanism isprovided to emphasize subgraphs of users interest.The framework has an architecture suitable for easy customization of the toolfor end-users? speciï¬c needs as well. Also Chisio oï¬ers several layout styles fromthe basic spring embedder to hierarchical layout to compound spring embedder tocircular layout. Furthermore, new algorithms are straightforward to add, makingChisio an ideal test environment for layout algorithm developers.iiiivKeywords: information visualization, graph layout, graph editing, software sys-tem, graph editor, compound graphs."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Birleşik olarak hareket yakalama verilerinin ï¬ltrelenmesi ve ana şerşevelerins ccazaltılması işin iki yeni yüntem sunulmuştur. Hareket yakalama bilgilerininc o ssü zü lmesi, hareket işerisindeki gü rü ltü nü n yok edilmesi işin gereklidir. Hareketuu c uu u u cişerisindeki gü rü ltü nü n yok edilmesi daha gerşekşi bir canlandırma sağlar. Ancakc uu u u cc ggerekli olşude yapılmadığı takdirde cok fazla dü zleştirilmiş, gerşekşi olmayan birü cü g ş us s cccanlandırma ortaya cıkarır. Diğer yandan, ana cerşevelerin azaltılması, bu ver-ş g şcilerin canlandırma eğrileri ile tanımlandığı işin, canlandırıcıların bu verileri cokg gc şdaha az ana cerşeve sayısı ile kolaylıkla düzenleyebilmelerini sağlar. Belirtilenşc u gyüntemlerden ilki, dinamik programlama yüntemi ile beraber hareket yakalamao overisine Hermite eğrisi oturtarak aynı anda hem ana cerşeve azaltımını yapmak-g şctadır, hem de verilerin gürü ltü den ayrılmasını sağlamaktadır. Diğer bir yüntemuu u g g oise aynı sonucu eğri basitleştirme yüntemlerini istenilen duyarlılık erişilinceyeg s o skadar kullanımıdır. Bu araştırmada bu yüntemlerin sonuşları değerlendirilmişs o c g sve karşılaştırılmıştır. Ek olarak oznel ve nesnel sonuşlar da sunulmuştur.ss s ü c sAnahtar süzcükler : Hareket Yakalama, Ana Cerşevelerin Azaltılması, Eğriou şc gOturtma, Eğri Sadeleştirme, Gü rü ltü nü n Filtrelenmesi.g s uu u uv","Two new methods for combined ï¬ltering and key-frame reduction of motioncapture data are proposed. Filtering of motion capture data is necessary toeliminate any jitter introduced by a motion capture system. Although jitterremoval is needed to obtain a more realistic animation, it may result in an over-smoothed motion data if it is not done properly. Key-frame reduction, on theother hand, allows animators to easily edit motion data by representing animationcurves with a signiï¬cantly smaller number of key frames. One of the proposedtechniques achieves key frame reduction and jitter removal simultaneously byï¬tting a Hermite curve to motion capture data using dynamic programming.Another method is to use curve simpliï¬cation algorithms on the motion capturedata until the desired reduction is reached. In this research, the results of thesealgorithms are evaluated and compared. Both subjective and objective resultsare presented.Keywords: Motion Capture, Keyframe Reduction, Curve Fitting, Curve Simpli-ï¬cation, Noise Filtering.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar graï¬klerinde geniş geometrik ortamların modellenmesi ve gürü ntü len-s ou umesi popü ler bir araştırma alanıdır. Bu tezde, geniş ve karmaşık şehir ortam-u s s sslarının uretilmesi ve stereoskopik olarak gürü ntü lenmesi işin bir cerşeve sunul-ü ou u c şcmaktadır. Kullanıcının güreceËi gürü ntü ye katkıda bulunmayan geometrinino g ou uşoËunun elenmesi işin, kapatılan alanların atılması ve bakış piramidi dışında kalancg c s salanların ayıklanması yüntemleri uygulanmaktadır. Kapatılan alanların atılmasıoişlemi işin daraltma yüntemi, yeni bir Minkowski farkına dayanan yaklaşıms c o sile uygulanmaktadır. KısmË gürü ntü lemeyi saËlayabilmek işin, dilimsel temsilı ou u g cadı verilen yeni bir bina temsil yüntemi geliştirilmiştir. Bu yüntem sayesinde,o s s okısmË gürü nü rlü k, depolama ihtiyacında muazzam azaltmalar saËlanarak tem-ı ou u u gsil edilebilmektedir. Elde edilen gürü ntü listesi graï¬k işlemci unitesi tabanlıou u s übir algoritma aracılıËıyla gürü ntü lenmektedir. Stereoskopik gürü ntü leme, ge-g ou u ou uzinti esnasında hesaplanan güz pozisyonlarına dayanmakta ve gürü ntü listelerio ou utespit edilmiş kapatılan alanların bilgisi kullanılarak elde edilmektedir. Stereos-skopik gürü ntü leme işin bakış piramidi dışındaki nesnelerin ayıklanması işlemi,ou u c s s süher iki güz işin iki yerine bir kez uygulanmaktadır. Onerilen algoritmalarockişisel bilgisayarlarda kodlanmıştır. Performans deneyleri, kapatılan alanlarıns satılması yüntemi ile dilimsel veri yapısı kullanımının, standart gürü ntü lemenino ou ukullanıldıËı bina seviyesindeki kapatılan alanların ayıklanması yüntemine güreg o operformansı; gürü ntü karesi hızı olarak % 81 arttırdıËını; graï¬k işlemci unitesiou u g s ütabanlı yüntem kullanımının da buna % 315 ilave artış saËladıËını ve depolamao sg güihtiyacını % 97 azalttıËını güstermektedir. Onerilen cerşevenin kullanılmasının,g o şcbü yü k ve karmaşık şehir modellerinin dü zgü n ve gerşek zamanlı gürü ntü lenmesiniuu ss uu c ou usaËladıËı güsterilmiştir.g go sAnahtar süzcükler : Stereoskopik gürü ntü leme, dilimsel veri yapısı, uzay altou ou ubülü mleme, sekizli ağaşlar, kapatılan alanların ayıklanması, kapatanların daral-ou gctılması, Minkowski farkı, bülgeden gürüş, şehir gürü ntü leme, gürü nü rlü k işleme.o o us s ou u ou u u sv","Modeling and visualization of large geometric environments is a popular researcharea in computer graphics. In this dissertation, a framework for modeling andstereoscopic visualization of large and complex urban environments is presented.The occlusion culling and view-frustum culling is performed to eliminate mostof the geometry that do not contribute to the user?s ï¬nal view. For the occlu-sion culling process, the shrinking method is employed but performed using anovel Minkowski-diï¬erence-based approach. In order to represent partial visibil-ity, a novel building representation method, called the slice-wise representationis developed. This method is able to represent the preprocessed partial visibilitywith huge reductions in the storage requirement. The resultant visibility list isrendered using a graphics-processing-unit-based algorithm, which perfectly ï¬tsinto the proposed slice-wise representation. The stereoscopic visualization de-pends on the calculated eye positions during walkthrough and the visibility listsfor both eyes are determined using the preprocessed occlusion information. Theview-frustum culling operation is performed once instead of two for both eyes.The proposed algorithms were implemented on personal computers. Performanceexperiments show that, the proposed occlusion culling method and the usage ofthe slice-wise representation increase the frame rate performance by 81 %; thegraphics-processing-unit-based display algorithm increases it by an additional315 % and decrease the storage requirement by 97 % as compared to occlusionculling using building-level granularity and not using the graphics hardware. Weshow that, a smooth and real-time visualization of large and complex urban en-vironments can be achieved by using the proposed framework.Keywords: Stereoscopic visualization, slice-wise representation, space subdivi-sion, octree, occlusion culling, occluder shrinking, Minkowski diï¬erence, from-region visibility, urban visualization, visibility processing.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,,"Example-Based Machine Translation (EBMT) is a corpus based approach to Machine Translation (MT), that utilizes the translation by analogy concept. In our EBMT system, translation templates are extracted automatically from bilingual aligned corpora, by substituting the similarities and differences in pairs of translation examples with variables. As this process is done on the lexical-level forms of the translation examples, and words in natural language texts are often morphologically ambiguous, a need for morphological disambiguation arises. Therefore, we present here a rule-based morphological disambiguator for Turkish. In earlier versions of the discussed system, the translation results were solely ranked using confidence factors of the translation templates. In this study, however, we introduce an improved ranking mechanism that dynamically learns from user feedback. When a user, such as a professional human translator, submits his evaluation of the generated translation results, the system learns ?contextdependent co-occurrence rules? from this feedback. The newly learned rules are later consulted, while ranking the results of the following translations. Through successive translation-evaluation cycles, we expect that the output of the ranking mechanism complies better with user expectations, listing the more preferred results in higher ranks. The evaluation of our ranking method, using the precision value at top 1, 3 and 5 results and the BLEU metric, is also presented. Keywords: Example-Based Machine Translation, Learning from User Feedback, Morphological Disambiguation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETTASARIM VE ÜRETİMİN SANALLAŞTIRILMASIBaşar ErdenerGrafik Tasarımı Yüksek Lisans ProgramıDanışman: Yard. Doç. Dr. Mahmut MutmanTemmuz, 2006Bu çalışma, tasarım ve üretim teknolojilerindeki son gelişmeleri göz önüne alarak, ürünlerinanlamlarını analiz etmeyi amaçlamaktadır. Ürünlerin kullanım, değişim ve gösterge değerleri,tüketim mantığı içinde, ilgili örnekler vasıtasıyla araştırılmakta ve sorgulanmaktadır.Teknolojideki en son gelişmelerin bir sonucu olarak seri uyarlama, pazardaki ürünlerinanlamları ve sunumları üzerindeki etkileri bağlamında tartışılmaktadır. Bunun yanısıra,bilgisayar destekli tasarım ve üretim metodlarının, üretim biçimlerine olan etkisi ve bu etkininkarşılaştığımız ürünlerin sunumlarına olan yansımaları irdelenmektedirAnahtar Kelimeler: Bilgisayar Destekli Tasarım, Seri Uyarlama, Kullanım Değeri, DeğişimDeğeri, Gösterge Değeri, Hipergerçeklik, Endüstriyel Tasarım","ABSTRACTVIRTUALIZATION OF DESIGN AND PRODUCTIONBaşar ErdenerMFA in Graphic DesignSupervisor: Assist. Prof. Dr. Mahmut MutmanJuly, 2006This study aims to make an analysis on the meaning of products with regards to recentdevelopments in design and production technologies. The notions of the use, exchange andsign values of products are aimed to be questioned and explored through relevant instanceswithin the consumption logic. Mass customization, as an outcome of recent advances intechnology is argued through its effects on the meaning and presentations of products withinthe market. It is also discussed in this thesis how the computer aided design andmanufacturing affects the mode of production and how this reflects on the representations ofproducts that we encounter.Keywords: Computer Aided Design, Mass Customization, Use Value, Exchange Value, SignValue, Hyperreality, Industrial Design"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË ËËËTEK VIDEO DIZISINDEN HAREKET YAKALANMASIËIbrahim DemirBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Assoc. Prof. Dr. Uğur Güdükbayo g uuAğustos, 2006gücUş boyutlu insan pozunun elde edilmesi şeşitli uygulama alanlarının ol-csmasından dolayı popüler bir konudur. Su anda var olan yüntemlerin şoğununu ş o cgşeşitli kısıtlamalara dayanması uygulamada bazı zorluklar doğurmaktadır. Bucs gyüntemlerin getirmiş olduğu kısıtlamalar birden fazla kamera gürüntüsüne veo s g ou uukamera ayarlarının bilinmesine ihtiyaş duyulması veya tek kamera gürüntüsüc ou uunedeniyle kullanıcının yoğun şabasına ihtiyaş duyulmasıdır. Coğu zaman bug c c şgkısıtlamalardan dolayı mevcut veriler uzerinde bu yüntemler uygulanamamak-ü otadır. Bu tezde bu kısıtlamalara bağlı olmadan tek kameradan elde edilengvideodan uş boyutlu animasyonda kullanılacak insan pozunun elde edilmesi işinüc cbir şerşeve şalışma ünerilmektedir. Buna güre video karelerindeki arka plancc cs o ohesaplanmakta, daha sonra gürüntü şıkarma yüntemi ile her bir video karesin-ou uc odeki insan silueti bulunmaktadır. Daha sonra vücudun parşaları iki boyutluu cresim uzerinde otomatik olarak tanımlanmaktadır. Son adım olarak da vücutü uparşaları tanımlanmış olan siluetten dikey izdüşum kullanılarak uş boyutlu pozc s usü ücoluşturulmaktadır. Bu tezde sunulan yüntem şoklu video zorunluluğu veya kam-s o c gera kalibrasyonu gerektirmemektedir. Bu tezde anlatılan şerşeve şalışma video-cc csnun değişmeyen bir arka planın olması, videonun dikey izdüşumünü etkileye-gs usü u ucek perspektif etkisinin fazla olmaması ve gürüntüdeki kişinin ayakta olmasıou u svarsayımlarına dayanmaktadır.Anahtar süzcükler : hareket yakalanması, şerşeve, tek kamera, gürme temelli,ou cc oanimasyon.1","ABSTRACTMOTION CAPTURE FROM SINGLE VIDEOSEQUENCEËIbrahim DemirM.S. in Computer EngineeringSupervisor: Assoc. Prof. Dr. Uğur Güdükbayg uuAugust, 20063D human pose reconstruction is a popular research area since it can be usedin various applications. Currently most of the methods work for constrained en-vironments, where multi camera views are available and camera calibration isknown, or a single camera view is available, which requires intensive user eï¬ort.However most of the currently available data do not satisfy these constraints,thus they cannot be processed by these algorithms. In this thesis a frameworkis proposed to reconstruct 3D pose of a human for animation from a sequence ofsingle view video frames. The framework for pose construction starts with back-ground estimation. Once the image background is estimated, the body silhouetteis extracted by using image subtraction for each frame. Then the body silhou-ettes are automatically labeled by using a model-based approach. Finally, the 3Dpose is constructed from the labeled human silhouette by assuming orthographicprojection. The proposed approach does not require camera calibration. Theproposed framework assumes that the input video has a static background andit has no signiï¬cant perspective eï¬ects and the performer is in upright position.Keywords: motion capture, framework, single camera, uncalibrated camera,vision-based, animation.1"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETSAĞLAMLIK VE ESNEKLIK: BILIŞSEL AJANLAR YARATMAKÖge BozyiğitBilgisayar Mühendisliğ Yüksek Lisansi,şDanıman: Yrd. Doç. Dr. David DavenportEylül, 20061959'da Newell ve Simon'nin geliştirdiğ genel insan zeka kabiliyeti modelleme projesiişyapay zeka ile ilgili çalımalarıbaş çın langı olarak biliniyor. Buna rağmen, günümüzdeyapay zeka alaninda genel beyin kabiliyeti modelleme yöntemleri değ dar biril,alanda spesifik zeka problemleri için özel cözüm üreten yöntemler hakim. Bu tezin amacıgenel biliş fonksiyonlar için yeni tasarlanmişmodellerin özeliklerini analiz edip,selonları sı rların nı nıhedef alan bir modelin araşrma zeminini oluştı turmak. Modelleribirbirinden ayı özellik temel biliş kapasitelerinin hangisinin daha üstün veya yayginran selsayı ğ Temel farklarin bir diğ ise, bazı nı sembolik altyapidan oluşıldı . eri ları n turulmuş,bazı nı ise daha ilksel mekanizmalardan kurulmuşları n olmalarıBunlarin yaninda, genel.biliş kapasitelerinin gerekçelerini daha da somutlaşrmak için bu alanda yapilansel tıfelsefik ve psikolojik araşrmalar da tezin konusu dahilinde.tıAnahtar Sözcükler: beyin fonksiyon modelleme, biliş dizge, mantik modelleme, genelselyapay zeka, tahmin kabiliyeti","ABSTRACTSTABILITY AND PLASTICITY: CONSTRUCTING COGNITIVE AGENTSÖge BozyiğitM.S. in Computer EngineeringSupervisor: Asst. Professor Dr. David DavenportSeptember, 2006The AI field is currently dominated by domain-specific approaches to intelligence andcognition instead of being driven by the aim of modeling general human intelligence andcognition. This is despite the fact that the work widely regarded as marking the birth ofAI was the project of creating a general cognitive architecture by Newell and Simon1959. This thesis aims to examine recently designed models and their various cognitivefeatures and limitations in preparation for building our own comprehensive model thatwould aim to address their limitations and give a better account for human cognition. Themodels differ in the kind of cognitive capabilities they view as the most important. Theyalso differ in whether their foundation is built on symbolic or sub-symbolic atomicstructures. Furthermore, we will look at studies in the philosophy and cognitivepsychology domain in order to better understand the requirements that need to be met inorder for a system to emulate general human cognition.Keywords: Cognitive architecture, human-level intelligence, knowledge representation,prediction, spreading activation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sözdizimsel çözümleme veya ayrıştırma, bir tümcenin dilbilgisel yapısını yanikelimeleri arasındaki ilişkiyi ortaya çıkarmak amacıyla verilen bir gramere göreinceleme işlemidir. Bu çalışmada, Türkçe için bir bağ grameri geliştirilmiştir.Sistemimizde Türkçe gibi çekimli ve bitişken biçimbirimlere sahip diller içinçok önemli olan, tam kapsamlı, iki aşamalı bir biçimbirimsel tanımlayıcınınsonuçları kullanılmıştır. Geliştirdiğimiz gramer sözcükseldir ancak, bazı işlevselkelimeler oldukları gibi kullanılırken, diğer kelime türleri için kelimelerinkendilerinin yerine biçimbirimsel özellikleri kullanılmıştır. Ayrıca sistemimizdekelimelerin ara türeme formlarının sözdizimsel rollerinin bazıları muhafazaedilmiştir.Anahtar Kelimeler: Doğal Dil şleme, Türkçe Dilbilgisi, Türkçe sözdizimi,Sözdizimsel Çözümleme, Bağ Grameri.","Syntactic parsing, or syntactic analysis, is the process of analyzing an inputsequence in order to determine its grammatical structure, i.e. the formalrelationships between the words of a sentence, with respect to a given grammar.In this thesis, we developed the grammar of Turkish language in the linkgrammar formalism. In the grammar, we used the output of a fully describedmorphological analyzer, which is very important for agglutinative languages likeTurkish. The grammar that we developed is lexical such that we used thelexemes of only some function words and for the rest of the word classes weused the morphological feature structures. In addition, we preserved the some ofthe syntactic roles of the intermediate derived forms of words in our system.Keywords: Natural Language Processing, Turkish grammar, Turkish syntax,Parsing, Link Grammar."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Spyware programları bilgisayar kullanıcıları işin ünemli bir problem teşkil et-co smektedirler. Genel olarak ?spyware? terimi kullanıcılara reklam güstermek veyaointernet tarayıcılarından alışkanlıklarını takip etmek işin kullanılan ?adware? adlıs ckütü niyetli programlar ile aynı anlamda kullanılır. Bu üzelliklerine ek olarak spy-ou oware programları varlıklarını gizleme eğilimleri ile de bilinirler, fakat bugüne kadarg ubu konudaki yeteneklerini ya kullanmadılar ya da oldukşa kıstlı kullandılar. Diğerc gbir deyişle, kullanıcıyı takip etmede oldukşa gelişmiş olan spyware programlarıs c sskendilerini saklama konusunda bu kadar başarılı değillerdi. Bu sebepten dolayı das gdosya tarama veya windows kütüğu tarama teknikleri ile kolayca yakalanabiliyor-u ugülardı. Yeni spyware programları ?rootkit? denilen kendilerini saklama konusundauzman programlarla birleşerek kendilerini ustaca saklayabilen spyware'ler ha-sline geldiler. Kısaca ?ghostware? adını verdiğimiz bu programlar işletim sistem-g slerinin uygulamalara sunduğu programlama arayüzlerini etkileyerek kendilerini veg ukaynaklarını saklayabilemektedirler. Bu şalışmamızda ghostware programlarınıncskullandıkları teknikleri ve onlara karşı kullanılabilecek karşı teknikleri inceledik.s sAyrıca popüler anti-virüs ve anti-spyware programlarına ve karşı teknik kullananu u saraşlara karşı etkililiklerini araştırdık. Sonuşlara güre anti-virüs ve anti-spywarec s s c o uprogramları ghostware programlarını yakalamada ve kaldırmada yetersiz kaldı.Karşı teknik kullanan araşlar nispeten başarılıydı fakat bu araşlar sadece enfek-s c s csiyon sonrası kullanılabildiğinden ve sorundan kurtulmak işin herhangi bir yüntemg c oişermediğinden, ghostware programlarının tehlikelerini ve kullanım alanlarını an-c glayarak yeni yakalama teknikleri geliştirilmesi zorunluluğunu güsterdik.Anahtar süzcükler : spyware, ghostware, rootkit.","Spyware is a signiï¬cant problem for most computer users. In public, the termspyware is used with the same meaning as adware, a kind of malicious softwareused for showing advertisements to the user against his will. Spyware programsare also known for their tendency to hide their presence, but advanced stealthtechniques used to be either nonexistent or relatively primitive in terms of eï¬ec-tiveness. In other words, most of the spyware programs were eï¬cient at spyingbut not very eï¬cient at hiding. This made spyware easily detectable with sim-ple ï¬le-scanning and registry-scanning techniques. New spyware programs havemerged with rootkits and gained stealth abilities, forming spyware with advancedstealth techniques. In this work we focus on this important subclass of spyware,namely ghostware. Ghostware programs hide their resources from the Operat-ing System Application Programming Interfaces that were designed to query andenumerate them. The resources may include ï¬les, Windows Registry entries,processes, and loaded modules and ï¬les. In this work, we enumerated thesehiding techniques and studied the stealth detection methodologies. We also in-vestigated the eï¬ectiveness of the hiding techniques against popular anti-virusprograms and anti-spyware programs together with publicly available ghostwaredetection and rootkit detection tools. The results show that, anti-virus programsor anti-spyware programs are not eï¬ective for detecting or removing ghostwareapplications. Hidden object detection or rootkit detection tools can be useful,however, these tools can only work after the computer is infected and they donot provide any means for removing the ghostware. As a result, our work showsthe need for understanding the potential dangers and applications of ghostwareand implementing new detection and prevention tools.Keywords: spyware, ghostware, rootkit, stealth, detection."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË Ë ËşËPARALEL METIN GETIRME ICINËMODELLER VE ALGORITMALARBerkant Barla CambazoğlugBilgisayar Mühendisliği, Yü ksek Lisansu g uTez Yüneticisi: Prof. Dr. Cevdet AykanatoOcak, 2006Son on yılda arama motorları hayatımızla bütü nleşik bir hale gelmişlerdir. Aramauu s smotorları teknolojisi şu anda paralel metin getirmeye dayanmaktadır. Bir par-salel metin getirme sistemi temel olarak uş bileşenden oluşmaktadır: tarayıcı,üc s sindeksleyici ve sorgu işleyici. Tarayıcı bileşeni Ağ'da bulunan sayfaları bulmayı,s s gËgetirmeyi ve yerel bir metin ambarında saklamayı amaşlar. Indeksleme bileşenic ssaklanmış olan dü zensiz metinleri sorgulanabilir bir yapıya dünüştü rü r ki bu yapıs u o us u uşoğu zaman bir ters dizindir. Sorgu işleme bileşeni ise indekslenmiş işerik uzerindecg s s sc üaramayı gerşekleştirir. Bu tezde, etkin Ağ tarama ve sorgu işleme işin modellerc s g s cve algoritmalar onerilmiştir. Paralel Ağ tarama işin, işlemciler arası iletişim mik-ü s g c s starını en aza indiren ve işlemcilerin sayfa indirme isteklerinin sayısını ve saklamasyü klerini dengeleyen karma bir model ünerilmiştir. Ek olarak, metin ve kelimeu o sbazlı ters dizin bülü mleme işin modeller onerilmiştir. Metin bülü mlemeye dayalıou c ü s oumodelimizde saklama yü kü dengelenirken sorgu işleme sırasında karşılaşılacakuu s ssdisk erişim miktarı en aza indirilmektedir. Kelime bülü mlemeye dayalı mod-s ouelimizde ise yine saklama yü kü dengelenirken toplam iletişim hacmi en azauu sindirilmektedir. Bunlara ek olarak, sıralamaya dayalı metin getirme sistem-leri işin şok sayıda sorgu işleme algoritması uygulanmış ve değerlendirilmiştir.cc s s g süOnerilen algoritmalar 48 düğumlü bir PC kü mesi uzerinde şalışmakta olan deney-ug ü u u ü cssel paralel metin getirme sistemimiz Skynet uzerinde denenmiştir. Tezde ayrıcaü sgride uyarlanmış SE4SEE arama motorumuzun tasarım ve uygulama detay-sları tartışılmıştır. Pratikteki katkılarımız arasından, SE4SEE işinde kullanılans s cHarbinger metin sınıï¬andırma sistemi ve onerilen modellerde kullanılmak uzereü ügeliştirilen K-PaToH hiperşizge bülü mleme aracı sunulmuştur.s c ou sAnahtar süzcükler : Arama motoru, paralel metin getirme, Ağ tarama, ters dizinou gbülü mleme, sorgu işleme, metin sınıï¬andırma, hiperşizge bülü mleme.ou s c ouv","ABSTRACTMODELS AND ALGORITHMS FORPARALLEL TEXT RETRIEVALBerkant Barla CambazoğlugPh.D. in Computer EngineeringSupervisor: Prof. Dr. Cevdet AykanatJanuary, 2006In the last decade, search engines became an integral part of our lives. The cur-rent state-of-the-art in search engine technology relies on parallel text retrieval.Basically, a parallel text retrieval system is composed of three components: acrawler, an indexer, and a query processor. The crawler component aims to lo-cate, fetch, and store the Web pages in a local document repository. The indexercomponent converts the stored, unstructured text into a queryable form, mostoften an inverted index. Finally, the query processing component performs thesearch over the indexed content. In this thesis, we present models and algo-rithms for eï¬cient Web crawling and query processing. First, for parallel Webcrawling, we propose a hybrid model that aims to minimize the communicationoverhead among the processors while balancing the number of page download re-quests and storage loads of processors. Second, we propose models for document-and term-based inverted index partitioning. In the document-based partitioningmodel, the number of disk accesses incurred during query processing is minimizedwhile the posting storage is balanced. In the term-based partitioning model, thetotal amount of communication is minimized while, again, the posting storageis balanced. Finally, we develop and evaluate a large number of algorithms forquery processing in ranking-based text retrieval systems. We test the proposedalgorithms over our experimental parallel text retrieval system, Skynet, currentlyrunning on a 48-node PC cluster. In the thesis, we also discuss the design andimplementation details of another, somewhat untraditional, grid-enabled searchengine, SE4SEE. Among our practical work, we present the Harbinger text clas-siï¬cation system, used in SE4SEE for Web page classiï¬cation, and the K-PaToHhypergraph partitioning toolkit, to be used in the proposed models.Keywords: Search engine, parallel text retrieval, Web crawling, inverted indexpartitioning, query processing, text classiï¬cation, hypergraph partitioning.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"A˘g arama motorları, sorgu sonucunda gelen sayfaları sıralamak i¸cin birtakım sıralama y¨ontemleri uygular. SayfaDe˘geri, a˘g sayfalarını A˘g'ın ba˘g yapısına g¨ore sıraya koyan ¨onemli bir y¨ontemdir. SayfaDe˘geri hesaplamasının etkin olması ¨onemlidir, ¸c¨unk¨u A˘g'ın s¨urekli de˘gi¸sen do˘gası bu hesaplamanın sıklıkla tekrarlanmasını gerektirir. SayfaDe˘geri hesaplaması tekrarlayan seyrek matris-vekt¨or ¸carpımları i¸cerir. Matris-vekt¨or ¸carpımı, SayfaDe˘geri hesaplamasının anahtar i¸slemidir. C¸ arpılan matrisin ¸cok b¨uy¨uk olmasından dolayı SayfaDe˘geri genellikle paralel sistemlerde hesaplanır. Fakat bu ¸cok b¨uy¨uk matrisin d¨uzensiz yapısından dolayı SayfaDe˘geri hesaplamasının verimli bir ¸sekilde paralelle¸stirilmesi kolay biri¸s de˘gildir. C¸izge ve hiper¸cizge b¨ol¨umleme y¨ontemleri matris-vekt¨or ¸carpımlarını etkin olarak paralelle¸stirilmesinde sık¸ca kullanılan y¨ontemlerdir. Yakın zamanda matris-vekt¨or ¸carpımından kaynaklanan haberle¸sme y¨uk¨un¨u azaltarak hızlı paralel SayfaDe˘geri hesaplamak i¸cin hiper¸cizge b¨ol¨umleme tabanlı bir y¨ontem ¨one s¨ur¨ulm¨u¸st¨ur. Fakat sunulan y¨ontem y¨uksek ¨on i¸sleme zamanı gerektirir. Bu da y¨ontemi s¨urekli de˘gi¸sen A˘g i¸cin pratikte elveri¸ssiz kılar. Bu ¸calı¸smada, makul bir ¨on i¸slemeyle paralel SayfaDe˘geri hesaplamasının haberle¸sme y¨uk¨un¨u azaltacak A˘g sitesi tabanlı ¸cizge ve hiper¸cizge b¨ol¨umleme modelleri sunuyoruz. Sundu˘gumuz modeller tek boyutlu (satır sıralı ve s¨utun sıralı) ve iki boyutlu (ince taneli ve dama tahtası) b¨ol¨umleme modelleridir. Modeller sadece matris-vekt¨or ¸carpımını kapsamakla kalmayıp, b¨ut¨un dolaylı algoritmayı kapsar. Y¨ur¨ut¨ulen deneyler, sunulan modellerin, ¸su ana kadar yapılan ¸calı¸smalarla kıyaslandı˘gında, d¨u¸s¨uk bir ¨on i¸sleme zamanıyla beraber hızlı SayfaDe˘geri hesaplamasını ba¸sardı˘gını g¨oz ¨on¨une koyar. Anahtar s¨ozc¨ukler : SayfaDe˘geri, Paralel Seyrek Matris-Vekt¨or C¸ arpımı, C¸izge ve Hiper¸cizge B¨ol¨umleme.","Web search engines use ranking techniques to order Web pages in query results. PageRank is an important technique, which orders Web pages according to the linkage structure of the Web. The efficiency of the PageRank computation is important since the constantly evolving nature of the Web requires this computation to be repeated many times. PageRank computation includes repeated iterative sparse matrix-vector multiplications. Due to the enormous size of the Web matrix to be multiplied, PageRank computations are usually carried out on parallel systems. However, efficiently parallelizing PageRank is not an easy task, because of the irregular sparsity pattern of the Web matrix. Graph and hypergraphpartitioning-based techniques are widely used for efficiently parallelizing matrixvector multiplications. Recently, a hypergraph-partitioning-based decomposition technique for fast parallel computation of PageRank is proposed. This technique aims to minimize the communication overhead of the parallel matrix-vector multiplication. However, the proposed technique has a high prepropocessing time, which makes the technique impractical. In this work, we propose 1D (rowwise and columnwise) and 2D (fine-grain and checkerboard) decomposition models using web-site-based graph and hypergraph-partitioning techniques. Proposed models minimize the communication overhead of the parallel PageRank computations with a reasonable preprocessing time. The models encapsulate not only the matrix-vector multiplication, but the overall iterative algorithm. Conducted experiments show that the proposed models achieve fast PageRank computation with low preprocessing time, compared with those in the literature. Keywords: PageRank, Parallel Sparse-Matrix Vector Multiplication, Graph and Hypergraph Partitioning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETü ËOTOMATIK OZET VE ANAHTAR KELIME CIKARMAşGünenş Ercano cBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ilyas Cişeklio c şcEylül, 2006uElektronik dokümanların sayısı arttıkşa, onların bizim ihtiyaşlarımıza olanu c cüyakınlığını ülşebileceğimiz otomatik tekniklere ihtiyaş da artmaktadır. Ozetler,g oc g cËdokümanın kısa ve üz bir sunumu olarak kabul edilebilir. Ideal bir üzet işin,u o o cdokümanın tamamıyla anlaşılması cok ünemlidir. Ancak, bilgisayarların otomatiku s şoolarak dokümanı anlaması imkansız değil ise bile cok zordur. Bunun işin,u g ş cdokümandan ünemli kelime veya cümleleri seşmek ve bunları üzet olarak sun-u o u c omak, otomatik üzet cıkarma araştırmalarında sık kullanılan bir yüntemdir.o ş s oDokümandaki kelime bütünlüğu ünemli kelime veya cümleleri belirlemekteu u u ugü o ukullanılabilir. Kelime zincirleri, kelime bütünlüğunü analiz etmekte kul-u u ug ü ulanılabilecek bir araştır. Bu tezde kelime zincirleri kullanarak, otomatikcüzet ve anahtar kelime cıkarma calışmalarımız anlatılıyor. Bu tezde kelimeo ş şsbütünlüğunün, anahtar kelime bulmadaki etkileri bir üğreticiyle üğrenme pro-u u ug ü u og ogügramı aracılığıyla araştırılıyor. Ozet cıkarma sistemimiz bir dokümanın kelimeg s ş uzincirlerini cıkarıp, konuları kelime zincirlerinden kabaca bulup, yazıyı konuyaşgüre parşalara bülüp, en ünemli parşalardan cümle seşiyor. Anahtar kelimeo c ou o c u cbulma deneylerimizde, kelime bütünlüğunün anahtar kelime bulmanın başarısınıu u ug ü u sg o u us u üarttırdığı gürülmüştür. Ozet cıkarma sistemimiz diğer kelime bütünlüğu kullananş g u u ug üüzet sistemleriyle karşılaştırılınca, iyi sonuşlar almıştır.o ss c süAnahtar süzcükler : Otomatik Ozet Cıkarma, Anahtar Kelime Cıkarma, Kelimeou ş şËsZincirleri, Kelime Bütünlüğu, Doğal Dil Işleme.u u ug ü giv","ABSTRACTAUTOMATED TEXT SUMMARIZATION ANDKEYPHRASE EXTRACTIONGünenş Ercano cM.S. in Computer EngineeringËSupervisor: Asst. Prof. Dr. Ilyas CişeklişcSeptember, 2006As the number of electronic documents increase rapidly, the need for faster tech-niques to asses the relevance of documents emerges. A summary can be consideredas a concise representation of the underlying text. To form an ideal summary, afull understanding of the document is essential. For computers, full understand-ing is diï¬cult, if not impossible. Thus, selecting important sentences from theoriginal text and presenting these sentences as a summary is a common techniquein automated text summarization research.The lexical cohesion structure of the text can be exploited to determine theimportance of a sentence/phrase. Lexical chains are useful tools to analyze thelexical cohesion structure in a text. This thesis discusses our research on auto-mated text summarization and keyphrase extraction using lexical chains. We in-vestigate the eï¬ect of the use of lexical cohesion features in keyphrase extraction,with a supervised machine learning algorithm. Our summarization algorithmconstructs the lexical chains, detects topics roughly from lexical chains, segmentsthe text with respect to the topics and selects the most important sentences. Ourexperiments show that lexical cohesion based features improve keyphrase extrac-tion. Our summarization algorithm has achieved good results, compared to someother lexical cohesion based algorithms.Keywords: Automated Text Summarization, Keyphrase Extraction, LexicalChain, Lexical Cohesion, Natural Language Processing.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETü Ë şü Ë ËSUREKLI AKIS YONLENDIRMESI (SAY): TASARSIZğAGLARDA COKLU ORTAM SUNULARINIşËşË ü Ë ü ËDESTEKLEMEK ICIN YONLENDIRME YONTEMIAhmet KARABilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gTemmuz, 2006Coklu ortam sunuları; sahip oldukları üzel nitelikler ve paketlerdeki ge-ş ocikme, gecikme miktarındaki dalgalanma ve paket kaybı oranı gibi üzel oihtiyaşları nedeniyle, tasarsız ağlarda yünlendirme seviyesinden desteğe ihtiyaşc g o g cduymaktadırlar. Bu tez işerisinde, tasarsız ağlarda şoklu ortam sunularınınc g cbaşarımını artırmak amacıyla gerşeklenmiş, yünlendirme keşif ve bakım mekaniz-s c so smalarından oluşan, SAY yüntemini sunmaktayız. Coklu ortam sunuları uzuns o şümürlerinden dolayı hatalardan uzak dayanıklı yünlendirmelere ihtiyaş duyul-ou o cmaktadır. Yünlendirmelerin dayanıklılığını artırmak işin enerji seviyesindekio g cazalma oranı ve bileşenlerin kalan ümürlerini hesaba katarak en iyi yünlendirmes ou oseşilmektedir. Ayrıca sunduğumuz bakım sistemi sayesinde bir bileşenin enerji-c g ssinin şok azaldığı durumlarda, daha bir hata ortaya şıkmadan yünlendirmelerc g c odeğiştirilerek tehlikeli bileşen dışarıda bırakılmaktadır.gs s s Bileşenlerin kalansËümürlerine bağlı iki eşik değeri tanımlanmıştır. Ilk eşik değerine erişildiğindeou g s g s s g sgbileşenler uzerlerindeki akışların bir kısmını bakıma alarak uzerlerindeki yüküs ü s ü uuazaltmaktadırlar. Bu sayede ağ uzerindeki yükün tüm bileşenlere mümküngü uu u s uuolduğunca eşit dağıtılması da sağlanmaktadır. Bileşenin şok az ümrü kaldığındag s g g s c ou gikinci eşik değerine erişilir ve bileşenin uzerindeki tüm akışlar bakıma alınarak her-s g s s ü u shangi bir akışın kesintiye uğraması engellenir. Yüntemimizi ns2 tabanlı simülatürs g o uouzerinde gerşekleyerek yüntemin doğruluğunu kanıtladık. Ayrıca diğer yüntemlerü c o g g g oile kıyaslayarak yüntemimizin başarısını ülştük. Elde ettiğimiz sonuşlara güreo s oc u g c oSAY, şoklu ortam sunularının başarımını artırmaya destek sağlamaktadır.c s gAnahtar süzcükler : Tasarsız Ağlar, Yünlendirme Bakımı, Coklu Ortam Sunuları.ou g o şiv","ABSTRACTCONTINUOUS FLOW ROUTING (CFR): A WIRELESSAD HOC NETWORK ROUTING PROTOCOL FORSUPPORTING MULTIMEDIA FLOWSAhmet KARAM.S. in Computer EngineeringËSupervisor: Assist. Prof. Dr. Ibrahim Kürpeoğluo gJuly, 2006Multimedia ï¬ows require special routing layer support in ad hoc networksdue their unique characteristics and certain requirements on packet delay, jitter,loss rate and bandwidth. In this thesis we propose a wireless ad hoc routingprotocol (called CFR) with route discovery and maintenance mechanisms, thatis speciï¬cally designed for better supporting multimedia ï¬ows in wireless ad hocnetworks. Since multimedia ï¬ows are long durational, it is important to routethem through stable routes in order to minimize route failures and disturbanceon the ï¬ows. We propose to improve the stability by considering the energydrain rates and estimated remaining lifetimes of nodes while selecting the bestroutes. Additionally we provide a maintenance scheme that acts pro-activelyand re-routes the traï¬c if a node starts getting very low energy. For this wedeï¬ne two thresholds on remaining lifetime of a node. After the ï¬rst threshold isreached, the node re-routes some of its ï¬ows so that the traï¬c load on the nodeis reduced. This helps to an even distribution of traï¬c to nodes of an ad hocnetwork. After the second threshold is reached, the node has very few energyand therefore redirects all its traï¬c to some other routes so that the ï¬ows arenot disturbed by the route failure when the node depletes all of its energy andcan not route anymore. We implemented the protocol as part of an ns2-basedsimulator and proved that it works correctly. Additionally, we compared theprotocol against some other similar protocols. The results show that CFR canindeed help supporting multimedia ï¬ows better.Keywords: Ad-Hoc Networking, Route Maintanence, Multimedia Streaming.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETMUAF KULLANICILARLA EN UYGUN YAYINşË ËSIFRELENMESIMurat AkBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Assist. Prof. Dr. Ali Aydın Selşuko cEylül, 2006uüYayın şifreleme planları bir merkezin şifreli mesajlar yayınlamasını sağlar. Oyles s gki herbir mesajı sadece o mesaj icin belirlenmiş imtiyazlı bir grup deşifre edebilir.s sStandart teknik en başta tüm alıcılara anahtarlar dağıtıp herbir yayında mesajıs u gsadece gerekli anahtarlarla şifrelemektir. Yapılan şifreleme sayısı günderim mas-s s orafını oluşturmaktadır. Bu masraf bakımından şu ana kadar yayınlanmış en iyis s splan Altküme Farkı (AF) planıdır. Bir yayın şifreleme planında imtiyazlı olmayanu sbir takım alıcıların da yayını deşifre edebilmesine izin verilmesi suretiyle günderims omasrafının azaltılması mümkündür. Fakat bunun işin muaf kullanıcıların akıllıcauuu cseşilmesi gerekmektedir. Bu tez AF planında günderim masrafını azaltmak işinc o cizin verilen sayıda muaf kullanıcının etkin olacak şekilde seşilmesi probleminis cühedef almaktadır. Oncelikle uc adet haris algoritma sunuyoruz. Bunlardan birin-üşcisi hızlı calışma süresine, ikincisi daha yavaş calışmasına karşın daha az günderimşs u sş s s omasrafına sahiptir. Sonuncusu ise bu iki unsur arasında dengeyi sağlamaktadır.gDaha sonra belli bir muaf kullanıcı kotası verildiğinde asgari günderim masrafınag oulaşan iki algoritma sunuyoruz. Yürütmüş olduğumuz deneyler güstermektedirs u u us g oki günderim masrafı büyük ülşude azaltılabilmektedir.o u u o cüAnahtar süzcükler : Yayın Sifreleme, Muaf Kullanıcı.şouiv","ABSTRACTOPTIMAL BROADCAST ENCRYPTION WITH FREERIDERSMurat AkM.S. in Computer EngineeringSupervisor: Assist. Prof. Dr. Ali Aydın SelşukcSeptember, 2006Broadcast encryption schemes allow a center to broadcast encrypted mes-sages so that each particular message can only be decrypted by a set of privilegedreceivers designated for it. The standard technique is to distribute keys to all re-ceivers at the beginning and to use only the necessary keys to encrypt the messagefor each particular broadcast. The number of encryptions needed constitutes thetransmission cost in broadcast encryption schemes. The most eï¬cient scheme interms of transmission cost published so far is the subset diï¬erence (SD) scheme. Itis possible to reduce the transmission overhead of a broadcast encryption schemeby allowing a number of free riders to be able to decrypt the message althoughthey are not privileged. However, unless the free riders are chosen cleverly, thecost may even increase. In this thesis, we deal with the problem of choosing agiven number of free riders eï¬ectively to reduce the transmission overhead in SDscheme. We ï¬rst present three greedy algorithms. First algorithm has a fastexecution, the second one is slower but has lower transmission overhead, and thelast one has a trade-oï¬ between running time and transmission cost. Then wepresent two algorithms which ï¬nd the minimum transmission overhead possiblegiven a free rider quota. The experiments we conduct show that the transmissioncost can be signiï¬cantly reduced.Keywords: Broadcast Encryption, Free Riders.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Cağımız toplumları, iletişimi kolaylaştıran büyük ülşekli bilgi ve cağdaş teknolo-jilere sahiptirler. Ancak, bir şahsın cok büyük miktarlardaki verileri tek başınakümeleyip bu verilerden yararlı bilgiler elde etmesi olanaklı değildir. Imecelibilgi sistemleri bir toplumun bilgilerini basit, hızlı ve adil bir şekilde bir arayatoplama cabalarının bütünüdür. Imeceli süzgeşleme, bilgi kaynağının kolaycaayrıştırılamadığı ve geleneksel bilgi süzgeşleme tekniklerinin uygulanmasında zor-luklarla karşılaşıldığı alanlarda başarıyla uygulanmaktadır. Imeceli süzgeşleme,kullanıcılar tarafından oylanan bir madde değerlendirme veri tabanı uzerindecalışmaktadır. Bu yüntemlerin işlemsel karmaşıklıkları tipik ticari uygulamalardamilyonları bulabilecek kullanıcı sayısına doğrusal orantılı olarak artmaktadır. Butür ülşeklenirlik kaygılarını ortadan kaldırmak işin, kullanıcı topaklaması uygu-layan ve topaklanmış ortamlara uydurulmuş belirli evirilmiş dizin yapısında olan(topak atlamalı evirilmiş dizin yapısı da denebilir) verimli imeceli süzgeşlemetekniğini geliştirdik. Bu sistemin üngürücü doğruluğunun topaklama uygu-lanmayan imeceli süzgeşleme algoritmalarıyla aynı ülşekte olmasına rağmenverimliliğinin cok daha iyileştirilmiş olduğunu güsterdik.","Collectively, a population contains vast amounts of knowledge and moderncommunication technologies that increase the ease of communication. However,it is not feasible for a single person to aggregate the knowledge of thousandsor millions of data and extract useful information from it. Collaborative infor-mation systems are attempts to harness the knowledge of a population and topresent it in a simple, fast and fair manner. Collaborative filltering has been suc-cessfully used in domains where the information content is not easily parse-ableand traditional information filltering techniques are difficult to apply. Collabora-tive filltering works over a database of ratings for the items which are rated byusers. The computational complexity of these methods grows linearly with thenumber of customers which can reach to several millions in typical commercialapplications. To address the scalability concern, we have developed an efficientcollaborative filltering technique by applying user clustering and using a specifilcinverted index structure (so called cluster-skipping inverted index structure) thatis tailored for clustered environments. We show that the predictive accuracyof the system is comparable with the collaborative filltering algorithms withoutclustering, whereas the efficiency is far more improved."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETPLANSIZ PEER-TO-PEER SISTEMLERDE ?ROTAü GRENME? YONTEMI ILE SORGULAMA YUKUNUNğ ü üüüOAZALTıLMASıSelim CıracışBilgisayar Mühendisliği, Yüksek Lisansu g uË üuTez Yüneticileri: Assist. Prof. Dr. Ibrahim Kürpeoğlu, Prof. Dr. Ozgü r Ulusoyo o gAğustos, 2005gGnutella gibi belli bir planı olmayan peer-to-peer (P2P) sistemlerde, akranlarsorgu mesajlarını kaynak tutucularına sel baskını yüntemi ile P2P ağı uzerindeno güaktarır. Fakat bu operasyon cok verimli değildir, cunkü sel baskını yüntemi akran-ş g şü u oügların bant genişliği gibi kaynaklarını gereksiz yere harcamaktadır. Orneğin, birsgakranın bir sorgu mesajına cevap verecek kaynağa sahip olmadığı, veya o kaynağag g ggidecek yolu bilmediği zaman, bu sorgu mesajının rotasını belirlemede gürev al-g omasına gerek yoktur.Anlamsal rota belirleme tekniği sorgu mesajlarını sadece cevaplarınggelebileceği akranlara yollamaya calışan bir mekanizmadır. Bu tezde plansızg şsP2P sistemlerde sorgulama yü kü nü ?Parzen Windows? sınıï¬andırma algorit-uuumalarını kullanarak azalatmaya calışan ve bir anlamsal rota belirleme sistemişsü grenme) sistemini sunuyoruz. Sunduğumuz bu sis-olan ?Route Learning? (Rota Oğ gtemde, akranlar sorgu mesajlarını cevapların yü ksek olasılıkla gelebileceği komşuu g sakranlara iletirler. Bu sayede, bir sorgu mesajı bir akranın, sel baskını tekniğindegolduğu gibi, tüm komşularına değil, sadece bu komşuların bir kısmına aktarılır.g u s g sEğer bir akranın komşularının gelen bir sorguya cevap veremeyecekleri tah-g smin edilirse, o sorgu mesajı düşurü lü r. Sistemimiz ayrıca sorgulardaki anahtarusü u usüzcü k değişikleri gibi dinamik yapılar ile başa cıkabilecek mekanizmalara sahip-ou gs sştir. Büylece sistemimiz daha onceden gürmediği sorguların bile rotasını tahmino ü o gedebilir. Sistemimiz uc fazdan oluşmaktadır; bunlar: ogrenme, değerlendirme veüş s üğ gyeniden ogrenmedir. Yaptığımız testler sonucunda sistemimizin sorgularda bantüğ ggenişliği kullanımını, kullanıcı memnuniyetini cok düşurmeden, yüksek düzeydesg ş usü u uazalttığı gürü lmüştü r.g o u us uiv","ABSTRACTREDUCING QUERY OVERHEAD THROUGH ROUTELEARNING IN UNSTRUCTURED PEER-TO-PEERNETWORKSSelim CıracışM.S. in Computer EngineeringË üuSupervisors: Assist. Prof. Dr. Ibrahim Kürpeoğlu, Prof. Dr. Ozgü r Ulusoyo gAugust, 2005In unstructured peer-to-peer networks, such as Gnutella, peers propagate querymessages towards the resource holders by ï¬ooding them through the network.This is, however, a costly operation since it consumes node and link resourcesexcessively and most of the time unnecessarily. There is no reason, for example,for a peer to receive a query message if the peer does not own any matchingresource or if the peer is not on the path reaching to a peer holding a matchingresource.Semantic routing is a technique that tries to forward the queries only to thosenodes where replies are likely to come from. In this thesis, we present ?RouteLearning?, a semantic routing scheme, which aims to reduce query traï¬c in un-structured peer-to-peer networks by utilizing a well-known estimation techniquecalled ?Parzen Windows?. In Route Learning, peers try to ï¬nd the most likelyneighbors through which replies can be obtained for submitted queries. In thisway, a query is forwarded only to a subset of the neighbors of a peer, or is droppedif no neighbor, which is likely to return a reply, is found. The scheme has alsomechanisms to cope with variations in user submitted queries, like changes inthe keywords. This way the scheme can also evaluate the route for a query forwhich it is not trained. The proposed scheme consists of three phases: training,evaluation, and recursive learning. The last phase enables the scheme to adaptitself to changes in a dynamic peer-to-peer network. Our simulation results showthat our scheme reduces the bandwidth overhead signiï¬cantly without scarifyinguser satisfaction compared to a pure ï¬ooding based querying approach.Keywords: Peer-to-peer query routing, Parzen Windows estimation, querycaching, distributed hash tables."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GUAP - GSM İÇİN KUVVETLİ KULLANICI ASUXAMA PROTOKOLÜ Özer Aydemir Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Ali Aydın SELÇUK Bilgisayar Mühendisliği Bölümü, Bilkent Üniversitesi, Ankara Ocak, 2005 Hücresel telefon ağlan için asıllama sistemleri geleneksel olarak kullanıcı asıllaması yerine sistemin kullanımı üzerine kısıtlamalar ve sınırlar getiren cihaz asıllamalarını kullanmak üzere tasarlanmışlardır. Bu tezde biz kullanıcı asıllamasını GSM'e uygulayan bir protokol tasarladık. Bizim protokolümüz zayıf kullanıcı şifrelerinin kullanılmasına müsaade etmekte ve GSM kullanıcılarına çeşitli esneklikler sağlamaktadır. GUAP ve literatürdeki ana kullanıcı asıllaması protokollerinin similasyonları da aynı zamanda bu tezde yer almaktadır. Protokol aynı zamanda ufak bir değişiklikle çalınmalara karşı hesapların güvenliğini sağlayabilmektedir. Anahtar Kelimeler: Kablosuz ağ güvenliği, kullanıcı asıllaması, GSM, kuvvetli şifre protokolleri çalınmaya karşı dayanıklılık 111","ABSTRACT GUAP - A STRONG USER AUTHENTICATION PROTOCOL FOR GSM Özer Aydemir M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Ali Aydın SELÇUK January, 2005 Traditionally, the authentication protocols for cellular phone networks have been designed for device authentication rather than user authentication, which brings limitations and restrictions on the functionality of the system. in this thesis we propose a user authentication protocol for GSM (Global System for Mobile) based cellular phone netvvorks. Our protocol permits the use of weak secrets (e.g. passvvords ör PINs) for authentication and provides certain flexibilities for GSM users. The simulation results on currently established user authentication protocols and GUAP are presented. Our proposal also has a capture resilience extension to disable captured cellular phones securely. Keywords: vvireless network security, user authentication, GSM, strong password protocols, capture resilient"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET CONTEXTPROXY: AĞ TABANLI BAĞLAMDAN- HABERD AR HİZMETLERİ VE UYGULAMALARI DESTEKLEMEK İÇİN YERDEN-HABERDAR HTTP PROXY SUNUCUSU Alper R. Uluçmar Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Asst. Prof. Dr. David Davenport Ocak, 2005 Gündelik yaşamımıza hızla giren bilgi işleme ve haberleşme yeteneğine sahip taşınabilir cihazlar, bizlere sadece her an her yerde hesaplama yapabilme ve haberleşebilmeden daha fazlasını vaat etmektedirler. Hızla gelişen ve gün geçtikçe yaşamımıza daha derinden nüfuz eden bu altyapı bileşenleri tamamen yeni etkileşim yöntemlerini ve uygulamaları mümkün hale getirmişlerdir. Bu yeni uygulama ve hizmet sağlama yaklaşımlarından bir tanesi de bağlamdan-haberdar uygulamalardır. Bağlamdan- haberdar uygulamalar, kullanıcılarına sağladıkları hizmetleri ve bilgiyi kullanıcılarının içlerinde bulundukları bağlama göre devingen olarak uyarlayabilen uygulamalardır. Bağlamdan-haberdar uygulamaları desteklemek için gerekli altyapısal bileşenlerin hızla yaygınlaşmalarına karşın bu uygulamalarda rol alacak çeşitli bileşenlerin birbirleriyle etkileşimlerinde kullanılacak kabul görmüş yaklaşımlar ve standartlar henüz mevcut değildir. Bu çalışmamızda ContextProxy adını verdiğimiz yaygın olarak kullanılan Symbian düzleminde çalışan yerden-haberdar bir HTTP proxy sunucusu geliştirdik. ContextProxy' nin istemci uygulama tarafından standart bir HTTP proxy sunucusu olarak algılanmasına karşın esas işlevi, istemciye ait istekleri hizmet sağlayıcıya aktarırken kullanıcının bağlamından çıkardığı yer bilgisiyle zenginleştirmesidir. Böylelikle halihazırda kullanılmakta olan uygulamalar veya yeni geliştirilecek olanlar standart bir HTTP proxy sunucusu kullanmaya ayarlanabilir oldukları sürece yerden-haberdar hale gelebilmektedirler. Anahtar sözcükler. Bağlamdan-haberdar programlama, Symbian, Bluetooth, GSM iv","ABSTRACT CONTEXTPROXY: A LOCATION-AWARE HTTP PROXY SERVER TO SUPPORT WEB BASED CONTEXT-AWARE SERVICES AND APPLICATIONS Alper R. Uluçınar M.S. in Computer Engineering Supervisor: Asst Prof. Dr. David Davenport January, 2005 The pervasion of computing in our physical world promises more than the ubiquitous availability of computing resources; totally new and exciting interaction schemes are to be explored. Context-awareness, one of the most important aspects of ubiquitous computing, enables applications that make use of their users' context to provide dynamically adapting information and services to their users or to other applications. Although the technological infrastructure to support ubiquitous and context-aware applications is being deployed rapidly, the standards and the best practices for the interactions of various components in a context-aware application are still missing. In our work we have developed a location-aware HTTP proxy server, called ContextProxy that runs on the popular Symbian platform. ContextProxy acts as a standard HTTP proxy server from the client application's perspective but it augments the service request of the client with the available location information while submitting the request to the service provider. This allows the existing nomadic applications to immediately become location- aware if they can be configured to make use of a standard HTTP proxy which is a common scheme for web based applications. And also it is possible to write new nomadic applications without considering the context-awareness aspect at the service requestor level. The contextual information added by ContextProxy can then be utilized by the service provider to dynamically adapt its services according to the service requestor's context. Keywords: Context- Aware/Nomadic Computing, Symbian, Bluetooth, GSM m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Hücresel işlemler canlıların en alt seviyedeki donanımlarıdır. Bu düzeydekiu s ubozukluklar halihazırda tedavi edilemeyen pek şok hastalıktan sorumludur.cHücresel işlemler hakkındaki bilgilerimiz hızla artmaktadır. Ancak, günümüzu s uu umoleküler biyolojisi bu işlemleri kurallı bir şekilde güsterecek yüntemlerden yok-u s s o osundur. Bilgi dağarcığının büyük bir kısmı bilimsel yazında, bilgisayarlı mod-g g uuelleme ve şüzümlemeye uygun olmayan bir bişimde durmaktadır. Cüzülmeyeco u c şo uşalışılan sistemin karmaşıklığı güzününe alındığında, uygun bir güsterim sistemics s g oou g ove araşlarına duyulan ihtiyac aşıktır.c cBu şalışmada hücresel sistemleri modellemek icin bir ontoloji üneriyoruz. Bucs u oontoloji, soyutlamaları, şoklu ayrıntı düzeylerini, hücresel bülmeleri ve molekülc u u o uhallerini güsterebilmek gibi tekil üzelliklere sahiptir. Ayrıca tümleme, sorgu-o o ulama, şüzümleme ve gürselleştirme kolaylığı gibi birtakım kullanıcı ve sistemco u o s gihtiyaşlarını karşılamak uzere tasarlanmıştır.c s ü sBu ontolojiyi taban alarak bir dizi yazılım aracı geliştirdik. Patikasaraşlarının temel kullanım hedeï¬eri tümleme, gürselleme ve sorgulamadır. Buc u ohedeï¬erde elde ettiğimiz tatmin edici sonuşların ontolojinin kullanılabilirliğinig c gdoğruladığını düşunüyoruz.g g usü uHalihazırdaki yolak güsterme ve sorgulama aracları ile karşılaştırıldığındao ss gPatika düzenli bir güsterim sistemi, ileri sorgulama yüntemleri, aşık gürsellemeu o o c oarabirimi gibi avantajlara sahiptir. Bunun yanısıra Patika'dan elde edilen mod-eller, akım analizi ya da yolak etkinliği şıkarımı gibi yüntemlerle şüzümlenebilir.gc o co uAnahtar süzcükler : Biyoenformatik, ontoloji, yolak.ouvi","Cellular processes form the hardware layer of living organisms. Malfunctions incellular processes are responsible for most of the currently incurable diseases. Notsurprisingly, knowledge about cellular processes are growing at an enormous rate.However, today?s molecular biology suï¬ers from lack of a formal representationsystem for cellular processes. Most of the knowledge is locked in literature, thatare not accessible to computational analysis and modeling. Given the complexityof the system we are attacking, the need for a representation system and modelingtools for cellular processes are clear.In this dissertation, we describe an ontology for modeling processes. Ourontology possesses several unique features, including ability to represent abstrac-tions and multiple levels of detail, cellular compartments and molecular states.Furthermore, it was designed to meet several user and system requirements, in-cluding ease of integration, querying, analysis and visualization.Based on this ontology we also implemented a set of software tools withinthe Patika project. Primary use cases of Patika are integration, querying andvisualization, and we have obtained satisfactory results proving the feasibility ofour ontology.Compared with existing alternative methods of representing and querying in-formation about cellular processes, Patika provides several advantages, includinga regular representation system, powerful querying options, an advanced visual-ization. Moreover Patika models can be analyzed by computational methodssuch as ï¬ux analysis or pathway activity inference. Although it has a more steeplearning curve compared to existing ad hoc representation systems, we believethat tools like Patika will be essential for molecular biology research in thefuture.ivvKeywords: bioinformatics, ontology, pathway."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Son zamanlardaki coklu ortam bilgi artışı, bu bilgileri saklama tekniklerininş sgelişmesine yol aşmıştır. Coklu ortam veritabanları, ülşeklenebilir olma ve or-s cs ş octam üzelliklerine güre sorgulanabilme üzellikleri ile bu tekniklerin en popülerio o o uolmuşlardır. Bu veritabanlarının bir kütü yanı ise saklama ve sorgulamadan ünces ou oortamların üzelliklerinin ayrıştırılması işin işlenmesi gereksinimidir. Sürekli artano s cs uortamlar yığını bu işlenmenin elle yapılmasını zorlaştırmaktadır. Gürüntü Veri-g s s ou utabanı sistemimiz BilVideo işin de durum büyledir. Nesne bulma ve takip etmec ogibi bilgisayarlı gürüş tekniklerindeki gelişmeler bu işlemenin otomatik olarako us s syapılmasına olanak vermektedir. Bu tezde, gürüntüden nesnelerin yerleşim-ou u szaman üzelliklerini otomatik olarak ayrıştırmak işin bir araş ünerilmektedir.o s c coüOnerilen sistem, gürüntü işleme işin geliştirilen ülşeklenebilir mimariyi ve resimou u s c s ocsınırı bulma, nesne bulma ve takip etme işin kullandığımız yüntemi kapsamak-c g oütadır. Resim sınırı bulmak işin renk histogramı kullanılmıştır. Onerilen sistemc sbulunan resim sınırları işerisinde ünemli nesneleri renk bülgelerine ve kamera odakc o otahminine güre bulmaktadır. Bulunan nesneler yerlerine, şekillerine ve tahminio shızlarına güre takip edilmektedir.oAnahtar süzcükler : Video veritabanları, gürüntüde nesne bulma, nesne takipou ou uetme, odak tahmini.i","Recently, the increase in the amount of multimedia data has unleashed the devel-opment of storage techniques. Multimedia databases is one of the most popularof these techniques because of its scalability and ability to be queried by themedia features. One downside of these databases is the necessity for processingof the media for feature extraction prior to storage and querying. Ever growingpile of media makes this processing harder to be completed manually. This is thecase with BilVideo Video Database System, as well. Improvements on computervision techniques for object detection and tracking have made automation of thistedious manual task possible. In this thesis, we propose a tool for the automaticdetection of objects of interest and deriving spatio-temporal relations betweenthem in video frames. The proposed framework covers the scalable architecturefor video processing and the stages for cut detection, object detection and track-ing. We use color histograms for cut detection. Based on detected shots, thesystem detects salient objects in the scene, by making use of color regions andcamera focus estimation. Then, the detected objects are tracked based on theirlocation, shape and estimated speed.Keywords: Video Databases, Video Object Detection, Object Tracking, CameraFocus Estimation.i"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Seyrek matris-vektür carpımı (MxV) bir cok bilimsel hesaplama uygula-oş şmasının cekirdeğini oluşturmaktadır. Dolayısıyla, MxV carpımlarının par-ş g s şalelleştirilmesi, bilimsel hesaplama cevrelerinin ünem verdiği bir konudur. Bus ş o gkonuda yapılmış calışmalar yü k dengelemeye ve toplam haberleşme hacminisş s u sazaltmaya odaklanmıştır. Bu tezde, toplam haberleşme sayısının da ünemlis s oolabileceği güsterilmiştir.go s Ayrıca, işlemcilere düşen en bü yü k haberleşmes us uu shacminin ve sayısının niceliğinin de ünemli olabileceği güsterilmiştir. Bu dürtg o go s ohaberleşme ülşutü nü n azaltılmasını sağlayacak hiperşizge modelleri ve bu mod-s o cü u u g cellerin bülü mlenmesini sağlayacak yüntemler ünerilmiştir. Bu ünerilen model-ou g o o s olerin ve yüntemlerin, tek boyutlu ve iki boyutlu matris bülü mlendirilmesindeo ounasıl kullanılacağı güsterilmiştir. MxV işleminin en cok kullanıldıgı yer lineergo s s şsistem cüzü mlemelerinde kullanılan dolaylı yüntemlerdir. Bu dolaylı yüntemlerşo u o ocoğu zaman matris iyileştirme teknikleri kullanırlar. Matrislerin yaklaşık ters-şg s sleriyle iyileştirme tekniği, bir cok simetrik ve simetrik olmayan matris ceşitlerines g ş şsuygulanabilen ve cokşa kullanılan bir tekniktir. Bu teknik, temel olarak, MxVşcişleminin yerine ardışık MxV işlemlerini koyar. Yani, bir MxV işlemi, matrislerins s s syaklaşık tersleriyle iyileştirme tekniğini kullanan dolaylı yüntemlerde daha bü yü ks s g o uubir hesaplama işleminin sadece kü cuk bir parşasıdır. Ardışık MxV carpımlarınıns uşü c s şarasında etkileşim vardır. Bu etkileşimler, verimli paralelleştirme işin matris-s s s clerin bir arada bülü mlendirilmesini zorunlu kılmaktadır. Bu tezde, bir aradaoubülü mlendirmenin, değişik dolaylı yüntemler işin değişik matris bülü mlendirmeou gs o c gs oumodellerine yol aştıgı güsterilmiştir. Sıkşa kullanılan bir cok dolaylı yünteminc o s c ş ohangi matris bülü mlendirme modelleriyle paralelleştirilebileceği güsterilmiştir.ou s go sBu matris bülü mlendirme modellerinin elde edilmesini sağlamak işin, üncedenou g c oünerilmiş hiperşizge modellerini birleştirerek bileşik hiperşizge modelleri geliştireno s c s s c sviviiişlemler tanımlanmıştır. Bileşik hiperşizge modellerinin bülü mlenmesi ile ma-s s s c outrislerin bir arada bülü mlendirilebileceği güsterilmiştir. Yukarıda bahsedilenou go scalışmaların pratikte işe yarayıp yaramadıklarını gürmek işin, paralel MxVşs s o cişlemini yapan bir program yazdık. Bu programla yaptığımız deneyler sırasında,s gdaha genel bir paralel program sınıfının calışma sü resinin günder işlemlerininşs u o ssırasına bağlı olduğunu gürdü k. En iyi günder işlemi sırasının bazı varsayımlarg g ou o saltında nasıl bulunabileceğini güsterdik.g oAnahtar süzcükler : Seyrek matrisler, paralel matris-vektür carpımı, dolaylıou oşyüntemler, matris iyileştirme, matrislerin yaklaşık tersleriyle iyileştirme,o s s shiperşizge bülü mleme.c ou","parse matrix-vector multiply (SpMxV) operations are in the kernel of manyscientiï¬c computing applications. Therefore, eï¬cient parallelization of SpMxVoperations is of prime importance to scientiï¬c computing community. Previousworks on parallelizing SpMxV operations consider maintaining the load balanceamong processors and minimizing the total message volume. We show that the to-tal message latency (start-up time) may be more important than the total messagevolume. We also stress that the maximum message volume and latency handledby a single processor are important communication cost metrics that should beminimized. We propose hypergraph models and hypergraph partitioning methodsto minimize these four communication cost metrics in one dimensional and twodimensional partitioning of sparse matrices. Iterative methods used for solvinglinear systems appear to be the most common context in which SpMxV operationsarise. Usually, these iterative methods apply a technique called preconditioning.Approximate inverse preconditioningâwhich can be applied to a large class ofunsymmetric and symmetric matricesâreplaces an SpMxV operation by a se-ries of SpMxV operations. That is, a single SpMxV operation is only a piece of alarger computation in the iterative methods that use approximate inverse precon-ditioning. In these methods, there are interactions in the form of dependenciesbetween the successive SpMxV operations. These interactions necessitate parti-tioning the matrices simultaneously in order to parallelize a full step of the subjectclass of iterative methods eï¬ciently. We show that the simultaneous partitioningrequirement gives rise to various matrix partitioning models depending on theiterative method used. We list the partitioning models for a number of widelyused iterative methods. We propose operations to build a composite hypergraphby combining the previously proposed hypergraph models and show that par-titioning the composite hypergraph models addresses the simultaneous matrixpartitioning problem. We strove to demonstrate how the proposed partitioningivvmethodsâboth the one that addresses multiple communication cost metrics andthe other that addresses the simultaneous partitioning problemâhelp in practice.We implemented a library and investigated the performances of the partitioningmethods. These practical investigations revealed a problem that we call messageordering problem. The problem asks how to organize the send operations to min-imize the completion time of a certain class of parallel programs. We show howto solve the message ordering problem optimally under reasonable assumptions.Keywords: Sparse matrices, parallel matrix-vector multiplication, iterative meth-ods, preconditioning, approximate inverse preconditioner, hypergraph partition-ing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETüü Ë şË ş Ë ËşË şËYONSUZ BILESIK CIZGELER ICIN YERLESIMËALGORITMASIErhan GiralBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Yard. Doş. Dr. Uğur Doğrusüzo c g goAğustos, 2005gCizge yerleşimi bilgi gürselleme alanındaki onemli bir problemdir. Bütün veriyeş s o ü uudayalı cizge tabanlı bilgi gürselleme sistemleri bir ozdevimli geometri yaratmaş o üdüzeneğine ihtiyaş duymaktadır. Cunkü geometri bilgisi coğunlukla modellenenu g c şü u şgbilgide bulunmaz. Bu nedenle cizge yerleşim probleminin etraï¬ıca incelenmesineş srağmen bileşik cizgeler durumu aynı kapsamda araştırılmamıştır. Bu calışmadag sş s s şsyünsüz bileşik cizgeler işin yeni bir yerleşim algoritması sunulmaktadır. Al-ou sş c sgoritma, geleneksel güce-dayalı yerleşim şablonunu esas almakta ve iş işelik,u s s ccdeğişebilir dügum şekli ve muhtemel diğer uygulamaya ozel kısıtları halledebile-gs uğü s g ücek şekilde geliştirmektedir. Deneysel sonuşlar hesaplama zamanı ve geneldes s ckabul edilen yerleşim niteliği aşısından algoritmanın son derece başarılı olduğunus gc s gortaya koymaktadır. Algoritma, bir yolak bütünleştirme ve analiz araş takımıuu s colan Patika işin de başarılı bir şekilde gerşekleştirilmiştir. Patika son derecec s s c s skarmaşık yolak bilgisini gürsellemektedir ve birşok değişik ceşit biyolojik yolağıs o c gs ş s ggürselleyebilmek işin, alansal kısıtlar ve rastgele iş işelik ilişkileri işermektedir.o c cc s cAnahtar süzcükler : Gürselleme, Cizge Gürselleme, Cizge Cizimi, Cizge Yerleşimi,ou o ş o ş ş ş sGüce-dayalı Cizge Yerleşimi, Bileşik Cizgeler.u ş s sşi","ABSTRACTA LAYOUT ALGORITHM FORUNDIRECTED COMPOUND GRAPHSErhan GiralM.S. in Computer EngineeringSupervisor: Assist. Prof. Dr. Uğur Doğrusüzg goAugust, 2005Graph layout is an important problem in information visualization. All data-driven graph-based information visualization systems require some sort of anautomatic geometry generation mechanism, as it is generally not directly availablefrom the data being modeled. This is why graph layout problem has been studiedextensively. However, for the case of compound graphs, there are still importantgaps in this area. We present a new, elegant algorithm for undirected compoundgraph layout. The algorithm is based on the traditional force-directed layoutscheme with extensions to handle nesting, varying node sizes, and possibly otherapplication-speciï¬c constraints. Experimental results show that the executiontime and quality of the produced drawings with respect to commonly acceptedlayout criteria are quite satisfactory. The algorithm has also been successfullyimplemented as part of a pathway integration and analysis toolkit named Patikafor drawing complicated biological pathways with compartmental constraints andarbitrary nesting relations to represent molecular complexes and various types ofpathway abstractions.Keywords: Visualization, Graph Visualization, Graph Drawing, Force DirectedGraph Layout, Compound Graphs.i"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Maddelerin kristal parametrelerinin belirlenmesi, kristallograï¬ de onemli birükonudur. Kristal parametrelerinin bilinmesi, maddelerin ï¬ziksel ozelliklerininüanlaşılmasına yardımcı olur. Karmaşık yapılı maddelerde, ozellikle eËer maddes s ü gglobal simetri yanında lokal simetri de barındırıyorsa, kristal parametrelerininbelirlenmesi oldukşa zor olabilir. Bu calışmada, primitif vektürler, temelc şs ovektürler ve uzay grubu gibi kristal parametrelerini, kristal yapısını oluşturano satomların koordinat bilgilerini kullanarak belirleyecek bir araş ortayacşıkarılmıştır. Ayrıca, kristalleri incelemeye yarayan bir gürüntüleme aracı dac s ou usunulmuştur. Dolayısıyla, bu calışma, kristal bilimcilere, madde bilimcilere ves şskimyacılara, kristal yapılarını daha verimli bir şekilde analiz etmeyi saËlayans gfaydalı bir araş sunmaktadır.c","Determining crystal structure parameters of a material is a quite importantissue in crystallography. Knowing the crystal structure parameters helps tounderstand physical behavior of material. For complex structures, particularlyfor materials which also contain local symmetry as well as global symmetry,obtaining crystal parameters can be quite hard. This work provides a tool thatwill extract crystal parameters such as primitive vectors, basis vectors andspace group from atomic coordinates of crystal structures. A visualization toolfor examining crystals is also provided. Accordingly, this work presents auseful tool that help crystallographers, chemists and material scientists toanalyze crystal structures eï¬ciently."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Insan Genome Proje'sinin tamamlanmasının ardından, biyolojik sistem veri-lerinde ustsel bir artış oldu. Fazla sayıdaki ontoloji, standart ve araş yaratabilmeü s ccşabalarına rağmen, mevcut biyo-enformatik yapısı bu verilerin karmaşıklığı ileg sgmücadele etmekten şok uzaktır. Patika Projesi topluluğa hücresel işlemleri mod-u c gu sellemek, şüzümlemek ve birleştirmek işin tümleşik bir ortam sağlamayı amaşlar.co u s c u s g cPatika projesi Patika veritabanındaki veriye ulaşılmasını, şüzümlenmesinis co uve gürüntülenmesini sağlayan yazılım araşları sağlamaktadır.ou u g c g Bu tezde,Patikaweb isimli ağ tabanlı, kullanışlı, kayıt, yükleme gerektirmeyen yazılımg s uaracının tasarım ve uygulanmasını sunmaktayız. Gelişmiş veri şüzümlemess co uolanağını sağlamak işin, Patikaweb iki seviyede şoklu gürüntü olanağı, molekülerg g c c ou u g ukomplekslerin, yolakların ve kara kutu reaksiyonların gürüntülenmesi işin bileşikou u c sşizge sağlamakta ve kompartmanları desteklemektedir.c gSorgulama bileşeni, geribildirimli düngüleri, ortak hedef ve düzenleyici veyas ou ukullanıcının ilgili genine dayalı ilgili alt şizgeleri bulmak işin hem SQL benzeric csorguları hem de şizge kuramsal sorgu dizilerini destekler. Oluşturulan mod-c seller XML formatında saklanabilir; BioPAX, SBML gibi standart bişimlere ak-ctarılabilir veya sabit gürüntülere şevirilebilir. Patikaweb fazla etkileşimli veou u c skullanışlı sorgulama arayüzünü destekler.s uuuYolak araştırmalarında, karmaşık bilginin gürsel olarak güsterimi kritik birs s o orol oynamaktadır. Kullanışlı bir arayüz oluşturmaya şalışırken bilgi doğrus u s cs golarak, mümkün olan en geniş kapsamla güsterilmelidir. Bu tezde, aynı zamandauu s okarmaşık yolak bilgisini gürselleştirmede ontoloji ve şizgesel güsterimden kay-s o s c onaklanan sınırlamalarla mücadele eden yeni bir yaklaşım sunmaktayız.u sPatikaweb'in benzersiz gürselleştirme ve sorgulama olanakları ile, şu andao s siiiiimevcut araşlar ve veritabanları havuzunda ünemli bir boşluk doldurmaktadır.c o sAnahtar süzcükler :ou Biyo-enformatik, yolak gürsellenmesi, karmaşık gürüntüo s ou uidaresi, ağ hizmeti.g","After completion of Human Genome Project, there has been an exponential in-crease in the available biological data. Although there has been an enormous ef-fort for creating ontologies, standards and tools, current bioinformatics infrastruc-ture is far from coping with this data. The Patika Project aims to provide thecommunity an integrated environment for modeling, analyzing and integratingcellular processes.Patika project develops software tools providing access, visualization andanalysis on the data in Patika database. In this thesis, we present analysis, de-sign and implementation of Patikaweb, a Web-service having a user-friendly in-terface without requiring any registrations, installations. To achieve an enhanceddata analysis , Patikaweb provides a multiple-view schema , compartments andcompound graphs for visualizing molecular complexes, pathways and black-boxreactions.Querying component supports SQL-like queries and an array of graph-theoretic queries for ï¬nding feedback loops, common targets and regulators, orinteresting subgraphs based on user?s genes of interest. Constructed models canbe saved in XML, exported to standard formats such as BioPAX, SBML or con-verted to static images. A highly interactive and user friendly querying interfaceis supported with Patikaweb.Visual representation of complex information in pathway research is very im-portant. The information should be presented with high coverage, while providinga user friendly interface. In this thesis we also present a new approach to visualizecomplex pathway information coping with the limitations introduced by ontologyand graphical representation.iiiiiPatikaweb ?s unique visualization and querying features ï¬ll an important gapin the pool of currently available tools and databases.Keywords: Bioinformatics, pathway visualization, complex view management,Web service."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETğ ËşËPARALEL AG TARAYICILARI ICIN SAYFA ATAMAü ËYONTEMLERIAta Tü rkuBilgisayar Mü hendisliği, Yü ksek Lisansu g uTez Yüneticisi: Prof. Dr. Cevdet AykanatoEylü l, 2004uOn yıldan kısa bir sü re işerisinde, Web (World Wide Web), bir araştırma pro-uc sjesinden, toplumumuzun her yü zü nde etkili, kü ltü rel bir fenomene dünüşmüştü r.uu uu o us us uË sËInternetin popü laritesindeki ve kullanımındaki artış, Internette bilgi aramayıusağlayan tekniklerin etkinliklerinde de bir artışa neden olmuştur. Ağ taramag s s gbu tü r tekniklerden birisidir. Bir ağ tarayıcı, genellikle arama motorlarını veu gağ depolarını beslemek işin Web sayfalarını indiren ve kaydeden bir programdır.g cBir ağ tarayıcısının faydalı olabilmesi işin, kısa bir sü re işerisinde yü ksek miktar-g c uc ularda bilgiyi tarayabilmesi gerekmektedir. Genellikle, etkin bir tarama işin gereklicolan yü ksek indirme hızlarına tek işlemcili sistemlerde erişilinemez. Bu yü zden,u s s ugü nü mü zdeki bü yü k caplı uygulamalar, ağ tarama problemini cozmek işin cokuu u uu ş g şü cşişlemcili paralel sistemleri kullanırlar. Paralel ağ tarama, eşit yü k dağıtımı ves g s u ghaberleşme hacminin ya da mesaj sayısının azaltılması gibi bilinen problemlerinsyanında, cakışmaların onlenmesi ve yü ksek kalitedeki sayfaların erken taranmasışs ü ugibi problemlerin de cozü mü nü gerektirir. Bu tez, ağ tarama işleminin par-şü u u u g salelleştirilmesi konuludur ve temel olarak ana katkısı paralel ağ tarayıcılarındas gsayfaların işlemcilere atanması işlemindedir. Bu tezde, cizge ve hiper-şizges s ş cmodellerini bülü mlemeye dayanan, iki yeni sayfa atama yüntemi onermekteyiz.ou o üYüntemlerimiz, toplam haberleşme hacmini ve toplam mesaj sayısını azaltırken,o sişlemci başına düşen depolama yü kü nü ve taranması gereken sayfa miktarını den-s s us uuugelemektedir. Tez sırasında onerdiğimiz modeller uygulamaya dünüştü rü lmüş veü g o us u u usteorik yaklaşımlarımızın doğruluğu deneysel sonuşlarla kanıtlanmıştır. Ayrıcas g g c sonerilen yüntemleri kullanan etkin bir ağ tarama programı yazılmıştır.ü o g sAnahtar süzcükler : Paralel ağ tarama, cizge bülü mleme, hiper-şizge bülü mleme,ou g ş ou c ousayfa atama.iv","ABSTRACTPAGE-TO-PROCESSOR ASSIGNMENT TECHNIQUESFOR PARALLEL CRAWLERSAta Tü rkuM.S. in Computer EngineeringSupervisor: Prof. Dr. Cevdet AykanatSeptember, 2004In less than a decade, the World Wide Web has evolved from a research projectto a cultural phenomena eï¬ective in almost every facet of our society. The increasein the popularity and usage of the Web enforced an increase in the eï¬ciency ofinformation retrieval techniques used over the net. Crawling is among such tech-niques and is used by search engines, web portals, and web caches. A crawler is aprogram which downloads and stores web pages, generally to feed a search engineor a web repository. In order to be of use for its target applications, a crawlermust download huge amounts of data in a reasonable amount of time. Gener-ally, the high download rates required for eï¬cient crawling cannot be achievedby single-processor systems. Thus, existing large-scale applications use multipleparallel processors to solve the crawling problem. Apart from the classical paral-lelization issues such as load balancing and minimization of the communicationoverhead, parallel crawling poses problems such as overlap avoidance and earlyretrieval of high quality pages. This thesis addresses parallelization of the crawl-ing task, and its major contribution is mainly on partitioning/page-to-processorassignment techniques applied in parallel crawlers. We propose two new page-to-processor assignment techniques based on graph and hypergraph partitioning,which respectively minimize the total communication volume and the number ofmessages, while balancing the storage load and page download requests of proces-sors. We implemented the proposed models, and our theoretic approaches havebeen supported with empirical ï¬ndings. We also implemented an eï¬cient parallelcrawler which uses the proposed models.Keywords: Parallel crawling, graph partitioning, hypergraph partitioning, pageassignment.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu şalışmada, doğrusal eşitsizlikler sistemine dünüştürülmüş olan gürüntücs g s o us u u us ou uonarımı problemi uzerinde durulmuştur. Bu yüntemle elde edilen matrisler belliü s obir yapısal dizilime sahip olmayan seyrek matrislerdir. Ayrıca, küşuk ülşekliuc ü o cgürüntülerde dahi şok büyük ülşekli matrisler oluşmaktadır. Dolayısıyla, prob-ou u c u u oc slemin şozümünde, büyük ülşekli problemler işin verimli şalışabilen ve paralelcü u u u u oc c csügerşekleştirmelere uygun olan aracı kısıtlar yüntemleri kullanılmıştır. Onerilenc s o syüntemler arasından, sağlanmayan kısıtların tümünü dikkate alan ve her adımdao g u uutek bir izdüşum gerşekleştiren temel yüntem ve sağlanmayan kısıtların altus ü c s o gkümelerini dikkate alıp oluşan izdüşumlerin dışbükey birleşimini alan paralelu s usü su syüntem kullanılmıştır. Ceşitli bülümleme stratejileri ve farklı iletişim model-o s şs ou sleri kullanılarak bir şok paralel gerşekleştirimler yapılmıştır. Hiper-şizge temellic c s s cbülümlemeler kullanılarak iletişim maliyeti azaltılırken işlemciler arasındaki yükou s s udengesi sağlanmıştır. Gerşekleştirimler yineleme bazında ve toplam bazdag s c sdeğerlendirilmiştir. Aynı zamanda, bülümlemelerin yakınsama hızına olan etkisig s ouaraştırılmıştır. Deney sonuşları, ünerilen paralel yüntemlerin, gürüntü onarımıs s c o o ou uprobleminde ve doğrusal eşitsizlikler sistemine şevrilebilen gerşek uygulamalardag s c cpratik kullanımı olduğunu güstermiştir.Anahtar süzcükler : Paralel gürüntü onarımı, bozunum, paralel algoritmalar, li-ou ou uneer ï¬zibilite, aracı kısıtlar yüntemi, hiper-şizge parşalama, sırasal parşalama,o c c cdamatahtası parşalama, ince tane parşalama, noktasal iletişim, herkes-herkesec c siletişim, yakınsama hızı.s","In this thesis, we are concerned with the image restoration problem which hasbeen formulated in the literature as a system of linear inequalities. With this for-mulation, the resulting constraint matrix is an unstructured sparse-matrix andeven with small size images we end up with huge matrices. So, to solve therestoration problem, we have used the surrogate constraint methods, that canwork eï¬ciently for large size problems and are amenable for parallel implemen-tations. Among the surrogate constraint methods, the basic method considers allof the violated constraints in the system and performs a single block projectionin each step. On the other hand, parallel method considers a subset of the con-straints, and makes simultaneous block projections. Using several partitioningstrategies and adopting diï¬erent communication models we have realized severalparallel implementations of the two methods. We have used the hypergraph par-titioning based decomposition methods in order to minimize the communicationcosts while ensuring load balance among the processors. The implementationsare evaluated based on the per iteration performance and on the overall perfor-mance. Besides, the eï¬ects of diï¬erent partitioning strategies on the speed ofconvergence are investigated. The experimental results reveal that the proposedparallelization schemes have practical usage in the restoration problem and inmany other real-world applications which can be modeled as a system of linearinequalities.Keywords: Parallel image restoration, distortion, parallel algorithms, linear feasi-bility, surrogate constraint method, hypergraph partitioning, rowwise partition-ing, checkerboard partitioning, ï¬ne-grain partitioning, point-to-point communi-cation, all-to-all communication, convergence rate."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË Ë ËË ËË Ë Ë ü ËMIRAS BILGI SISTEMLERININ ILGIYE-YONELIKËş Ë Ë ËGELISTIRIMIYasemin SatıroğlugBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Prof. Dr. H.Altay Güveniro uAğustos, 2004gMiras sistem, birşok yıl ünce geliştirilen ve bir kuruluşta kullanılmaya de-c o s svam edilen sistemdir. Yazılım gereksinimleri değiştikşe miras sistemler de uy-gs cgun olarak geliştirilmelidir. Miras sistemlerin bakımı icin sarma, taşıma ves syeniden geliştirme gibi birşok yüntem ünerilmiştir. Maalesef, bu yüntemler teks c o o s obir bileşende yakalanması güş, ve birşok bileşeni enine kesme eğiliminde olans uc c s güzellikleri aşıkşa güz ününde bulundurmamışlardır. Dağıtım, eş zamanlama, de-o c c o ou s g svamlılık, güvenlik, kayıt tutma ve gerşek zaman davranışı, enine kesen üzelliku c s oürnekleri arasındadır. Bu üzelliklerin enine kesme niteligi miras sistemlerino obakımını ciddi anlamda karmaşıklaştırır, şunkü, sistemin kodunun birden fazlas s cü uyerde değiştirilmesini gerektirir ve geleneksel bakım teknikleri bu işlemi etkiligs solarak gerşekleştirmede yetersiz kalmaktadır.c sËIlgiye-Yünelik Yazılım Geliştirme enine kesen üzellikler ile başa şıkmak işino s o sc cËkesin mekanizmalar saglar. Fakat geşerli Ilgiye-Yünelik Yazılım Geliştirmec o steknikleri, esas olarak, sıfırdan geliştirilen yazılım sistemleri işerisindeki enines ckesen üzellikler ile baş etmek uzerine odaklanmış durumdadır. Bu sistemlerdeo s ü senine kesen üzellikler başlangışta birer ilgi olarak gerşekleştirilerek tek bir bileşeno s c c s sişerisine yerleştirilebilir. Bu şekilde, enine kesen üzelliklerin gerşekleştirim vec s s o c sbakımı büyük şapta düzenlenebilir, ki bu da sistemin ilerideki bakımını ko-uu c ulaylaştıracaktır. Ne yazık ki, miras sistemler daha sert gereksinimler yüklerler,s uşunkü miras sistemlerde enine kesen üzellikler ünceden aşık olarak tanımlanamazcü u o o cve düzenlenemez. Bununla beraber, enine kesen üzellikler ile baş etmek işin uygunu o s ctekniklerin eksikliği miras sistemlerin bakımını şarpıcı bir bişimde engeller.g c cBu tezde, miras sistemlerin analizi işin sistematik bir süreş tanımlanmaktadır.c ucË ËIlgiye-Yünelik Miras Analiz Süreci isimli bu süreş, Olurluk Analizi, Ilgiye-Yüneliko u uc oAnaliz ve Bakım Analizi olmak uzere uş alt süreşten olusur. Herbir alt süreş, birü üc uc ucvviËbuluşsal kurallar kümesi ve bunlara ilişkin kontrol mekanizmasından oluşur. Ikis u s saşamadan oluşan Olurluk Analizi, birinci aşamada miras sistemlerin kategoriza-s s ssyonu ile ilgili kuralları, ikinci aşamada da miras sistemlerin, statik ve dinamiksenine kesme gerşekleştirim yeteneğine güre değerlendirilmesi ile ilgili kurallarıc s g o gËtanımlar. Ilk aşamada tanımlanan kurallar, miras sistemler hakkında derinleme-ssine bir şalışma sonrasında tanımladığımız miras sistem kategorilerine dayan-cs gËmaktadır. Ikinci aşamada tanımlanan kurallar da bu kategorilerin enine kesmesgerşekleştirimi uzerine yaptığımız tartışmaya dayanmaktadır. Miras sistem kate-c s ü g sËgorize edilip enine kesme gerşekleştirimine güre değerlendirildikten sonra, Ilgiye-c s o gYünelik Analiz, miras sistemdeki ilgilerin teşhis edilmesi ve belirtilmesi ile ilgilio sËkuralları tanımlar. Bakım Analizi, Olurluk Analizi ve Ilgiye-Yünelik Analiz altosüreşlerinin sonuşlarına dayanarak miras sistem işin uygun bakım yaklaşımınınuc c c sseşimi ile ilgili kuralları tanımlar.cBu alt süreşler, herbir alt süreşle ilgili kuralları gerşekleştiren, ve buuc uc c sşekilde, miras sistemin bakımını yapan kişiye, miras sistemin analizi ve uyguns sËbakım yaklaşımının belirlenmesinde yardım sağlayan Ilgiye-Yünelik Miras Anal-s g oizi Aracı'nda gerşekleştirilmiştir.c s sËAnahtar süzcükler : Miras Bilgi Sistemleri, Ilgiye-Yünelik Yazılım Geliştirme,ou o sBuluşsal Kural Modellemesi.s","ABSTRACTASPECT-ORIENTED EVOLUTION OF LEGACYINFORMATION SYSTEMSYasemin SatıroğlugM.S. in Computer EngineeringSupervisor: Prof. Dr. H. Altay GüveniruAugust, 2004A legacy information system is an old system that typically has been developedseveral years ago, and remains in operation within an organization. Since the soft-ware requirements change, legacy systems must be evolved accordingly. Variousapproaches such as wrapping, migration and redevelopment have been proposedto maintain legacy information systems. Unfortunately, these approaches havenot explicitly considered the concerns that are diï¬cult to capture in single com-ponents, and tend to crosscut many components. Examples of such crosscuttingconcerns include distribution, synchronization, persistence, security, logging andreal-time behavior. The crosscutting property of concerns seriously complicatesthe maintenance of legacy systems because the code of the system needs to bechanged at multiple places, and conventional maintenance techniques fall shortto do this eï¬ectively.Aspect-Oriented Software Development (AOSD) provides explicit mechanismsfor coping with these crosscutting concerns. However, current AOSD approacheshave primarily focused on coping with crosscutting concerns in software systemsthat are developed from scratch. Hereby, the crosscutting concerns are imple-mented as aspects at the beginning, hence localized in single modules. In thisway the implementation and maintenance of crosscutting concerns can be pre-pared to a large extent so that the maintenance of these systems will be easierlater on. Unfortunately, legacy systems impose harsher requirements, becausecrosscutting concerns in legacy systems are neither explicitly identiï¬ed nor havebeen prepared before.We provide a systematic process for analyzing the impact of crosscutting con-cerns on legacy systems. The process, which is called Aspectual Legacy AnalysisProcess (ALAP), consists of three sub-processes, Feasibility Analysis, AspectualiiiivAnalysis and Maintenance Analysis. All the three sub-processes consist of a set ofheuristic rules and the corresponding control. Feasibility Analysis, which consistsof two phases, describes rules for categorizing legacy systems, in the ï¬rst phase;and describes the rules for evaluating legacy systems with respect to the abilityto implement static crosscutting and ability to implement dynamic crosscutting,in the second phase. The rules of the ï¬rst phase are based on the categories oflegacy systems that we have deï¬ned after a thorough study to legacy informationsystems, and the rules of the second phase are based on our discussion of thesecategories with respect to crosscutting implementation. Once the legacy systemhas been categorized and evaluated with respect to crosscutting implementation,the Aspectual Analysis sub-process describes rules for identifying and specifyingaspects in legacy systems. Based on the results of the Feasibility Analysis andAspectual Analysis sub-processes, the Maintenance Analysis describes the rulesfor the selection of the appropriate legacy maintenance approach.ALAP has been implemented in the Aspectual Legacy Analysis Tool (ALAT),which implements the rules of the three sub-processes and as such helps to sup-port the legacy maintainer in analyzing the legacy system and identifying theappropriate maintenance approach.Keywords: Legacy Information Systems, Aspect-Oriented Software Development,Heuristic Rule Modelling."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETTÜRKÇE VAROLUŞSAL CÜMLELERİNDESTEK VEKTÖR MAKİNELERİ KULLANILARAKANLAMBİLİMSEL ARGÜMAN SINIFLANDIRILMASI VEANLAMBİLİMSEL GRUPLANMASIAylin KocaBilgisayar Mühendisliği, Yüksek LisansTez Yöneticisi: Prof. Dr. Varol AkmanEylül 2004Bütün doğal diller üç çeşit cümleden oluşur: fiil cümleleri (ör. ?Ben kitabı okudum.?),isim cümleleri (ör. ?Kitap masanın üzerinde.?), ve varoluşsal cümleler (ör. ?Masanınüzerinde kitap var.?). Bu cümle çeşitlerinin sözdizimsel ve anlambilimsel tanımalarıbilişimsel dilbilim için çok önemli olduğu halde, bununla ilgili yapılmış belli başlı birçalışma bulunmamaktadır. Bu tez, varolan sözkonusu açığı kısmen de olsa kapatmakamacıyla, Türkçe varoluşsal cümlelerin anlambilimsel tanınması ve sınıflamasıüzerinedir. Türkçe varoluşsal cümleler, asgari olarak var ve yok işlevsel sözcükleriyleıralanır. Bu işlevsel sözcüklerin, varlık bildiren en temel anlamlarının dışında, başkaanlamları da mevcuttur. Bunları, örneğin, sahiplik veya hâl/durum bildirenler olaraksınıflamak mümkündür. Sistemimiz öncelikle, destek vektör makineleri yardımıyla,varoluşsal cümlelerin yüklem ve diğer öğeleri arasındaki ilişkileri tanımlamak içinkelimeleri baz alan sığ anlambilimsel ayrıştırmasını yapmaktadır. Bunu takiben decümlelerin anlambilimsel gruplamasını gerçekleştirmektedir. İlk işlem için aldığımızdoğruluk, ve ikinci işlem için aldığımız duyarlık/geri çağırma sonuçları oldukça umutvericidir. Bu çalışmamızın bir katkısı da ODTÜ-Sabancı Türkçe Ağaç YapılıDerlemi'nin bir kısmının anlambilimsel bilgi ile etiketlendirilmesi olmuştur.Anahtar sözcükler: sığ anlambilimsel ayrıştırma, anlambilimsel rol etiketlendirilmesi,anlambilimsel roller, destek vektör makineleri, Türkçe varoluşsal cümleler, Türkçe ağaçyapılı derlem.iv","ABSTRACTSEMANTIC ARGUMENT CLASSIFICATION ANDSEMANTIC CATEGORIZATION OFTURKISH EXISTENTIAL SENTENCES USINGSUPPORT VECTOR LEARNINGAylin KocaM.S. in Computer EngineeringSupervisor: Prof. Dr. Varol AkmanSeptember, 2004There are three types of sentences that form all existing natural languages: verbalsentences (e.g. ?I read the book.?), copulative sentences (e.g. ?The book is on thetable.?), and existential sentences (e.g. ?There is a book on the table.?). Syntactic andsemantic recognition of these sentence types are crucially important in computationallinguistics although there has not been any significant work towards this end. Thisthesis, in an attempt to fill this evident gap, is on identifying and assigning semanticcategories of Turkish existential sentences in print. Existential sentences in Turkish areminimally characterized by the two existential particles var, meaning there is/are, andyok, meaning there is/are no. In addition to these most basic meanings, other senses ofexistential particles are possible, which can be categorized into groups such as caseexistentials and possession existentials. Our system does shallow semantic parsing indefining the predicate-argument relationships in an existential sentence on a word-by-word basis, via utilizing Support Vector Machines, after which it proceeds with thesemantic categorization of the whole sentence. For both of these tasks, our systemproduces promising results, in terms of accuracy and precision/recall, respectively. Partof this research contributes to the annotation of the METU-Sabancı Turkish Treebankwith semantic information.Keywords: shallow semantic parsing, semantic role labeling, thematic roles, supportvector machines, Turkish existential sentences, Turkish Treebank.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË Ë Ë Ë ËBIR VIDEO VERITABANI SISTEMINDE ANLAMSALSORGULARIN CALISTIRILMASIş şCemil ALPERBilgisayar Mühendisliği, Yüksek Lisansu g uü ur Ulusoy veTez Yüneticileri: Prof. Dr. OzgüoAssist. Prof. Dr. Uğur Güdükbayg uuAğustos, 2004gBu tezde, BilVideo isimli video veritabanı yünetim sistemine anlamsal sorgulamaoyeteneğini kazandırılmıştır. Calışmamız, videoların anlamsal işeriğinin şıkartılıpg s şs cg csaklanması işin tanımlanmış olan video veri modeline ve video verisini sorgula-c smayı destekleyen sorgu diline sahip olan bir şalışmaya dayanmaktadır. Anlamsalcssorguların gürsel ve yazılı olarak girilmesini desteklemesi işin BilVideo'nun Inter-o cnet tabanlı arayüzünde de değişiklikler yapılmıştır.uu gs sAnahtar süzcükler : video veritabanları, anlamsal video modelleme, video veri-oulerinden şıkarımlar yapma, video verilerini anlamsal sorgulama, anlamsal sorgu-cların şalıştırılması.csv","ABSTRACTSEMANTIC QUERY EXECUTION IN A VIDEODATABASE SYSTEMCemil ALPERM.S. in Computer EngineeringüuSupervisors: Prof. Dr. Ozgür Ulusoy andAssist. Prof. Dr. Uğur Güdükbayg uuAugust, 2004In this thesis, we have extended a video database management system, calledBilVideo, with semantic querying capability. Our work is based on a video datamodel for the extraction and storage of the semantic contents of videos, and aquery language to support semantic queries on video data. The Web based queryinterface of BilVideo has also been modiï¬ed to handle semantic queries bothvisually and textually.Keywords: video databases, semantic video modeling, annotation of video data,semantic querying of video data, semantic query execution.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ivÖZETTÜRKÇE SANAL KLAVYELERİNPERFORMANS OPTİMİZASYONUSinan UşşaklıBilgisayar Mühendisliği, Yüksek LisansTez Yöneticisi: Prof. Dr. Bülent ÖzgüçMayıs, 2004Tablet ve el bilgisayarları için en önemli sorunlardan biri metin girişidir. Butezde, Türkçeyi analiz ederek ODTÜ Derleminden ve ek$i sözlükten oluşturduğumuz ikideğişik çift yönlü grafik değerleri oluşturulmaktadır. Fitts'in kanunu ile insan elininhareketlerinin nasıl modelleneceğini açıklanmakta, Metropolis algoritması ile Kernighanve Lin'in buluşsalından nasıl en iyi sanal Türkçe klavyenin oluşturulacağıanlatılmaktadır. Kernighan ve Lin'in buluşsalının Metropolis algoritmasına göreavantajları tartışılmaktadır.Anahtar Sözcükler: Sanal klavye tasarımı, Metropolis algoritması, KL buluşsalı Türkçeklavye, el bilgisayarları, çift yönlü grafikler","iiiABSTRACTPERFORMANCE OPTIMIZATION OFTURKISH VIRTUAL KEYBOARDSSinan UşşaklıM. S. in Computer EngineeringSupervisor: Prof. Dr. Bülent ÖzgüçMay, 2004Text input is one of the major problems of handheld and tablet computers. In thisthesis we analyze Turkish language to create different digraph tables from Middle EastTechnical University Turkish Corpus and ek$i sözlük. We describe how to model humanhand movement with Fitts? Law, and using it, we describe two different quantitativedesign methods: Metropolis Algorithm and Kernighan and Lin?s heuristic to create thebest virtual keyboard layout for Turkish. We discuss why Kernighan and Lin?s heuristicis preferable for our problem.Keywords: Virtual keyboard design, Metropolis Algorithm, KL heuristic, Turkishkeyboard, handheld computers, digraphs"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETğ üş Ë ËKABLOSUZ ALGILAYICI AGLARINDA GUC VERIMLIË ğşVERI TOPLAMA VE YIGISIMIüuHüseyin Ozgür TANuBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gOcak, 2004ËsIşlemci, hafıza ve radyo teknolojilerindeki son gelişmeler, herhangi bir alan-sdan kullanışlı bilgileri toplamak işin konuşlandırılan kablosuz algılayıcı ağlarınıns c s ggeliştirilmesini mümkün kılmıştır. Bu tür ağlarda, algılanan veriler toplandıktans uu s ugsonra, kullanıcıların sorgu yapabilecekleri bir baz istasyonuna iletilmeleri gerek-mektedir. Genel olarak bu tür ağlar düşuk maliyetli ve kısıtlı pil gücü olanug us ü uudügumlerden oluştuğundan, ağ ümrünün uzatılabilmesi işin enerjiyi verimli biruğü sg go u u cbişimde harcayan metodları kullanmak zorundadırlar.cHer bir iletişim turunda bütün düğumlerin baz istasyonuna günderecek verisis u u ug ü oolduğu ortamlarda, sistemin ümrünün en yüksek değerine ulaşabilmesi işin sistemg o uu u g s ctarafından bir turda harcanan toplam enerjinin mümkün olduğunca azaltılmasıuu goldukşa ünemlidir. Eğer bir yandan tur başına harcanan toplam enerji miktarıco g sveri yığışım ve kaynaşım teknikleri ile azaltılırken, aynı zamanda dügum başınags s uğü sdüşen enerji tüketimi dengelenebilirse, sistem ümrü aşısından en iyiye yakın birus u o ucveri toplama ve yol belirleme yüntemi elde edilmiş olur.o sSu ana kadar, sistem ümrünü artıran farklı yol belirleme protokolleriş o uuünerilmiştir. Bu tez şalışmasında, PEDAP ve PEDAP-PA adlarında en kısa kap-o s cssayan ağaş tabanlı iki yeni yol belirleme metodu ünerilmiştir. Bunlardan birigc o sdiğerinin güş-haberdar halidir. Yaptığımız benzetim sonuşları protokollerimizin,g uc g cbaz istasyonunun hem uzakta hem de alanın ortasında olduğu sistemlerde şok iyig cşalıştığını güstermiştir.c sg o sAnahtar süzcükler : Algılayıcı Ağları, Yol Atama, Güş Verimliliği, Veri Toplama,ou g uc gVeri Yığışımı.gsiv","ABSTRACTPOWER EFFICIENT DATA GATHERING ANDAGGREGATION IN WIRELESS SENSOR NETWORKSüuHüseyin Ozgür TANuM.S. in Computer EngineeringËSupervisor: Asst. Prof. Dr. Ibrahim Kürpeoğluo gJanuary, 2004Recent developments in processor, memory and radio technology have enabledwireless micro-sensor networks which are deployed to collect useful informationfrom an area of interest. The sensed data must be gathered and transmitted to abase station where it is further processed for end-user queries. Since the networkconsists of low-cost nodes with limited battery power, power eï¬cient methodsmust be employed for data gathering and aggregation in order to achieve longnetwork lifetimes.In an environment where each of the sensor nodes has data to send to a basestation in a round of communication, it is important to minimize the total energyconsumed by the system in a round so that the system lifetime is maximized.A near optimal data gathering and routing scheme can be achieved in terms ofnetwork lifetime, while minimizing the total energy per round with the use ofdata fusion and aggregation techniques, if power consumption per node can bebalanced as well.So far, diï¬erent routing protocols have been proposed to maximize the lifetimeof a sensor network. In this thesis, we propose two new protocols PEDAP (PowerEï¬cient Data gathering and Aggregation Protocol) and PEDAP-PA (PEDAP-Power Aware), which are minimum spanning tree based routing schemes, whereone of them is the power-aware version of the other. Our simulation results showthat our protocols perform well both in systems where base station is far awayfrom and where it is in the center of the ï¬eld.Keywords: Sensor Networks, Routing, Power Eï¬ciency, Data Gathering, DataAggregation.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VERİMLİ BLUETOOTH SERPME AĞ OLUŞTURMA Tağmaç Topal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. ibrahim Körpeoğlu Ocak, 2004 Çeşitli kısa mesafeli kablosuz ağ teknolojileri arasında, Bluetooth hem kul lanıcılardan hem de üreticilerden belirgin bir ilgi görüyor. Bluetooth, kablo suz kişisel alan ağları destekleyen ana teknoloji durumunda. Teknolojiyi geliştirmek ve tanıtmak amacıyla kurulan Bluetooth Special Interest Group (SIG), Bluetooth standartlarının belirtimlerini üretmekte. Bluetooth standart ları, değişen boyutlarda Bluetooth ağları, serpme ağlar (scatternet), oluşturmak için gerekli yapı taşlarını belirtiyor ama bu serpme ağların oluşturulmasında kul lanılabilecek politika ve algoritmaları belirtmiyor. Bluetooth serpme ağlarının biçimlendirilmesinde, aynı düğüm kümesinden farklı ilingelerin elde edilmesini sağlayacak değişik yaklaşımlar olabilir. Tez çalışmamızda, ilk olarak farklı serpme ağ biçimlendirme algoritmalarının çıktısı olan serpme ağ ilingelerini değerlendirmek için bazı başarım metrikleri tanımladık. Daha sonra, serpme ağ kurulmasında kullanıcı trafik desenlerini (düğümlerin birbirleri arasındaki bilgi trafiği gereksinimleri) dikkate: almasıyla diğer algoritmalardan ayrılan yeni bir Bluetooth serpme ağ oluşturma algorit ması geliştirdik. Ardından, benzetimlerle, oluşturulan serpe ağların özelliklerini gözlemleyerek algoritmamızın başarımmı inceledik. Algoritmamızın ürünü olan bir serpme ağda özellikle, trafik akışlarının izlediği en kısa ağırlıklı ortalamalı yol uzunluklarına, tatmin edilen kullanıcı oranına ve tahsis edilen trafik kapasitesi oranına baktık. Sonuçlar bize, algoritmamızı kullanarak yüksek oranlarda kul lanıcı tatminine, yüksek oranlarda kapasite tahsisine ve düşük değerlerde en kısa ağırlıklı ortalamalı yol uznuklarına ulaşabileceğimizi gösterdi. Şu anda, algorit mamız merkezi yapıda ama gelecekte dağıtık yapıya dönüştürülebilir. Anahtar sözcükler: Bluetooth, Serpme Ağ Oluşturma, Verimli İlingeler, Kişisel Kablosuz iletişim Ağları, Trafik Gereksinimleri.","ABSTRACT CONSTRUCTING EFFICIENT BLUETOOTH SCATTERNETS Tağmaç Topal M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Ibrahim Körpeoğlu January, 2004 Among various technologies for short-range wireless networking, Bluetooth has received a particular attention from users as well as from vendors. It is the main technology that supports wireless personal area networking. Bluetooth Special Interest Group (SIG), which is a consortium established to develop and promote the technology, produces the specifications of Bluetooth standards. The Bluetooth standards specify the building blocks to construct Bluetooth networks of arbitrary size, i.e. scatternets, but they do not specify the policies and algo rithms that can be used in constructing these scatternets. There may be various approaches for forming Bluetooth scatternets which will result in different topolo gies for the same set of nodes. In this thesis, we first define and provide some performance metrics that can be used to evaluate various scatternet topologies that can be the output of different scatternet formation algorithms. Then, we provide a new Bluetooth scatternet construction algorithm that differs from other algorithms in that it also considers the traffic pattern of users (i.e. traffic requirements of nodes among themselves) in establishing a scatternet. Then we evaluate the performance of our algorithm through simulations by observing the properties of the constructed scatternets. In a scatternet that is the result of our algorithm we particularly look to the weighted average shortest path lengths that traffic flows follow, the ratio of satisfied users, and the utilization of the scatternet capacity. The results show that we can achieve a good ratio of satisfied users, a high network utilization, and a reasonably small value for average path lengths using our algorithm. The algorithm is currently centralized, but can be extended to a distributed one in the future. mIV Keywords: Bluetooth, Scatternet Construction, Efficient Topologies, Wireless Personal Area Networks, Traffic Requirements."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZETELEKTRONİK DEVLETTE GENİŞLETİLEBİLİRBİÇİMLEME DİLİ (XML) : BAZI ÖRNEK SENARYOLARAyışığı B. SevdikBilgisayar Mühendisliği, Yüksek LisansTez Yöneticisi: Prof. Dr. Varol AkmanOcak, 2004Son on ila yirmi yılda bilişim teknolojisi, Internet'in hayatımızın vazgeçilmez birerparçası olmasından da anlaşılabileceği gibi, muazzam bir şekilde gelişmiştir. Internet'inhayatımızın her alanına girmesiyle enformasyon ve bilgi çağında yaşadığımızsöylenmektedir. Fakat bilgiyle diğerlerine göre daha düzenli şekilde uğraşmasınarağmen bir kurum var ki, bilişim teknolojilerindeki bu gelişmelerdenfaydalanmamaktadır. Çalışma çarkının her evresinde veri ile uğraşan ve yeni bilgi üretenbu kurum devlet kurumudur. Bilişim teknolojisindeki gelişmelerin devlete uygulanmasıkarşımıza ?elektronik devlet? ya da kısaca ?e-devlet? kavramını çıkarmaktadır.Gelişmekte olan bilişim teknolojilerinden biri de Genişletilebilir Biçimleme Dili(XML)'dir. XML'in ortaya çıkması platformdan-bağımsız, makine tarafındanokunabilir, yapısal veri değişimi sağlayarak, Web adını verdiğimiz evrensel bilgihavuzuna hem bir bilgi boyutu, hem de servis boyutu getirmiştir. Hızlı bir şekildeWeb'in veri değişim standardı haline gelmesiyle, iş ve araştırma çevreleri halihazırdaXML'i ve bağıntılı Web servislerini kullanmaktadır. O halde devleti elektronik devletedönüştürmek için neden yeni bilişim teknolojileri arasından XML seçilmesin?XML, devletten-devlete hizmetlerde birlikte-işlerlik, devletten-vatandaşa hizmetlerdedaha vatandaş-yanlısı bir bakış açısı ve daha kolay, platformdan-bağımsız erişim, vedevletten-iş dünyasına hizmetlerde verimlilik sağlamak için kullanılabilir. Böylelikle,geleneksel devleti daha üretken, kağıtsız elektronik devlete dönüştürür. Bu araştırmadaXML'in ?elektronik devlet? kavramına uygulanması incelenmektedir. Bazı örnek e-devlet senaryoları ve bunların uygulanmasında XML'in nasıl kullanılabileceği, XML'ine-devlette kullanımının avantaj ve dezavantajları tartışılarak sunulmaktadır.Anahtar sözcükler: Genişletilebilir Biçimleme Dili (XML), Elektronik Devlet (e-devlet),Web servisleri.v","ABSTRACTEXTENSIBLE MARKUP LANGUAGE (XML) INELECTRONIC GOVERNMENT: SOME EXEMPLARYSCENARIOSAyışığı B. SevdikM.S. in Computer EngineeringSupervisor: Prof. Dr. Varol AkmanJanuary, 2004In the last decade or two, information technology (IT) has evolved remarkably as can beperceived by the fact that the Internet has become an indispensable part of our lives.With the advent of the Internet in almost every aspect of our lives, we are said to beliving in the information and knowledge age. However, there is one institution that is notmaking use of these developments in IT, despite the fact that it is the one which dealswith information more regularly than any other. This institution that handles data andproduces new information in every aspect of its work cycle is the government. Hence,applying the advances in information technology to the government, brings before us theconcept of ?electronic government? or as it is briefly called ?e-government?.One of the evolving information technologies is the Extensible Markup Language(XML). The advent of XML has brought both a knowledge dimension and a servicedimension to the universal information repository we call the Web, enabling platform-independent, machine-readable, structured data exchange. As it has quickly become thedata exchange standard of the Web, the business and research communities are readilymaking use of XML and related Web services. Why not pick XML amongst the newinformation technologies to transfer the government into an e-government?XML can be used to achieve interoperability in government-to-government services, amore citizen-centric approach and easier, platform-independent access in government-to-citizen services, and efficiency in government-to-business services; thus,transforming the traditional government into a more productive, paperless electronicgovernment. In this research we explore the application of XML to the ?e-government?concept. We present some exemplary e-government scenarios and show how XML canbe used to implement them as we discuss the advantages and disadvantages of usingXML in e-government.Keywords: Extensible Markup Language (XML), Electronic Government (e-government), Web services.iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETüş ËË ËüUC BOYUTLU GIYSI TASARIM VE SIMULASYONË ËSISTEMIFunda DurupınarBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Asst. Prof. Dr. Uğur Güdükbayo g uuTemmuz, 2004Bu tez şalışmasında, uc boyutlu bir sanal giysi tasarım ve simülasyon sistemics üş uütanıtılmıştır. Onerilen sistem, uş boyutlu bir giysinin, giysiyi oluşturan pan-s üc sellerden yapılandırılmasına olanak sağlamaktadır. Panellerin temelini bir yay-gparşacık modeli oluşturmaktadır. Giysinin oluşturulması, kesme, dikme, şevresinic s s cdüzeltme ve büyütme/küşultme aşamalarından geşerek gerşekleştirilmektedir.u uu uc ü s c c sDaha sonra gerşek hayatta bir terzinin atülyesindeki olduğu gibi bir mankenc o guzerinde prova yapmak mümkündür. Kumaşın değişik şevresel koşullardakiü uuu s gs c sdavranışı, ï¬ziksel bir yaklaşım izlenerek gerşekleştirilmiştir. Giysilerin salınımınıns s c s syanısıra, verimli ve gerşekşi gürüntülenmesi de kumaş modellemede ünemli birc c ou u s okonudur. Kumaşların değişik materyal tipleri ve yansıma üzellikleri vardır. Sis-s gs otemimizde ürgü, dokuma ve standart boyama yüntemleri gibi şeşitli materyalou o csüzellikleri ve boyama seşenekleri gerşekleştirilmiştir. Programın performanso c c s ssonuşları tezde sunulmuştur.c sAnahtar süzcükler : giysi tasarımı, giysi simülasyonu, kumaş boyama, ï¬zikselou u smodelleme.iv","ABSTRACTA 3D GARMENT DESIGN AND SIMULATIONSYSTEMFunda DurupınarM.S. in Computer EngineeringSupervisor: Asst. Prof. Dr. Uğur Güdükbayg uuJuly, 2004In this thesis study, a 3D graphics environment for virtual garment design andsimulation is presented. The proposed system enables the three dimensionalconstruction of a garment from its two dimensional cloth panels, for which theunderlying structure is a mass-spring model. Construction of the garment is per-formed through cutting, boundary smoothing , seaming and scaling. Afterwards,it is possible to do ï¬tting on virtual mannequins like in the real life as if in a tai-lor?s workshop. The behavior of cloth under diï¬erent environmental conditions isimplemented applying a physically-based approach. As well as the simulation ofthe draping of garments, eï¬cient and realistic visualization of garments is an im-portant issue in cloth modelling. There are various material types and reï¬ectanceproperties for fabrics. We have implemented a number of material and renderingoptions such as knitwear, woven cloth and standard shading methods such asGouraud shading. Performance results of the system are presented at the end.Keywords: garment design, garment simulation, fabric rendering, physically-based modeling.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ivÖZETPROTEİN-PROTEİN ETKİLEŞİMLERİNİNKORUNMUŞ BÖLGE BENZERLİĞİNE GÖREÖBEKLENMESİAslı AyazBilgisayar Mühendisligi, Yüksek LisansTez Yöneticisi: Yard. Doç. Dr. Uğur DoğrusözAğustos, 2004Protein protein etkileşimleri (PPE) sinyal iletimi, transkripsiyonel düzenleme vemetabolizma gibi pek çok hücresel işlemi yürütürler. Saccharomyces ceravisae'de 16,000civarında PPE olduğu tahmin edilmektedir. Bu etkileşimlerin pek azının rastgelemutasyonlarla evrimleştiği (keşif), diğerlerinin ise gen çiftlenmesi ve exon değişimi gibiolaylarla oluştuğu (doğum) kabul edilmektedir. Bölgeler, proteinlerin işlevselbirimleridir, ve yeniden derlenip düzenlenebildikleri için doğumların çoğundansorumludurlar. Dolayısıyla doğumlar yoluyla evrimleşmiş eşköklü etkileşimlerin ortakbölgelerden oluşan bir imzası olması beklenir. PPE'leri tespit eden bir kaç yükseküretimli yöntem sayesinde, hızla artan miktarlarda PPE verisi elde edilmektedir. Buverileri literatür ve gen ifadesi gibi diğer yüksek verimli verilerle tümleştirmek içinçabalar bulunsa da, hali hazırdaki verilerle ilişkilendirilmiş bilgiler etkileşiminmekanizmasını ve işlevini anlamak için yetersizdir. Etkileşim eşköklülüğünün tespiti,yeni ya da bilinmeyen etkileşimlerin, eşköklü bilinen etkileşimler yoluyla tanımlanmasınısağlayabilir. Bu çalışmada bölge benzerliğini esas alarak etkileşimleri eşköklü gruplaraatamak için olasılıksal bir model tanımlıyoruz. Bu modeli temel alarak, en olasıöbeklemeyi bulmak için bir Beklenti-Maksimizasyon algoritması geliştirdik. Algoritmayısentetik ve gerçek veriler üzerinde sınadık ve ilk sonuçların oldukça umut vericiolduğunu gösterdik. Son olarak bu çalışmanın geliştirilebilmesi için bir kaç öneri sunduk.","iiiABSTRACTCLUSTERING PROTEIN-PROTEIN INTERACTIONSBASED ON CONSERVED DOMAIN SIMILARITIESAslı AyazM.S. in Computer EngineeringSupervisor: Assist. Prof. Dr. Uğur DoğrusözAugust, 2004Protein interactions govern most cellular processes, including signal transduction,transcriptional regulation and metabolism. Saccharomyces ceravisae is estimated to have16,000 protein interactions. Appereantly only a small number of these interactions wereformed ab initio (invention), rest of them were formed through gene duplications andexon shuffling (birth). Domains form functional units of a protein and are responsible formost of the interaction births, since they can be recombined and rearranged much moreeasily compared to innovation. Therefore groups of functionally similar, homologousinteractions that evolved through births are expected to have a certain domain signature.Several high throughput techniques can detect interacting protein pairs, resulting in arapidly growing corpus of protein interactions. Although there are several efforts forcomputationally integrating this data with literature and other high throughput data suchas gene expression, annotation of this corpus is inadaquate for deriving interactionmechanism and outcome. Finding interaction homologies would allow us to annotate anunannotated interaction based on already annotated known interactions, or predict newones. In this study we propose a probabilistic model for assigning interactions tohomologous groups, according to their conserved domain similarities. Based on thismodel we have developed and implemented an Expectation-Maximization algorithm forfinding the most likely grouping of an interaction set. We tested our algorithm withsynthetic and real data, and showed that our initial results are very promising. Finally wepropose several directions to improve this work."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETËş Ë ğ Ë Ë Ë ËBANT GENISLIGINI VERIMLI KULLANANğ ğBLUETOOTH SERPME AGLARININ DAGITIKOLUSTURULMASI VE BAKIMIşMetin TekkalmazBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gAğustos, 2004gBluetooth, düşuk enerji tüketimi ve düşuk maliyeti dolayısı ile, şu anda,usü u usü skısa mesafeli kablosuz iletişim işin kullanılan başlıca teknolojidir. Haberleşmes c s ssağlayabilmek işin Bluetooth donanımlı cihazlar, en fazla sekiz düğumden oluşang c ugü sve piconet adı verilen ağlar kurabilmektedirler. Scatternet adı verilen daha büyükg uuBluetooth ağlarını oluşturmak işin piconetler birleşebilirler. Piconet oluşturmag s c s syüntemi işin Bluetooth Special Interest Group tarafından bir standart belirlenmişo c solmasına rağmen scatternet oluşturma yüntemleri henüz tanımlanmamıştır.g s o u sSu ana kadar, oluşturma işlemininin verimliliği, oluşturulan scatternette yolş s s g satama kolaylığı ya da scatterneti meydana getiren piconet sayısının asgariye in-gdirmesi gibi farklı ana hedeï¬eri olan şok sayıda scatternet oluşturma yüntemic s oüünerilmiştir. Onerilen yüntemlerde farklı bir şok nokta güz ününde bulun-o s o c o oudurulmuş olmasına rağmen bant genişliğinin verimli kullanılması konusu yeterlis g sgdikkati şekememiştir.c sBu tezde, toplam bant genişliği kullanımını asgariye indirmeyi amaşlayansg cdağıtık ve değişen koşullara uyum sağlayabilen bir scatternet oluşturma ve idameg gs s g syüntemi ünerilmektedir. Bant genişliğinin verimli kullanımına bağlı olarak, or-o o sg gtalama paket gecikmesi ve toplam enerji tüketiminde düşuşun yanı sıra yeniu usüsühaberleşme ihtiyaşları işin kullanılmayan bant genişliğini arttırmak da hedeï¬en-s c c sgmektedir. Değerlendirme kriterleri ve bunlara dayalı olarak, ünerilen yünteming o obaşarımı da tez şalışmasında sunulmaktadır.s csAnahtar süzcükler : Bluetooth, Scatternet (Serpme Ağ) Oluşturma, Bantou g sËGenişliğini Verimli Kullanan Ilingeler.sgiv","ABSTRACTDISTRIBUTED CONSTRUCTION ANDMAINTENANCE OF BANDWIDTH-EFFICIENTBLUETOOTH SCATTERNETSMetin TekkalmazM.S. in Computer EngineeringËSupervisor: Assist. Prof. Dr. Ibrahim Kürpeoğluo gAugust, 2004Bluetooth is currently the mainstream technology used for short range wirelesscommunication due to its low power and low cost properties. In order to com-municate, Bluetooth enabled devices can form networks called piconets, whichconsist of at most eight members. To construct larger Bluetooth networks, whichare called scatternets, any number of piconets can be combined. Although pi-conet construction process is standardized by Bluetooth Special Interest Group,scatternet construction policies and algorithms are not yet clariï¬ed.There have been many solution proposals for the scatternet construction prob-lem each of which focuses on diï¬erent aspects of it like the eï¬ciency of the con-struction algorithm, ease of routing in the resulting scatternet and number ofpiconets that constitute it. Although various considerations came into picture,bandwidth eï¬ciency of the resulting scatternet topology, which depends on theplacement of nodes and communication demand among them, did not take muchattention.In this thesis, we provide a distributed and adaptive algorithm that constructsa scatternet and based on collected traï¬c ï¬ow information, modiï¬es it to min-imize the overall bandwidth usage. As consequences of eï¬cient use of availablebandwidth, reduce in average latency and total energy consumption as well asincrease in available bandwidth for new communication demand are also aimed.Moreover, performance of the proposed algorithm is presented, based on the eval-uation criteria described.Keywords: Bluetooth, Scatternet Construction, Bandwidth-Eï¬cient Topologies.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË ü Ë Ë ËşË ËAKILLI VIDEO GOZETIMI ICIN HAREKETLI NESNEËBULMA, TAKIP ETME VE SINIFLANDIRMAYiğithan Dedeoğlug gBilgisayar Mühendisliği, Yüksek Lisansu g uTez Yüneticisi: Yrd. Doş. Dr. Uğur Güdükbayo c g uuAğustos, 2004gVideo güzetimi hassas güvenlik gerektiren banka, alışveriş merkezi, otoyol gibio u s skamuya aşık kalabalık alanları izlemek işin uzun süredir kullanılmaktadır. Bilgic c uişlem gücündeki artış, yüksek kapasiteli kayıt cihazlarının uretilmesi ve hızlı ağs uu su ü galtyapısı ucuz ve şok algılayıcılı sistemlerin uretilmesine ün ayak olmuştur. Eldec ü o sedilen video gürüntüsü canlı olarak operatürlerce izlenir ve daha sonra adli birou uu oolayda kullanılmak uzere kaydedilir. Sıradan bir güzetim sisteminde bulunanü okamera sayısındaki teknolojinin gelişmesine bağlı artış, hem operatürleri hems g s ode kayıt cihazlarını aşırı hacimdeki bilgiye maruz bırakmış ve hasas güvenliks s ubülgelerinin uzun süreli video ile güzetimini verimsiz kılmıştır. Bir dizi kamerao u o starafından uretilen ve şoğunlukla gereksiz olan gürüntü bilgisini elemek ve adliü cg ou uolaylara müdahale zamanını kısaltmak işin operatürlere videodaki ünemli olaylarıu c o obelirleyerek yardımcı olacak ?akıllı? video güzetim sistemlerini geliştirmek kri-o stik bir ihtiyaş haline gelmiştir. Video güzetim sistemlerini ?akıllı? hale getirmekc s ohızlı, güvenilir ve hatasız nesne bulma, sınıï¬andırma ve takip etme algoritmalarınıugerektirmektedir.Bu tezde, nesne bulma, sınıï¬andırma ve takip etme yeteneklerine sahipbir ?akıllı? video güzetim sistemi sunulmuştur. Sistem sabit bir kameradano sËcelde edilen renkli ve renksiz gürüntüler uzerinde şalışabilmektedir. Iş veou u ü csdış mekanlarda, değişen ışık koşulları altında şekilen video gürüntülerinde yers gs s s c ou ualan nesneler bulunabilmektedir. Nesne sınıï¬andırma algoritması bulunan nes-neleri şekillerinden ve nesne takip etme algoritmasından yararlanarak üncedens otanımlanmış olan insan, insan grubu ve araş gibi sınıï¬ara ayırabilmektedir.s cüOnerilen sistem bina ve aşık alan güvenliğinde şok ünem arzeden yangını dac u g cogüvenilir bir şekilde bulabilmektedir. Nesne takip algoritması başka nesneleru s starafından perdelenen nesneleri de takip edebilmektedir. Tüm bu üzelliklere eku ovviolarak video güzetim sistemlerinin ünemli ihtiyacı olan gülge bülgelerinin silin-o o o omesi, ani ışık değişimlerinin algılanması ve bırakılan ya da alınan nesnelerinins gsbulunması da gerşekleştirebilmektedir.c sAnahtar süzcükler : Akıllı Video Güzetimi, Hareketli Nesne Bulma, Arka Planou oKestirimi, Nesne Takip Etme, Siluete Dayalı Nesne Sınıï¬andırma, Ateş Bulma.s","ABSTRACTMOVING OBJECT DETECTION, TRACKING ANDCLASSIFICATION FOR SMART VIDEOSURVEILLANCEYiğithan Dedeoğlug gM.S. in Computer EngineeringSupervisor: Assist. Prof. Dr. Uğur Güdükbayg uuAugust, 2004Video surveillance has long been in use to monitor security sensitive areas suchas banks, department stores, highways, crowded public places and borders. Theadvance in computing power, availability of large-capacity storage devices andhigh speed network infrastructure paved the way for cheaper, multi sensor videosurveillance systems. Traditionally, the video outputs are processed online byhuman operators and are usually saved to tapes for later use only after a forensicevent. The increase in the number of cameras in ordinary surveillance systemsoverloaded both the human operators and the storage devices with high volumesof data and made it infeasible to ensure proper monitoring of sensitive areas forlong times. In order to ï¬lter out redundant information generated by an array ofcameras, and increase the response time to forensic events, assisting the humanoperators with identiï¬cation of important events in video by the use of ?smart?video surveillance systems has become a critical requirement. The making ofvideo surveillance systems ?smart? requires fast, reliable and robust algorithmsfor moving object detection, classiï¬cation, tracking and activity analysis.In this thesis, a smart visual surveillance system with real-time moving ob-ject detection, classiï¬cation and tracking capabilities is presented. The systemoperates on both color and gray scale video imagery from a stationary camera.It can handle object detection in indoor and outdoor environments and underchanging illumination conditions. The classiï¬cation algorithm makes use of theshape of the detected objects and temporal tracking results to successfully cat-egorize objects into pre-deï¬ned classes like human, human group and vehicle.The system is also able to detect the natural phenomenon ï¬re in various scenesreliably. The proposed tracking algorithm successfully tracks video objects evenin full occlusion cases. In addition to these, some important needs of a robustiiiivsmart video surveillance system such as removing shadows, detecting sudden il-lumination changes and distinguishing left/removed objects are met.Keywords: Video-Based Smart Surveillance, Moving Object Detection, Back-ground Subtraction, Object Tracking, Silhouette-Based Object Classiï¬cation,Fire Detection."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Web'in gelişimi ile beraber, bilgiye erişim ve sorgulamada yeni problemler or-taya şıkmıştır. Coğunlukla arama motorları tarafından kullanılan anahtar süzkarşılaştırmaya dayalı sorgulama yüntemleri tek bir sorgu işin binlerce Web bel-gesi getirmekte ve bu belgelerin şoğu kullanıcıların bilgi ihtiyaşları ile ilgisiz ol-maktadır. Web kullanıcılarının bilgi arama ihtiyaşlarını iyileştirmek amacınayünelik olarak, son umut verici yaklaşım Web'in metadata ve ek aşıklama kul-lanılarak dizinlenmesidir.Bu tezde, Web arama yeteneklerini iyileştirmek işin, Web'deki bilgi kaynaklarımetadata kullanılarak modellenmekte ve sorgulanmaktadır. Web sorgulamasınınmetadata kullanılarak yapılması, daha anlamlı sorgu sonuşlarının uretilmesinisağlamaktadır. ?Web bilgi uzayı modeli? adını verdiğimiz Web veri modeli, Webtabanlı bilgi kaynaklarından (Web uzerindeki HTML/XML formundaki belgeler-den), uzman üneri veritabanlarından (bilgi kaynakları işin alan uzmanı tarafındanhazırlanmış metadatadan), ve kullanıcılarla ilgili kişiselleştirilmiş bilgiden (kul-lanıcıların uzmanlarla ilgili tercihleri ve konular hakkındaki bilgi seviyesini be-lirleyen kullanıcı proï¬llerinden) oluşmaktadır. Uzman ünerisi, yakın zamandaünerilmiş olan konu haritaları standardı doğrultusunda, konular ve konulararasındaki ilişkiler (metalink'ler) kullanılarak tanımlanmaktadır. Konular vekonular arasındaki ilişkiler, Web'deki bilgi kaynaklarının işeriğini tanımlayanmetadata'yı oluştururlar. Uzmanlar, konulara, konular arasındaki ilişkilereve bilgi kaynaklarına onların ünem derecesini belirten sayısal değerler verir-ler. Kullanıcı proï¬lleri kullanıcıların tercihlerini ve kullanıcıların ziyaret et-tikleri bilgi kaynaklarını işeren tarihşeyi saklamaktadırlar. Kullanıcı tercihleri,konular uzerindeki bilgi seviyeleri ve Web dolaşım tarihşesi Web'deki aramayıkişiselleştirmek ve kullanıcıya dündürülen sonucun duyarlılığını arttırmak işinkullanılır.Uzman ünerileri ve kullanıcı proï¬lleri nesneye dayalı ilişkisel veritabanındasaklanmakta ve Web tabanlı bilgi kaynaklarını Web bilgi uzayı modeli kullanaraketkin şekilde sorgulayabilmek işin SQL dili genişletilmektedir. SQL uzantıları,i sağlayan yantümceleri, sorguyudurdurma koşulunu tanımlayan yantümceyi ve yeni işleşleri (metin benzerliğinedayalı seşim, metin benzerliğine dayalı birleşim, ve konu kapsamı) işerir. Onemdeğerinin iletimi ve sorguyu durdurma koşulu sorgu şıktısının sıralanmasını veşıktı boyutunun sınırlandırılmasını sağlar. Metin benzerliğine dayalı işleşler vekonu kapsamı işleci karmaşık sorgulama olanaklarını desteklemektedir. Bu SQLeklentilerini işleyebilmek amacıyla ?Yan Değer ureten Cebir? adı verilen yeni bircebir geliştirilmiştir.Yan değer ureten cebir tanımlandıktan sonra, metin benzerliğine dayalı yünlübirleştirme işlecinin algoritması ve bu işlecin performansı uzerine olan deney-sel sonuşlar sunulmaktadır. Tüm bunlara ek olarak, Web bilgi uzayı modeliuzerinde SQL eklentileri kullanılarak yapılan metadataya dayalı kişiselleştirilmişWeb sorgulamasının etkinliği, anahtar süz karşılaştırmaya dayalı Web aramateknikleri ile karşılaştırmalı olarak güsterilmiştir.Anahtar süzcükler : metadataya dayalı Web sorgulaması, konu haritaları, kul-lanıcı proï¬li, kişiselleştirilmiş Web sorgulaması, Yan Değer ureten Cebir, değeryünetimi, metin benzerliğine dayalı birleştirme.","The advent of the Web has raised new searching and querying problems. Key-word matching based querying techniques that have been widely used by searchengines, return thousands of Web documents for a single query, and most of thesedocuments are generally unrelated to the users? information needs. Towards thegoal of improving the information search needs of Web users, a recent promisingapproach is to index the Web by using metadata and annotations.In this thesis, we model and query Web-based information resources usingmetadata for improved Web searching capabilities. Employing metadata forquerying the Web increases the precision of the query outputs by returning seman-tically more meaningful results. Our Web data model, named ?Web informationspace model?, consists of Web-based information resources (HTML/XML docu-ments on the Web), expert advice repositories (domain-expert-speciï¬ed metadatafor information resources), and personalized information about users (capturedas user proï¬les that indicate users? preferences about experts as well as users?knowledge about topics). Expert advice is speciï¬ed using topics and relationshipsamong topics (i.e., metalinks), along the lines of recently proposed topic mapsstandard. Topics and metalinks constitute metadata that describe the contents ofthe underlying Web information resources. Experts assign scores to topics, met-alinks, and information resources to represent the ?importance? of them. Userproï¬les store users? preferences and navigational history information about theinformation resources that the user visits. User preferences, knowledge level ontopics, and history information are used for personalizing the Web search, andimproving the precision of the results returned to the user.We store expert advices and user proï¬les in an object relational databasemanagement system, and extend the SQL for eï¬cient querying of Web-based in-formation resources through the Web information space model. SQL extensionsinclude the clauses for propagating input importance scores to output tuples, theclause that speciï¬es query stopping condition, and new operators (i.e., text sim-ilarity based selection, text similarity based join, and topic closure). Importancescore propagation and query stopping condition allow ranking of query outputs,and limiting the output size. Text similarity based operators and topic closureoperator support sophisticated querying facilities. We develop a new algebracalled Sideway Value generating Algebra (SVA) to process these SQL extensions.We also propose evaluation algorithms for the text similarity based SVA direc-tional join operator, and report experimental results on the performance of theoperator. We demonstrate experimentally the eï¬ectiveness of metadata-basedpersonalized Web search through SQL extensions over the Web information spacemodel against keyword matching based Web search techniques.Keywords: metadata based Web querying, topic maps, user proï¬le, personal-ized Web querying, Sideway Value generating Algebra, score management, textsimilarity based join."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETğ ËşË ËKABLOSUZ TASARSIZ AGLAR ICIN BIR ESLERşË ËARASI DOSYA PAYLASIM SISTEMIşHasan SüzeroBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gAğustos, 2004gSon yıllarda, eşler arası (P2P) ağların artan popülaritesine tanık olunmuştur.s g u süOzellikle dosya paylaşım uygulamaları, Internet kullanıcılarının büyük ilgisinis uucşekmiştir ve şu anda Internet uzerinde fonksiyonel olan birkaş eşler arası dosyas s ü cspaylaşım sistemi bulunmaktadır.sAynı zamanda, seyyar cihazlar ve kablosuz haberleşme teknolojilerindeki sonsgelişmeler, kişisel dijital asistanların (PDA) kolay ve otomatik bir şekilde tasarsızs s sağlar oluşturmalarına olanak sağlamıştır. Fakat, kablosuz tasarsız ağların ortayag s g s gkoyduğu zorluklar, Internet gibi kablolu ağlar uzerinde şalışan eşler arası sistem-g gü cs slerin bu ortama uygulanmasını olanaksız kılmaktadır. Bilgi ve iş dağılımı iles gbirlikte yol atama, sadece haberleşme menzili işine düşen eşlerden haberdar olans c us skablosuz tasarsız ağ uyeleri işin ünemli problemlerdir.gü coBu tez şalışmasında, kablosuz tasarsız ağlar uzerinde eşler arası dosyacs g ü süpaylaşım problemini şüzen bir sistem ünerilmektedir. Onerilen sistem, merkezis co obir sunucuya ihtiyaş duymaksızın, eşler arası sistemlerin prensiplerine gürec s ocsşalışmakta ve paylaşılan dosyaların konum bilgilerini ağın uyeleri arasındas güdağıtmaktadır. Dağıtık bir kıyım tablosu (DHT) ve ağın yapısına dayanan ağaşg g g gcşeklinde bir yerpaylaşan ağ oluşturmak suretiyle, sistem hem konum sorgularınıs s g scevaplandırabilmekte, hem de dosyaların kaynak eşten diğer eşlere aktarımındas g skullanılan yol atama bilgisini keşfedip bu bilgiyi güncel tutabilmektedir.s uAnahtar süzcükler : Kablosuz Tasarsız Ağlar, Dosya Paylaşımı, Eşler Arası Ağlar.ou g s s giv","ABSTRACTA PEER-TO-PEER FILE SHARING SYSTEM FORWIRELESS AD-HOC NETWORKSHasan SüzeroM.S. in Computer EngineeringËSupervisor: Assist. Prof. Dr. Ibrahim Kürpeoğluo gAugust, 2004In recent years, we have witnessed an increasing popularity of peer-to-peer (P2P)networks. Especially, ï¬le sharing applications aroused considerable interest of theInternet users and currently there exist several peer-to-peer ï¬le sharing systemsthat are functional on the Internet.In the mean time, recent developments in mobile devices and wireless commu-nication technologies enabled personal digital assistants (PDA) to form ad-hocnetworks in an easy and automated way. However, ï¬le sharing in wireless ad-hocnetworks imposes many challenges that make conventional peer-to-peer systemsoperating on wire-line networks (i.e. Internet) inapplicable for this case. Informa-tion and workload distribution as well as routing are major problems for membersof a wireless ad-hoc network, which are only aware of peers that are within theircommunication range.In this thesis, we propose a system that solves peer-to-peer ï¬le-sharing prob-lem for wireless ad-hoc networks. Our system works according to principles ofpeer-to-peer systems, without requiring a central server, and distributes informa-tion regarding the location of shared ï¬les among members of the network. Bymeans of constructing a distributed hash table (DHT) and forming a tree shapedoverlay network based on the topology of the network itself, the system is able toanswer location queries, and also discover and maintain routing information thatis used to transfer ï¬les from a source-peer to another peer.Keywords: Wireless Ad-Hoc Networks, File Sharing, Peer-to-Peer Networks.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETËşËBLUETOOTH TABANLI SENSOR AGLARI ICINË Ë ËENERJI ETKIN SCATTERNETLERINOLUSTURULMASIşSain SaginbekovBilgisayar Mühendisliği, Yüksek Lisansu g uËTez Yüneticisi: Yrd. Doş. Dr. Ibrahim Kürpeoğluo c o gAğustos, 2004gSu ana kadar kablosuz komunikasiyon ve micro-sensor alanında olan ilerlemelerşsayesinde yüzlerce hatta binlerce ucuz ve az enerji harcayan sensorlardan oluşanu sağların kurulması ve kullanılması mümkün hale gelmiştir. Bu tür ağları oluşturang uu s ug ssensorlar bir alana doşendikten sonra, o alan ile ilgili bilgiler bir merkezde insan-sların alana gitmesini gerektirmeden toplanabilmekte ve işlenebilmektedir. Fakat,sbu şekilde kullanılan sensorların tek enerji kaynağı pillerdir ve bu sebeple sensor-s glar sınırlı enerji kaynağına sahiptir. Bundan dolayıdır ki enerji, sensor ağları işing g cdikkatle kullanılması gereken en ünemli kaynaklardan biridir.oSensor ağlarında kullanılan iletişim teknolojisi genelde kablosuzdur. Bug samaş işin kullanılabilecek bir şok kablosuz ağ teknolojisi günümüzde mevcut-cc c g uu utur (mesela, Bluetooth, 802.11, ZigBee, gibi). Kullanılan kablosuz ağ teknolojisi,gsensor ağlarının yaşam süreleri uzerinde etkili olmaktadır. Bunun başlıca nedenig s u ü sdeğişik teknolojilerin değişik miktarlarda enerji harcamalarıdır. Günümüzde pop-gs gs uu uuler hale gelen Bluetooth teknolojisi, düşuk enerji harcayan ve düşuk maaliyeteus ü us üsahip olan bir teknoloji olarak sensor ağlarında kullanılmak işin oldukşa elverişlig c c sbir teknolojidir. Sensor ağlarının yaşam sürelerini etkileyen faktürlerden birg s u obaşkası olarak, toplanan verinin sensor düğumlerinden bir merkeze, yani baz ista-s ug üsyonuna, aktarılmasında kullanılacak yolları belirleyen yünlendirme metodlarınınoda ünemi büyüktür.o uu uBluetooth teknolojisinin bir sensor ağında altyapı olarak kullanılması işing cünce scatternet adı verdiğimiz bir Bluetooth ağının oluşturulması gerekmekte-o g g sdir. Bir scatternet oluştururken ise bir şok değişik objektif güzününde bulun-s c gs ooudurulabilir. Fakat, sensor ağları işin en ünemli objektif, oluşturulan scatternetin,g c o sverinin sensorlardan baz istasyonuna taşınması sırasında az enerji harcanmasısvviişin uygun bir topolojiye sahip olmasıdır. Bu tez şalışmasındaki amacımız, sen-c cssor ağları işin oluşturan Bluetooth scatternetlerinin mümkün olduğunca enerjig c s uu gverimli olarak oluşturulması işin gerekli algoritmalar geliştirmektir. Bu amaşlas c s cgeliştirdiğimiz algoritma, ünce her bir düğumün baz istasyonuna olan bağlantısınıs g o ug ü u gmümkün olan en kısa yoldan yapıp, sonra eğer varsa yediden fazla komşusuuu g solan dügümlerin komşu sayısını en fazla yedi olmak uzere indirgemeye dayalıdır.uu s üBu şekilde, her bir dügümün oluşturduğu veri baz istasyonuna en az enerji iles uu u s gtaşınmış olacak, ve aynı zamanda oluşturulan topoloji Bluetooth teknolojisi kul-s s slanılarak gerşekleştirilebilecektir (bir Bluetooth dügumü en fazla 7 tane komşuyac s uğ ü u ssahip olabilir). Yine baz istasyonuna bağlı dügumlerin yükünün dengeli olmasıg uğü uuuişin, ünerdiğimiz algoritma ağaş şeklinde olan scatternetinin birinci seviyesin-c o g gcsdeki dügumlerde harcanan enerjiyi dengelemeye şalısmakta, ve bu şekilde enuğü c sşabuk ülecek olan dügumün hayatını uzatmaya şalışmaktadır. Yaptığımız simu-c o uğü u cs glasyon sonuşları algoritmamızın Bluetooth tabanlı sensor ağlarının hayatlarınınc guzatılmasında etkili olduğunu gostermektedir.gAnahtar süzcükler : Kablosuz Sensor Ağları, Bluetooth, Scatternet, yol belirleme,ou gkısa yol ağacı.g","ABSTRACTCONSTRUCTING ENERGY EFFICIENT BLUETOOTHSCATTERNETS FOR WIRELESS SENSORNETWORKSSain SaginbekovM.S. in Computer EngineeringËSupervisor: Assist. Prof. Dr. Ibrahim Kürpeoğluo gAugust, 2004The improvements in the area of wireless communication and micro-sensor tech-nology have made the deployment of thousands, even millions, of low cost andlow power sensor nodes in a region of interest a reality. After deploying sensornodes in a target region of interest, which can be inaccessible by people, peoplecan collect useful data from the region remotely. The sensor nodes use wirelesscommunication and can collaborate with each other. However, sensor nodes arebattery powered and therefore they have limited energy and lifetime. This makesenergy as the main resource problem in sensor networks. The design process forsensor networks has to consider energy constraints as the main factor to extendthe lifetime of the network.The wireless technology used for communication among sensor nodes can af-fect the lifetime of the network, since diï¬erent technologies have diï¬erent energyconsumption parameters. Bluetooth, being low power and low cost, is a goodcandidate for being the underlying wireless connectivity technology for sensornetworks tailored for various applications. But in order to build a large networkof Bluetooth-enabled sensor nodes, we have to ï¬rst form a Bluetooth scatter-net. The topology of the Bluetooth scatternet aï¬ects the routing scheme to beused over that topology to collect and route informaton from sensor nodes to abase station. And routing scheme, in turn, aï¬ects how much energy is consumedduring transport of information. Therefore, it is important to build a Bluetoothscatternet wisely to reduce and balance the energy consumption, hence extendthe lifetime of a sensor network.In this thesis work, we propose a new Bluetooth scatternet formation algo-rithm to be used in Bluetooth-based sensor networks. Our algorithm is based oniiiivï¬rst computing a shortest path tree from the base station to all sensor nodes andthen solving the degree constraint problem so that the degree of each node in thenetwork is not greater than seven (a Bluetooth constraint). We also propose abalancing algorithm over the degree constrained tree to balance the energy con-sumption of the nodes that are closer to the base station. The closer nodes arethe nodes that will consume more energy in the network since all traï¬c has tobe forwarded over these nodes. Our simulation results show that our proposedalgorithm improves the lifetime of the network by trying to reduce the energyconsumed during data transfer and also by balancing the load among the nodes.Keywords: Wireless Sensor Networks, Bluetooth, Scatternet, Routing, ShortestPath Tree."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,ÇÃÃÃ Ã Ã ÃÃ Ã ÃÃ ÃÃÃÃÃ Ã ÃÃÃ ÃÃÃÃÃ ÃÇÃÃ ÃÃ ÃÃÃ ÇÃÃ Ã ÇÃ ÃÃÃÃ Ã ÃÇÃ ÃÃ ÃÃÃ ÃÃÃ Ã ÇÃÃ Ã ÃÃÃ Ã ÃÃÃÃÃ Ã ÇÃÃ ÃÃ ÃÃÃ Ã ÃÃ Ã ÃÃÃ ÃÃ Ã ÃÃÃ ÃÃÃÃ ÃÃÃÃÃ Ã ÖÃ İÃ Ã İ Ö ÃÃ Ã ÃÃ ş ÃÃ Ã Ã ÃÃÃÃ ÃÃ Ã Ã ÃÖÃ Âº ÖÂº Ã Ã İ ÃÃÃÃÃÃÃş Â¾Â¼Â¼ÇÖÃ ÃÃİ ÃÃÃ Ã Ã ÃÃ Ã ÃÖ ÃÃ Ö Ã ÃÃÖ Ã ÃÃÃ İ Ã ÃÃ Ã ÃÃÃÃ Ã ÃÖÃ ÃÂ¹Ã ÖÃ Ã Ã ÃÃ ÃÃ Ã ÃÃÃ Ã Ã ÃÃ Ö Ö ÃÖ Ã Ö Ã ÃÃ ÃÃ Ãİ ÃÃ Ã Ã ÖÃÃÃ Ã ÃÃ Ã ÖÂº Ã ÃÖÃ Ã Ã Ã ÃÃ ÃÃ Ö Ã Ã ÃÃÃÃÃ Ã İÃÃÃ ÃÃ Ö Ã ÃÃÖ Ã Ã Ã ÃÃ İ Öİ Ã ÃÃ Ã ÃÖ ÃÃ Ö Ã ÃÃ Ã Ã Ã Ã ÃÃ Ö Ã ÃÃÃ Ã ÃÃÃÃÃ Ö Ö Ö Ã ÖÃ Ã Ã ÃÃ Ã Ã İ ÃÃ Ã ÖÂº ÃÃÃ ÃÃ Ö Ã Ã ÃÖ şÃ ÃÃÖ Ö Ö Ö Ã ÖÃ Ã Ã ÃÃ Ã ÃÃ Ã Ã Ã Ã ÃÃ Ö Ã Ã ÃÖÃ ÃÃİ ÃÃÂ¹Ã ÃÃ Ã ÃÃÃ Ã Ã İ Ã ÃÃ ÖÃÃ İ Ã ÖÃ ÃÃ Ã Ã ÃÃ İ Ã Ã İ Ã ÃÃÃ Ö ÃÃ ÖÃ Ã ÃÃÃ Ã Ã Ã Ã ÖÂº Ã ÃÃÖ ÃÖ ÃÃ Ö Ã Ã Ã ÃÃ ÃÃ Ã ÃÖÃ Â¹Ã ÃÃÃ ÃÃÃÃÃ ÃÃ Ã Ã ÃÃ Ã ÃÖ Ã Ã Ã Ãş Ã Ã Ã ÖÃ Ã ÃÃ Ö ÃÃ Ã ÃÃÖ Ã İÃÖÃÃÃ ÃÖ Ã Ã Ã Ã Ã ÖÃ ÖÃÂ¿Ã Ã İÃ Öİ Ã Ã Ã ÃÃ Ö ÂºÃÃ Ã ÃÃ ÖÃ Ã Ã Ã Ãİ ÃÃ Ã Ã Ö ÖÖ ÃÖ Ã ÃÃ Ö ş ÃÖ Ã ÃÃÃÖÃ Ã Ã Ã ÃÖ Ã ÃÃÃÃÃ Ã ÃÃ Ö Ö ÃÖÃ Ã Ã ÃÃ ÃÃ ÃÃ ÃÃ ÖÂºÇÃ Ö Ã Ã İ Ã Ã Ã Ã İ Ã ÃÃ ÖÃ Ã ÃÃ ÃÃÃÃÃ Ã İÃÃÃ ÃÃ Ö Ã Ã ÃÃ Ã Ö ÃÃ ÖÃ ÃÃÃ Ã Ã Ã ÃÃ Ö Ã Ã ÃÃ Ã İÃÃÃ Ö Ã ÃÃ ÃÃÃÃ ÃÃ ÃÃ Ã Ã ÖÂºÃ Ã ÃÃ Ã Ö ÃÃ ÃÃÃ Ã Ã Ö Ã Ã ÃÃ Ã Ã Ã İ Ã ÃÃ Ã Ã Ö Ã Ã İÃÖÃÃÃÃ Ã ÃÃ ÃÃÃÃÃ İ Ã ÃÃ ÃÃÃÃ ÃÃ ÃÃ Ã Ã Ã İÃ ÃÃ Ã İÃÃÃÃ Ö Ã Ö Ã Ã ÃÃ ÃÃ ÖÂº Ã İ ÃÃ Ö İ Ö Ã Ã Ã Ãİ ÃÃ Ã Ã Ö Ã ÃÃÖ Ã ÃÃÃ İ ÃÃÃÃÃ Ã ÃÃ Ã ÃÖÃ ÃÃ Ö Ã Ö Ã ÃÃ Ö Ã Ã Ã İÃ Ö Ã ÃÃÃÃ Ã Ö ÃÃ Ö Ã Ã Â¿ Ã Ãİ Ã ÃÃÃ ÃÃÃÃ Ö Ã Ã Ã ÃÃ İ İ Ã ÃÃ ÃÖ Ã ÖÃ ÃÃ Ã ÃÃÃÃÃÃ ÖÃ ÃÃ ÖÂºÃ Ã Ö ÃÃÃ Ã Ã Ö ÃÖ Ã Ã Ã ÃÃ Ã ş ÃÖÃ ÃÃİ ÃÃÃ Ã Ã ÃÖ ÃÃ Öş ÃÃÖ Ã ÃÃÂ¹Ã İ Ã ÃÃ Ã ÃÃÃÃ Ã ÃÖÃ ÃÃ Ö ş İ Ã Ã Ã Ã İ Ã ÃÃ ÖÃ ÂºÃ,ÃÃÃ ÃÃÃ Ã ÃÃÃ Â¹ÃÃÃÃÇÃ Ã ÃÃÂ¹ Ã Ã ÃÃÃÃÃÃ ÃÇÃ ÃÃÃÃ Ã Ã ÃÃÃÃ Ç Ã ÃÃÃ ÃÃ ÃÃÃÃÃ Ã ÇÃ Ã Ã ÃÇ Ã ÇÃÃ Ã ÃÃ ÃÂ¹ÃÃ ÃÃÃÃÃÇÃÃ ÃÃÃÃ Ã ÖÃ İÃÂºÃÂº Ã ÃÃÃÃÃ Ö Ã Ã ÖÃÃÃÃ ÖÃ ÃÃÖ ÃÖÃ Âº ÖÂº Ã Ã İ ÃÃÃ ÃÃÃş Â¾Â¼Â¼Ã Ã ÃÃ Ã Ã Ã ÃÃ ÃÃ ÃÃ Â¬Ã Â¹Ã ÖÃ Ã Ã Ã ÃÃ Ã ÖÃ Ã ÃÃÃ Ã ÃÃ ÖÂ¹ÃÃ Ã ÃÃ Ã ÃÖÃÃ ÃÖ ÃÃÃİ ÃÃÃ ÃÃÃÖÃ ÃÃ ÃÃÃ Ã ÃÃÃ Ã Ö ÃÃ ÖÃÃÃ ÃÃÃÂºÃ Ã ÃÃ Ã ÃÖ ÃÃ ÃÖ ÃÃÃİ ÃÖÃÃÃÃ ÃÖ Ã Ã ÃÖÃ Ã Ã Ö ÃÃ ÃÃÃÃÖÃ Ã Ã ÃÃ ÃÃÖ Ã Ã ÃÃ ÃÃÃÃÃ Ö İ Ö Ã Ö ÃÃ Û Ã Ã Ã ÃÃ Ã ÃÃÃ ÃÃ ÖİÃÃÃÃ Ã ÃÃ Ã Ã Ã ÃÃ Ã Ã Ã Ã Ã ÃÂº Ã Ã ÃÛ Ã ÃÃ Ã Ö İ Ã ÃÃ Ö Ã Ö ÃÃÃ Ã ÃÖÃ ÃÃ Ã Ã Ã ÜÃÃÃ Ã Ã Ã Â¬Ã Â¹Ã ÖÃ ÃÃ Ö Ã ÃÃ ÃÃÃ Ã Ã ÃÃÃ ÃÃÂ¹ÃÃ Ã ÃÃ Ã Ã Ã ÃÃ Ã Ö Ã ÃÃ Ã ÃÃ ÜÃÖ ÃÃ ÃÃ ÃÃ ÛÃ Ã Ã ÃÃ Ö Ã ÃÃÂºÃ ÃÖÃÃÃÃ ÃÖ Â¹Ã Ã Ã ÃÃ Ã ÃÃÖÃ Û ÃÃÃÃÃ Ã Ã Ã ÃÃ Ã ÃÃ ÃÂ¹Ã ÃÃş Ö Â¬Ã Ã ÃÃ Ã Ü ÃÃ ÃÃ ÃÖ ÖÃ Ã Ã ÃÂº ÃÖ Ã Ö Â¬Ã Ã ÃÃ Ã ÃşÛÃÃ ÃÃ ÃÖ Ã ÃÃÃ Ã ÃÃ Ã İÃ Ö Ö Ã Ã ÛÃ Ã Ã ÃÃ İÃ Ö Ö Ã Â¹Ã ÖÃ Ã ÃÃ Ã Â¹Ã ÃÖÃÃÃ Ã ÃÃş Û ÃÖÃÃÃÃ ÃÃ ÃÃ Ã Ö Ã Ã Â¹ ÃÃÖÃÃ Ã ÃÃ ÃÃÖ ÃÃ Ã ÃÖ Ö Â¬Ã Ã Ã ÃÃ ÃÃ ÃÃ ÃÃÃ ÃÖ Ã ÃÃ ÃÛÃ ÃÃÃ Ã Ã ÃÃ ÃÃ Â¹Ã ÃÃÃÂº ÃÃÃ Ã ÃÃ Ã Ã ÃÃ Ã ÃÃÃş Ã ÃÃÃÃÃ Ã ÃÃ Ã ÃÖÃÃÃÃ Ã ÃÃÃÃ Ã ÃÃÃ Ã ÃÃÃ ÃÃ Ã Ã Ö Ã Ã Â¹ ÃÃÖÃÃ Ã ÃÃÂ¹ Ã ÃÖ ÃÃ Ã ÃÃ ÃÃ ÃÃÃİşÃÃ Ã Ö Â« Ã Ã Ã ÃÃ Ã Ã Ãİ ÃÃ ÃÃ Ã ÃÃÃÃÃ Ã ÃÃ Ã Ã Ã ÃÃÃÃ Ã ÃÃÂº ÜÃ Ö Ã ÃÃ Ã Ö ÃÃÃÃÃ ÃÃ Û ÖÃ Ã ÃİÃÃ Ã ÃÃİ ÃÖÃ Ã ÖÃÂ¹Ã ÃÃÃ Ã ÃÃ ÖÂ¹ÃÃ Ã Ö Ã ÛÃÖ Ã Ã ÃÛ Ã ÃÃ ÃÖÃÃÃÃ ÃÖ Â¹Ã Ã Ã ÃÃ ÃÃÃÖÃ Ã Ö ÃÖÃÃ ÃÃ ÃÃ Ö Ã ÃÃ Ö İ ÃÃÃÃÖÃ Ã Ã ÃÃÖÃ ÂºÃ İÛÃÖ Ã ÃÃ Ã ÃÃ Ã ş Â¬Ã Â¹Ã ÖÃ Ã Ã Ãş Ã ÖÃ Ã ÃÃÃ Ã ÃÃ ÖÂ¹ÃÃ Ã ÃÃ ÃÂ¹ÃÖÃÃş Ã Ö Ã Ã Â¹ ÃÃÖÃÃ Ã ÃÃÂº
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"üOZETË ËË Ëş Ë ËË ËGIYDIRILMIS INSAN MODELLERININ ANIMASYONUËIlknur Kaynar KabulBilgisayar Mühendisliği, Yüksek Lisansu g uü ucTez Yüneticileri: Prof. Dr. Bülent Ozgüş veo uYrd. Doş. Dr. UËur Güdükbayc g uuTemmuz, 2004Bu şalışmada, kıyafet giydirilmiş insan modelinin değişik hareket bişimleriylecs s gs cübenzetimini sağlayan bir sistem sunulmaktadır. Onerilen sistem uş modüldeng üc uoluşmaktadır: Vücut hareketleri modülü, kıyafet tasarım modülü, kıyafet ben-s u uu uuzetim modülü. Vücut hareketleri modülü, kullanıcının her vücut parşası işinuu u uu u c cfarklı hareket şablonlarını düzenlemesini ve büylece şeşitli hareket bişimlerinis u o cs ctasarlamasını sağlamaktadır. Kıyafet tasarım modülü, kıyafet modellerining uuoluşturulmasını, ülşulerinin belirlenmesini ve parşacık şıkarılmasıyla kesilmesinis o cü c cişermektedir. Kıyafetin insan modeli uzerine giydirilmesi, kumaşın deforme ol-c ü sması ve şakışmaların ele alınması sırasında kıyafet parşalarının dikilmesi ilecs cyapılmaktadır. Kıyafet benzetim modülü kıyafet giymiş insan modelinin hareketuu settirilmesini sağlamaktadır. Bu modüldeki en ünemli problem hareketli insang u omodeli ile kıyafet modeli arasındaki şakışmaların ele alınması ve ünlenmesidir.cs oBu şalışmada bu problem uzerinde durulmaktadır. Cakışmaların belirlenmesin-cs ü şsdeki hesaplamalar insan ve kumaş modelleri işin kısıtlayan hacimler kullanılmasıs cyoluyla azaltılmıştır. Ayrıca kenar-kenar kesişimlerinin hesaplanmasını ünlemeks s oişin insan vücudu uzerinde küşuk bir hacim oluşturulmuştur. Cakışmalarınc u ü uc ü s s şsünlenmesi işin kumaş parşacığı ile uşgen düzlemi arasındaki uzaklığa ve parşacığıno c s cg üc u g cguşgene güre pozisyonuna bağlı olarak ceza kuvvetleri ve sınırlamalar uygu-üc o glanmıştır. Gerşekleştirilen sistem ile oluşturulan gürsel sonuşlar ve başarıms c s s o c ssonuşları tezde sunulmuştur.c sAnahtar süzcükler : kütle-yay sistemleri, kıyafet tasarımı, kıyafet benzetimi,ou uşarpışma testi, kumaş.c s siv","ABSTRACTANIMATION OF DRESSED VIRTUAL HUMANSËIlknur Kaynar KabulM.S. in Computer Engineeringü uşSupervisors: Prof. Dr. Bülent Ozgüc anduAssist. Prof. Dr. Uğur Güdükbayg uuJuly, 2004In this study, we present a system that enables the simulation of a dressedvirtual human with diï¬erent motion behaviors. The proposed system is composedof three modules: human body motion module, garment design module, andgarment simulation module. The human body motion module enables the userto design diï¬erent motion behaviors by adjusting the motion patterns for eachbody part. The garment design module consists of creating garment patterns,setting their sizes, and cutting them by removing particles. The adjustment ofgarment patterns onto human body is achieved by seaming garment patternswhile applying cloth deformation and collision handling. Garment simulationmodule provides animation of virtual humans with the garments on. The mainproblem in garment simulation is collision handling between the animated virtualhuman and its garments. This study focuses on this problem. Collision detectioncalculations are reduced by using bounding volumes for both virtual human andgarments. In addition, for avoiding edge-edge collision detection, a heuristic thatconstructs a very thin volume around the human body is applied. For the collisionresponse, penalty forces and constraints are applied depending on the distancebetween the particle and the triangle?s plane, and the particle?s position withrespect to triangle. Visual results and performance experiments produced by theimplementation are presented.Keywords: mass-spring model, garment design, garment simulation, collision han-dling, cloth.iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇOK KATMANLI BİR İNSAN MODELİNİN GERÇEKÇİ GÖRÜNTÜLENMESİ Mehmet Şahin YEŞİL Bilgisayar Mühendisliği Bölümü, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Güdükbay Ağustos, 2003 Bu tez çalışmasında, insan vücudunun hareket ederken gerçekçi olarak görüntülenmesi (boyanması) için çok katmanlı bir insan modeli önerilmiş ve gerçekleştirilmiştir. Önerilen insan modeli, iskelet katmanı, kas katmam ve deri katmanı olmak üzere üç katmandan oluşmaktadır. Bir grup eklem ve kemikle temsil edilen iskelet katmam, insan modelinin ters kinematik yöntemler kul lanılarak hareket ettirilmesini sağlamaktadır. Kaslar, bir grup kontrol noktasıyla tanımlanan etki doğrularıyla modellenmiştir. Bu etki doğrulan, kas tarafından kemik ve deri yüzeyi üzerine uygulanan kuvvetleri temsil etmektedir. Deri kat manı üç boyutlu bir modelleyici kullanılarak modellenmiştir ve bu katmanın iskelet katmam ile kas katmanına bağlanması suretiyle canlandırma süresince derinin deforme edilmesi sağlanmıştır. Deri yüzeyi, iskelet ve kas katmanının mevcut durumlarına bağlı olarak iki aşamalı bir algoritma kullanılarak deforme edilmiştir. Birinci aşamada, deriyi iskeletin hareketine göre deforme eden bir deri kaplama algoritması kullanılmıştır, ikinci aşamada ise, deri yüzeyi, alttaki kas katmanı vasıtasıyla deforme edilmiştir. Gerçekleştirilen sistem ile üretilen görsel sonuçlar da tezde sunulmuştur. Başarım sonuçlan sistemin orta karmaşıklıkta(deri katmanında 33,000 üçgen içeren) bir model için gerçek zamanlı hızlarda çalıştığını göstermiştir. Anahtar kelimeler: insan vücudunun modellenmesi ve canlandırılması, çok kat manlı modelleme, eklemli vücut, kinematik, ters kinematik, etki doğrusu, deri kaplama, deformasyon. vi","ABSTRACT REALISTIC RENDERING OF A MULTI-LAYERED HUMAN BODY MODEL Mehmet Şahin YEŞİL M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay August, 2003 In this thesis study, a framework is proposed and implemented for the real istic rendering of a multi-layered human body model while it is moving. The proposed human body model is composed of three layers: a skeleton layer, a muscle layer, and a skin layer. The skeleton layer, represented by a set of joints and bones, controls the animation of the human body model using inverse kine matics. Muscles are represented by action lines, which are defined by a set of control points. The action line expresses the force produced by a muscle on the bones and on the skin mesh. The skin layer is modeled in a 3D modeler and deformed during animation by binding the skin layer to both the skeleton layer and the muscle layer. The skin is deformed by a two-step algorithm according to the current state of the skeleton and muscle layers. In the first step, the skin is deformed by a variant of the skinning algorithm, which deforms the skin based on the motion of the skeleton. In the second step, the skin is deformed by the underlying muscular layer. Visual results produced by the implementation is also presented. Performance experiments show that it is possible to obtain real-time frame rates for a moderately complex human model containing approximately 33,000 triangles on the skin layer. mKeywords: Human body modeling and animation, multi-layered modeling, artic ulated figure, kinematics, inverse kinematics, action line, skinning, deformation. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET AYRIŞIK ÇİZGE YERLEŞTİMİNDE POLİGON PAKETLEME YAKLAŞIMI Cihad Baskoy Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Doğrusöz Ocak, 2003 Çizge yerleştirimi konusu, Bilgisayar Bilimlerinde son birkaç on yıl içerisinde önem kazanmıştır. Kullanım alanları oldukça geniş olmak ile beraber, veri yapılan, veritabanlan, yazılım mühendisliği, VLSI teknolojileri, elektrik mühendisliği, üretim planlaması, kimya ve biyoloji örnek olarak verilebilir. Yerleştirme algoritmalarının bir çoğu, çizgenin bağlaşık olduğunu varsayarak işlem yapmaktadır. Bununla birlikte, karşılaşılan çizgelerin ayrık olması durumunda kullanılabilecek algoritmalara ihtiyaç duyulmaktadır. İki boyutlu kap içerisine paketleme problemleri tekstil ve çelik endüstrilerinde geniş uygulama alanına sahiptir, örneğin çelik endüstrisinde, dikdörtgen plakalar üzerinde poligonsal şekillerin işaretlenmesi ve kesilmesi önemli bir problemdir. Bütün bu uygulamalardaki ana amaç, kullanılmayan alanın en aza indirimesi olarak özetlenebilir. Son zamanlarda, iki boyutlu paketleme algoritmaları, ayrık çizge yerleştirimi araştırmalarında, dikdörtgenler şeklinde tanımlanan ve bağlantılı olmayan çizge elemanlarını yüzey üzerine kaplamak için kullanılmaya başlanmıştır. Bu algoritmaların gereklerinde birisi, oluşturulacak yerleştirimde önceden tanımlanan boyut oranının korunmasıdır. Çizge yerleştirimi konusunda, elemanların polyomino'lar kullanılarak tanımlanması, hesaplama zamanlarını artırmakla birlikte daha doğru sonuçlar elde edilmesini sağlayan, yeni bir yaklaşımdır. Çizim performansı ve hesaplama zamanı arasındaki denge önemlidir. Bu çalışmada, çizge elemanlarının detaylı olarak tanımlanabilmesi için poligonlar kullanılmakta ve ayrık çizge yerleştirimini gerçekleştiren yeni bir algoritma sunulmaktadır, özetle, ayrık çizge yerleştirimlerinin iki boyutlu paketlenmesi için No-Fit poligon yaklaşımı uygulanmaktadır. Bu yeni yaklaşım kullanılarak elde edilen sonuçlar sunulmakta ve değerlendirilmektedir. Ayrıca bu sonuçların, önceki yaklaşımların sonuçlan karşılaştmlması yapılmaktadır. Anahtar Sözcükler: Çizge yerleştirimi, Aynk çizge yerleştirimi, İki boyutlu paketleme. iv","ABSTRACT POLYGON PACKING APPROACH TO DISCONNECTED GRAPH LAYOUT Cihad Başköy M.S. in Computer Engineering Supervisor: Asst. Prof. Uğur Doğrusöz January, 2003 Graph layout has become an important area of research in Computer Science for the last couple of decades. There is a wide range of applications for graph layout including data structures, databases, software engineering, VLSI technology, electrical engineering, production planning, chemistry, and biology. Most layout algorithms assume the graph to be connected. However, most graphs are disconnected and a method for putting the disconnected graph objects together is needed. Two-dimensional packing algorithms have wide area of application such as in the steel and textile industry. In steel industry, problems frequently occur when the need to stamp polygonal figures from a rectangular board arises. In the textile industry, similar problems exist. The aim is same: to maximize the use of the contiguous remainder of the board. Recently, two-dimensional packing has also been used in disconnected graph layout yielding algorithms that 'tile' the disconnected graph objects, which are represented by rectangles. These algorithms are also required to respect the specified aspect ratio for the final layout. A more recent approach to disconnected graph layout has been the use of polyominoes for representing the graph objects resulting in more accurate packings at the cost of increased execution times. In this thesis, we use polygons for a more accurate representation of graph objects and present new algorithms for disconnected graph layout. Specifically, we apply the No-Fit Polygon approach in two- dimensional packing to disconnected graph layout. We present and analyze the graph layouts resulting from our new approach and contrast the new approach with previous ones. Keywords: Graph Layout, Disconnected Graph Layout, Two-Dimensional Packing. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SİNYAL İLETİM YOLAKLARININ MİKRODİZİ VERİSİNE DAYALI ÇÖZÜMLEME YÖNTEMLERİ Özgün Babur Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Uğur Doğrusöz Eylül, 2003 Mikrodizi, bilim adamlarına ilk kez hücre çapında gen ifadesi verisi sağlayan, yüksek üretimli bir moleküler biyoloji tekniğidir. Mikrodizi deneyleri ile veri üretilmesi görece kolay olmasına rağmen bu verilerin düzeltilmesi ve çözümlenmesi oldukça güçtür. Görünen odur ki, bu verilerin çözümlenmesi ancak sistem tabanlı bir yaklaşımla olasıdır. Bu tezde bilinen sinyal ileti yolakları için bir mikrodizi veri çözümleme yöntemi önerilmektedir. Sinyal yolakları hücrenin metabolizma, bölünme, apoptoz gibi işleyiş ve davranışlarını yöneten düzeneklerdir. Bir sinyal olayının etkinliği düzenekte yer alan moleküllerin derişimleri ile belirlenir. Bu moleküllerden bir kısmı ise mRNA lardır. Mikrodizi deneyi işte bu mRNA ların derişimlerinin, kontrol ile test arasında ne kadar farklılık gösterdiğini ölçer. Eğer değişiklik istatistiksel olarak belirgin ise, bu moleküllerin etkinliğinin değiştiği söylenebilir. Ancak mikrodizi verisi deneysel hatalardan dolayı tüm RNA ların temsil edildiğini garantileyemez. Daha da önemlisi, sonuçların güvenilirliği alışıldık moleküler biyoloji tekniklerine göre düşüktür. Son olarak mikrodizi deneyleri bize proteinlerin hallerini ve etkinliklerini doğrudan söyleyemez. Temsil edilmeyen ya da belirgin olmayan moleküllerin hal ve etkinliklerinin tahmin edilmesini sağlayan yeni bir çözümleme yöntemi geliştirilmiş ve PATİKA isimli tümleşik yolak ortamına uygulanmıştır. Bu sistem ayrıca tahmin edilen yeni bilginin kul lanılmasına yönelik sorgu ve görselleme araçları da içermektedir. Anahtar sözcükler: Biyo-enformatik, sinyal iletim yolakları, mikrodizi veri çözümlemesi, yolak etkinlik tahmini.","ABSTRACT METHODS FOR SIGNALING PATHWAY ANALYSIS USING MICROARRAY DATA Özgün Babur M.S.in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Doğrusöz September, 2003 Microaxray is a new high throughput molecular biology technique which pro vided scientists with cell scale expression data for the first time. Although col lecting microaxray data is relatively easy, filtering, normalizing and analyzing its results are notoriously difficult. It is becoming clear that in order to able to analyze such data we need a system-oriented methodology. In this thesis a new analysis method for known signal transduction pathways in the cell using microarray data is proposed. Signaling pathways are the mechanisms that control cellular machinery and behavior such as metabolic events or decisions about the cell fate. The activity of a signaling event is determined by the concentration of actor molecules. Mi croarray data give the concentration fold change of RNA of genes between the control state and the experimented state of the cell line. In the case of signaling proteins, this significant fold change data can be converted to the status data (i.e., being effectively present or absent). However microarray data only partially signify the levels of RNA in the cell; not all genes are guaranteed to be represented. Also the precision and fidelity of expression data is quite low, thus even for represented genes, changes might not be statistically significant enough to understand the status of the genes. Moreover signaling paths contain non-RNA actors with a greater ratio and these are not represented on the microarray data. So it provides relatively less amount of useful data compared to the size of the array. Finally microarray data only implicitly indicates which protein states are active or inactive. In this thesis a new approach (and algorithm) for inferring possible temporal iiiIV status of non-represented or non-significant signaling molecules is proposed. This new analysis method is implemented as part of a microarray data analysis com ponent within PATİKA (Pathway Analysis Tool for Integration and Knowledge Acquisition), which is a software environment for pathway storage, integration and analysis. This component also includes facilities that help users make use of the new inferred data by operations such as highlighting upstream, downstream, up-regulation and down-regulation relations between entities. Keywords: Bioinformatics, signal transduction pathways, high throughput tech niques, microarray data analysis, pathway activity inference."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VERİ BİRLEŞTİRME YÖNTEMLERİ KULLANARAK BİLGİ ERİŞİM SİSTEMLERİNİN PERFORMANSININ OTOMATİK OLARAK DEĞERLENDİRİLMESİ Rabia Nuray Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. H. Al tay Güvenir Ağustos, 2003 Deneysel olarak bir bilgi erişim sisteminin (arama motorunun) etkinliğinin ölçümü belgeler, bir sorgu kümesi ve her sorguya ilişkin bir küme belgeden oluşan bir test koleksiyonu gerektirir. İnsanlar tarafından yapılan değerlendirmeleri pahalı ve özneldir. Buna ek olarak veri tabanları ve kullanıcıların ilgi alanları çok çabuk değişmektedir. Bu nedenle arama motorlarının performansını otomatik olarak değerlendirecek bir yönteme büyük gereksinim duyulmaktadır. Ayrıca son çalışmalar insan değerlendirmelerindeki faklılığın sistemlerin bağıl performansını etkilemediğini göstermiştir. Bu gözlemlere dayanarak, bu tezde veri birleştirme yöntemlerini kullanarak insan değerlendirmelerini otomatik değerlendirmeler ile değiştirmeyi öneriyor, kullanıyor, ve yeni bir yöntem sunuyoruz ve bu yöntemin birçok Text Retrieval Conference (TREC)' de uygulamasının sonuçlarını gerçek insan değerlendirmeleri ile anlamlı ve pozitif uyuşumunu ayrıntılı gösteren istatistiksel değerlendirmelerini gösteriyoruz. Bu tezin önemli katkıları şunlardır: (1) veri birleştirme algoritmalarını literatürde ilk defa kullana bir otomatik değerlendirme yöntemi (2) özdevinimli yöntem ile insan değerlendirmeleri arasında yüksek uyuşum amaçlayan sistem seçme yöntemleri (3) önerilen bu yöntemin bulduğu duyarlık değerlerinin gerçek duyarlık değerlerine güçlü uyuşumunun olduğu gerçeğinden kaynaklanan birkaç farklı pratik faydalar ve yeniliklerdir. Anahtar Sözcükler: otomatik performans değerlendirme, veri birleştirme, bilgi erişim sistemleri, sosyal refahlık fonksiyonları, sistem performans tahmini, TREC. iv","ABSTRACT AUTOMATIC PERFORMANCE EVALUATION OF INFORMATION RETRIEVAL SYSTEMS USING DATA FUSION Rabia Nuray M.S. in Computer Engineering Supervisor: Prof. Dr. jh. Altay Güvenir August, 200b The empirical investigation of the effectiveness of information retrieval systems (search engines) requires a test collection composed of a set of documents, a set of query topics and a set of relevance judgments indicating which documents are relevant to which topics. The human relevance judgments are expensive and subjective. In addition to this databases and user interests change quickly. Hence there is a great need of automatic way of evaluating the performance of search engines. Furthermore, recent studies show that differences in human relevance assessments do not affect the relative performance of information retrieval systems. Based on these observations, in this thesis, we propose and use data fusion to replace human relevance judgments and introduce an automatic evaluation method and provide its comprehensive statistical assessment with several Text Retrieval Conference (TREC) systems which shows that the method results correlates positively and significantly with the actual human based evaluations. The major contributions of this thesis are: (1) an automatic information retrieval performance evaluation method that uses data fusion algorithms for the first time in the literature, (2) system selection methods for data fusion aiming even higher correlation among automatic and human-based results, (3) several practical implications stemming from the fact that the automatic precision values are strongly correlated with those of actual information retrieval systems. Keywords: automatic performance evaluation, data fusion, information retrieval system, social welfare functions, system performance prediction, TREC. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HÜCRESEL YOLAK ÇİZGELERİNİN YÖNETİMİ İÇİN ÇOKLU GÖRÜNTÜLEME YÖNTEMİ Gürcan Güleşir Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Uğur Doğrusöz Eylül, 2003 Gen araştırmalarındaki gelişmeler, moleküler seviyedeki hücresel işlemlerin in celenmesi sonucu elde edilen verinin saklanması, birleştirilmesi ve analiz edilmesi için ileri tekniklerin ihtiyacını doğurmuştur. Sözkonusu veri büyük ve karmaşık olduğundan dolayı, anlaşılabilirliğini arttırmak için ileri görselleme ve karmaşıklık yönetimi tekniklerinin geliştirilmesine ihtiyaç vardır. Bu tezde, yönlü çizge halinde bulunan karmaşık yolak verisini değiştirmek için bir tek özne - çoklu görüntü yöntemi sunuyoruz. Sunulan yöntem değişen şekillerde ve ebatlardaki potansiyel olarak büyük olan yolak verisinin görsellenmesini sağlar. Bir taraftan özne ve görüntüleri korurken, diğer yandan da izleyen yazılım desenini kullanarak bütün görüntüleri düzenleme yetisine de sahiptir. Sunulan yöntem özne verisinin üzerindeki görüntülerin doğruluğunu ve tutarlılığını da garanti altına almaktadır. Karmaşıklığın (yüksek dereceli yolak nesneleri) azaltılması için ihtiyaç duyulan biyolojik verinin yinelen mesi de yöntemimizin başka bir faydasıdır. Çalışan bir yolak değiştiricisinin düzgünce modüllenmiş bir parçası olarak bu iskelet, yolak biyo-enformatiğinin alana has ihtiyaçlarına karşılık vermesi ile her hangi bir tek özne - çoklu görüntü yöntemine sahip çizge değiştirme ortamından ayrılmaktadır. Anahtar sözcükler: Biyo-enformatik, hücresel yolaklar, çizge görüntülemesi, çoklu görüntüleme, görüntü koordinasyonu, yazılım mühendisliği. iv","ABSTRACT A FRAMEWORK FOR MANAGEMENT OF MULTIPLE VIEWS OF CELLULAR PATHWAY GRAPHS Gürcan Güleşir M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Doğrusöz September, 2003 The enhancements in genomic studies have given birth to the necessity of advanced techniques for storing, integrating and analyzing the accumulated data regarding molecular level cellular processes. Since this data is huge and complex, advanced visualization and complexity management techniques need to be devel oped to improve its understandability. In this thesis, we present a single subject - multiple view framework for manipulating complex pathway data, which is in the form of a directed graph. The framework facilitates visualization of potentially huge pathway data in possibly varying forms and sizes. While maintaining the subject data (i.e., path way graph) and its views, the presented framework coordinates all the views using an observer software pattern. It ensures the validity and consistency of subject data across all views. Support for replication of biological data, which is desired to reduce complexity (i.e., high degree ubique pathway objects), is another benefit of our framework. Being a neatly modularized, isolated component of a functional pathway edi tor, this framework is distinguished from any other single subject - multiple view graph editing environment by addressing the domain specific needs of pathway informatics. Keywords: Bioinformatics, software engineering, cellular pathways, graph visual ization, multiple views, view coordination. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NEON - HÜCRESEL YOLAKLARI SORGULAMAK İÇİN BETİK TABANLI BİR YAZILIM ÇERÇEVE PROGRAMI Gürkan Nişancı Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Attila Gürsoy Ocak, 2003 Hücresel yolak kısaca hücre içerisinde olan olayların gözle okunabilir şekilde soyutlanması olarak tanımlanabilir. PATİKA projesindeki amacımız bol miktar daki hücresel yolak bilgisini ınodellemek ve merkezi bir veri tabanında saklamaktı. Bilimsel veri ambarlarında karşılaşılan ortak bir problem veri içerisindeki çok değerli gizli ilişkilerin kaybedilmesi riskidir. Eğer hücresel yolak bilgilerini incele mek için gerekli sorgu araçları kullanıcılara sağlanamazsa, yeni bilgiler verebilecek bu gizli ilişkiler farkedilemeyebilir. Bu yüzden PATİKA kullanıcılarının sorgu lama sırasında ihtiyaç duyacakları form tabanlı görsel sorgu araçları yapılmıştır. Fakat görsel sorgu araçları var olan sorguların değiştirilebilmesini yada yeni sorgu tiplerinin tanımlanmasını sağlayacak kadar esnek değildir. Aynı zamanda sorguların güçlü bir şekilde ifade edilememesi, kullanıcıları sorgu tanımlamaları sırasında kısıtlamaktadır. Bu tezde PATİKA projesi için geliştirilmiş, yolak verisini bir betik dili kulla narak sorgulayan, gelişmiş bir sorgulama çerçeve programı NEON'u öneriyoruz. NEON, çok bilinen bir betik dili olan Jython'un PATİKA projesiyle entegre bir yazılım ortamı sağlayabilmesi için Java ve Jython'un kaynaşmasıyla oluşacak sis temlerden yararlanır. Çerçeve programı kullanıcılara kendi yazacakları betiklerle veri ambarının istedikleri kısımlarını incelemelerine olanak sağlayacak şekilde es nek tasarlanmıştır. PATİKA sisteminde yapılan testler sonucunda metodolojinin yolak veri tabanında başarıyla çalıştığı ve hücresel yolakların PATİKA ile anal izinde önemli ilerlemelere yol açtığı gözlemlenmiştir. Anahtar sözcükler: NEON, PATİKA projesi, hücresel yolak, veri ambarı, betik dili kullanarak sorgulama, çerçeve programı, Java, Jython, fonksiyon tabanlı hesaplama. iv","ABSTRACT NEON - A SCRIPT-BASED SOFTWARE FRAMEWORK FOR QUERYING CELLULAR PATHWAYS Gürkan Nişancı M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Attila Gürsoy January, 2003 A cellular pathway can be defined briefly as a readable abstraction of events by human happening in the cell. With the PATİKA project, our aim was to model and store vast amount of cellular pathway data in a centralized database. A common problem of every scientific database is the risk of losing highly valuable hidden relations in the data. If appropriate query tools can not be provided to users for analyzing pathway data, these hidden relations can not be detected, which may infer new information. For that reason form-based visual query tools are built to provide querying facility that will be needed by PATİKA users. How ever, queries made with visual tools are not flexible enough to let users modify the existing queries or define new queries to execute on the PATİKA database. Also lack of powerful expression with these tools restricts the users while defining queries. In this thesis we propose an advanced query framework, NEON, which is developed for the PATİKA project for querying cellular pathways by using and extending a scripting language. It utilises a novel integration of the well-known Jython scripting language with the PATİKA project to provide a software en vironment where systems can be developed in a seamless mixture of Java and Jython. The framework is designed flexible enough to enable users analyze any part of the PATİKA database by writing their own query scripts. According to the tests done on the PATİKA system, querying through scripting methodol ogy works fine on our pathway database and more effective analysis of cellular pathways through PATİKA seems to be facilitated. Keywords: NEON, the PATİKA project, cellular pathway, database, query through scripting, framework, Java, Jython, computing with function. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GÖRÜŞ TABANLI ELYAZISI HARF TANINMASI Özcan Öksüz Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Asst. Prof. Dr. Uğur Güdükbay Eylül, 2003 Etkileşimli otomatik el yazısı tanınması son 40 yıldır araştırma konusu olmuştur. Otomatik posta adresi ve ZIP kodu okunmasında, formlara girilen bilgilerin okun masında, banka çeklerinden bilgi alınmasında, kurumsal arşivlerin işlenmesinde, otomatik olarak pasaportların denetlenmesinde vb kullanılmıştır. Avuçiçi bil gisayarların, sayısal taşınır bilgisayarların ve gelişmiş cep telefonlarının popüler olmasından dolayı son zamanlarda çok ilgi görmüştür. Geleneksel olarak insan ve bilgisayar iletişimi klavye ve fare ile sağlanmaktadır. Kalem benzeri aletle etk ileşimli elyazısı tanıma bilgisayarla, klavye dışında dinamik iletişim kurma yolları sunar. Bu bilgisayara veri girişinin daha doğal yolla yapılmasını sağlamaktadır. Bu tezde, hem etkileşimli, hem de etkileşimsiz sistemlerin avantajını kullanan bir karakter tanıma sistemi sunulmuştur. Standart video kamera kullanılarak, karakterler kağıt üzerine yazılırken, kalemin uç noktası bulunup kaydedilmiştir. Sonra kaydedilen koordinatlar yönlerin hesaplanmasında kullanılmıştır. Sonunda, elyazısı karakter bir dizi yazı yönünün birbiri ardısıra görüntülerde değişmesiyle bulunmuştur. Kalem hareketinin yön bilgisi karakterin temel sınıflandırılmasında, pozisyonu ise karakterin bulunmasında kullanılmıştır. Karakterler bulunduk tan sonra, LaTeX kodu hazırlama metoduna yollanmıştır. Desteklenen LaTeX çevrebirimleri, dizi hazırlanması, referans, bölüm, listeleme, formül, harfi harfine ve normal yazıdır. Deneylerde harflerin 90% oranında doğru tanıma yüzdesine erişilmiştir. Temel tanıma hataları düzensiz yazımdan ve benzer şekilli harflerin belirsizliğinden kaynaklanmaktadır. Anahtar sözcükler: desen tanıma, karakter tanıma, etkileşimli tanıma sistemleri, LaTeX. iv","ABSTRACT VISION BASED HANDWRITTEN CHARACTER RECOGNITION Özcan Öksüz M.S. in Computer Engineering Supervisor: Asst. Prof. Dr. Uğur Güdükbay September, 2003 Online automatic recognition of handwritten text has been an ongoing research problem for four decades. It is used in automated postal address and ZIP code and form reading, data acquisition in bank checks, processing of archived institutional records, automatic validation of passports, etc. It has been gaining more interest lately due to the increasing popularity of handheld computers, digital notebooks and advanced cellular phones. Traditionally, human-machine communication has been based on keyboard and pointing devices. Online handwriting recognition promises to provide a dynamic means of communication with computers through a pen like stylus, not just an ordinary keyboard. This seems to be a more natural way of entering data into computers. In this thesis, we develop a character recognition system that combines the advantage of both on-line and off-line systems. Using an USB CCD Camera, positions of the pen-tip between frames are detected as they are written on a sheet of regular paper. Then, these positions are used for calculation of directional information. Finally, handwritten character is characterized by a sequence of writing directions between consecutive frames. The directional information of the pen movement points is used for character pre-classification and positional information is used for fine classification. After characters are recognized they are passed to LaTeX code generation subroutine. Supported LaTeX environments are array construction, citation, section, itemization, equation, verbatim and normal text environments. During experiments a recognition rate of 90% was achieved. The main recognition errors were due to the abnormal writing and ambiguity among similar shaped characters. Keywords: pattern recognition, character Recognition, on-line recognition sys tems, LaTeX. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET MOBIL SİSTEMLERDE VERİ MADENCİLİĞİ KULLANILARAK KULLANICI HAREKETLERİNİN TAHMİNİ Gökhan Yavaş Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Özgür Ulusoy Aralık, 2003 Mobil bilgisayar sistemleri kapsamında yürütülen en önemli araştırma konularından biri de konum bilgisi yönetimidir. Konum bilgisi yönetimi, sistemden servis alan mobil kullanıcıların, zamana bağlı olarak değişen konum bilgilerinin uygun metodlar kullanarak güncellenmesi, saklanması ve gerektiğinde kullanılması konularını içerir. Son zamanlarda, hareket tahmini de konum bilgisi yönetimi alanında başlıca araştırma konularından biri haline gelmiştir. Bu tezde, mobil kullanıcıların hareket modellerinin veri madenciliği kullanılarak çıkarılması ve bu modeller kullanılarak mobil kullanıcıların daha sonraki hareketlerinin tahmin edilmesi için yeni bir algoritma geliştirilmiştir. Üç aşamadan oluşan bu algoritmanın ilk aşamasında kullanıcı hareket modelleri, kullanıcıların önceden kaydedilmiş mobil yörüngelerinden veri madenciliği kullanılarak çıkarılmaktadır. İkinci aşamada bulunan hareket modellerinden hareket kuralları üretilmekte, son aşamada ise bu hareket kuralları kullanıcının bir sonraki hücreler arası hareketinin tahmini için kullanılmaktadır. Sunulan algoritmanın performansı simülasyonlar yardımıyla iki farklı tahmin yöntemiyle karşılaştmlmıştır. Performans sonuçlan algoritmamızın diğer metodlardan daha doğru tahminler yapabildiğini göstermektedir. Anahtar Sözcükler. Yer tahmini, veri madenciliği, mobil bilgisayar sistemleri, hareket modelleri, hareket tahmini iv","ABSTRACT USING A DATA MINING APPROACH FOR THE PREDICTION OF USER MOVEMENTS IN MOBILE ENVIRONMENTS Gökhan Yavaş M.S. in Computer Engineering Supervisor: Prof. Dr. Özgür Ulusoy December, 2003 Mobility prediction is one of the most essential issues that need to be explored for mobility management in mobile computing systems. In this thesis, we propose a new algorithm for predicting the next inter-cell movement of a mobile user in a Personal Communication Systems network. In the first phase of our three-phase algorithm, user mobility patterns are mined from the history of mobile user trajectories. In the second phase, mobility rules are extracted from these patterns, and in the last phase, mobility predictions are accomplished by using these rules. The performance of the proposed algorithm is evaluated through simulation as compared to two other prediction methods. The performance results obtained in terms of Precision and Recall indicate that our method can make more accurate predictions than the other methods. Keywords: Location prediction, data mining, mobile computing, mobility patterns, mobility prediction m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TERS KİNEMATİK YÖNTEMİ KULLANILARAK İNSAN HAREKETİNİN KONTROLÜ Aydemir MEMİŞOĞLU Bilgisayar Mühendisliği Bölümü, Yüksek Lisans Tez Yöneticileri: Prof. Dr. Bülent Özgüç ve Yrd. Doç. Dr. Uğur Güdükbay Ağustos, 2003 Eklemli vücutların animasyonu bilgisayar grafiği alanında en fazla ilgi çeken konulardan biridir. Eklemli vücutların animasyonu için geliştirilen teknikler, anahtar resimler arasında ara değer kestirimi metodlarmdan hareket yakalama tekniklerine kadar geniş bir yelpaze oluşturmaktadır. Bu tekniklerden biri olan ve robotbilimden uyarlanan ters kinematik yöntemi, canlandırıcıya birçok hareket parametresini tanımlayabilme imkanı vererek gerçekçi bir animasyona olanak sağlamaktadır. Bu çalışmada, insan vücudunun yürüme hareketi için kul lanılan etkileşimli, hiyerarşik bir hareket kontrol sistemi anlatılmaktadır. Ayrıca, çalışma kapsamında, hedefe yönelik hareket ve yürüme gibi değişik seviyelerde hareket kontrol teknikleri kullanılarak, eklemli vücut canlandırması yapan bir sistem geliştirilmiştir. Eklemli vücudun hareketim ters kinematik yöntemi kulla narak kontrol etmek için, Pennsylvania Üniversitesi'nde geliştirilen İKAN yazılım paketinden yararlanılmıştır. Anahtar kelimeler: kinematik, ters kinematik, eklemli vücut, hareket kontrolü, eğri, yürüyüş. iv","ABSTRACT HUMAN MOTION CONTROL USING INVERSE KINEMATICS Aydemir MEMİŞO?LU M.S. in Computer Engineering Supervisors: Prof. Dr. Bülent Özgüç and Assist. Prof. Dr. Uğur Güdükbay August, 2003 Articulated figure animation receives particular attention of the computer graph ics society. The techniques for animation of articulated figures range from simple interpolation between keyframes methods to motion-capture techniques. One of these techniques, inverse kinematics, which is adopted from robotics, provides the animator the ability to specify a large quantity of motion parameters that results with realistic animations. This study presents an interactive hierarchical motion control system used for the animation of human figure locomotion. We aimed to develop an articulated figure animation system that creates movements, like goal-directed motion and walking by using motion control techniques at different levels. Inverse Kinematics using Analytical Methods (IKAN) software, which was developed at the University of Pennsylvania, is utilized for controlling the motion of the articulated body using inverse kinematics. Keywords: kinematics, inverse kinematics, articulated figure, motion control, spline, gait. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET PC KÜMELERİ ÜZERİNDE PARALEL METİN ERİŞİMİ Aytül Çatal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Cevdet Aykanat Eylül, 2003 Ters dizin bölümleme problemi paralel metin erişim sistemleri için araştırıldı. Hedef, bir PC kümesi üzerine dağıtılmış ters dizin üzerinde hızlı ve verimli sorgulamayı başarmaktır. Dizin kayıtlarının belge numaraları veya kelime numaralarına göre dağıtıldığı ters dizin bölümlemesi için alternatif stratejiler düşünülmüş ve değerlendirilmiştir. Her iki bölümleme planının performansı toplam disk erişim sayısına ve sistemdeki toplam iletişim hacmine bağlıdır. Belge numarası bazlı bölümlemede, toplam disk erişim sayısı kelime numarası bazlı bölümleme ile kıyaslandığında daha büyük olabilirken, toplam iletişim hacmi doğal olarak en az miktardadır. Diğer bir taraftan, kelime numarası bazlı bölümlemede, toplam iletişim hacmi oldukça büyük olabilse de, toplam disk erişim sayısı seri algoritma tarafından ulaşılan alt sınıra zaten eşittir. Şu ana kadar yapılmış çalışmalar, bu bölümleme planlarını sıralı bir biçimde icra etmek tedirler ve performanslarını simulasyonla karşılaştırmaktadırlar. Bu çalışmada, paralel metin erişim sistemi bir PC kümesi üzerinde tasarlandı ve programlan ması gerçekleştirildi. Hiperçizge kuramsal bölümleme modellerini seçtik ve sıralı ve hiperçizge kuramsal bölümleme planlarının performans karşılaştırmadım par alel metin erişim sistemimiz üzerinde gerçekleştirdik. Bundan başka, sistemimizin sorgulama arayüzünü ve kullanıcı arayüzünü tasarladık ve programlanmasını gerçekleştirdik. Anahtar sözcükler: Paralel metin erişimi, ters dizin, paralel sorgulama, ters dizin bölümlemesi, sistem performansı. iv","ABSTRACT PARALLEL TEXT RETRIEVAL ON PC CLUSTERS Aytül Çatal M.S. in Computer Engineering Supervisor: Prof. Dr. Cevdet Aykanat September, 2003 The inverted index partitioning problem is investigated for parallel text retrieval systems. The objective is to perform efficient query processing on an inverted index distributed across a PC cluster. Alternative strategies are considered and evaluated for inverted index partitioning, where index entries are distributed ac cording to their document-ids or term-ids. The performance of both partitioning schemes depend on the total number of disk accesses and the total volume of communication in the system. In document-id partitioning, the total volume of communication is naturally minimum, whereas the total number of disk accesses may be larger compared to term-id partitioning. On the other hand, in term-id partitioning the total number of disk accesses is already equivalent to the lower bound achieved by the sequential algorithm, albeit the total communication vol ume may be quite large. The studies done so far perform these partitioning schemes in a round- robin fashion and compare the performance of them by simu lation. In this work, a parallel text retrieval system is designed and implemented on a PC cluster. We adopted hypergraph-theoretical partitioning models and carried out performance comparison of round-robin and hypergraph-theoretical partitioning schemes on our parallel text retrieval system. We also designed and implemented a query interface and a user interface of our system. Keywords: Parallel text retrieval, inverted index, parallel query processing, in verted index partitioning, system performance. ui"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HİYERARŞİK KENDİ KENDİNE ÖĞRENEN SİNİR AĞLARIYLA FİLOGENETİK AĞAÇ YAPISININ OLUŞTURULMASI Hayretdin Bahsi Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Yrd. Doç. Dr. Atilla Gürsoy, Yrd. Doç. Dr. Rengül Çetin Atalay Ocak, 2002 Biyoloji alanında fılogenetik ağaç yapılan, şu anda varolan türlerin ortak atalarının belirlenerek bu türlerin evrimsel geçmişinin açıklanması amacıyla oluşturulur. Özellikle moleküler biyoloji alanında, bu ağaç yapıları, proteinlerin veya DNA dizilerinin evrimsel ilişkilerini ortaya çıkarmak için kullanılır. Çoğu zaman, fılogenetik ağaç yapılarının oluşturulması zor ve karmaşık bir işlem gerektirmektedir. Örneğin, 30 DNA dizisine sahip bir girdi için 1036,dan fazla farklı ağaç yapısının arasından en iyisini seçmek gerekir. En iyi olan ağaç yapısının uygun bir zaman içinde belirlenebilmesi için literatürde bir çok hiyerarşik kümeleme teknikleri mevcuttur. Diğer yandan, kendi kendine öğrenen sinir ağları, girdi türleri hakkında her hangi bir bilgi sahibi olmadan çok boyutlu girdilerin iki boyutlu çıktı uzaylarına indirgenmesinde çok başarılı olmaktadır. Bu çalışmada, kendi kendine Öğrenen sinir ağları yöntemi ard arda kullanılarak ağaç yapıları oluşturulmaktadır. Bu amaçla, iki farklı algoritma tasarlanmıştır. Birincisi, ağacı kökten başlayıp yapraklara doğru giderek oluşturmaktadır. İkincisi ise ağacı oluşturmaya yapraklardan başlamakta ve köke doğru ilerlemektedir. Tasarlanan algoritmalar ağaç topolojisinin doğruluğu göz önüne alınarak test edilmiştir. Bu algoritmalar, çok kullanılan UPGMA ve Komşu birleştirme metotlarından çok daha iyi sonuç vermektedir. Ayrıca bu çalışma, ağaç kollarının uzunluklarını tahmin etme problemi için de bazı çözümler sunmaktadır. Anahtar Kelimeler: Fılogenetik Ağaç Yapısı, Kendi Kendine Öğrenen Sinir Ağları, Evrim, DNA","ABSTRACT INFERRING PHYLOGENETICAL TREE BY USING HIERARCHICAL SELF ORGANIZING MAPS Hayretdin Bahsi M.S in Computer Engineering Supervisor: Assist. Prof. Dr. Atilla Gürsoy Co-supervisor: Assist. Prof. Dr. Rengül Çetin Atalay January, 2002 In biology, inferring phylogenetical tree is an attempt to describe the evolutionary history of today's species with the aim of finding their common ancestors. Specifically in molecular biology, it is used in understanding the evolution relationships between proteins or DNA sequences. Inferring phylogenetical tree can be a very complicated task since even for the input data having thirty sequences, the best tree must be chosen among 1 036 possible trees. In order to find the best one in a reasonable time, various hierarchical clustering techniques exist in the literature. On the other side, it is known that Self Organizing Maps (SOM) are very successful in mapping higher dimensional inputs to two dimensional output spaces (maps) without having any priori information about input patterns. In this study, SOM are used iteratively for tree inference. Two different algorithms are proposed. First one is hierarchical top-down SOM method which constructs the tree from the root to the leaves. Second one uses a bottom-up approach that infers the tree from the leaves to the root. The efficiency of Hierarchical SOM is tested in terms of tree topology. Hierarchical SOM gives better results than the most popular phylogeny methods, UPGMA and Neighbor-joining. Also this study covers possible solutions for branch length estimation problem. Keywords: Phylogenetic Tree, Self Organizing Map, Kohonen Map, Evolution, DNA \^>"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KIŞISELLEŞTIRILMIŞ YÜZ MODELLEME VE ANİMASYONU Fatih Erol Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Güdükbay Ocak, 2002 Kişisel insan yüzünü gerçekçi olarak modellemek ve canlandırmak bilgisayar grafiği alanında önemli ve zor bir problemdir. Bu tezde, bahsedilen prob lemi çözmeye yönelik olarak geliştirilen bir yüz modelleme ve animasyon sis temi anlatılmaktadır. Sistemde kullanılan kas tabanlı genel yüz modeli defor masyon teknikleri ile deforme edilerek kişisel yüz modelleri oluşturulabilmektedir. Bunun için insan yüz modelinin birbirine dik olan ön ve yanal fotoğrafları kul lanılmaktadır. Resim işleme teknikleri ile fotoğraf üzerinde bazı özellikler bu lunduktan sonra, kullanıcı arabiriminin sağladığı fonksiyonlar aracılığıyla yüz üzerindeki tanımlayıcı noktaların yerleri tam olarak belirlenmektedir. On ve yanal yüz fotoğraflarındaki bu noktalar kullanılarak genel model deforme edilmektedir. Daha sonra, sistem kişiselleştirilmiş modeldeki kas vektörlerinin yeni pozisyon larını belirlemektedir. Bu yöntemle oluşturulan kişisel yüz modelleri parametrik interpolasyon teknikleri ile hareket ettirilmektedir. Anahtar sözcükler: yüz animasyonu, boyama, doku eşleme, deformasyon, özellik çıkarımı. iv","ABSTRACT MODELING AND ANIMATING PERSONALIZED FACES Fatih Erol M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay January, 2002 A very important and challenging problem in computer graphics is modeling and animation of individualized face models. In this thesis, we describe a fa cial modeling and animation system attempting to address this problem. The system uses muscle-based generic face model and deforms it using deformation techniques to model individualized faces. Two orthogonal photos of the real faces are used for this purpose. Image processing techniques are employed to extract certain features on the photographs, which are then refined manually by the user through the facilities of the user interface of the system. The feature points lo cated on the frontal and side views of a real face are used to deform the generic model. Then, the muscle vectors in the individualized face model are arranged accordingly. Individualized face models produced in this manner are animated using parametric interpolation techniques. Keywords: facial animation, rendering, texture mapping, deformation, feature extraction. iii rrr-........"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET WEB TABANLI BİLGİ KAYNAKLARI İÇİN BİR KAVRAM HARİTASI VERİ MODELİ GERÇEKLEŞTİRİMİ Mustafa Kutlutürk Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos, 2002 Web son yıllarda yoğun bir bilgi kaynağı olmuştur. Milyonlarca insan düzenli olarak Web'i kullanmaktadır ve kullanıcı sayısı hızla artmaktadır. Web hemen hemen tüm sosyal, ekonomik, eğitimsel v.b. alanlardaki uğraşları sunan dünyadaki en geniş bilgi merkezidir ve dünyanın herhangibir yerinden bir kişi bu büyük merkezi yerinden kalkmak zorunda bile kalmadan ziyaret edebilir. Çok büyük olmasından dolayı, istenilen veriye Web' de zaman ve maliyet açısından verimli bir yolla erişebilmek, önemli bir araştırma konusudur. Web kullanıcılarına istedikleri bilgiye erişebilme konusunda yardımcı olmak için, son birkaç yılda bir çok arama motoru üretilmiştir. Bununla beraber, bu arama motorlarının birçoğu kavram bağımsız arama metodla rı kullanmaktadır ve kullanıcılara birçok gereksiz arama sonuçlan sunan anahtar kelime tabanlı yaklaşımlara dayanmaktadır. Bu tezde, Web tabanlı bilgi kaynaklan için kavram haritaları standartlarım kullanan bir veri modeli sunulmaktadır. Bu modelde, kavramlar, kavram ilişkileri ve kavram oluşumları (bu çalışmada kavram metalinkleri ve kavram kaynakları olarak anılacaklar) temel unsurlardır. Aslında, sunulan model bir ""metaveri"" model olup Web tabamı bilgi kaynağının içeriğini tanımlayarak modellenen bilgi kaynağı üzerinde ""gerçek bilgi haritalan"" üretmektedir. Böylece, verimli bir veri araması ve sorgulaması için Web tabamı bilgi kaynağının kavramsal endeksi oluşturulur.VI Ayrıca geniş kabul gören ters çevrilmiş dosya endeksi kullanılarak, sunulan modelde tam kelime endeksi de uygulanmıştır. Verinin hızlı artışına bağlı olarak, yeni dokümanlar eklenmesi ve ters çevrilmiş dosya endeksinin dinamik olarak güncellenmesi kaçınılmazdır. Sunulan modelde, kullanılan ters çevrilmiş dosya endeksi için verimli bir dinamik güncelleme metodu uygulanmıştır. Sunulan kavram haritası veri modeli, anahtar kelime tabanlı arama ve kavram merkezli arama metodlarının güçlerini birleştirmektedir. Sunulan modelin Web tabanlı bilgi kaynaklarının verimli ve etkin bir şekilde aranması ve sorgulanması problemine büyük katkıda bulunduğunu gösteren bir prototip arama motoru da sunulmaktadır. Anahtar sözcükler. Metadata, XML, Web tabanlı bilgi kaynağı, Web araması, ters çevrilmiş dosya endeksi, dinamik güncelleme, Web veri modellemesi, kavramsal endeksleme.","ÖZET WEB TABANLI BİLGİ KAYNAKLARI İÇİN BİR KAVRAM HARİTASI VERİ MODELİ GERÇEKLEŞTİRİMİ Mustafa Kutlutürk Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos, 2002 Web son yıllarda yoğun bir bilgi kaynağı olmuştur. Milyonlarca insan düzenli olarak Web'i kullanmaktadır ve kullanıcı sayısı hızla artmaktadır. Web hemen hemen tüm sosyal, ekonomik, eğitimsel v.b. alanlardaki uğraşları sunan dünyadaki en geniş bilgi merkezidir ve dünyanın herhangibir yerinden bir kişi bu büyük merkezi yerinden kalkmak zorunda bile kalmadan ziyaret edebilir. Çok büyük olmasından dolayı, istenilen veriye Web' de zaman ve maliyet açısından verimli bir yolla erişebilmek, önemli bir araştırma konusudur. Web kullanıcılarına istedikleri bilgiye erişebilme konusunda yardımcı olmak için, son birkaç yılda bir çok arama motoru üretilmiştir. Bununla beraber, bu arama motorlarının birçoğu kavram bağımsız arama metodla rı kullanmaktadır ve kullanıcılara birçok gereksiz arama sonuçlan sunan anahtar kelime tabanlı yaklaşımlara dayanmaktadır. Bu tezde, Web tabanlı bilgi kaynaklan için kavram haritaları standartlarım kullanan bir veri modeli sunulmaktadır. Bu modelde, kavramlar, kavram ilişkileri ve kavram oluşumları (bu çalışmada kavram metalinkleri ve kavram kaynakları olarak anılacaklar) temel unsurlardır. Aslında, sunulan model bir ""metaveri"" model olup Web tabamı bilgi kaynağının içeriğini tanımlayarak modellenen bilgi kaynağı üzerinde ""gerçek bilgi haritalan"" üretmektedir. Böylece, verimli bir veri araması ve sorgulaması için Web tabamı bilgi kaynağının kavramsal endeksi oluşturulur.VI Ayrıca geniş kabul gören ters çevrilmiş dosya endeksi kullanılarak, sunulan modelde tam kelime endeksi de uygulanmıştır. Verinin hızlı artışına bağlı olarak, yeni dokümanlar eklenmesi ve ters çevrilmiş dosya endeksinin dinamik olarak güncellenmesi kaçınılmazdır. Sunulan modelde, kullanılan ters çevrilmiş dosya endeksi için verimli bir dinamik güncelleme metodu uygulanmıştır. Sunulan kavram haritası veri modeli, anahtar kelime tabanlı arama ve kavram merkezli arama metodlarının güçlerini birleştirmektedir. Sunulan modelin Web tabanlı bilgi kaynaklarının verimli ve etkin bir şekilde aranması ve sorgulanması problemine büyük katkıda bulunduğunu gösteren bir prototip arama motoru da sunulmaktadır. Anahtar sözcükler. Metadata, XML, Web tabanlı bilgi kaynağı, Web araması, ters çevrilmiş dosya endeksi, dinamik güncelleme, Web veri modellemesi, kavramsal endeksleme."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET Comet-KO: BİLGİ DÜZENLEMESİ İÇİN BİR ANLAMSAL AĞ ORTAMI Tayfun Küçükyılmaz Bilgisayar Mühendisliği Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. David Davenport Eylül, 2003 Bu tez temellerini instrüktivist ve konstrüktivist eğitim teorilerinden alan bir bilgi inşaa ve düzenleme olan Comet-KO' u sunmaktadır. Comet-KO, bilgi inşaası ve düzenlemesi sürecinde kullanıcıya bilgiyi toplama, yaratma ve sunmada, bu bilgileri ilşkilendirmede ve bu süreçte diğer kullanıcılarla etkileşmede yardımcı olmayı amaçlamaktadır. Comet- KO bilginin ifadesini yazıya dayalı klasik bilgi sunum methodları yerine, bilgi inşaasında bu tekniklerden çok daha üstün olan mantıksal ağ teknikleri ile yapmaktadır. Bilinen anlamsal ağ tekniklerini gruplama ve hipergraf çizim gücü ile geliştiren Comet-Ko, bilginin sunumu konusunda en kapsamlı anlamsal ağ geliştirme araçlarından biri olmaya adaydır. Anahtar sözcükler: İnstrüktivizm, Konstrüktivizm, Anlamsal Ağlar, Bilgi Düzenlemesi iv","ABSTRACT Comet-KO: A CONCEPT MAPPING ENVIRONMENT FOR KNOWLEDGE ORGANIZATION Tayfun Küçükyılmaz M.S. in Computer Engineering Supervisor: Asst. Prof. David Davenport September 2002 This thesis presents Comet-KO, a software tool for knowledge construction and organization, which takes its basis from instructivist and constructivist learning theories. Comet-KO aids users to collect, create and represent information, visualize relationships between concepts, and collaborate during the knowledge construction process. The representation of knowledge in Comet-KO is based on semantic networking techniques, which proved to be more powerful than conventional text based note-taking methodologies. In addition to formal semantic networking language, grouping and hypergraph drawing is also supported by Comet-KO, which brings a new perspective on visualization of knowledge construction. Keywords: Instructivism, Constructivism, Concept Maps, Semantic Networking, Knowledge Organization,"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÖZNÎTELİK ARALIKLARIYLA FAYDA MAKSİMİZAS YONUNA YÖNELİK SINIFLANDIRMA Nazlı İkizler Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. H. Altay Güvenir Eylül, 2002 Uzun zamandır, sınıflandırma algoritmaları bütün olası hataların sonuçlarının benzer olacağı varsayımıyla, tahmine dayalı hataların sayısını azaltma üzerinde yoğunlaşmışlardır. Fakat bu varsayım, gerçek hayattaki pek çok durum için elverişli değildir. Örneğin, tıbbi tanı alanında, hasta olan bir kimseyi sağlıklı olarak sınıflandırmak tam tersi duruma oranla çok daha ciddi bir hatadır. Bu nedenle, sınıflandırmaların bu tip asimetrik maliyet ve fayda kriterleri göz önünde bulunduracâk yeni sınıflandırma metotlarına büyük ihtiyaç vardır. Bu tezde, maliyete duyarlı sınıflandırma kavramları üzerinde durulmakta ve öznitelik izdüşümü tabanlı bilgi gösterimini kullanan, Öznitelik Aralıklarıyla Fayda Arttırma (BMFI) olarak isimlendirilen yeni bir sınıflandırma algoritması sunulmaktadır. BMFI çatısı altında, farklı veri kümelerinde etkili olduğu gösterilen beş ayrı oylama yöntemi tanıtılmıştır. Bununla birlikte, birkaç genelleme ve budama yöntemi geliştirilmiş ve denenmiştir. Deneysel değerlendirmelerde BMFI, performansın problemin veri kümesindeki fayda kriterlerine ve sınıf dağılımlarına çok bağlı olması gerçeğine rağmen, sarma prensibine dayalı yeni maliyete duyarlı sınıflandırma algoritmalarıyla karşılaştırıldığında, umut verici bir performans sergilemiştir. Ek olarak, maliyete duyarlı ve fayda arttırımına yönelik metotların değerlendirilmesi için, fayda doğruluğu olarak isimlendirilmiş yeni bir metrik önerilmiştir. Bu metrik, elde edilen toplam faydanın, mümkün olan en yüksek fayda değerine oranla göreceli doğruluğunu hesaplamaktadır. Anahtar sözcükler, makine öğrenmesi, sınıflandırma, maliyet duy maksimizasyonu, öznitelik aralıkları, oylama, budama. ^P^i s iv","ABSTRACT BENEFIT MAXIMIZING CLASSIFICATION USING FEATURE INTERVALS Nazlı İkizler M.S. in Computer Engineering Supervisor: Prof. Dr. H. Altay Güvenir September, 2002 For a long time, classification algorithms have focused on minimizing the quantity of prediction errors by assuming that each possible error has identical consequences. However, in many real-world situations, this assumption is not convenient. For instance, in a medical diagnosis domain, misdiagnosing a sick patient as healthy is much more serious than its opposite. For this reason, there is a great need for new classification methods that can handle asymmetric cost and benefit constraints ofclâssificatıöhs. In this thesis, we discuss cost-sensitive classification concepts and propose a new classification algorithm called Benefit Maximization with Feature Intervals (BMFI) that uses the feature projection based knowledge representation. In the framework of BMFI, we introduce five different voting methods that are shown to be effective over different domains. A number of generalization and pruning methodologies based on benefits of classification are implemented and experimented. Empirical evaluation of the methods has shown that BMFI exhibits promising performance results compared to recent wrapper cost-sensitive algorithms, despite the fact that classifier performance is highly dependent on the benefit constraints and class distributions in the domain. In order to evaluate cost- sensitive classification techniques, we describe a new metric, namely benefit accuracy which computes the relative accuracy of the total benefit obtained with respect to the maximum possible benefit achievable in the domain. Keywords: machine learning, classification, cost-sensitivity, benefit maximization, feature intervals, voting, pruning. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KAPLAMA KATSAYISI TABANLI KÜME OLUŞTURMA METODOLOJİSİ KULLANARAK ANINDA YENİ OLAY BELİRLEME VE KÜME OLUŞTURMA Ahmet Vural Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Ör. Fazlı Can Ağustos, 2002 Bu çalışmada, anında yeni olay belirlemek ve olay kümeleri oluşturmak amacıyla, kaplama katsayısı tabanlı küme oluşturma metodolojisi (C^M) kavramları kullanıldı. Çahşmanın ana teması, yeni olay belirlemek için Ç?M algoritmasının tohum seçme işlemini kullanmaktır. C3M'in çalışma prensibi arımda kümelemeye uygun olmadığından, algoritmada değişiklikler yapıldı. Ayrıca, çok büyük olay kümelerinin oluşumunu önlemek ve bütün dokümanlara, tohum olabilmeleri için eşit şans tanımak amacıyla, pencere yöntemi kullanıldı. Tohum dokümanlarının miktarını kontrol etmek maksadıyla, olay kümeleme işi için bir eşik kavramı ortaya çıkarıldı. Bu kavramı, çok küçük değişikliklerle, yeni olay belirlemede de kullanıldı. Deneyler esnasında, orjinal konu belirleme ve takip çalışmasında da kullanıldı. TDT1 yığınından yararlanılmıştır. Yeni olay belirleme ve olay kümeleme işlemlerinde TDT1 yığınının ağırlıklı ve düz uyarlamaları kullanıldı. Düz uygulamalar için daha iyi sonuçlar elde edildi. Aranda olay belirleme alanındaki sonuçlar DMASS yaklaşımınınkilerle karşılaştırıldığında yanlış alarm oranlan açısaldan daha iyi performans elde edilmiştir. Anahtar Sözcükler. Kümeleme, anında olay kümelemesi, anında olay belirleme.","ABSTRACT ONLINE NEW EVENT DETECTION AND CLUSTERING USING THE CONCEPTS OF THE COVER COEFFICIENT-BASED CLUSTERING METHODOLOGY Ahmet Vural M.S. in Computer Engineering Supervisor: Prof. Dr. Fazlı Can August, 2002 In this study, we use the concepts of the cover coefficient-based clustering methodology (C3M) for on-line new event detection and event clustering. The main idea of the study is to use the seed selection process of the C M algorithm for the purpose of detecting new events. Since C3M works in a retrospective manner, we modify the algorithm to work in an on-line environment. Furthermore, in order to prevent producing oversized event clusters, and to give equal chance to all documents to be the seed of a new event, we employ the window size concept. Since we desire to control the number of seed documents, we introduce a threshold concept to the event clustering algorithm. We also use the threshold concept, with a little modification, in the on-line event detection. In the experiments we use TDT1 corpus, which is also used in the original topic detection and tracking study. In event clustering and event detection, we use both binary and weighted versions of TDT1 corpus. With the binary implementation, we obtain better results. When we compare our on-line event detection results to the results of UMASS approach, we obtain better performance in terms of false alarm rates. Keywords: Clustering, on-line event clustering, on-line event detection. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET YENİ BİR İŞLEM VERİSİ PARÇALAMA ŞEMASI TABANLI ETKİN PARALEL FREKANS TARAMA Eray Özkural Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Cevdet Ay kanat Ocak, 2002 Son yıllarda, gelişen veri toplama yetenekleriyle birlikte büyük miktarlarda veri toplanmıştır. Bilimsel ve iş alanlarından elde edilen çok geniş veriler için yararlı bilgilerin otomatik olarak bulunmasi gerekmektedir. Veri tarama bu tarz bilgi keşfi için etkin algoritmik çözümlerin değişik büyük veriler üzerinde uygu lanmasıdır. Frekans tarama bir işlem ya da ilişkisel veri tabanındaki bütün sık desenleri keşfeder ve ilişki kuralı tarama ve dizi kurali tarama gibi bir çok veri tarama algoritmalarının özünü oluşturur. Sık desen keşfi paralel programlama için dev veriler üzerinde karmaşık bir işlem olması itibariyle önemli bir problem haline gelmiştir. Bu tezde, yeni bir sınıf paralel frekans tarama algoritması öneriyoruz. Frekans tarama işini tepeden aşağı bölmek için kullanılabilecek bir işlem verisi parçalama şeması takdim ediyoruz. Yöntemimiz iki uzunluğundaki sık desenlerin çizgesi [Gp2) üzerinde çalışmakta olup, bu çizgenin köşe ayracıyla parçalanması (GPVS) işlem kümesi üzerinde iki-yollu bir parçalamaya eşlenmektedir. Elde edilen iki parça bağımsız olarak taranabilir ve bu sayede eş zamanlılık için kullanilabilir. Bu özelliğin tutması için Gf2 'deki ayraç tarafından belirlenen ve GPVS tarafından minimize edilen bir yineleme bulunmaktadır. Genel bir paralel frekans taramasi algoritmasında kullanılan bir k-yollu parçalama iki-yollu parçalama şemasından türetilmektedir. İlk olarak Gf2'yi paralel olarak hesaplarız ve ertesinde veri tabanının k-yollu bir parçalaması k işlemci için paralel kendini çağıran bir yöntemle belirlenir. Veri tabanı her işlemciye bir parça düşecek şekilde yeniden dağıtılır, izleyen tarama her işlemcide verilen seri bir tarama algoritmasıyla eş vıvıı zamanlı biçimde devam eder. FP~Growth'u seri algoritma olarak kullandiğımız tam bir program gerçekleştirilmiştir. Bir Beowulf sistemi üzerinde yapılan per formans çalışması sentetik veritabanları için iyi hızlanma kaydettiğimizi göster mektedir. Problemin zor örneklerinde gelişmiş bir algoritmanın yaklaşık iki katı hızlanma kazanmış bulunmaktayız. Ayrıca FP-Growth için bir düzeltme ve hızlandırma sunuyoruz. Anahtar sözcükler: Paralel Veri Tarama, Frekans Tarama.","ABSTRACT EFFICIENT PARALLEL FREQUENCY MINING BASED ON A NOVEL TOP-DOWN PARTITIONING SCHEME FOR TRANSACTIONAL DATA Eray Ozkural M.S. in Computer Engineering Supervisor: Prof. Cevdet Aykanat January, 2002 In recent years, large quantities of data have been amassed with advances in data acquisition capabilities. Automated detection of useful information is required for vast data obtained from scientific and business domains. Data Mining is the application of efficient algorithmic solutions on a variety of immense data for such knowledge discovery. Frequency mining discovers all frequent patterns in a transaction or relational database and it comprises the core of several data mining algorithms such as association rule mining and sequence mining. Frequent pattern discovery has become a challenge for parallel programming since it is a highly complex operation on huge datasets demanding efficient and scalable algorithms. In this thesis, we propose a new family of parallel frequency mining algo rithms. We introduce a novel transaction set partitioning scheme that can be used to divide the frequency mining task in a top-down fashion. The method op erates on the graph of frequent patterns with length two (Gp2) from which a graph partitioning by vertex separator (GPVS) is mapped to a two-way partitioning on the transaction set. The two parts obtained can be mined independently and therefore can be utilized for concurrency. In order for this property to hold, there is an amount of replication dictated by the separator in Gp2 which is minimized by the GPVS algorithm. A k-way partitioning is derived from recursive applica tion of 2- way partitioning scheme which is used in the design of a generic parallel frequency mining algorithm. First we compute Gp2 in parallel, succeeding that we designate a k-way partitioning of the database for k processors with a parallel IVrecursive procedure. The database is redistributed such, that each processor is as signed one part. Subsequent mining proceeds simultaneously and independently at each processor with a given serial mining algorithm. A complete implemen tation in which we employ FP- Growth as the sequential algorithm has been achieved. The performance study of the algorithm on a Beowulf system demon strates favorable performance for synthetic databases. For hard instances of the problem, we have gained approximately twice the speedup of a state-of-the-art algorithm. We also present a correction and optimization to FP- Growth algorithm. Keywords: Parallel Data Mining, Frequency Mining."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VİDEO VERİ TABANLARI İÇİN VERİ MODELLEME VE SORGULAMA Mehmet Emin Dönderler Bilgisayar Mühendisliği, Doktora Tez Yöneticileri: Doç. Dr. Özgür Ulusoy ve Yard. Doç. Dr. Uğur Güdükbay Temmuz, 2002 Bilgi teknolojisindeki gelişmeler ile, elde edilen, üretilen ve saklanan mültimedya veri miktarı hızlı bir şekilde artmakta ve bu veriler günümüzde birçok uygulamada kullanılmaktadır. Bu nedenle, bu verilerin düzenlenmesi ve bu verilere büyük miktarlarda bilgi bulunduran saklama alanlarından erişim gereksinimi, hem ticari hem de akademik olarak, bir tetikleyici etken oluşturmuştur. Kaçınılmaz olan bu eğilime bağlı olarak, ilk olarak resim ve özellikle daha sonra da video veri tabanı yönetim sistemleri, geleneksel veri tabanı sistemlerinin mültimedya için uygun olmaması nedeniyle, büyük bir ilgi çekmiştir. Bu tezde, yeni bir video veri tabam sistem mimarisi önerilmektedir. Bu mi marinin özelliği, yerleşimsel, zamansal, nesne görünüm, harici önerme, hareket izdüşüm ve benzerlik tabanlı nesne hareket koşullarının herhangi bir kombinas yonunu içeren yerleşim-zamansal sorgulara bir bilgi tabanı üzerine kurulu kural tabanlı bir sistem ile, anlamsal (anahtar kelime, olay/aktivite ve kategori tabanlı) ve alt seviyedeki (renk, şekil ve desen) video sorgularına da nesneye yönelik ve ilişkisel bir veri tabanı kullanılarak tam bir desteğin sağlanmasıdır. Bu tez kapsamında elde edilen araştırma sonuçları, BilVideo olarak isimlendirdiğimiz bir video veri tabanı yönetim sistemi prototipinin gerçekleştirilmesinde kul lanılmıştır. BilVideo sisteminin parçaları olan Gerçek Çıkartıcı, Video Anlamsal îlişkilendirici, Web tabanlı görsel sorgu arayüzü ve SQL benzeri metne dayalı sorgu dili de tanıtılmaktadır. Ayrıca, BilVideo sisteminin sorgu işlemcisi ve yerleşim-zamansal sorgu işleme yöntemimiz de tartışılmaktadır. Anahtar sözcükler: video veri tabanları, mültimedya veri tabanları, bilgi sistem leri, video veri modelleme, içerik-tabanlı veri alma, yerleşim-zamansal ilişkiler, yerleşim-zamansal sorgu işleme, video sorgu dilleri.","ABSTRACT DATA MODELING AND QUERYING FOR VIDEO DATABASES Mehmet Emin Dönderler Ph.D. in Computer Engineering Supervisors: Assoc. Prof. Dr. Özgür Ulusoy and Asst. Prof. Dr. Uğur Güdükbay July, 2002 With the advances in information technology, the amount of multimedia data captured, produced and stored is increasing rapidly. As a consequence, multime dia content is widely used for many applications in today's world, and hence, a need for organizing this data and accessing it from repositories with vast amount of information has been a driving stimulus both commercially and academically. In compliance with this inevitable trend, first image and especially later video database management systems have attracted a great deal of attention since tra ditional database systems are not suitable to be used for multimedia data. In this thesis, a novel architecture for a video database system is proposed. The architecture is original in that it provides full support for spatio-temporal queries that contain any combination of spatial, temporal, object-appearance, external-predicate, trajectory-projection and similarity-based object-trajectory conditions by a rule-based system built on a knowledge-base, while utilizing an object-relational database to respond to semantic (keyword, event/ activity and category-based) and low-level (color, shape and texture) video queries. Research results obtained from this thesis work have been realized by a prototype video database management system, which we call BilVideo. Its tools, Fact-Extractor and Video- Annotator, its Web-based visual query interface and its SQL-like tex tual query language are presented. Moreover, the query processor of BilVideo and our spatio-temporal query processing strategy are also discussed. Keywords: video databases, multimedia databases, information systems, video data modeling, content-based retrieval, spatio-temporal relations, spatio- temporal query processing, video query languages. iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KARAKTER DİZİSİ ARGÜMANLI ÖNERGELERİN GENELLEŞTİRİLMESİ Göker Canıtezer Yüksek Lisans, Bilgisayar Mühendisliği Tez Yöneticileri: Prof. Dr. H. Altay Güvenir, Yrd. Doç. Dr. İlyas Çiçekli Ocak, 2002 Karakter dizilerinin genelleştirilmesi, makine öğrenimi, örnek tabanlı otomatik çeviri, DNA sırası hizalama gibi pek çok alanda kullanılmaktadır. Bu tezde, verilen örneklerden oluşan karakter dizisi parametreli yüklemlerin genelleştirilmiş hallerini bulan bir yöntem sunulmaktadır. Verilen örneklerden öğrenmeye çalışmak gerçekten zor bir problemdir, çünkü genelleştirmeyi durdurmak için global optimum noktayı bulmak zor ve zaman alıcı bir işlemdir. Şu ana kadar yapılan bütün işler, en iyi çözümü bulmak için kullanılan deneme-yanılma yöntemleridir. Bu çalışma da onlardan birisidir. Bu projede, iki karakter dizisinin mümkün olan bütün hizalanmalarını bulmak için ÖEGG (Özel Enaz Genel Genelleştirme) algoritmasında uygulanan bazı kısıtlamalar kaldırılmıştır. Ek olarak, en özel genelleştirmeleri bulmak için Euclid uzaklığına benzer bir puanlandırma mekanizması kullanılmıştır. Üretilen kalıplarlardan bazıları dört farklı seçim/eleme yöntemi ile elenmiştir. Son olarak, sonuç kümesi karar listesi halinde sunularak, istisnai durumların yakalanması sağlanmıştır. Anahtar Kelimeler: genelleştirme, öegg, sıra hizalama iv","ABSTRACT GENERALIZATION OF PREDICATES WITH STRING ARGUMENTS Göker Camtezer M.S in Computer Engineering Supervisors: Prof. H. Altay Güvenir, Asst. Prof. İlyas Çiçekli January, 2002 String/sequence generalization is used in many different areas such as machine learning, example-based machine translation and DNA sequence alignment. In this thesis, a method is proposed to find the generalizations of the predicates with string arguments from the given examples. Trying to learn from examples is a very hard problem in machine learning, since finding the global optimal point to stop generalization is a difficult and time consuming process. All the work done until now is about employing a heuristic to find the best solution. This work is one of them. In this study, some restrictions applied by the SLGG (Specific Least General Generalization) algorithm, which is developed to be used in an example-based machine translation system, are relaxed to find the all possible alignments of two strings. Moreover, a Euclidian distance like scoring mechanism is used to find the most specific generalizations. Some of the generated templates are eliminated by four different selection/filtering approaches to get a good solution set. Finally, the result set is presented as a decision list, which provides the handling of exceptional cases. Keywords: generalization, slgg, sequence alignment m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE METİN SESLENDİRME Barış Eker Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. İlyas Çiçekli Ocak, 2002 Bilim adamları sesin yapay olarak üretilmesi konusunda iki yüzyılı aşkın bir süredir çalışıyorlar. Bilgisayarın icadından sonra, ses üretmek için bilgisayarlar kullanılmaya başlandı. Bu yeni teknolojinin yardımıyla girdi olarak bir metin alıp bu metnin sesli olarak okunmuş halini üreten ""Metin Seslendirme"" sistemleri üretilmeye başlandı. İngilizce ve Fransızca gibi bazı diller araştırmacıların ilgisini çekerken, Türkçe gibi diller konusunda çok fazla çalışma yapılmadı. Bu tezde Türkçe için ikili fonem birleştirme tekniğini kullanan bir ""Metin Seslendirme"" sistemi anlatılmaktadır. Sistem girdi olarak bir metin alır ve çıktı olarak bu metne karşılık gelen Türkçe sesleri üretir. Türkçe fonetik bir dil olduğu için bu sistem, ufak değişikliklerle benzer fonetik diller için de kullanılabilir. Eğer sisteme bir telaffuz ünitesi entegre edilirse, sistem fonetik olmayan diller İçin de kullanılabilir. Anahtar Kelimeler: Metin Seslendirme, Türkçe, Ses Sentezi, İkili Fonem Birleştirme IV","ABSTRACT TURKISH TEXT TO SPEECH SYSTEM Barış Eker M.S. in Computer Engineering Supervisor: Prof. Dr. H. Altay Güvenir April, 2002 Scientists have been interested in producing human speech artificially for more than two centuries. After the invention of computers, computers have been used in order to synthesize speech. By the help of this new technology, Text To Speech (TTS) systems that take a text as input and produce speech as output have been created. Some languages like English and French have taken most of the attention and some languages like Turkish have not been taken into consideration. This thesis presents a TTS system for Turkish that uses the diphone concatenation method. It takes a text as input and produces corresponding speech in Turkish. The output can be obtained in one male voice only in this system. Since Turkish is a phonetic language, this system also can be used for other phonetic languages with some minor modifications. If this system is integrated with a pronunciation unit, it can also be used for languages that are not phonetic. Keywords: Text To Speech, TTS, Turkish, Speech Synthesis, Diphone Concatenation III"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VİDEO VERİLERİ İÇİN ANLAMSAL VERİ MODELİ VE SORGULAMA DİLİ Umut Arslan Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Doç. Dr. Özgür Ulusoy ve Yrd. Doç. Dr. Uğur Güdükbay Ocak, 2002 Sıkıştırma tekniklerindeki gelişmeler, veri saklama maliyetlerinin düşmesi ve yüksek hızda veri transferi, videonun oluşturulması, saklanması ve dağıtılmasını kolaylaştırmıştır. Bundan dolayı, video günümüzde birçok uygulama alanında kullanılmaktadır. Uygulama alanlarında oluşturulan ve kullanılan video sayısında ve veri miktarındaki artış, hem videonun sadece bir mültimedya (çoklu iletişim aracı) veri tipi olarak daha çok ilgi çekmesini sağlamış, hem de video verilerinin daha verimli bir şekilde kullanılması gereğini doğurmuştur. Video verilerinin yönetimi, videonun yer-zaman, görsel ve anlamsal içeriklerine göre endekslenmesi ve yeniden edinilmesi gibi araştırma alanlarının oluşmasına olanak sağlamıştır. Bu tezde videonun anlamsal içeriği üzerine çalışılmıştır. Videonun anlamsal içeriği video ile ilgili bibliyografik verileri, video içinde geçen etkinlikleri, aksi yonları (hareketleri) ve ilginç nesneleri kapsar. Videonun anlamsal içeriğini mo dellemek için bir video veri modeli önerilmektedir. Videonun anlamsal içeriği bir çıkarım aracı ile elde edilir. Video verileri üzerinde anlamsal sorgular yapabilmek amacıyla bir video sorgulama dili sağlanmıştır. Bu tezin sonuçları, yer-zaman, nesne yörüngesi, nesne görünüm sorgula malarını destekleyen video veritabanı mimarisinin önerildiği bir projede kul lanılacaktır. Buradaki amaç yer-zaman, görsel ve anlamsal sorguları destekleyen eksiksiz bir video arama sistemi oluşturmaktır. Anahtar sözcükler: video veritabanları, anlamsal video modelleme, video veri lerinden çıkarımlar yapma, video- verilerini anlamsal sorgulama. iv","ABSTRACT A SEMANTIC DATA MODEL AND QUERY LANGUAGE FOR VIDEO DATABASES Umut Arslan M.S. in Computer Engineering Supervisors: Assoc. Prof. Dr. Özgür Ulusoy and Assist. Prof. Dr. Uğur Güdükbay January, 2002 Advances in compression techniques, decreasing cost of storage, and high-speed transmission have facilitated the way video is created, stored and distributed. As a consequence, video is now being used in many application areas. The increase in the amount of video data deployed and used in today's applications not only caused video to draw more attention as a multimedia data type, but also led to the requirement of efficient management of video data. Management of video data paved the way for new research areas, such as indexing and retrieval of videos with respect to their spatio-temporal, visual and semantic contents. In this thesis, semantic content of video is studied, where video metadata, activities, actions and objects of interest are considered within the context of video semantic content. A data model is proposed to model video semantic content, which is extracted from video data by a video annotation tool. A video query language is also provided to support semantic queries on video data. The outcome of this thesis work will be used in a project, which proposes a video database system architecture with spatio-temporal, object-trajectory and object-apperance query facilities so as to build a complete video search system to query video data by its spatio-temporal, visual and semantic features. Keywords: video databases, semantic video modeling, annotation of video data, semantic querying of video data."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BÜTÜNLEŞİK MODELLEME DİLİ AKTİVİTE DİYAGRAMLARINI ÇİZEBİLMEK İÇİN KISITLANMIŞ HİYERARŞİK YERLEŞİM PLANININ GELİŞTİRİLMESİ H. Mehmet Yüksel Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Doğrusöz Ocak, 2002 Nesneye Yönelik bir yazılımı modellerken, Bütünleşik Modelleme Dili (UML) adında görsel bir dil kullanılabilir. UML, yazılım sistem modellerinin belirtilmesi, yapılandırılması, görselleştirilmesi ve dökümasyonu amaçlı bir dil ve notasyondur. UML, sınıf ve aktivite diyagramları da dahil olmak üzere birçok çeşit diyagramdan oluşmaktadır. Çizge yerleşim düzeni Bilgisayar Bilimi'nde son yirmi yıldır önemli bir araştırma alanı haline gelmiştir. Veri yapıları, veri tabanları, yazılım mühendisliği, VLSI teknolojisi, elektrik mühendisliği, üretim planlama, kimya ve biyoloji gibi alanlar dahil olmak üzere, çizge yerleşim düzeniyle ilgili geniş bir uygulama alanı vardır. Diyagramlar ilgisel veriyi daha etkin açıklama yollarıdır ve otomatik çizge yerleşim düzeni, diyagramları daha anlaşılabilir yapmaktadır. Diğer bir deyişle, çizge yerleşim düzeni teknikleri sayesinde, çizgelerin okunabilirliği ve anlaşılabilirliği artmakta ve karmaşıklığı azalmaktadır. Bu durum UML diyagramları için de geçerlidir. Bu tezde, UML aktivite diyagramları için çizge yerleşim düzeni algoritmaları sunulmaktadır. UML aktivite diyagramlarını çizmek için varolan bir kısıtlanmış hiyerarşik yerleşim düzeni uygulaması kullanılmıştır. Ayrıca, önerilen çizim algoritmalarının sonuçları sunulmuş ve analiz edilmiştir. Anahtar Sözcükler: Çizge çizimi, UML aktivite diyagramları, kısıtlanmış çizim, hiyerarşik yerleşim düzeni. iv","ABSTRACT EXTENDING CONSTRAINED HIERARCHICAL LAYOUT FOR DRAWING UML ACTIVITY DIAGRAMS H. Mehmet Yüksel M.S. in Computer Engineering Supervisor: Asst. Prof. Uğur Doğrusöz January, 2002 While modeling an object-oriented software, a visual language called Unified Modeling Language (UML) may be used. UML is a language and notation for specification, construction, visualization, and documentation of models of software systems. It consists of a variety of diagrams including class diagrams and activity diagrams. Graph layout has become an important area of research in Computer Science for the last couple of decades. There is a wide range of applications for graph layout including data structures, databases, software engineering, VLSI technology, electrical engineering, production planning, chemistry, and biology. Diagrams are more effective means of expressing relational information and automatic graph layout makes them to be more comprehensible. In other words, with graph layout techniques, the readability and the comprehensibility of the graphs increases and the complexity is reduced. UML diagrams are no exception. In this thesis, we present graph layout algorithms for UML activity diagrams based on constrained hierarchical layout. We use an existing implementation of constrained hierarchical layout to draw UML activity diagrams. We analyze and present the results of these new layout algorithms. Keywords: Graph layout, UML activity diagrams, constrained layout, hierarchical layout. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇOK YÖNLÜ PARALEL BİRLEŞTİRME SIRALAMA ALGORİTMASININ DENEYSEL ÇALIŞMALARI İLE BİRLİKTE ÇOKLU KOMUT ÇOKLU DATA MİMARİLERİNE UYARLANMASI Levent Cantürk Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Cevdet Aykanat Nisan, 2002 Elemanları sıralama problemi, hesaplamalarda muhtemelen üzerinde en çok çalışmış olan problemlerin başında gelmektedir. Bu konuda oldukça fazla optimum algoritmalar geliştirilmiştir. Bu algoritmalar birçok paralel model üzerinde denendi. Bunlar arasında tabi ki çoklu komut çoklu data (ÇKÇD) mimarileri için önerilen ve oldukça iyi çalışan algoritmalar da yer aldı. Bu çalışmamızda biz de, esasen ürün ağları için tasarlanmış çok yönlü birleştirme paralel sıralama algoritmasını ÇKÇD mimarilerine uygun hale getirdik. Çalışmamız, iş yükünün parallel makinalara dengeli dağıtılması, bilgisayarlar arasındaki iletişim yükünün azaltılması ve kendine has performans özellikleriyle oldukça başarılı bir uyarlamadır. Çok yönlü birleştirme sıralama algoritması, sıralanacak eleman sayısından bağımsız olarak sadece iki kere bütün bilgisayarlardan bütün bilgisayarlara kişisel iletişim ve iki kere de bilgisayardan bilgisayara iletişime ihtiyaç duymaktadır. Ek olarak, bu algoritma en kötü olasılıkla 2N/P kadar lokal belleğe ihtiyaç duymaktadır. Burada N sıralanacak eleman sayısını, P ise sıralamada kullanılacak işlemci sayısını temsil etmektedir. Algoritmayı Bilkent Üniversitesi Bilgisayar Mühendisliğinde kurulmuş olan dağıtık bellekli bilgisayar kümesi üzerinde programlayarak test ettik. Sonuçlan karşılaştırma açısından bir tane örneklemeye dayalı paralel sıralama algoritmalarından (PSRS) birtane de paralel hızlısıralama algoritması örneğini (Hyperquicksort) aynı sistem ü/erinde geliştirdik. Deneylerimizde girdi verilerinin dağılımlarına dayanan üç farklı kalite testi ""uniformly"", ""Gaussian"" ve ""Zero"" olmak üzere uyguladık. Çok yönlü birleştirme algoritması diğer iki algoritmaya göre daha iyi sonuçlar elde etmemesine rağmen, ""Zero"" kalite testinde olduğu gibi bazı durumlarda da diyer aluoritmaları geçmiştir. Deneylerin sonuçları raporda detaylı olarak sunulmuştur. Çok yönlü birleştirme algoritması en iyi sıralama algoritması olmamasına rağmen, bir çok ÇKÇD mimarisindeki bilgisayarda çalışabilecek ve kabul edilebilir performans verebilecek bir algoritmadır. Anahtar sözcükler: Sıralama, paralel sıralama, algoritmalar, çokyönlü-birleştirme sıralaması, bilgisayar kümelerinde sıralama. vı","ABSTRACT ADAPTATION OF MULTIWAY-MERGE SORTING ALGORITHM TO MIMD ARCHITECTURES WITH AN EXPERIMENTAL STUDY Levent Cantürk M.S. in Computer Engineering Supervisor: Prof. Dr. Cevdet Aykanat April, 2002 Sorting is perhaps one of the most widely studied problems of computing. Numerous asymptotically optimal sequential algorithms have been discovered. Asymptotically optimal algorithms have been presented for varying parallel models as well. Parallel sorting algorithms have already been proposed for a variety of multiple instruction, multiple data streams (MIMD) architectures. In this thesis, we adapt the multiway- merge sorting algorithm that is originally designed for product networks, to MIMD architectures. It has good load balancing properties, modest communication needs and well performance. The multiway-merge sort algorithm requires only two all-to-all personalized communication (AAPC) and two one-to-one communications independent from the input size. In addition to evenly distributed load balancing, the algorithm requires only size of 2N/P local memory for each processor in the worst case, where N is the number of items to be sorted and P is the number of processors. We have implemented the algorithm on the PC Cluster that is established at Computer Engineering Department of Bilkent University. To compare the results we have implemented a sample sort algorithm (PSRS Parallel Sorting by Regular Sampling) by X. Liu et all and a parallel quicksort algorithm (HyperQuickSort) on the same cluster. In the experimental studies we have used three different benchmarks namely Uniformly, Gaussian, and Zero distributed inputs. Although the multiway- merge algorithm did not achieve better results than the other two, which are 't theoretically cost optimal algorithms, there are some cases that the multiway-merge algorithm outperforms the other two like in Zero distributed input. The results of the inexperiments are reported in detail. The multivvay-merge sort algorithm is not necessarily the best parallel sorting algorithm, but it is expected to achieve acceptable performance on a wide spectrum of MIMD architectures. Keywords: Sorting, parallel sorting, algorithms, multivvay-merge sorting, sorting in clusters. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VİDEO VERİTABANLARINDA YERLEŞİM-ZAMAN SORGULARI İÇİN ETKİLİ BİR SORGU OPTİMİZASYON STRATEJİSİ Gülay Unol Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Doç. Dr. Özgür Ulusoy and Yrd. Doç. Dr. Uğur Güdükbay Temmuz, 2002 Mültimedya veritabanı yönetim sistemlerine olan ilgi büyük hacimlerde mültimedya verilerini saklama ihtiyacından dolayı hızla artmıştır. Sorgu işlemcisi, bir mültimedya veritabanı sisteminin önemli yapı taşlarından biridir ve sorgu ları verimli bir şekilde yanıtlayabilmek için sorgu işlemcisine yerleştirilmiş bir sorgu eniyileyicisine ihtiyaç vardır. Sorgu optimizasyonu problemi konvansi- yonel veritabanları için kapsamlı olarak araştırılmış olup, mültimedya verita ba nı sistemleri için yeni bir araştırma alanıdır. Sorgu işleme stratejilerindeki farklılıklardan dolayı mültimedya veritabanı sistemlerinde kullanılan sorgu opti- mizasyon teknikleri, geleneksel veritabanlarında kullanılanlardan farklıdır. Bu tezde, video veritaba nı sistemlerindeki sorgu optimizasyon problemi ana hat larıyla ele alınmış ve bu probleme çözüm olarak bir sorgu optimizasyon stratejisi önerilmiştir. Ayrıca, sorgu çalışma ağacına uygulanacak sıralama algoritmaları tanımlanmıştır. Son olarak, önerilen algoritmaların test edilmesi sonucu elde edilmiş olan performans sonuçları sunulmuştur. Anahtar sözcükler: video veritabanları, sorgu optimizasyonu, sorgu ağacı, video verilerini sorgulama. iv","ABSTRACT AN EFFICIENT QUERY OPTIMIZATION STRATEGY FOR SPATIO-TEMPORAL QUERIES IN VIDEO DATABASES Gülay IJnel M.S. in Computer Engineering Supervisors: Assoc. Prof. Dr. Özgür Ulusoy and Assist. Prof. Dr. Uğur Güdükbay July, 2002 The interest for multimedia database management systems has grown rapidly due to the need for the storage of huge volumes of multimedia data in computer systems. An important building block of a multimedia database system is the query processor, and a query optimizer embedded to the query processor is needed to answer user queries efficiently. Query optimization problem is widely studied for conventional database systems, however it is a new research area for multime dia database systems. Due to the differences in query processing strategies, query optimization techniques used in multimedia database systems are different from those used in traditional databases. In this thesis, query optimization problem in video database systems is outlined and a query optimization strategy is proposed as a solution to this problem. Reordering algorithms, to be applied on query execution tree, are also described. Finally, the performance results obtained by testing the proposed algorithms are presented. Keywords: video databases, query optimization, query tree, querying of video data. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ZAMAN-DİZİNSEL VERİLERDE GENİŞ ZAMAN BOYUTLARINDA İLİNTİ KURALLARI ÇIKARIMI Aykut Ünal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doçent Özgür Ulusoy Eylül, 2002 Birçok farklı veri merkezli uygulamalarda büyük miktarlardaki veriler zaman- dizinsel olarak toplanır ve depolanır. Zaman-dizinsel veri olay tabanlı bir sis temde belli bir zaman aralığında meydana gelen olaylar topluluğundan oluşur. Bir zaman-dizinsel verinin zaman boyutu saniye, dakika, veya yün gibi her hangi bir zaman birimi olabilir. Uzaklık, zaman-dizinsel verideki olayın oluşumları arasındaki tik sayısı olarak tanımlanır. Bu çalışmada, verilen zaman- dizinsel verideki uzaklık dağılımını inceleyerek verinin daha geniş zaman boyut larındaki sıklık derecesini düşük hatayla yaklaşık olarak bulan etkin bir yöntem önerilmektedir. Daha sonra bu yöntemin daha geniş zaman boyutlarında ilinti kuralları bulmak için ilinti kuralları analizi algoritmalarıyla birlikte nasıl kul lanılabileceği gösterilmektedir. Önerilen yöntem farklı gerçek veriler üzerinde denenmiştir ve sonuçlar önerilen yöntemin etkinliğini kanıtlamak için ayrıntılı olarak verilmektedir. Anahtar sözcükler: Veri Analizi, Zaman-dizinsel Veri, Zaman Boyutu, İlinti Ku ralları Analizi. iv","ÖZET ZAMAN-DİZİNSEL VERİLERDE GENİŞ ZAMAN BOYUTLARINDA İLİNTİ KURALLARI ÇIKARIMI Aykut Ünal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doçent Özgür Ulusoy Eylül, 2002 Birçok farklı veri merkezli uygulamalarda büyük miktarlardaki veriler zaman- dizinsel olarak toplanır ve depolanır. Zaman-dizinsel veri olay tabanlı bir sis temde belli bir zaman aralığında meydana gelen olaylar topluluğundan oluşur. Bir zaman-dizinsel verinin zaman boyutu saniye, dakika, veya yün gibi her hangi bir zaman birimi olabilir. Uzaklık, zaman-dizinsel verideki olayın oluşumları arasındaki tik sayısı olarak tanımlanır. Bu çalışmada, verilen zaman- dizinsel verideki uzaklık dağılımını inceleyerek verinin daha geniş zaman boyut larındaki sıklık derecesini düşük hatayla yaklaşık olarak bulan etkin bir yöntem önerilmektedir. Daha sonra bu yöntemin daha geniş zaman boyutlarında ilinti kuralları bulmak için ilinti kuralları analizi algoritmalarıyla birlikte nasıl kul lanılabileceği gösterilmektedir. Önerilen yöntem farklı gerçek veriler üzerinde denenmiştir ve sonuçlar önerilen yöntemin etkinliğini kanıtlamak için ayrıntılı olarak verilmektedir. Anahtar sözcükler: Veri Analizi, Zaman-dizinsel Veri, Zaman Boyutu, İlinti Ku ralları Analizi. iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET RENK VE DOKU ARAMALARI İÇİN İÇERİK-TABANLI BİR GÖRÜNTÜ ERİŞİM SİSTEMİ Eyüp Sabri Konak Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Yard. Doç. Dr. Uğur Güdükbay ve Doç. Dr. Özgür Ulusoy Ağustos, 2002 Son yıllarda, çok büyük resim ve video veritabanları oluşagelmiştir. Bu büyümeye paralel olarak, görsel bilgiye erişebilmek için içerik-tabanlı erişim ve indekslenmiş veritabanları üzerinde arama yapabilme ihtiyaçları doğmaktadır. Görsel bilginin ana bileşenlerinden ikisi renk ve dokudur. Bu tezde, resim ler arasında renk ve doku benzerliğim hesaplayan bir içerik-tabanlı erişim sis temi sunulmaktadır. Kullanılan teknik, doku analizinde istatistiksel yaklaşıma dayanmaktadır. Her resim için özellikler vektörünü, aynı anda hem mümkün olduğunca bilgilendirici kılmak, hem de düşük vektör boyutlarının sağlayacağı etkin hesaplama imkanından yararlanabilmek için, her resmin Uzaysal Gri Düzey Bağıntı Matrisinden beş adet optimal ikinci-seviyeden doku istatistiği hesaplan maktadır. Renk analizinde kullanılan yöntem renk histogramları olup bun larda bulunan bilginin elde edilmesi, renk dönüşümü, basamaklandırma ve filt- relemeyi içeren bir önişleme fazı sonucunda olmuştur. Böylelikle elde edilen ve özellik vektörlerinde saklanan özellikler daha sonra bir kesişim yöntemiyle karşılaştırılmıştır. Sistem ayrıca, görüntünün tümü üzerinde işlem yapmak yer ine resmi önce farklı doku özelliğindeki bölgelerine ayıran başka bir önişleme fazıyla genişletilmiştir. Sisteme dahil edilen bir diğer özellik de, dikdörtgensel bölgeleri nesne olarak ele almanın yol açabileceği benzerlik hatalarını azalta- bilmeye yönelik nesne-bazlı renk ve doku arama için bir yön haritası sun masıdır. Deney sonuçları ve analizler bu içerik-tabanlı erişim sisteminin erişim ve ölçeklenebilirlik bakımından etkin olduğunu göstermektedir. Anahtar sözcükler: Doku Analizi, Renk Histogramları, Doku Benzerlik Ölçümü, içerik- Tabanlı Görüntü Erişimi, Görüntü Veritabanları.","ABSTRACT A CONTENT-BASED IMAGE RETRIEVAL SYSTEM FOR TEXTURE AND COLOR QUERIES Eyüp Sabri Konak M.S. in Computer Engineering Supervisors: Assist. Prof. Dr. Uğur Güdükbay and Assoc. Prof. Dr. Özgür Ulusoy August, 2002 In recent years, very large collections of images and videos have grown rapidly. In parallel with this growth, content-based retrieval and querying the indexed col lections are required to access visual information. Two of the main components of the visual information are texture and color. In this thesis, a content-based image retrieval system is presented that computes texture and color similarity among images. The underlying technique is based on the adaptation of a statistical ap proach to texture analysis. An optimal set of five second-order texture statistics are extracted from the Spatial Grey Level Dependency Matrix of each image, so as to render the feature vector for each image maximally informative, and yet to obtain a low vector dimensionality for efficiency in computation. The method for color analysis is the color histograms, and the information captured within histograms is extracted after a pre-processing phase that performs color transfor mation, quantization, and filtering. The features thus extracted and stored within feature vectors are later compared with an intersection-based method. The sys tem is also extended for pre-processing images to segment regions with different textural quality, rather than operating globally over the whole image. The sys tem also includes a framework for object-based color and texture querying, which might be useful for reducing the similarity error while comparing rectangular re gions as objects. It is shown through experimental results and precision-recall analysis that the content-based retrieval system is effective in terms of retrieval and scalability. Keywords: Texture Analysis, Color Histograms, Texture Similarity Measurement, Content-Based Image Retrieval, Image Databases. iii C-»'"" II."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DÖRT YAKLAŞIMSAL YÜZEY ALT BÖLÜMLEME YÖNTEMİNİN KARŞILAŞTIRILMASI Tekin Kabasakal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Güdükbay Ağustos, 2002 Yüzey alt bölümleme fikri ilk olarak 1978 yılında öne sürülmüş ve o günden beri bir çok yöntem ortaya konmuştur. Yüzey alt bölümleme, bir sınır değerine kadar kendi kendini yineleyen bir iyileştirme olarak tanımlanmaktadır. Bu araştırmada yaklaşımsa! yüzey alt bölümleme yöntemlerinin özellikleri incelenmiştir. Karmaşık yüzeylerin kama eğrileri ile modellenmesi, çok sayıda yivli kama yamasının incelikle birleştirilmesini gerektirdiğinden kullanılmaları zahmetlidir. Geleneksel yüzey kama yamalarının tersine yüzey alt bölümleme yöntemleri iyi tanımlanmıştır ve karalıdırlar. Yüzey bölümleme yöntemlerinin kama eğrilerini düzensiz yapılara uygulanacak şekilde genellemesi, karma şık yüzeylerin yamalarla uğraşmadan modellenebilmesine imkan vermektedir. Bu çalışmada, Catmull-Clark, Doo-Sabin, Loop ve v^-bölümleme yöntemleri ince lenmiştir. Yöntemlerden ilk ikisi dörtgen, diğer ikisi üçgen yüzeyler için önerilmiştir. Yüzey bölümleme ile kırışıklar, kıvrımlar ve bükülme noktaları gibi keskin hatlı yüzeylerin modellenmesi yüzey bölümleme yöntemlerinin düzenlenmesi ve ağın işaretlenmesini gerektirmektedir, v^-bölümleme yönteminin kuralları keskin hatlı yüzeyleri mo- delleye- cek şekilde geliştirilmiş ve sonuçları geliştirilmiş Loop yönteminin sonuçları ile karşılaştırılmıştır. Ara kestirim normallerinin kullanılması keskin hatların yumuşak görünmesine neden olduğundan Loop ve \/3-bölümleme yöntemlerinin kesin normal değerleri hesaplanarak kullanılmıştır. Anahtar Sözcükler: sayısal geometri ve nesne modelleme, yüzey alt bölümleme, Loop, Catmull-Clark, Doo-Sabin, v^-bölümleme, keskin hatların modellenmesi. iv","ABSTRACT COMPARISON OF FOUR APPROXIMATING SUBDIVISION SURFACE SCHEMES Tekin Kabasakal M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay August, 2002 The idea of subdivision surfaces was first introduced in 1978, and there are many- methods proposed till now. A subdivision surface is defined as the limit of repeated recursive refinements. In this thesis, we studied the properties of approximating sub division surface schemes. We started by modeling a complex surface with splines that typically requires a number of spline patches, which must be smoothly joined, making splines burdensome to use. Unlike traditional spline surfaces, subdivision surfaces are defined algorithmically. Subdivision schemes generalize splines to domains of arbitrary topology.. Thus, subdivision functions can be used to model complex surfaces without the need to deal with patches. We studied four well-known schemes Catmull-Clark, Doo-Sabin, Loop and the y/%- subdivision. The first two of these schemes are quadrilateral and the other two are triangular surface subdivision schemes. Modeling sharp features, such as creases, cor ners or darts, using subdivision schemes requires some modifications in subdivision procedures and sometimes special tagging in the mesh. We developed the rules of \/3- subdivision to model such features and compared the results with the extended Loop scheme. We have implemented exact normals of Loop and \/3-8ubdivision since using interpolated normals causes creases and other sharp features to appear smooth. Keywords: computational geometry and object modeling, subdivision surfaces, Loop, Catmull-Clark, Doo-Sabin, -\/3-subdivision, modeling sharp features. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SKALAR DEĞER ATAMA YÖNTEMİ İLE HACİMSEL VERİLERİN YALINLAŞTIRILMASI Emre Can Sezer Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Prof. Dr. Cevdet Aykanat and Assist. Prof. Dr. Uğur Güdükbay Eylül, 2002 Düzensiz dörtyüzlü ızgaralar üzerinde tanımlanmış hacimsel verilerin yakılaştırılması için yeni bir yöntem sunulmaktadır. Veri, bir hacim üzerinde tanımlı skalar alan dan alınmış örnek değerler ve bu hacimi bölen dörtyüzlüsel ızgaradan oluşur. Yalınlaştırma, dörtyüzlülerin ya da kenarlarının noktaya indirgenmesiyle sağlanır. Yalınlaştırma metodu, istenilen derecedeki doğrulukta modeller sağlayabilir. Dolayısıyla, bu metod çoklu-çözünürlük modellemesi için uygundur. Metodun güzelliği, dörtyüzlünün ya da kenarının indirgeneceği noktanın değişken olabilmesinden kaynaklanır. Diğer metodların aksine, indirgenilecek noktanın asıl ızgaranın bir köşesi olması gerekmez, indirgeme sonunda indirge nilen noktanın skalar değeri son derece basit bir yöntemle belirlenir ve bu yöntem hatanın tahmini bir değerini de sağlar. Önerilen metod, hacimsel verilerin yalınlaştırılmasındaki verimliliğinin görülmesi açısından, iki farklı hacimsel veri örneğinde denenmiştir. Anahtar sözcükler: Hacimsel veri, dörtyüzlüsel ağ, dörtyüzlü indirgeme, kenar indirgeme, skalar değer atama. iv","ABSTRACT SIMPLIFICATION OF TETRAHEDRAL MESHES BY SCALAR VALUE ASSIGNMENT Emre Can Sezer M.S. in Computer Engineering Supervisors: Prof. Dr. Cevdet Aykanat and Assist. Prof. Dr. Uğur Güdükbay September, 2002 A new approach to simplification of volumetric data over an unstructured tetra- hedral mesh is presented. The data consist of sample values of a scalar field defined over a spatial domain, which is subdivided with a tetrahedral mesh. Sim plification is performed by means of contraction of the tetrahedra and also of the edges. The simplification algorithm can provide a continuum of aproximate models of the given dataset with any desired degree of accuracy. Hence, the simplification method is suitable for multi-resolution modeling. The novelty of the approach comes from the arbitrariness in the selection of the point to which a tetrahedron or an edge is contracted. Unlike most of the existing methods, the final vertex of the contraction need not be a vertex of the original mesh. The scalar value to be assigned to the final vertex of contraction is determined by an extremely simple method (both conceptually and computationally), which also provides an estimate of the error of simplification. The proposed method is applied to two volumetric grids to illustrate its effec tiveness in simplification of volumetric data. Keywords: Volumetric dataset, tetrahedral mesh, tetrahedron contraction, edge contraction, scalar value assignment. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KÜTLE-YAY SİSTEMLERİ KULLANARAK KUMAŞ MODELLEMEK Serkan Bayraktar Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Bülent Ozgüç Eylül, 2002 Şeklini değiştirebilen cisimleri modellemek on yıldan fazla bir süredir araştırma konusudur. Geleneksel animasyon teknikleri bu tür cisimleri içeren ortamları modellemekte yetersiz kalmaktadırlar. Bu noktada dinamik modelleme teknikleri önemli avantajlar içermektedirler. Bu tür dinamik modellemeler genellikle fiziksel tabanlı animasyon tekniğini kullanırlar. Fiziksel tabanlı animasyon tekniklerinde cisimler tıpkı gerçek hayatta olduğu gibi kütle, hız ve ivme gibi fiziksel özelliklere sahiptirler. Şeklini değiştirebilen cisimleri modellemenin bir yolu cisimlerin toplam kütlesi belirli noktalarda yoğunlaşmış gibi tasarlayıp bu noktaları yay veya parçacık kuvvetleriyle birbirlerine bağlamaktır. Kütle-yay sistemleri tasarım ve uygulama kolaylıkları, tatmin edici hızları ve sağladıkları gerçeklikle tercih edile gelmişlerdir. Bu çalışma bu konudaki çeşitli teknikleri gözden geçirip kütle-yay sistemleri üzerinde durmaktadır. Anahtar sözcükler: kütle-yay sistemleri, çarpışma testi, kumaş, similasyon. iv","ABSTRACT SIMULATING CLOTH BEHAVIOR BY USING MASS-SPRING NETWORKS Serkan Bayraktar M.S. in Computer Engineering Supervisor: Prof. Dr. Bülent Özgüç September, 2002 Simulating deformable objects has been a topic of interest for more than a decade. Classical methods of animation such as key framing require a tiresome and rigorous process to simulate scenes containing deformable models. Dynamic simulation techniques offer a solution to this problem. Often, dynamic simulation methods use physically based animation methods where models have physical properties which are simulated through time by using Newtonian laws. One of the ways to model and simulate models physically is to discretise models in some points and connect these points by some forces such as damped spring forces or forces governed by particle dynamics. Using mass-springs networks has been very popular in modeling deformable objects, because of the ease of modeling, satisfactory speed and reality they offer. This work review several methods employed to simulate deformable objects, and emphasizes mass-spring network based methods. Keywords: mass-spring networks, collision detection, collision response, cloth, simulation. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET WEB GEÇİCİ BELLEKLERİNDE YERLEŞTİRME PROBLEMİ Seda Çakıroğlu Elektrik ve Elektronik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erdal Ankan Haziran 2002 World Wide Web, dünya çapında ağ, paylaşılmış veri nesnelerine erişim sağlayan büyük, dağıtık bir bilgi sistemidir. Web'in büyüklüğünün üstel artışı iletişim ağında tıkanıklık ve sunucularda fazla yüklenmeye sebep olur. Web'de geçici bellek kullanımı servisteki darboğazları engelleyecek ve ağ trafiğini azaltacak bir çözüm yöntemi olarak kabul edilmiştir. Bu şekilde kul lanıcıların maruz kaldığı gecikme azaltılmış olur. Bu çalışmada belleklerdeki yerleştirme prob lemi üzerinde durulmuştur. Problem, erişim maliyetini minimize etmek için sınırlı büyüklükteki belleklerde hangi sayfaların tutulması gerektiğine karar vermektir. Herbiri birbirine bağlı ve C kadar saklama alanına sahip İV tane geçici belleğin üzerinde sayfa yerleştirmenin en iyi yolu aranmıştır. Veri nesneleri uzayının P elemanlı olduğu varsayılmış ve nesneler için hem tek- tip hem değişken büyüklük modelleri kullanılmıştır. Problem, çözümü standart metodlarla İV, C ve P cinsinden üstel olan bir optimizasyon problemi olarak formüle edilmiştir. Ayrıca yaklaşık çözümler elde etmek için FIFO (ilk-gelen-ilk-atılan), LFU (en-az-sıklıkla-kullanılan), LRU (en-uzak-zamanda-kullanılmış) gibi daha az işlem gerektiren yerine yerleştirme algorit maları denedik. Bunların performanslarını belli bir olasılık dağılımına göre yarattığımız sanal istekler ve gerçek Web trafiği kayıtlarıyla test ettik. Farklı algoritmlarm elde ettiği, sayfaların geçici bellekte bulunma oranlarını ve toplam maliyetlerini karşılaştırdık. Optimum çözüme ulaştığı düşünülen çevrimdışı algoritma LFD'nin (en-uzak-zamanda-istenecek), kullanılan trafik için en iyi sonucu vermediği görülmüşür. Onun yerine kayan pencere metodu kullanılmalıdır. Elde edilen sonuçlar gösteriyor ki, istekler zamandan bağımsız bir olasılık dağılımına sahipse basit bir sabit yerleştirme algoritması sayfaların geçici bellekte bulunmasında maksimum oranı ve toplam maliyette de iyi bir seviyeyi elde elder. Eğer istekler sıklıkla değişiyorsa, çabuk uyum sağlayabilmek ve olası en kötü durumu iyileştirmek için raslantısal bir algoritma seçilmelidir. Algoritmaların sonuca ulaşma zamanlarının analizi gösteriyor ki sayfaların istenme olasılıklarını kullanan algoritmalar ancak az elemanlı uzaylarda kullanılabilir. Farklı istek dağılımlarının geçici belleğin performansına etkileri tartışılmış ve son olarak özelliğine en uygun yöntemi önermek için Web trafiğinin bir analizi yapılmıştır. Anahtar sözcükler: Web'de geçici bellek, internet, geçici bellekte yeniden yerleştirme. iv","ABSTRACT REPLACEMENT PROBLEM IN WEB CACHING Seda Çakıroğlu M.S. in Electrical and Electronics Engineering Supervisor: Prof. Dr. Erdal Ankan June 2002 World Wide Web is a large distributed information system that provides access to shared data objects. Exponential growth of Web's size results in network congestion and server over loading. Web caching has been recognized as an effective scheme for avoiding service bottleneck and reducing network traffic. In this way it minimizes user access latency. Our work focus on the replacement problem, that is deciding which pages to keep in a memory of limited size to minimize the retrieval cost. We seek the best configuration for a network of N caches with capacities C, where all caches are connected to each other. The universe of data objects is assumed to contain P items. Both uniform and nonuniform size models are used for data ob jects. The problem is formulated as a discrete optimization problem whose solution by standard methods is exponential in N, C and in P. We also study a number of low-complexity heuristics to obtain approximate solutions such as FIFO (first-in-first-out), LRU (least-recently-used), and LFU (least-frequently-used). We test the performances of the algorithms both by fictitious re quests generated according to a probabilistic distribution and by access logs of real Web traffic. The hit ratios and total costs of the algorithms are compared. For the traffic used, it is shown that LFD (longest-forward-distance), the classical optimal off-line paging algorithm, is not op timal. Instead a window scheme should be used. Results obtained indicate that if requests follow a stationary probabilistic distribution, a simple static placement algorithm achieves the maximum hit rates and reasonably good cost levels by using the arrival probabilities. Oth erwise, for a quick adaptation to changing requests and for better worst-case performances a randomized algorithm should be chosen. Analysis of the convergence-times shows that the algorithms using probability information have higher order run-time complexities, which limit their usage to small object space cases. The effects of the request sequence on the caches' performance are discussed. Finally, we give an analysis of Web data to propose best heuristics for its characteristics. - & $.'3 i '?'? Keywords: Web caching, internet, removal algorithms, cache replacement. J fl 3m iii S © y g Si """"W-"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇİZGE GÖRÜNTÜLEMESİNDE KARMAŞIKLIĞIN YÖNETİMİ İÇİN BİR ÇERÇEVE ÇALIŞMA Burkay Genç Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Assist. Prof. Dr. Uğur Doğrusöz Eylül, 2002 Bu tezde, çizge görüntüleme uygulamalarında karmaşıklık yönetim teknikleri geliştirmek için etraflı ve verimli bir çerçeve çalışması sunuyoruz. Sunulan mi mari birbiriyle ilişkili birden çok çizgeyi yönetme ve çizgeleri yuvalandırma yeti sine sahip olduğu gibi, istenmeyen çizge elemanlarını bulanıklaştırma, dosyalama ve gizleme imkanı da tanımaktadır. Teorik incelemeler, kullanılan veri yapılarının ve bu yapılar üzerine bina edilen operasyonların oldukça verimli olduğunu göstermektedir. Bu yapılar ve operasyonlar bir çizge çizim uygulamasında kul lanılmış ve başarısı ispatlanmıştır. Anahtar sözcükler: Çizge Çizimi, Bileşik Çizgeler, Bilgi Görüntülemesi, Yazılım Mühendisliği. iv","ABSTRACT A FRAMEWORK FOR COMPLEXITY MANAGEMENT IN GRAPH VISUALIZATION Burkay Genç M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Doğrusöz September, 2002 In this thesis we describe a comprehensive and efficient framework for devel opment of complexity management techniques in graph visualization tools. The presented architecture is capable of managing multiple associated graphs with navigation links and nesting of graphs as well as ghosting, folding and hiding of unwanted graph elements. The theoretical analyses show that the involved data structures and operations on them are quite efficient, and an implementation in a graph drawing tool has proven to be successful. Keywords: Graph Drawing, Compound Graphs, Information Visualization, Soft ware Engineering. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KAYNAKLARIN ETKİN KULLANILARAK BİR SONRAKİ SAYFANIN ZAMAN FAKTÖRÜNE DAYALI OLARAK TAHMİN EDİLMESİ Berkan YALÇINKAYA Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi : Prof. Dr. Halü Altay GÜVENİR Eylül,2002 Web erişim dosyası web kullanım madenciliği için gerekli olan değerli bilgileri içeren ve keşfedilmeyi bekleyen hazine sandığı gibidir. Bu dosyalan analiz ederek içinde saklı bu bilgileri kullanılabilir bilgi haline dönüştürebiliriz. Web erişim dosyalarının analizi, kullanıcının davranışını anlamada ve etkinliği artırmak için web sitesinin tekrar dizaynının nasıl yapılacağı hakkında bilgi sağlamaya yarar ve böylece kaynak topluluğumuzun dizaynını geliştirme imkanına sahip olabiliriz. Bu tezde ziyaretçileri web üzerinde tanıyarak ilgilerine göre önermelerde bulunacak yeni bir sistem dizayn ederek geliştirdik. Sistem tüm kullanıcı bilgilerini korur ve siteyi ziyaret eden diğer bir kullanıcıyı tanımak için bu bilgileri kullanır. Ziyaretçi tanındıktan sonra, sistem bu ziyaretçinin daha önceden web sitesini ziyaret edip etmediğini kontrol eder. Eğer ziyaretçi bu siteyi daha önceden ziyaret ettiyse bu kullanıcının geçmiş hareketlerine dayalı bir önermede bulunur. Aksi takdirde, bu kullanıcılım bağlı bulunduğu üst etki alanından gelen ziyaretçilerin ilgi alanlarına göre bir önermede bulunur. Burada, ""Ata Etki Alanı"" o kullanıcının bağlı olduğu üst etki alanını ifade eder. Örneğin, ""bilkent.edu.tr"" ""cs.bilkent.edu.tr"" etki alanının ata etki alanıdır. Ziyaretçilerin gerçekten en çok ilgi duydukları sayfa özelliği ve kimlik bilgisi sistemimizin iskeletini oluşturmaktadır. Bir kullanıcının bir sayfadageçirdiği zaman faraziyesi, o sayfanın kullanıcı için, geçiş veya içerik sayfası olarak sınıflandırılması ile ilgilidir. Diğer kriter, yani kimlik bilgisi tezimizin diğer önemli noktasını teşkil etmektedir. Ziyaretçinin geçmiş tecrübelerine dayalı önerme mevcut olmadığında, aynı etki alanındaki ziyaretçilerin benzer ilgilere sahip olacağını farz ettiğimizden dolayı bu kullanıcının ait olduğu ata etki alanındaki ziyaretçilerin ilgilerine göre önermede bulunabilmek için İçimlik bilgisi uygun ata etki alanı veya sınıf içerisine yerleştirilmektedir. Bunun yanında, sistem kaynaklan daha verimli kullanacak şekilde tasarlanmıştır. Sistemimizde ""Bellek Yönetimi"", ""Disk Kapasitesi"" ve ""Zaman Faktörü"" opsiyonları ""Kaynakların Etkin Kullanımı"" konsepti dahilinde kullanılmıştır. Sistemi Bilkent Üniversitesi Bilgisayar Mühendisliği web sitesinde test ettik. Deneylerin sonucu bize sistemin verimliliği ve kullanılabilirliğini göstermiştir. Anahtar sözcükler: erişim dosyası, kişiselleştirme, kimlik bilgisi, önerme. vı","ABSTRACT PREDICTING NEXT PAGE ACCESS BY TIME LENGTH REFERENCE IN THE SCOPE OF EFFECTIVE USE OF RESOURCES Berkan YALÇINKAYA M.S. in Computer Engineering Supervisor: Prof. Dr. Halil Altay GÜVENİR September 2002 Access log file is like a box of treasure waiting to be exploited containing valuable information for the web usage mining system. We can convert this information hidden in the access log files into knowledge by analyzing them. Analysis of web server access data can help understand the user behavior and provide information on how to restructure a web site for increased effectiveness, thereby improving the design of this collection of resources. We designed and developed a new system in this thesis to make dynamic recommendation according to the interest of the visitors by recognizing them through the web. The system keeps all user information and uses this information to recognize the other user visiting the web site. After the visitor is recognized, the system checks whether she/he has visited the web site before or not. If the visitor has visited the web site before, it makes recommendation according to his/her past actions. Otherwise, it makes recommendation according to the visitors coming from the parent domain. Here, ""parent domain"" identifies the domain in which the identity belongs to. For instance, ""bilkent.edu.tr"" is the parent domain of the ""cs.bilkent.edu.tr"". The importance of the pages that the visitors are really interested in and the identity information forms the skeleton of the system. The assumption that the amount of time a user spends on inpage correlates to whether the page should be classified as a navigation or content page for that user. The other criterion, the identity information, is another important point of the thesis. In case of having no recommendation according to the past experiences of the visitor, the identity information is located into appropriate parent domain or class to get other recommendation according to the interests of the visitors coming from its parent domain or class because we assume that the visitors from the same domain will have similar interests. Besides, the system is designed in such a way that it uses the resources of the system efficiently. ""Memory Management"", ""Disk Capacity"" and ""Time Factor"" options have been used in our system in the scope of ""Efficient Use of the Resources"" concept. We have tested the system on the web site of CS Department of Bilkent University. The results of the experiments have shown the efficiency and applicability of the system. Keywords: access log file, personalization, identity information, recommendation. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET AGAÇ TABANLI PARALEL K-ORTALI GRUPLAMA İÇİN VERİ DAĞITIM TEKNİKLERİ Cenk Şen Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Attila Gürsoy Temmuz, 2002 K-ortalı gruplamada asıl olan hesaplama yükü veri vektörleri ile grupların ortalan arasındaki uzaklık hesaplamalarıdır. Veri vektörlerinin ve grup ortalarının sayılan arttırıldıkça, hesaplamaları tamamlamak için gerekli olan zaman artar. Bu hesaplama yükü yüksek performanslı bilgisayarlar ve/veya algoritmik gelişmeler gerektirir. Büyük veri kümelerini işlemek için dağınık hafizalı makinalardaki paralel ağaç tabanlı k-ortalı algoritması algoritmik iyileştirmeler ile paralel bilgisayarların yüksek hesaplama kapasitesini birleştirmiştir. Algoritmanın performansı veri dağıtım tekniğinden etkilenmektedir. Bu tezde, dağınık hafızalı makinalardaki paralel ağaç tabanlı k-ortalı algortimasının performansını arttıracak yeni bir veri dağıtım tekniği sunduk. Önerilen ağaç tabanlı dağıtım teknikleri işlemcilere sıkışık altalanlar vererek toplam uzaklık hesaplamalarının sayısını düşürmeyi amaçlamaktadır. Sıkışık altalanlar ağaç tabanlı k-ortalı algoritmasının budama fonksiyonunun performansını arttırmaktadır. Algoritmanın gerçekleştirilmesi ve performans deneyleri gruplandırılmış kişisel bilgisayarlar üzerinde yapılmıştır. Deney sonuçlanınız ağaç tabanlı dağıtım tekniğinin karışık dağıtım ve şeritvari dağıtım tekniklerinden daha iyi performansı olduğunu göstermiştir. Anahtar sözcükler : Gruplama, paralel algoritma, yük dengesi, veri dağıtımı. iv","ABSTRACT DATA DECOMPOSITION TECHNIQUES FOR PARALLEL TREE-BASED K-MEANS CLUSTERING Cenk Şen M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Attila Gürsoy July, 2002 The main computation in the k-means clustering is distance calculations between cluster centroids and patterns. As the number of the patterns and the number of centroids increases, time needed to complete computations increased. This computational load requires high performance computers and/or algorithmic improvements. The parallel tree-based k-means algorithm on distributed memory machines combines the algorithmic improvements and high computation capacity of the parallel computers to deal with huge datasets. Its performance is affected by the data decomposition technique used. In this thesis, we presented novel data decomposition technique to improve the performance of the parallel tree-based k-means algorithm on distributed memory machines. Proposed tree-based decomposition techniques try to decrease the total number of the distance calculations by assigning processors compact subspaces. The compact subspace improves the performance of the pruning function of the tree-based k-means algorithm. We have implemented the algorithm and have conducted experiments on a PC cluster. Our experimental results demonstrated that the tree-based decomposition technique outperforms the random decomposition and stripwise decomposition techniques. Keywords : Clustering, parallel algorithm, load balancing, data decomposition. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET WEB SİTE GELİŞİMİ İÇİN ZAMANA BAĞIMLI WEB KULLANIM MOTİFLERİNİN ÇIKARILMASI Alpay Erdem Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Yrd. Doç. Dr. Attila Gürsoy and Yrd. Doç. Dr. Uğur Doğrusöz Mart 2002 Web kullanımının artması ile birlikte siteler üzerinde sunucu log dosyalarında saklı olan Web kullanıcı verileri kullanıcı motiflerinin incelenmesi amacına yönelik işlenebilir durumdadır. Bağlantılarla gösterilen Web site tasarımı etkin Web içerik sunumu açısından oldukça önemlidir. Web Kullanıcı Motiflerinin işlenmesi kullanıcı motiflerinin yardımıyla Web sitesindeki eksiklikleri tespit ederek Web yapısının iyileştirilmesinde kullanılabilir. Web siteleri ortalama kullanıcıların etkin bir şekilde kullanabilmeleri için tasarlanmış olsa da sayfalar arasındaki kavramsal bağıntılar ve say faların sınıflandırılması kullanıcıların beklentilerini karşılamayabilir. Yanlış yönlendirmeler içeren Web sayfa tasarımı kullanıcıların yanlış yollar deneme sine sebep olarak ulaşmak istedikleri hedef sayfalara çok daha fazla zaman har cayarak ulaşması veya ""sanal-boşlukta"" hedefine ulaşamaması ile neticelenir. Bununla birlikte kullanıcıların zamanla ilgi odaklarının değişmesi ile birlikte kul lanıcı ihtiyaçlarına yönelik olarak Web sitesi tekrar düzenlenmelidir. Bu ne denle, en çok ziyaret edilen popüler sayfalar kolaylıkla ulaşabilmeli, kavramsal olarak birbirine yakın olan sayfalar birlikte kategorize edilmeli veya birbirler ine bağlantılarla ulaşabilmeli ve hatalı yönlendirici içeriğe sahip sayfalar tespit edilmelidir. Bununla birlikte yalnızca sık sıralamaların bulunmasi Web sitesinin geliştirilmesi için yeterli değildir. Çünkü sık sıralamalar ilgi duyulan sayfaları içerdiği gibi kullananın hedefine ulaşırken denemiş olduğu gereksiz sıralamaları da içerir. Geri sık sıralamalar gereksiz yere izlenen sıralamaları ve aynı zamanda ilgili sayfaların ilgilenilen veya ilgilenilmeyen sayfalar olmalarına göre farklılaşan bilgileri içerir. Bu tezde, ileri ve geri sık sıralamaların yorumlanması amacı ile her sayfa için sayfanın izlenme süresini bulmada kullanılan zaman boyutunu sıralamalara ekledik. Her sayfada bir sıralama dizisi içinde diğer sayfalara oranla sayfanın izlenmesi için harcanan zaman o sayfaya kullanıcının ilgi kriteri olarak kullanılabilir. vVI Bu amaçla, Web sitesinin iyileştirilmesine yönelik tekrar tasarlanması için önerilerde bulunan Web kullanım motifleri işleyen bir yöntem önerilmiştir. Kul lanıcının hedefine ulaşıp ulaşmadığı hatalı yönlendirmenin bulunması açısından önemlidir. Bununla birlikte, çoğu zaman geri dönüşlerin kullanımı ile uzun yollar izlenerek ilgili diğer sayfalara geçiş bu sayfaların bağlanması gerektiğini gösterir. Ancak bu şekildeki motiflerin yakalanabilmesi için zaman boyutunun kullanılması gerekmektedir. Kullanıcı motiflerinin işlenmesi kısmında tüm kullanıcı sayfa zi yaret sıralamaları ileri ve geri sıralamalar da dahil olmak üzere ve sayfa istem havuzundan getirilen sayfaların bulunması ve her sayfa üzerinde harcanan za manın bulunması problemlerinin göz önüne alındığı sıralamalar bir bütün olarak incelenmiştir. Popüler sayfalara kısayol ekleme, ilgili sayfaların bağlanması ve hatalı yönlendirici sayfaların bulunması amacı ile sık motiflerin buluması ve yo rumlanması kısmında ilgi kullanıcılar tarafından ilgi duyulan motifler ve gereksiz olanlar bulunmuştur. Anahtar sözcükler: Web Kullanıcı Motifleri, Web Sitesi Gelişimi, Sıralama Çıkarımı, Web Veri Çıkarımı..","ABSTRACT TIME DOMAIN BASED WEB USAGE MINING FOR WEB SITE IMPROVEMENT Alpay Erdem M.S. in Computer Engineering Supervisors: Assist. Prof. Dr. Attila Gürsoy and Assist. Prof. Dr. Uğur Doğrusöz March 2002 With the increased use of Web, large volumes of click-stream data, embed ded inside server logs, has become available for revealing user access patterns especially on specified Web sites. Efficient Web content presentation conveyed through links structure is a very important issue for efficient use of site. Web Usage Mining can be used to improve Web site design by finding deficiencies of the Web site by analyzing user access patterns. Although Web sites are intended to be designed for efficient usage for typical users, mostly conceptual relations between pages and categorization proposed by Web site designer may not meet expectations of the users. Misleading Web site design leads to users spending much more time for reaching target pages by reasoning redundant paths to be followed or lost in cyber-space without finding the target. Furthermore, changing needs and interests of users by the time require re-structuring of the Web site. Therefore Web sites should be updated according to user expectations. For that reason, most popular pages should be easily accessed, conceptually related pages either should be categorized close enough or should be linked and misleading guidance directing users to different pages other than target should be detected. However, barely finding frequent sequences is not sufficient for improving a Web site. This is because of the fact that explored frequent patterns cover both interested patterns used for reaching popular sites and redundant patterns that are followed previous to reaching target page(s). Frequent backward references embed knowledge of redundant and also related pages according to interest in these pages. In order to interpret backward and forward references in terms of interest we incorporated time domain that finds page viewing timing for each visited page. Relatively spent page viewing time for each page within a session is an important interest criterion for that page, a_.IV For that purpose, we proposed a Web usage mining framework that explores deficient points in the web site design according to user expectations. Whether user reached or not indicates misleading-guidance. Besides, jumping to related pages by using long paths, in many cases backtracks, shows that those pages should be linked. However, in order to be able to capture such patterns, page view time of each page is used. This franework advises re-design suggestions for Web site improvement. In the usage processing part of this framework, all user navigation sessions are analyzed and both forward and backward references are obtained by considering cached pages and also page viewing timing is computed for each page. In the mining process and interpretation part, frequent inter ested and redundant patterns are explored and interpreted for enabling popular pages more visible, linking related pages, reporting misleading categorization and detecting misleading guidance or categorization. Keywords: Web Usage Mining, Web Site Improvement, Sequence Mining, Web Mining.."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE'NİN ÖNEK-SONEK TABANLI İSTATİSTİKSEL MODELLERİ Umut Topkara Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. İlyas Çiçekli Temmuz, 2001 Teknolojik gelişmelerle beraber büyük derlemlerin ortaya çıkmasından sonra dil hakkındaki nicel bilgilerin özlü bir halde saklanması ve bu bilgi üzerinde çıkarımlar yapılması çekici bir bilimsel araştırma alanı haline geldi, istatistik sel dil modelleri u dil birimlerinden oluşan büyük derlemleri ürettiği varsayılan ve bilinmeyen bir P(u) olasılık dağılımını tahmin etmekte kullanılırlar. Bulu nan bu olasılık dağılımı tahmini, aralarında konuşma tanıma(speech recogni tion), yazım ve gramer hatalarını düzeltme, otomatik belge tercümesi ve otomatik belge sınıflandırmanın da bulunduğu birçok doğal dil işleme uygulamasının başarımını artırmak için kullanılabilir, istatistiksel dil modelleme, İngilizce'ye başarıyla uygulanmıştır, ancak istatistiksel modellerin bu başarısı Türkçe'nin is tatistiksel modellerine Türkçe'nin belirli özelliklerinden dolayı yeterince yansıma maktadır. Türkçe üretken sondan eklemeli bir dil yapısına sahiptir, yani bir ke lime kökünden arka arkaya eklemeler yoluyla binlerce kelime üretmek mümkün olmaktadır. Kelime birimleri üzerinden istatistiksel modeller kullanıldığında Türkçe'nin üretken sözlük yapısı genel olarak veri yetersizliğine ve konuşma tanıma gibi zaman-yer kritik uygulamalarda ciddi yer ve zaman problemleri oluşturmaktadır. Yakın zamanda tamamlanan Hakkani-Tür'e ait doktora tezindeki bulgu lara göre, Türkçe için konuşma tanıma uygulamalarının ürettiği aday lis telerinin yeniden değerlendirilmesinde, kelimelerin sabit büyüklükteki önek ve sonek birimleri üzerinden yapılan n-birimli istatiksel modeller kelime birimleri üzerinden yapılan n-birimli modellere göre daha iyi başarı sağlamaktadırlar. Bu başarılı sonuçlardan sonra, kelimeden küçük birimler üzerinden Türkçe'nin istatis tiksel modelleri konusunda daha fazla araştırma yaptık. Çalışmalarımızda önekVI ve sonek kısımları için sabit sayıda hece kullanılan çeşitli istatistiksel modeller denedik. Yaklaşımlarımızın güçlülüğünü değerlendirebilmek için önek ve sonek dağarcığımızı kısıtlı tuttuk. Ayrıca 2 kelime birimi bağlandı önek sonek modellerimizin başarımını kelime birimleri üzerinde 2-birimli istatistiksel modellerle karşılaştırdık. Araştırmalarımızın sonunda 2 kelime bağlamda kelime tabanlı dil modeliyle aynı perfor mansı gösteren, ancak yarı boyutta olan bir dil modeli geliştirdik. Anahtar sözcükler: istatistiksel Dil Modelleme, Doğal Dil İşleme, Sondan Ek lemeli Diller, Konuşma Tanıma, Aday Listesi Değerlendirme, n-birimli Dil Mod elleri, Önek Sonek Dil Modelleri.","ABSTRACT PREFIX-SUFFIX BASED STATISTICAL LANGUAGE MODELS OF TURKISH Umut Topkara M.S. in Computer Engineering Supervisor: Asst. Prof. Dr. Ilyas Çiçekli July, 2001 As large amount of online text became available, concisely representing quan titative information about language and doing inference on this information for natural language applications have become an attractive research area. Statisti cal language models try to estimate the unknown probability distribution P(u) that is assumed to have produced large text corpora of linguistic units u. This probability distribution estimate is used to improve the performance of many natural language processing applications including speech recognition (ASR), op tical character recognition (OCR), spelling and grammar correction, machine translation and document classification. Statistical language modeling has been successfully applied to English. However, this good performance of approaches to statistical modeling of English does not apply to Turkish. Turkish has a produc tive agglutinative morphology, that is, it's possible to derive thousands of word forms from a given root word through adding suffixes. When statistical modeling by word units is used, this lucrative vocabulary structure causes data sparseness problems in general and serious space problems in time-memory critical applica tions such as speech recognition. According to a recent Ph.D. thesis by Hakkani-Tür, using fixed size prefix and suffix parts of words for statistical modeling of Turkish performs better than using whole words for the task of selecting the most likely sequence of words from a list of candidate words emitted by a speech recognizer. After these successful results, we have made further research on using smaller units for statistical modeling of Turkish. We have used fixed number of syllables for prefix and suffix parts. In our experiments we have used small vocabulary of prefixes and suffixes to test the robustness of our approach. We also compared the performance of prefix-suffix language models having 2-word context with word 2-gram models. We have found a language model that uses subword units and can perform as well as a large word iiiIV based language model in 2-word context and still be half in size. Keywords: Statistical Language Modeling, Natural Language Processing, Agglu tinative Languages, Speech Recognition, N-best List Rescoring, n-gram Language Models, Prefix Suffix Language Models."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇOK KANALLI KAYNAK ORTAMINDA ÇEVRİMİÇİ YENİ OLAY BELİRLEME VE TAKİBİ Hakan Kurt Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. H. Altay Güvenir Eylül, 2001 Elektronik ortamdaki bilgi miktarı arttıkça bilgiye olan ihtiyaç da artmak tadır. Bugün, bir kişinin bütün bigi kaynaklarını takip etmesi ve yeni olaylari olabildiğince kısa zamanda bulması hemen hemen imkansızdır. Bu tezde, bir den fazla haber kaynağından otomatik olarak yeni olayları belirleyen ve olaylar geliştikçe onları anında takibe başlayan, çevrimiçi yeni olay belirleme ve takip sis temi sunuyoruz. Olay belirleme yaklaşımının çevrimiçi versiyonunu uyguladığımız için, haber metni hakkındaki yenilik kararı bir sonraki haber işleme alınmadan yapılmaktadır. Ayrıca, destek eşiği olarak adlandırdığımız, belirleme işleminde, bilgi verici ve bir seferlik haberler tarafından sebep olunan yeni olay alarmlarının sayısını azaltmak maksadıyla kullanılan yeni bir eşiği de tanıtıyoruz. Destek eşiği, haber kaynaklarının ağırlığını ayarlamak için de kullanılabilir. Takip etme safhasını, denetlemesiz öğrenme metodu şeklinde uyguladık, yani belirlenen olay lar bir olayın ilk haberini kullanarak otomatik olarak takip edilir. Olaylar zaman geçtikçe geliştiği için, takip etme sisteminin performansını arttırmak maksadıyla, bir denetlemesiz adaptasyon yöntemi takip sistemini tekrar eğitmek için kullanılır. Adaptasyon tahmin edilen dokümanların eğitim işlemine eklenmesiyle sağlanır. Toplanan haberlerin incelenmesinden, bir haber metninin birden fazla olaydan bahsedebileceği sonucuna vardık. Bu sebeple, takip sistemi bir haber metnini birden fazla olayla ilişkilendirecek şekilde uygulandı. Çevrimiçi yeni olay belir leme ve takip sistemi, Internet 'te mevcut olan, Reuters haber kaynağında teste dildi. Kullandığımız Reuters haber kaynağı dört bağımsız haber kaynağını içerir ve haberleri Türkçe'dir. Anahtar sözcükler: Olay belirleme, olay takibi, bilgi erişimi. iv","ABSTRACT ON-LINE NEW EVENT DETECTION AND TRACKING IN A MULTI-RESOURCE ENVIRONMENT Hakan Kurt M.S. in Computer Engineering Supervisor: Prof. Dr. H. Altay Güvenir September, 2001 As the amount of electronically available information resources increase, the need for information also increases. Today, it is almost impossible for a person to keep track all the information resources and find new events as soon as possible. In this thesis, we present an on-line new event detection and tracking system, which automatically detects new events from multiple news resources and immediately start tracking events as they evolve. Since we implemented the on-line version of event detection approach, the novelty decision about a news story is done be fore processing the next one. We also present a new threshold, called support threshold, used in detection process to decrease the number of new event alarms, that are caused by informative and one-time-only news. The support threshold can be used to tune the weights of news resources. We implemented the tracking phase as an unsupervised learning process, that is, detected events are automati cally tracked by training the system using the first news story of an event. Since events evolve over time, an unsupervised adaptation is used to retrain the track ing system in order to increase the tracking system performance. Adaptation is achieved by adding predicted documents to the training process. From the corpus observations, we conclude that one news story can be associated to more than one event. For this reason, the tracking system can relate a news story to more than one event. The on-line new event detection and tracking system has been tested on the Reuters news feed, available on the Internet. The Reuters news feed, that we used, comprises four independent news resources. The news stories are in Turkish. Keywords: Event detection, event tracking, information retrieval. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET HESAP VE HABERLEŞME YÜKÜNÜ ÇOK KATMANLI KENDİNDEN DÜZENLENEN HARİTALARLA DENGELEME Erdoğan BIKMAZ Bilgisayar Mühendisliği Bölümü, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Attila Gürsoy Temmuz, 2001 Bugün, veri analizi, bilimsel hesaplamalar, ve mühendislik hesaplamaları gibi uygulamalarda kullanılan bazı büyük programların bilgisayarlarda yürütme zamanı büyük bir problem olarak karşımıza çıkmaktadır. Yürütme zamanını azaltmak için genel yaklaşım, bu tür uygulamaları paralel makinalarda yürüt mektir. Bu tür uygulamaları ve hesaplamaları paralelleştirmekte önemli olan hesaplama yüklerini işlemcilere dengeli dağıtmaktır. Sadece hesaplama yüklerini eşit olarak dağıtmanın yük dengesi açısından yeterli olmadığını iddia ediy oruz, çünkü paralel makinalarda kaçınılmaz olan haberleşme fazla yük ge tirir. Haberleşme yükünü azaltmak ve dengelemek için komşuluk ilişkilerini topolojik olarak muhafaza eden Kohonen Kendinden Düzenlenen Harita Al goritmasını kullandık. Mesaj sayısını dengeleyerek sadece hesaplama yükünü değil, aynı zamanda haberleşme yükünü de dengeledik. Başarım deneyleri, yük dağılımı açısından bizim algoritmamızın diğer statik yük dağılım algorit malarından daha iyi olduğunu gösterdi. Kendinden Düzenlenen Haritalar Al goritmasının genel kötü yanı uzun yürütme zamanıdır. Kendinden Düzenlenen Haritalar Algoritmasının işlem süresini çok katlı yaklaşımla azalttık. Anahtar Kelimeler. Sinir ağları, Kohonen Kendinden Düzenlenen Haritalar Algoritması, görev atama, yük dengeleme, Haberleşme yükü.","ABSTRACT BALANCING COMPUTATION LOAD AND COMMUNICATION OVERHEAD WITH MULTILEVEL SELF ORGANIZING MAPS Erdoğan BIKMAZ M.S. in Computer Engineering Supervisor: Asst. Prof. Attila Giirsoy July, 2001 Today, execution time of big programs such as the programs for data- analysis tasks, scientific computations, and engineering problems remains as a big bottleneck. To reduce the execution time, a common approach is to run such applications on parallel machines. A major task in the paralleliza- tion of these applications or computations is to distribute the computational load to processors in a balanced way. We argue that distributing only the computational load equally is not enough for load balancing because commu nication cost, which is inevitable in parallel computations, brings some extra overhead. We used Kohonen Self-Organizing Maps (SOM) that preserves the neighborhood relationship of tasks to minimize and balance the communica tion overhead. We balance not only computation load but also communication overhead by balancing the number of messages. The performance experiments show that our algorithm outperforms the other static task mapping algorithms on the view of load balancing. One general drawback of Self-Organizing ap proaches is the high running time. We decreased the execution time of SOM algorithm with multilevel approach. Key words: Neural networks, Kohonen Self-Organizing Maps, task map ping, load balancing, communication overhead. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE'DEN KIRIMTATARCA'YA OTOMATİK ÇEVİRİ SİSTEMİ Kemal Altıntaş Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. İlyas Çiçekli Temmuz, 2001 Bilgisayarın keşfinden beri otomatik çeviri işlemi insanların ilgisini çekmiştir. Bu konuda bugüne kadar yapılan araştırmaların çoğu İngilizce ve Fransızca gibi Batı Dilleri üzerinde yapılmış, Türkçe ve Türk dilleri sahnenin dışında kalmıştır. Yakın diller arasındaki otomatik çeviri işlemi birbiriyle ilişkisi olmayan diller arasındaki çeviri işleminden daha kolaydır. Gramer ve kelime hazinelerinin önemli bir kısmı ortak olduğundan, yakın diller arasında çeviri yapacak bir sistem geliştirmek daha az çabayla mümkün olabilir. Yakın diller arasında tercüme yapmak için sınırlı tercüme kuralları ve karşılıklı sözlükler tarafından desteklenecek bir biçimbirimsel çözümleme çoğu zaman yeterli olacaktır. Genelde bir anlam çözümlemesine gerek olmayabilir. Bu tezde Türkçe ve Kırımtatarca arasında tercüme için sonlu durumlu teknikler kullanan bir otomatik çeviri sistemi anlatılmaktadır. Türkçe ve Kırımtatarca arasında geliştirilen otomatik çeviri sistemimizin yakın diller arasında geliştirilecek sistemlere bir model teşkil edeceğini ummaktayız. Geliştirdiğimiz sistem Türkçe bir cümleyi alıp biçimbirmsel olarak çözümlemekte, gramer yapılarını ve çevirisi duruma bağlı olan sözcükleri çevirmekte, kökleri çevirmekte ve son olarak da Kırımtatarca cümleyi biçimbirimsel olarak üretmektedir. Üretilen cümlelerden en az biri çoğu zaman girdi olarak alınan cümlenin doğru bir tercümesi olmaktadır. Anahtar Kelimeler: Doğal Dil İşleme, Otomatik Çeviri, Türkçe, Kırımtatarca, Tatarca, Türk Dilleri IV","ABSTRACT TURKISH TO CRIMEAN TATAR MACHINE TRANSLATION SYSTEM Kemal Altıntaş MS in Computer Engineering Supervisor: Asst.Prof.Ilyas Cicekli July, 2001 Machine translation has always been interesting to people since the invention of computers. Most of the research has been conducted on western languages such as English and French, and Turkish and Turkic languages have been left out of the scene. Machine translation between closely related languages is easier than between language pairs that are not related with each other. Having many parts of their grammars and vocabularies in common reduces the amount of effort needed to develop a translation system between related languages. A translation system that makes a morphological analysis supported by simpler translation rules and context dependent bilingual dictionaries would suffice most of the time. Usually a semantic analysis may not be needed. This thesis presents a machine translation system from Turkish to Crimean Tatar that uses finite state techniques for the translation process. By developing a machine translation system between Turkish and Crimean Tatar, we propose a sample model for translation between close pairs of languages. The system we developed takes a Turkish sentence, analyses all the words morphologically, translates the grammatical and context dependent structures, translates the root words and finally morphologically generates the Crimean Tatar text. Most of the time, at least one of the outputs is a true translation of the input sentence. Keywords: Natural Language Processing, Machine Translation, Turkish, Turkic Languages, Crimean Tatar."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET fc-NN ve FPTC TABANLI METİN KATEGORİZASYON ALGORİTMALARININ TÜRKÇE HABERLERE UYGULAMASI Ufuk ilhan Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Halil Altay Güvenir Şubat, 2001 İnternet ulaşım kolaylığı, optik okuyucular, yüksek hızlı ağlar ve pahalı ol mayan yüksek miktardaki bilgi depolama imkanlarındaki teknolojik gelişmeler, on-line metin ve makalelerine, elektronik posta ve teknik raporlara erişim ko- laylığıyla büyük bir artışa neden oldu. On-line bilgi erişimindeki, bu inanılmaz artış, kullanıcı ların bilgileri organize etme ihtiyacını yarattı. Metin sınıflandırması (Text Categorization), gelişen tekniklerin ihtiyaçlarına bir çare olabilir. Metin sınıflandırması, önceden belirlenmiş kategorilere göre, doğal dil metinlerinin sınıflandırılmasıdır. Bu tezde, metin sınıflandırması üzerinde çalışmak için Anadolu Ajansı adlı Türkçe bir veri kümesinin der lenmesi sunulmuştur. Türkçe gibi bitişken dillerde kelimeler, en küçük an lamlı parçasının sınırlarına dair bir belirti göstermez, üstelik, bu parçalar, morfolojik ve fonolojik şartlara bağlı olarak şekil alırlar. Türkçe'de, bir keli menin son ekine bir tane daha ekleyerek, nispeten uzun kelimeler elde edilebilir, üstelik, sadece bir tek Türkçe kelimeden çok miktarda değişik anlamlı kelimeler oluşturulabilir. Bu karmaşık morfolojik yapı yüzünden, Türkçe, ingilizce ve benzer dillerden daha farklı metin özel işlem teknikleri gerektirir. Bu nedenle, bütün kelimelerin küçük harfe çevrilmesi ve noktalama işaretlerinin atılması dışında; gövdeleme, gereksiz kelimelerin atılması ve anahtar kelime listesinin oluşturulması gibi, bazı önhazırlıklar yapılması gereklidir. ivBu tezde, ayrıca, literatürde yaygın olarak bilinen k en yakın komşu sınıflandırma algoritması (k-NN) ile k-NN'in bir değişiği olan FPTC algoritmasının Türkçe veri kümesi üzerinde değerlemesi ve karşılaştırılması da sunulmuştur. k-NN, bir örnek tabanlı öğrenme metodudur. k-NN, tahmin ve test örnekleri arasındaki benzerliği hesaplar ve girdi kategorilerini tahmin etmek için k adet üst sıranın en yakın örneklerini düşünerek, en benzer kategorileri bulur. FPTC algorit ması ise, tahmin örneklerinin izdüşümlerinin, herbir öznitelik boyutunda ifade edilmesi fikri esasına dayalıdır. Eğer, bir tahmin örneğinin değeri, bir öznitelik için belli değilse, tahmin örneği, öznitelik üzerinde ifade edilmez. Yapılan değerlemeler sonucu, FPTC algoritması, k-NN'le karşılaştırılabilir bir doğruluk oranını başarmıştır, ayrıca, zaman verimliliği açısından, k-NN algoritmasına belirgin bir üstünlük sağlamıştır.","ABSTRACT APPLICATION OF fc-NN and FPTC BASED TEXT CATEGORIZATION ALGORITHMS TO TURKISH NEWS REPORTS Ufuk Ilhan M.S. in Computer Engineering Supervisor: Assoc. Prof. Halil Altay Güvenir February, 2001 New technological developments, such as easy access to Internet, optical char acter readers, high-speed networks and inexpensive massive storage facilities, have resulted in a dramatic increase in the availability of on-line text-newspaper articles, incoming (electronic) mail, technical reports, etc. The enormous growth of on-line information has led to a comparable growth in the need for methods that help users organize such information. Text Categorization may be the remedy of increased need for advanced techniques. Text Catego rization is the classification of units of natural language texts with respect to a set of pre-existing categories. Categorization of documents is challenging, as the number of discriminating words can be very large. This thesis presents compilation of a Turkish dataset, called Anadolu Agency Newsgroup in or der to study in Text Categorization. Turkish is an agglutinative languages in which words contain no direct indication where the morpheme boundaries are, furthermore, morphemes take a shape dependent on the morphological and phonological context. In Turkish, the process of adding one suffix to another can result in a relatively long word, furthermore, a single Turkish word can give rise to a very large number of variants. Due to this complex morphologi cal structure, Turkish requires text processing techniques different than English and similar languages. Therefore, besides converting all words to lower case and removing punctuation marks, some preliminary work is required such as stemming, removal of stopwords and formation of a keyword list. iiiThis thesis also presents the evaluation and comparison of the well-known k-NN classification algorithm and a variant of the k-NN, called Feature Projection Text Categorization (FPTC) algorithm. The k-NN classifier is an instance based learning method. It computes the similarity between the test instance and training instance, and considering the k top-ranking nearest instances to predict the categories of the input, finds out the category that is most similar. FPTC algorithm is based on the idea of representing training instances as their projections on each feature dimension. If the value of a training instance is missing for a feature, that instance is not stored on that feature. Experiments show that the FPTC algorithm achieves comparable accuracy with the k-NN algorithm, furthermore, the time efficiency of FPTC outperforms the k-NN significantly. Keywords: text categorization, classification, feature projections, stemming, wild card matching, stopword. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"VI ÖZET Oleg Gusak Bilgisayar Mühendisliği, Doktora Tez Yöneticisi: Doç. Dr. Tuğrul Dayar Temmuz, 2001 Bu çalışma, rassal özdevinimli ağ olarak modellenen sonlu Markov zincirlerinin çözümlemesi alanında var olan araştırmaya katkıda bulunmaktadır, ilk olarak, bu tez Markov zincirlerinin neredeyse tamamen bölünebilirlik kavramını rassal özdevinimli ağlara genişleterek, alttaki Markov zincirinin çözülmesinde aslında var olan zorluğun kestirilmesini ve bu kavrama dayalı çözüm tekniklerinin araştırılmasını mümkün kılmaktadır. Bir rassal özdevinimli ağın altındaki Markov zincirinin neredeyse tamamen bölünebilir bloklara ayrışmış halini bul manın basit yolu, sisteme karşı gelen matrisin sıfırdan farklı elemanlarının hesap edilmesini gerektirir. Bu, çok büyük sistemler için bellek ve uygu lama zamanı sınırlamalarından dolayı seyrek matris gösterimiyle dahi mümkün değildir. Bu tezde, bu problem için, verilen bir rassal özdevinimli ağın herbir bileşenini çözümlemeye dayalı, bir etkili ayrıştırmalı çözüm algoritması sunul maktadır. Sayısal sonuçlar, verilen algoritmanın basit yöntemden çok daha iyi randıman verdiğini göstermektedir. ikinci olarak, bu çalışma bir rassal özdevinimli ağa karşı gelen matris için kontrol edilmesi kolay birleştirilebilirlik koşulları vermektedir. Sisteme karşı gelen matrisin tensör gösteriminin neden olduğu birleştirilebilir bir bölünme var olduğunda, rassal özdevinimli ağ modelinin altındaki Markov zincirinin değişmez durum dağılımının etkili bir dolaylı birleştirme-ayrıştırma algorit ması kullanılarak bulunabileceği gösterilmektedir. Sürekli-zamanlı ve kesintili- zamanlı rassal özdevinimli ağ modelleri üzerinde yapılan deneylerin sonuçları, önerilen algoritmanın son derece çetin bir rakip olan blok Gauss-Seidel'den hem ardışık tekrar sayısında hem de çözüme yakınsama için geçen süre yönünden daha iyi randıman verdiğini göstermektedir.vıı Son olarak, dolaylı birleştirme-ayrıştırma algoritmasının başarımı, birleştiri lebilir bölünmelerde nispi olarak büyük bloklara sahip sürekli-zamanlı rassal özdevinimli ağlar üzerinde araştırılmaktadır. Dolaylı birleştirme-ayrıştırma algoritmasının herbir ardışık tekrarında, büyük blokları çözmede karşılaşılan güçlükleri aşmak için rassal özdevinimli ağlar için blok Gauss-Seidel'm özyinele meli uygulaması kullanılmaktadır. Dolaylı birleştirme-ayrıştırma algoritmasının başarımı blok Gauss-Seidel'inki ile karşılaştırılmaktadır. Deney sonuçları, dolaylı birleştirme-ayrıştırmayı blok Gauss-Seidel'i geçecek şekilde ayarlamanın müm kün olduğunu göstermektedir. Anahtar kelimeler: Markov zinciri, rassal özdevinimli ağlar, neredeyse tama men bölünebilirlik, birleştirilebilirlik, dolaylı birleştirme-ayrıştırma.","IV ABSTRACT ANALYSIS OF LARGE MARKOV CHAINS USING STOCHASTIC AUTOMATA NETWORKS Oleg Gusak Ph.D. in Computer Engineering Advisor: Assoc. Prof. Tuğrul Dayar July, 2001 This work contributes to the existing research in the area of analysis of fi nite Markov chains (MCs) modeled as stochastic automata networks (SANs). First, this thesis extends the near complete decomposability concept of Markov chains to SANs so that the inherent difficulty associated with solving the un derlying MC can be forecasted and solution techniques based on this concept can be investigated. A straightforward approach to finding a nearly completely decomposable (NCD) partitioning of the MC underlying a SAN requires the computation of the nonzero elements of its global generator. This is not feasi ble for very large systems even in sparse matrix representation due to memory and execution time constraints. In this thesis, an efficient decompositional so lution algorithm to this problem that is based on analyzing the NCD structure of each component of a given SAN is introduced. Numerical results show that the given algorithm performs much better than the straightforward approach. Second, this work specifies easy to check lumpability conditions for the generator of a SAN. When there exists a lumpable partitioning induced by the tensor representation of the generator, it is shown that an efficient iterative aggregation-disaggregation algorithm (IAD) may be employed to compute the steady state distribution of the MC underlying the SAN model. The results of experiments with continuous-time and discrete-time SAN models show that the proposed algorithm performs better than the highly competitive block Gauss- Seidel (BGS) in terms of both the number of iterations and the time to converge to the solution. Finally, the performance of the IAD algorithm on continuous-time SANsV having relatively large blocks in lumpable partitionings is investigated. To overcome difficulties associated with solving large diagonal blocks at each iter ation of the IAD algorithm, the recursive implementation of BGS for SANs is employed. The performance of IAD is compared with that of BGS. The results of experiments show that it is possible to tune IAD so that it outperforms BGS. Key words: Markov chain, Stochastic automata network, Near complete de- composability, lumpability, iterative aggregation-disaggregation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET AKTİF VE HAREKETLİ VERİLERİN OLAY GEÇMİŞLERİNİN ANALİZİ YOLUYLA YÖNETİMİ Yücel Saygın Bilgisayar Mühendisliği, Doktora Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos 2001 Olay geçmişleri, olay tabanlı sistemlerde görülen olayların bir zaman dili mi içerisinde toplu olarak tutulduğu yerdir. Bir sistemde birçok olay buluna bilir, bunlara örnek olarak, güç yönetimi sistemlerinde sıcaklık değişimleri ve elektrik talepleri, veri yayın sisteminde verilere erişim işlemleri, borsada belirli hisse senetlerinin artışı gösterilebilir. Veri analizi teknikleriyle olay geçmişleri analiz edilerek birçok faydalı bilgi elde edilebilir. Bu tezde amacımız, tek ya da birbiriyle ilgili birden fazla olay geçmişinin analiz edilmesiyle olay dizilerinin ve olay ilintilerinin çıkarılmasıdır. Bunun yanında diğer bir hedefimiz de olay geçmişi analizi yoluyla elde edilen olay dizi ve ilintilerinin aktif ve hareketli veri yönetiminde nasıl kullanılabileceğini de gösterilmesidir. Olay geçmişi anali zi sonucu elde ettiğimiz bilgiler, genelde ilinti veya dizi şeklinde gösterilebilir. Olaylar arası ilişkilerin, olay-tabanlı sistemlerin olaylarının organize edilmesi, olayların önceden tahmin edilmesi ve kuralların önceden aktive edilmesi yoluyla geliştirilmesinde çok yararlı olduğu çalışmalarımızda tesbit edilmiştir. Bu tezde, hem tek hem de birden fazla birbiriyle ilişkili olay geçmişlerinin analizi üzerinde çalışılmıştır. Öncelikle tek bir olay geçmişinin analizi konusu ele alınmış ve böyle bir olay geçmişinden ikili olay ilişkilerinin çıkarılması için bir yöntem geliştirilmiştir. Olayların arasındaki bu ikili ilişkiler, olayların grup landırılması için kullanılmıştır. Bir sistemdeki olayların gruplandırılması önem lidir, çünkü sistemdeki olay sayısı çok artabilir ve bu durum olayların yönetimini zorlaştırır. Çalışmamızda, birbiriyle ilişkili olay grupları ve olaylar arasındaki ilişkiler aktif veri tabanı sistemlerinde tahmini olay saptaması ve kuralların önce den aktive edilmesi için kullanılmıştır. Birbiriyle ilişkili iki olay geçmişinin a nalizi üzerinde de çalışmalar yapılmıştır. Birbiriyle ilişkili olay geçmişlerinde ikiayrık olay kümesi vardır ve bir olay geçmişindeki olaylar, öteki olay geçmişindeki olaylarla ilgilidir. Tezimizde, iki olay geçmişini kapsayan olaylar çapraz ilintiler olarak adlandırılmış ve çapraz ilintilerin etkili bir şekilde nasıl çıkarılabileceği anlatılmıştır. Çalışmamızda, aktif veri yönetimi için, hareketli (mobil) sistem ortamlarında veri yayını konusu araştırma alanı olarak seçilmiştir. Hareketli sistem ortamların da önemli bir gerçek, sunucudan-istemciye, yani aşağı-hat, iletişim kapasitesinin, istemci den-sunucuya, yani yukarı- hat, kapasitesinden çok daha yüksek olmasıdır. Bu asimetri, verilerin yayınının arzu edilen bir veri iletme yöntemi olmasını sağlar. Öte yandan, verilerin yayın yoluyla iletilmesi, verilerin çok olması durumunda uzun bekleme sürelerine sebep olur. Bu tezde amaçlarımızdan birisi, hareketli verilerin yönetiminin, yayınlanan verilerin organize edilmesi ve verilerin önce den istemci hafızasına alınması yoluyla iyileştirilmesidir. Bunu sağlamak için, istemciden sunucaya yapılan veri istekleri birer olay olarak düşünülmüş ve bu şekilde istemciden gelen isteklerin diziminden bir olay geçmişi oluşturulmuştur. Yayın ortamında istemciden gelen veri isteklerinin tutulduğu olay geçmişleri, çalışmamızda yayın geçmişleri olarak adlandırılmıştır. İlk olarak, yayın geçmişi analiz edilerek istemcilerin veri ulaşım davranışlarını gösteren sıralı patern ler keşfedilmiştir. Elde edilen sıralı paternler, yayın diskindeki verilerin orga nize edilmesi amacıyla kullanılmıştır. Burada önemli olan, yayın diskindeki verilerin aynı anda sıralı olarak istenen veriler birbirine yakın olacak şekilde yerleştirilmesidir. Daha sonra, tahmini olay saptaması teknikleri kullanılarak istemci hafızasına yerinde ulaşım oranının iyileştirilmesi yoluyla istemcilerin veri ulaşım zamanının azaltılmasına çalışılmıştır. Olayların tahmini saptanması, is temcilerin yayın diskinden verileri önceden hafızalarına olay geçmişinden çıkarılan kurallar yardımıyla yüklemeleri için kullanılmıştır. Anahtar sözcükler: veri analizi, olay geçmişi analizi, birbiriyle ilişkili geçmişler, çapraz ilintiler, aktif veri tabanı sistemleri, tahmini olay saptaması, kuralların önceden aktive edilmesi, bulanık olay kümeleri, bulanık tetikler, bulanık kural uygulaması, yakınlık derecesine göre olay saptaması, yayın geçmişleri, hareketli veri tabanları, önceden hafızaya yükleme, yayın organizasyonu.","ABSTRACT ACTIVE AND MOBILE DATA MANAGEMENT THROUGH EVENT HISTORY MINING Yücel Saygın Ph. D. in Computer Engineering Supervisor: Assoc. Prof. Özgür Ulusoy August 2001 An event history is a collection of events that have occurred in an event-based system over a period of time. There can be various types of events, among which are temperature changes and power demands in a power management system, client requests for data items in a data broadcast system, price increase of a stock in a stock market, and so on. There is a lot of interesting information that can be extracted from an event history via data mining techniques. Our purpose in this thesis is to propose methods for extracting this useful information in the form of event sequences and event associations from a single or two correlated event histories. We also aim to show how the results of the mining process can be used for active and mobile data management. The results of the mining process demonstrate the relationships among the events which are generally captured as associations or sequences. The relationships among the events are shown to be a useful tool to enhance an event-based system via event organization, predictive event detection, and proactive rule execution. We consider the mining of both a single event history and two correlated event histories. We first propose a method for mining binary relationships from a single event history. The binary relationships among events are used to organize the events into related groups of event. This organization is important because the number of events in an event-driven system may become very high and unman ageable. The groups of related events and the relationships among the events are exploited for predictive event detection and proactive rule execution in active database systems. We also consider the mining of two correlated event histories which are disjoint and the events in one history are related to the events in the other history. We describe how we can efficiently extract associations among theevents spanning different event histories, which we call cross associations. We have chosen data broadcast in mobile computing environments as a case study for active data management. One of the important facts in mobile comput ing environments with wireless communication medium is that the server-to-client (downlink) communication bandwidth is much higher than the client-to-server (uplink) communication bandwidth. This asymmetry makes the dissemination of data to client machines a desirable approach. However, the dissemination of data by broadcasting may induce high access latency in case the number of broadcast data items is large. Our purpose is to show how the features of ac tive data management can be used to improve mobile data management through broadcast data organization and prefetching from the broadcast medium. In or der to achieve this, the client requests of data items at the server are considered as events and the chronological sequence of items that have been requested by clients is considered as an event history. An event history in broadcast medium is called a broadcast history. The first step in this work is to analyze the broadcast history to discover sequential patterns describing the client access behavior. The sequential patterns are used to organize the data items in the broadcast disk in such a way that the items requested subsequently are placed close to each other. Then, we utilize predictive event detection techniques to improve the cache hit ratio to be able to decrease the access latency. Prediction of future client access behavior enables clients to prefetch the data from the broadcast disk based on the rules extracted from the broadcast history. Keywords: Data mining, event history mining, correlated histories, cross associ ations, active database systems, predictive event detection, proactive rule execu tion, fuzzy event sets, fuzzy triggers, fuzzy rule execution, similarity based event detection, broadcast histories, mobile databases, prefetching, broadcast organi zation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET Hiyerarşik Yapıda Olan Bir Veritabanının Kategorizasyonu Ferhat Kutlu Bilgisayar Mühendisliği, Yüksek Lisans Programı Tez Yöneticisi: Doç. Dr. H. Altay Güvenir Şubat 2001 Son yirmi yıldır İnternet alanındaki gelişmelerin etkisiyle veritabanlarında saklanan verinin boyutunda ve on-line veri akışında büyük bir artış oldu. Bu artış beraberinde, bu büyüklükteki veri yığınını ve akışını yönetebilecek araçlara olan ihtiyaçları açığa çıkardı. Hiyerarşik yaklaşım, bu ihtiyaçları tat min için en iyi yoldur ve Internet ve veritabanlarıyla uğraşanlar arasında da çok yaygındır. Usenet haber grupları sistemi, içinde yapısal bir hiyerarşi bu lunduran on-line veritabanlarından biridir. Bizim hareket noktamız da katego- rizasyon işlerini daha kolay ve hızlı hale sokan bu hiyerarşik yapıdır. Aslında İnternetteki arama motorlarının çoğu İnternetin yapısal hiyerarşisinden fay dalanmaktadır. Verilerin artan boyutu birçok geleneksel kategorizasyon algo ritmasını kullanılmaz hale sokmuştur. Bu sebeple Usenet haberlerinden oluşan bir veri tabanından indeks çıkartan ve daha sonra bu indeks üzerinden katego rizasyon yaparak yeni bir haberin ilgili haber gruplarını belirleyen yeni bir kate- grizasyon öğrenme algoritması geliştirdik. Bu algoritma öğrenme safhasında birleştirici ve aşağıdan yukarıya hiyerarşik bir yaklaşıma sahiptir. Katego rizasyon safhasında ise örtüşümlü ve denetlemeli bir kategorizasyon yapmak tadır. Algoritmamızın kompleksite ölçütünü ve doğruluğunu kıyaslamak için k En Yakın Komşu kategorizasyon algoritması kullanılmıştır. Bu kıyaslama sadece iki algoritmanın kıyaslanması demek değil, hiyerarşik yaklaşımın düz yaklaşımla, benzerlik ölçütünün mesafe ölçütüyle ve doğruluğun öneminin hızın önemiyle kıyaslanmasıdır. Algoritmamız hiyerarşik yaklaşımı ve benzerlik ölçü tünü benimsemekte ve küçük bir doğruluk kaybıyla k En Yakin Komşu algo ritmasından çok daha hızlı çalışmaktadır. Anahtar sözcükler: öğrenme, kategorizasyon, bölümleme, hiyerarşi, Usenet, haber grubu, üst seviye, başlık satırı, postalama, frekans, norm ölçekleme, benzerlik ölçütü, mesafe ölçütü, birleştirici, aşağıdan yukarı, eklerinden ayırma, yaygın kelime, indeks iv","ABSTRACT Categorization in a Hierarchically Structured Text Database Ferhat Kutlu M.S. in Computer Engineering Supervisor: Assoc. Prof. H. Altay Güvenir February 2001 Over the past two decades there has been a huge increase in the amount of data being stored in databases and the on-line flow of data by the effects of improvements in Internet. This huge increase brought out the needs for intelli gent tools to manage that size of data and its flow. Hierarchical approach is the best way to satisfy these needs and it is so widespread among people dealing with databases and Internet. Usenet newsgroups system is one of the on-line databases that have built-in hierarchical structures. Our point of departure is this hierarchical structure which makes categorization tasks easier and faster. In fact most of the search engines in Internet also exploit inherent hierarchy of Internet. Growing size of data makes most of the traditional categorization algorithms obsolete. Thus we developed a brand-new categorization learning algorithm which constructs an index tree out of Usenet news database and then decides the related newsgroups of a new news by categorizing it over the index tree. In learning phase it has an agglomerative and bottom-up hierarchical approach. In categorization phase it does an overlapping and supervised cate gorization, k Nearest Neighbor categorization algorithm is used to compare the complexity measure and accuracy of our algorithm. This comparison does not only mean comparing two different algorithms but also means comparing hier archical approach vs. flat approach, similarity measure vs. distance measure and importance of accuracy vs. importance of speed. Our algorithm prefers hi erarchical approach and similarity measure, and greatly outperforms k Nearest Neighbor categorization algorithm in speed with minimal loss of accuracy. Keywords: learning, categorization, clustering, hierarchy, Usenet, newsgroup, top-level, header-line, posting, frequency, norm-scaling, similarity measure, dis tance measure, agglomerative, bottom-up, stemming, stopword, index iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BİR TÜRKÇE DİYALOG SİSTEMİ İÇİN HATA TOLERANSLI SONLU DURUM ÇÖZÜMLEME Atacan Çonduroğlu Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. İlyas Çiçekli Temmuz, 2001 Doğal dil işlemede, yüksek seviyeli gramer formalizmaları cümle çözümlemesi için sıkça kullanılmaktadır. Pratikde hiç bir formalizma doğal dilin esnekliği ve çeşitliliği ile baş edemediği için, bu formalizmalar sınırlı alanlarda, alt-dillerle kullanılmaktadır. Her ne kadar sınırlandırılmamış bir dünyada, cümlelerden anlam çıkarabilmek için sofistike metodlar kullanılması gerekteğine inansak da, sınırlı alanlarda durum böyle olmak zorunda değildir. Basit, ama daha zaman- verimli olan sonlu durum makina metodları, kapalı alanlarda çözümleme için kullanılabilir. Basitlik ve zaman verimlilikleri ile bu metodlar sadece etkile şimli değil, ayrıca onların hafif gramer dışı cümleleri çözümlemelerini sağlıyan hata toleransı ile geliştirilmeleri daha kolaydır. Bu tezde, Türkçe banka diyalogları alanındaki, diyalog cümlelerini çözümlemek için, hata toleranslı sonlu durum tanımaya dayanan bir çözümleme modülü ve grameri sunmaktayız. Sen tetik olarak yaratılmış hatalı cümleler üzerindeki test sonuçları, önerdiğimiz sis temin gramer dışı cümleleri verimli bir şekilde analiz edebildiğini ve büyüyen gramerle beraber ölçeklenebildiğini göstermektedir. Anahtar sözcükler: Doğal Dil İşleme, Sonlu Durum Çözümleme, Hata Toleranslı Tanıma, Sağlam Çözümleme, Hafif Çözümleme, Diyalog Sistemleri, Doğal Dil Arayüzleri.","ABSTRACT ERROR TOLERANT FINITE STATE PARSING FOR A TURKISH DIALOGUE SYSTEM Atacan Çonduroğlu M.S. in Computer Engineering Supervisor: Asst. Prof. Dr. İlyas Çiçekli July, 2001 In NLP (Natural Language Processing), high level grammar formalisms are frequently employed for parsing. Since in practice no formalism can cope with the diversity and the flexibility of the human languages, such formalisms are used in closed domains, with sub-languages. Even though we believe that in an open world sophisticated analysis is required for extracting meaning from natural language texts, this does not have to be the case for the closed domains. Simpler time-efficient finite state methods can be used in closed domains. With their simplicity and time-efficiency, finite state methods are not only responsive, but also easy to augment with error tolerance which allows these methods to flexibly parse mildly ungrammatical sentences. In this thesis, we present a parser module which is based on error tolerant finite state recognition and a grammar for parsing transcribed dialogue utterances in a closed Turkish banking domain. Test results on the syntheticly created erroneous sentences indicate that the proposed system can analyze ungrammatical sentences efficiently and can scale with the growth of the grammar. Keywords: Natural Language Processing, Finite State Parsing, Error Tolerant Recognition, Robust Parsing, Light Parsing, Dialogue Systems, Natural Language Interfaces. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET EŞLENİK GRADYAN TİPİ PARALEL ÖZYİNELİ ÇÖZÜCÜLERDE İLETİŞİM GECİKME SÜRELERİNİN EN AZA İNDİRGENMESİ M. Mustafa Özdal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Cevdet Aykanat Temmuz, 2001 Eşlenik Gradyan (EG) tipi özyineli çözücüler büyük, seyrek, doğrusal denklem sistemlerinin çözümlerinde sıkça kullanılmaktadırlar. Genelde, her özyinelemede uygulanan temel işlemler, seyrek matris vektör çarpımları (SyMxV) ve vektör iç çarpımlarıdır. Paralel EG algoritmalarında, SyMxV işlemleri noktadan nok taya tipinde iletişim gerektirirken, iç çarpım işlemleri herkesten herkese yayım (HHY) tipinde iletişim gerektirmektedirler. Bu tezde, noktadan noktaya iletişim işlemlerinin HHY işlemlerinin içine gömüldüğü yeni bir iletişim metodu öner ilmektedir. Buradaki amaç, her bir işlemci tarafından gönderilen mesaj sayısının en aza indirgenmesi ve böylece paralel EG algoritmasındaki iletişim gecikme sürelerinin azaltılmasıdır. Diğer yandan, böyle bir metodun iletişim hacmini arttırma gibi bir dezavantajı vardır. Bu yüzden, iletişim hacmindeki ek yükleri en aza indirgemek için bir yöntem maliyet modeli ile birlikte sunulmaktadır. Öner ilen yöntemlerin pratikteki geçerliliklerini gözlemlemek için deneyler yapılmıştır. Özellikle mesaj başlama maliyetlerinin toplam iletişim zamanlarında baskın olduğu durumlarda, işlemci zamanlarında bir azalma gözlemlenmiştir. Anahtar sözcükler: seyrek matrisler, paralel özyineli çözücüler, paralel matris- vektör çarpımı, haberleşme en aza indirme, hiperçizge bölümleme.","ABSTRACT MINIMIZING COMMUNICATION LATENCIES IN CONJUGATE GRADIENT TYPE PARALLEL ITERATIVE SOLVERS M. Mustafa Özdal MS in Computer Engineering Supervisor: Prof. Cevdet Aykanat July, 2001 Conjugate Gradient (CG) type iterative solvers are widely used for the solu tion of large, sparse, linear system of equations on multicomputers. Typically, the basic operations performed at each iteration are sparse matrix vector multiplica tions (SpMxV), and inner product computations. In the parallel CG algorithm, SpMxV operations require point-to-point type communications, whereas inner product computations require all-to-all broadcast (AABC) type communications. In this thesis, we propose a novel communication scheme in which the point-to- point communications are embedded into the AABC operations. The purpose here is to minimize the number messages sent by each processor, so that the com munication latencies of a parallel CG program are minimized. However, such a scheme has the disadvantage that the communication volume requirements might increase. For this reason, a cost model and a methodology to minimize the over head in communication volume is given. Some experiments have been performed to test the practical validity of the proposed scheme. It is observed that the execution times for communication operations decrease in this scheme, especially for the configurations in which the message start-up costs dominate the total communication times. Keywords: sparse matrices, parallel iterative solvers, parallel matrix-vector mul tiplication, communication minimization, hypergraph partitioning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET YOĞUN NESNE KÜMELERİNİN GÜNCELLENMESİ İÇİN SINIRLAMAYA DAYALI ARTIMLI BİR YAKLAŞIM Engin Demir Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkun Ağustos, 2001 Son yıllarda birçok çalışmaya konu olan veri araştırmasının temel amacı, birik tirilen ve depolanan yüksek hacimli verilerin içinde saklı olan bilgileri çıkartmak ve çözümlemektir. Bu işlemi yürütmek için çok sayıda bilgi bulma teknikleri geliştirilmiştir. Verilerdeki ilginç ve yoğun oluşumları bulmaya yarayan ilişki ku rallarının belirlenmesi veri araştırmasında önemli bir yaklaşımdır. Durağan veri- tabanları için kimi teknikler geliştirilmiş olmasına karşın; gerçek uygulamalarda ilişki kurallarının araştırılmasında yoğun oluşumları güncellemek için dinamik yaklaşımlara da gereksinim vardır. Ayrıca, kullanıcılar açısından bakıldığında, mevcut sistemlerde, arama ve denetim eksiklikleri, odaklanma eksikliği, ve ilişkilendirmelerde katı bir yalınlık vardır. Bu tezde, mevcut sistemlerdeki iki eksikliğe de çözüm önerilmektedir. Erken eliminasyon yöntemi ile yoğun nesne kümelerinin güncellenmesini gerçekleştiren UWEP algoritmasına, monoton ol mayan ve özlü sınırlamalara uyum yetenekleri de uyarlanmıştır. Anahtar sözcükler: Veri araştırması, sınırlamaya dayalı, ilişki kuralları, yoğun nesne kümelerinin güncellenmesi.","ABSTRACT A CONSTRAINT-BASED INCREMENTAL APPROACH FOR UPDATE OF LARGE ITEMSETS Engin Demir M.S. in Computer Engineering Supervisor: Prof. Dr. Erol Arkun August, 2001 The main focus of data mining, subject of numerous studies in recent years, is to extract and analyze hidden knowledge from the collected and stored massive amounts of data. In order to manage this process, many automated knowledge discovery techniques have been developed. Discovery of association rules, which is the process of mining interesting and frequent patterns from data, is an important class of data mining. Most of the researchers proposed techniques for static databases but in real applications dynamic approaches are necessary to maintain the frequent patterns in terms of association rule mining. Additionally, from the standpoint of users, previously developed systems have lack of user exploration and control, lack of focus, and rigid notion of relationships. In this thesis, we propose a solution to both of the shortcomings of the previous systems. We integrate the anti-monotonicity and succinctness constraints to the previously developed update with early pruning (UWEP) algorithm. Keywords: Data mining, constraint based, association rules, update of large item- sets, pruning. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ETKİLİ KİŞİSELLEŞTİRME İÇİN WEB GÜNLÜKLERİNİN VERİ MADENCİLİĞİ YÖNTEMİ İLE ANALİZİ Esra Satıroğlu Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Halil Altay Güvenir Eylül, 2001 Web, her gün milyonlarca kişi tarafından ziyaret edilmesi nedeniyle kullanımı açısından büyük bir veri kaynağıdır. Kullanım verileri, web dağıtıcılarına ulaşan her türlü veri isteğinin kaydedildiği web günlüklerinde saklanmaktadır. Yakın geçmişte, bu veri topluluğunun kullanıcı davranış örüntülerini anlamak amacı ile analiz edilmesinin önemli sonuçlar doğurabileceği anlaşıldı. Kullanıcıların davranışsal örüntülerinin anlaşılması özellikle web aracılığı ile müşteri kazan maya ve ürünlerini satmaya çalışan elektronik ticaret şirketleri için önemlidir. Kullanıcılarını tanıyabilen ve kendisini kullanıcılarına göre kişiselleştirebilen etkileşimli web sitelerine sahip olmak şirketlerin oldukça önemli miktarlarda kar etmesini sağlayabilir. Kullanıma dayanan kişiselleştirme, kişiselleştirilmiş web siteleri üretmeye yönelik araştırma alanının genel adıdır. Bu tezde, kul lanıma dayanan yeni bir kişiselliştirme sistemi tanıtılacaktır. Bizim tarafımızdan tasarlanıp gerçekleştirilen bu sistem, bir web sitesinin ziyaretçilerinin ziyaret lerinin geri kalan kısmında talep edebilecekleri web sayfalarını tahmin etme yeteneğine sahiptir. Sistem, bu sayfalardan en yüksek skorlu olanları zi yaretçilerin görüntüledikleri sayfalara ekler. Tasarlayıp gerçekleştirdiğimiz bu sis temin iki ana modülü bulunmaktadır. Çevrimdışı modül web günlük dosyalarını eski kullanıcıların davranışsal örüntülerini keşfetmek amacı ile çevrimdışı olarak analiz eder. Çevrimdışı modül tarafından elde edilen bilgiler yeni ziyaretçileri tanımak ve onlar için öneriler üretmek amacı ile çevrimiçi modül tarafından kul lanılır. Yeni ziyaretçileri tanımak amacı ile kullanılan ilk kriter bu ziyaretçilerin web sitesinde izledikleri yollardır. Yol, bir ziyaretçinin web sitesine ziyareti süresinde görüntülediği sayfalardan oluşur. Yeni ziyaretçileri tanımak amacı ilekullanılan diğer bir kriterde bu ziyaretçilerin siteye bağlandıkları makinanın kim lik bilgisidir. Kimlik bilgisi, makinanın İP adresi veya etki alanı ismidir. Kim lik bilgilerini kullanarak ziyaretçinin kendisinin veya benzer etki alanlarında bu lunan ziyaretçilerin eski tercihlerini öğrenmek mümkün hale gelmektedir. İki etki alanı arasındaki benzerlik etki alanı sıradüzeni yardımı ile belirlenir. Etki alanı sıradüzeni, sıradüzensel ağaç yapısı ile temsil edilebilir. Bu ağacın en alt seviyesindeki düğümler Internet'e bağlanan makinaların etki alanı isimlerini içerir. Diğer düğümler ise com veya edu.tr gibi genel etki alanlarını kapsar. Bu sıradüzende, iki etki alanı arasındaki benzerlik bu etki alanlarının ortak ata düğüm sayısı arttığı sürece artar. Bu bilgilerin ışığında, tasarladığımız sistem bir ziyaretçinin hangi sayfaları talep edebileceğini anlamak için bu zi yaretçinin İnternet'e bağlanmak için kullandığı makinanın etki alanının ata etki alanlarına ait olan kullanıcıların ortak davranışsal yönsemelerinden yararlanmak tadır. Bu sistem, Bilkent Üniversitesi Bilgisayar Mühendisliği bölümünün web sitesi üzerinde test edilmiş ve sonuçlar sistemin verimliliğini ve kullanılabilirliğini kanıtlamıştır. Anahtar sözcükler: Kişiselleştirme, Web Kullanım Analizi, Kullanıcı Erişim Örüntüleri.","ABSTRACT MINING USER ACCESS PATTERNS AND IDENTITY INFORMATION FROM WEB LOGS FOR EFFECTIVE PERSONALIZATION Esra Satır oğlu M.S. in Computer Engineering Supervisor: Prof. Dr. Halil Altay Güvenir September, 2001 Web is a huge source of data in terms of its usage as a result of being visited by millions of people on each day. Its usage data is stored in web server logs which contain a detailed description of every single hit taken by the corresponding web server. Recently, it has been discovered that analysis of this data for understand ing the user behavioral patterns may have critical implications. Understanding the behavioral patterns of visitors is especially important for e-commerce compa nies which try to gain customers and sell products through the web. Interactive sites that recognize their customer and customize themselves accordingly may save lots of money to the companies. Usage Based Personalization is a study on designing such personalized sites. In this thesis, we present a new usage based personalization system. The system we designed and implemented is capable of guessing the web pages that may be requested by the on-line visitors during the rest of their visits. The system shows the subset of these pages with highest scores as recommendations to the visitors as being attached to the original pages. The system has two major modules. The off-line module mines the log files off-line for determining the behavioral patterns of the previous visitors of the web site considered. The information obtained by the off-line module is utilized by the on-line module of the system for recognizing new visitors and producing on-line recommendations. The first criterion for identifying on-line visitors is the paths followed by them. A path of a particular visitor consists of pages retrieved by him throughout his visit to the web site. Another criterion considered by the system is the identity information (IP address or domain name) of the visitors. By using identity information, it is possible to learn old preferences of the visitor himself or visitors from similar domains. The similarity between domains is determined iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET AATx'in VERİMLİ HESAPLANMASI İÇİN GRUPLAMAYA DAYALI YAKLAŞIM Bülent Özel Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Cevdet Aykanat Eylül, 2001 Birçok bilimsel uygulama, bir seyrek dikdörtgensel matrisin aynı matrisin transpozunun bir vektör ile defalarca çarpılmasını içermektedir. Bu bileşik işlem iki yol izlenerek yapılabilir. Birincisinde y = ATx ve z = Ay gibi iki seyrek matris vektör çarpımı her bir tekrarlama sırasında yapılır, ikincisinde ise, tekrarli çarpma işlemine girmeden hemen önce B - AAT gibi bir matris oluşturulur. Daha sonra, her bir tekrar adımında z = Bx çarpımı yapılır. Bu ikinci yolda oluşturulan B matrisi A matrisinden daha çok sıfırdan farklı ele mana sahip olacağı için genellikle bu ikinci yolda yapılan z = Bx çarpımı ilk yolda yapılan iki matris vektör çarpımından daha uzun sürmektedir. Bazı ma trislerde seyreklik yapısına bağlı olarak ikinci yolda yapılan iş daha az olmak tadır. Bu çalışmada önerilen gruplama algoritmaları ile A matrisi alt matrislere ayrıştırılmış ve her bir alt matris için yukarıda adı geçen çarpma yollarının hangisinin kullanılacağı yine bu çalışmada önerilen ölçekler ile belirlenmiş ve verilen bir matrisi iki çarpma yolunu da kullanarak çarpan melez bir algoritma önerilmiştir. Gruplama problemi seyrek matrislerin iki parçalı çizge modelleri kul lanılarak ele alınmıştır. Önerilen gruplama algorithmaları A matrisine tekrarlı çarpma işleminden önce uygulanmış ve değişik uygulama alanlarından örneklenen bir çok matris üzerinde sınanmıştır. Matris elemanı çarpma- toplama işleminde yüzde 17'lere varan azalma sağlanmıştır. Anahtar kelimeler: matris-matris-transpoz-vektör hesabı, seyrek matris kümeleme algoritması, seyrek matris veri yapıları.","Ill ABSTRACT CLUSTERING BASED APPROACH FOR EFFICIENT AATx COMPUTATION Bülent Özel M.S. in Computer Engineering Supervisor: Prof. Dr. Cevdet Aykanat September, 2001 Many scientific applications involve repeated sparse matrix-matrix-transpose- vector multiplication computations. These computations are carried out by following two schemes. In the first one, two successive sparse matrix multipli cations in the form of y = ATx and z = Ay are performed at each iteration. In the second one, matrix B = AAT is obtained prior to the iterations and then the matrix-vector multiply z - Bx is performed at each iteration. Matrix B is usually denser than the A matrix, and hence the computation z = Bx is more expensive then the afore mentioned successive products. In some cases, due to sparsity pattern öf the A matrix, the computation z = Bx may require lesser computation time. In this work, we propose clustering metrics and algo rithms to decompose the A matrix into blocks to use both of the computation schemes in order to reduce the computation time. We give a hybrid algorithm for computing z = AATx that combines the two computation schemes. The clustering problem is modelled using bipartite graph representation of sparse matrices. Efficient algorithms are proposed based on the metrics and as pect ratios that measure the extra multiply-and-add operations and encourage the formation of blocks that are suitable for the hybrid scheme. The clustering algorithms are applied once to matrix A as a preprocessing step before the repeated matrix-matrix-transpose-vector computations. Very promising im provements on sparse matrices from various collections are obtained, where up to 17% reductions in the number of multiply-and-add operations are observed. Key words: Sparse matrix clustering algorithms, matrix-matrix-transpose- vector computation schemes, sparse matrix data structures."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DAĞITIK BELLEKLİ SİSTEMLERDE PARALEL DİZİ MADENCİLİĞİ Embiya KARAPINAR Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Atilla Gürsoy Şubat, 2001 Çok büyük veritabanlarında tüm sık dizileri bulmak çok zaman alan bir görevdir. Bununla birlikte, çok büyük veritabanları orjinal veritabanını birden çok veri yığınına parçalayarak ana bellekte işlemeyi zorunlu kılar. Çoğu güncel algoritmalar en uzun sık dizinin uzunluğu adedince veritabanını okumayı gerektirir. Spade, kafes-kuramı yaklaşımını kullanarak orjinal problemi ana hafızada işlenebilen küçük parçalara ( eşdeğer sınıflara) ayıran ve vertabanını üç kere okuyan çok hızlı bir algoritmadır. Bu tez çalışmasında, dağıtık bellekli sistemler için sık diziler kümesinin tamamını bulan ve Spade algoritmasını baz alan dSpade adlı paralel algoritmayı öneriyoruz. dSpade algoritması her işlemcinin eşit miktarda müşteri hareketi sakladığı yatay veritabanı parçalama metodunu kullanır. dSpade birli ve ikili sık dizileri bulan Fi ve Fi fazları süresince anauyumlu bir algorit madır. Her işlemci F1 ve F2 fazları süresince yerel verileri üzerinde yerel destek sayılarını bulur ve genel birli ve ikili sık dizileri bulmak için bu destek sayılarını diğer işlemcilere yayımlar. Birli ve ikili sik dizileri bulduktan sonra tüm sık diziler kafes içine yerleştirilir ve orjinal problemi küçük parçalara bölmek amacıyle kafes eşdeğer sınıflara ayrıştırılır. Eşdeğer sınıflar açgözlü kurami yöntemiyle en az görev yükü olan işlemciye döngüsel bir sırayla eşleştirilir. Bu aşamadan sonra, her işlemci zaman uyumsuz olarak kendisine eşleştirilen eşdeğer sınıfları üzerindeki tüm artan uzunluktaki sık dizileri, Fk, bulur. Sonuçlarını açıkladığımız başarım deneylerini 32-düğümlü Beowulf kümesinde yürüttük. Deneyler gösterdi ki, dSpade iyi bir hız oranı ve veritabanı boyutuna bağlı olarak lineer ölçekle artan sonuçlar verir. Anahtar Sözcükler: Kafes, eşdeğer sınıf, yatay veritabanı parçalama metodu iv","ABSTRACT PARALLEL SEQUENCE MINING ON DISTRIBUTED-MEMORY SYSTEMS Embiya KARAPINAR M.S. in Computer Engineering Supervisor: Asst. Prof. Dr. Atilla Gürsoy February, 2001 Discovering all the frequent sequences in very large databases is a time consuming task. However, large databases forces to partition the original database into chunks of data to process in main-memory. Most current algorithms require as many database scans as the longest frequent sequences. Spade is a fast algorithm which reduces the number of database scans to three by using lattice-theoretic approach to decompose origional problem into small pieces (equivalence classes) which can be processed in main-memory independently. In this thesis work, we present dSpade, a parallel algorithm, based on Spade, for discov ering the set of all frequent sequences, targeting distributed-memory systems. In dSpade, horizontal database partitioning method is used, where each processor stores equal number of customer transactions. dSpade is a synchronous algorithm for discovering frequent 1-sequences (F1) and frequent 2-sequences ( F%). Each processor performs the same computation on its local data to get local support counts and broadcasts the results to other processors to find global frequent sequences during F1 and F2 computation. After discovering all F1 and F2, all frequent sequences are inserted into lattice to decompose the original problem into equivalence classes. Equivalence classes are mapped in a greedy heuristic to the least loaded processors in a round- robin manner. Finally, each processor asynchronously begins to compute Fk on its mapped equivalence classes to find all frequent sequences. We present results of performance experiments conducted on a 32-node Beowulf Cluster. Experiments show that dSpade delivers good speedup and scales linearly in the database size. Keywords: Lattice, equivalence class, horizontal database partitioning method. 111 igggSSSSS"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VİDEO VERİ TABANI İÇİN İNTERNET TABANLI SORGU BELİRLEME ARAYÜZÜ Ediz Şaykol Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Yard. Doç. Dr. Uğur Güdükbay ve Doç. Dr. Özgür Ulusoy Eylül, 2001 Mültimedya teknolojisindeki gelişmeler, sorgulama ve sorgu belirleme tekniklerinin gelişmesini hızlandırmıştır. Geliştirilen yeni teknikler, video veri tabanları için etkili sorgu belirleme arayüzlerinin düzenlenmesini sağlamıştır. Genellikle birden çok kullanıcıya hizmet vermek amacında olan video veri ta banları, uygun çok kullanıcılı ortam olan İnternet üzerinde çalışacak şekilde planlanmıştır. Bu tezde, bir video veri tabanı için İnternet tabanlı sorgu belirleme arayüzü sunulmaktadır. Arayüz, hiyerarşik yapıdaki pencereler den oluşmakta ve mantıksal olarak ayrılabilen sorguların belirlenmesi ayrı pencerelerde yapılmaktadır. Örneğin, uzaysal sorgular ve hareket (yörünge) sorguları farklı pencerelerdedir. Bu şekilde kullanıcıların sorgu belirleme işlemi kolaylaşmaktadır. Ayrıca sorguların yanıtları da video kareleri olarak ayrı bir pencerede kullanıcıya sunulmaktadır. Tezde sunulan bir başka konu da nes nelerin renk ve şekillerine göre sorgulanması tekniğidir. Bu teknik, nesnelerin renk ve şekillerini özel histogram tabanlı yapılarda saklamak esasına dayan maktadır. Deney sonuçları, bu tekniğin literatürdeki tekniklerin eksiklerini giderdiğini ve onlardan daha başarılı olduğunu göstermiştir. Tezde ayrıca, bu teknikle birlikte kullanılmak üzere geliştirilmiş bir nesne ayrıştırma aracı sunul maktadır. Bu araç yardımıyla resimlerdeki ve video karelerindeki nesne bölgeleri ayrıştırılabilmektedir. Anahtar sözcükler: İnternet tabanlı sorgu belirleme arayüzü, nesne ayrıştırma, ayrıştırma için alan doldurma, uzaklık histogramı, açı histogramı, poligon ba sitleştirme için kinematik yöntem. iv","ABSTRACT WEB-BASED USER INTERFACE FOR QUERY SPECIFICATION IN A VIDEO DATABASE SYSTEM Ediz Şaykol M.S. in Computer Engineering Supervisors: Assist. Prof. Dr. Uğur Güdükbay and Assoc. Prof. Dr. Özgür Ulusoy September, 2001 The enhancements in multimedia technology have accelerated the development of new techniques for querying and retrieval in video databases. Based on these new techniques, effective user interfaces are designed especially for query specification. Since the Internet is a suitable multiple client environment, most of the video database systems are designed to work over the Internet. In this thesis, a web- based user interface for a video database system is presented. The interface is composed of a hierarchical windows-based structure where logically separable sub-queries are handled in separate windows. For example, spatial and motion queries are handled separately. The separation of the interface aims to facilitate the query specification process. Furthermore, the user is allowed to get partial results for the sub-queries. The retrieval and result presentation process displays the query results in the form of video frame sequences on a separate window with the capability of playing each result separately. As another contribution of this thesis, considering the fact that most of the systems include querying by object features, a novel approach for color and shape querying and retrieval is proposed. The proposed approach is histogram-based and it is shown through the performance experiments that it overcomes the deficiencies of the existing methods. This new approach is supported by an auxiliary object extraction tool in the query-by-feature sub-system of the video database system. The object extraction tool is semi-automatic, hence it can successfully capture salient object regions for most of the images and/or video frames. Keywords: Web-based query specification interface, object extraction, flood fill for extraction, distance histogram, color histogram, kinematics model for polygon simplification. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET insan modellerinin gerçeğe uygun olarak görüntülenmesi Gültekin Arabacı Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticileri: Prof. Dr. Bülent Özgüç ve Yrd. Doç. Dr. Uğur Güdükbay Eylül, 2001 İnsan modellerinin gerçeğe uygun olarak modellenmesi bilgisayar grafiği alanın da gittikçe artan bir öneme sahiptir. Eklemlerdeki derinin uygun şekil değiş tirmesinin ve kasların şişmesinin simulasyonu, insan animasyonunu daha ger çekçi kılar. Bu tezde derinin kaslara kasların kemiklere bağlı olarak hareket ettiği katmanlı bir insan animasyon modeli önerilmektedir. Kaslar elips şeklinde modellenmiş olup çubuk figürlerle gösterilmiş olan kemiklerin hareketlerine göre şekil değiştirmektedir. Deri kaslara bağlanmıştır ve kasların kasılmalarına göre şekil değiştirmektedir. Eklem hareketlerine göre her kasın kullanıcı tara fından belirlenmiş değişik kasılma oranları olabilmektedir. Anahtar Sözcükler: İnsan animasyonu, deri ve kas deformasyonu, 3 boyutlu modelleme, boyama. iv","ABSTRACT REALISTIC RENDERING OF HUMAN MODELS Gültekin Arabacı M.S. in Computer Engineering Supervisors: Prof. Dr. Bülent Özgüç and Assist. Prof. Dr. Uğur Güdükbay September, 2001 Realistic rendering of human models has an increasing importance in computer graphics. Simulation of muscle bulging and proper deformations of skin at joints, makes human animation more realistic. In this thesis, we describe a layered human animation system in which muscles move with respect to the bones and the skin deforms according to the muscles. Muscles are modelled as ellipsoids and their shapes are deformed with the movements of the bones represented by sticks in the skeleton. The skin is anchored to the muscles and it changes its shape as the muscles bulge. Every muscle may have different user defined bulging ratios according to the joint movements. Keywords: Human animation, skin and muscle deformation, 3D modelling, rendering. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DOĞRUSAL OLMAYAN PROGRAMLAMA KULLANAN TERS KİNEMATİK YÖNTEMİYLE İNSAN MODELLERİNİN ANİMASYONU A. Sezgin Abalı Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Güdükbay Eylül, 2001 Askeri uygulamalar, ergonomik tasarım gibi geniş uygulama alanları olan in san modellerinin animasyonu bilgisayar grafiğinin en önemli konularından biri olmuştur. Bir eklemli vücut genellikle eklemlerle bağlanmış segmanlar seti olarak modellenir. Eklem açılarındaki değişim, vücuda yeni bir duruş verir. Bir animator, yeni bir duruş için eklem açılarını belirleyebilir (ileri kinematik). Fakat, vücudu bir pozisyona yerleştirmek için gerekli kesin eklem açılarını tahmin etmek zordur. Bunun yerine bir animator vücuttaki uç bir nokta için istenen bir pozisyonu tanımlayabilir ve sonra ters kinematik algoritması vücudu yerleştirmek için gerekli eklem açılarını hesaplar. Bu tezde, insan modellerinin animasyonu için doğrusal olmayan optimizasyon ile ters kinematik yönteminin gerçekleştirilmesi anlatılmaktadır. Bu yöntem, son-etkileyici ve hedef olarak adlandırılan son etkileyicinin istenen pozisyonu için bir potansiyel fonksiyon tanımlar. Doğrusal olmayan optimizasyon algoritması bu fonksiyonun değerini küçültmeye çalışır. Eklem açılarının üst ve alt limitleri olduğundan dolayı fonksiyon değeri sıfırlanamayabilir. Bu durumda algoritma en iyi çözümde durur (lokal optimum). Kullanıcı, eklem açılarına, algoritma tarafından tahmin edilmiş ilk değerlerinin ağırlıklarını değiştirerek öncelik verebilir. Böylece, eklem açılarının animatörün önceliğine göre değişmesi sağlanmaktadır. Anahtar sözcükler: animasyon, insan hareketi, ters kinematik, doğrusal olmayan programlama, optimizasyon. İV","ABSTRACT ANIMATION OF HUMAN MOTION WITH INVERSE KINEMATICS USING NONLINEAR PROGRAMMING A. Sezgin Abalı M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay September, 2001 Animation of articulated figures has always been an interesting subject of com puter graphics due to a wide range of applications, like military, ergonomic de sign etc. An articulated figure is usually modelled as a set of segments linked with joints. Changing the joint angles brings the articulated figure to a new posture. An animator can define the joint angles for a new posture (forward kinematics). However, it is difficult to estimate the exact joint angles needed to place the articulated figure to a predefined position. Instead of this, an animator can specify the desired position for an end-effector, and then an algo rithm computes the joint angles needed (inverse kinematics). In this thesis, we present the implementation of an inverse kinematics algorithm using nonlinear optimization methods. This algorithm computes a potential function value be tween the end-effector and the desired posture of the end-effector called goal. Then, it tries to minimize the value of the function. If the goal cannot be reached due to constraints then an optimum solution is found and applied by the algorithm. The user may assign priority to the joint angles by scaling ini tial values estimated by the algorithm. In this way, the joint angles change according to the animator's priority. Keywords: animation, human motion, inverse kinematics, nonlinear program ming, optimization. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET WEB BİLGİ KAYNAKLARININ KONU-MERKEZLİ SORGULANMASI ismail Sengör Altmgövde Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Eylül, 2001 internetin insanlık tarafından bilinen hemen hemen en büyük bilgi kaynağı ol masıyla, aranılan bilgiye web üzerinde yeterince kısa bir zamanda ulaşabilmek önemli bir sorun ve araştırma konusu haline gelmiştir. Uzmanlarca hazırlanmış yüksek kaliteli indekslerden ve bilgi erişim tekniklerini kullanan (kimi zaman özelleştirilmiş) arama motorlarından anahtar-kelime tabanlı aramalar için fay- dalanılırken, çoğu akademik araştırma amaçlı bir çok web sorgu dili de literatürde önerilmiştir. Ancak, anahtar-kelime aramaya dayalı yaklaşımlar web üzerindeki ""bilgi kirliliğine"" karşı savunmasız olup aranılan konuyla ilgisi olmayan pek çok sonuç geri döndürebilmektedir. Diğer taraftan web sorgu dilleri ise pratikte kul lanımlarını sağlayabilecek hız ve genel geçerlikten uzak durumdadır. Bu tezde, web kaynak alanlarını temsil etmek ve yüksek kaliteli arama sonuçlarını kabul edilebilir sürede geri döndürebilecek sorgu kapasitesi sağlamak için ""metadata""da,rı ve beraberinde XML temelli bir takım standartlardan fay dalanılmaktadır. Bu tezde web bilgi kaynakları için ""web bilgi uzayı"" metadata modeli ve bu model üzerinde işlem yapan SQL-TC (Konu-merkezli SQL) sorgu dili sunulmaktadır. Bahsedilen web bilgi uzayı modelinin bileşenleri ise web- tabanlı bilgi kaynakları (internetteki XML veya HTML belgeleri), uzman öneri veri-tabanları (bilgi kaynakları için alan/konu uzmanlarınca belirlenmiş meta data) ve kullanıcılar için özelleştirilmiş bilgilerdir (XML belgeleri olarak saklanan kullanıcı profilleri ve tercihleri). Uzman önerileri, literatürde yakın zamanda sunulmuş olan konu haritaları çalışması göz önüne alınarak belli bir ilgi alanındaki ""konu'l&r ve konular-arası ""ilişkiler"" (metalink'ler) kullanılarak verilmektedir. Alan uzmanları, tanımladıkları konulara ve metalinklere bunların o alandaki önemlerini yansıtan sayısal önem değerleri de iliştirmektedirler. Uygun koşullarVI altında, bu konu ve rnetalinklerin web ortamındaki gerçek bilgi kaynaklarıyla ilişkilendirilmesiyle uzman öneri veritabanı bu bilgi kaynakları için bir kavramsal indeks işlevi kazanmaktadır. Modelin bir diğer bileşeni olan kullanıcı profilleri ise kullanıcının bilgi seviyesini ve web dolaşım tarihçesini yine konular ve zi yaret edilen web kaynakları cinsinden saklamaktadır. Kullanıcı tercihleri ise belli bir alan uzmanının önerilerine karşı kullanıcının yaklaşımını ve/veya güvenini yansıtmaktadır. Bu tezde sunulan SQL-TC sorgu dili uzman öneri veri-tabanlarmda sağlanan metadata'yı kullanmakta ve sorgu sonuçlarını daraltmak ve özelleştirmek için (yukarıda bahsedilen) kullanıcı bilgilerinden faydalanmaktadır. Sorgu sonucunda bulunan nesneler veya veri tablosu satırları ise uzmanlarca atanan ve kullanıcı ter cihlerine göre değerlendirilen önem değerleri göz önüne alınarak sıralanmaktadır. Bu sıralama sonrasında kullanıcıya (isteğine göre) sadece en önemli ilk n nesne ve/veya belli bir önem değerinin üstündeki nesneler ya da her ikisi birden sunula bilmektedir. Bunlardan dolayı, SQL-TC sorgu dilinin aranılan konuyla ""anlam sal"" olarak ilişkili sonuçları kısa zamanda bulabilmesi beklenmektedir. Anahtar sözcükler: metadata, XML, Konu Haritaları, web veri modellemesi, web sorgulaması, kavramsal indeksleme, kullanıcı profili.","ABSTRACT TOPIC-CENTRIC QUERYING OF WEB RESOURCES Ismail Sengör Altmgövde M.S. in Computer Engineering Supervisor: Assoc. Prof. Dr. Özgür Ulusoy September, 2001 As the world wide web (WWW) has evolved to be almost the largest source of information that is known by human beings, locating relevant information on the web in a reasonably short time has become a major struggle. High qual ity indices and (sometimes specialized) search engines that employ information retrieval techniques are widely used for keyword based searches, and a number of web query languages have also been developed, mostly for research purposes. However, most of the keyword-based approaches are vulnerable to the noise on the web, leading to unqualified results with lots of irrelevant documents; whereas the web-query languages lack the speed or generality to be used in practical cases. In this thesis, we make use of metadata (along with some XML-based stan dards) to characterize the web resource domains, and to provide sophisticated querying features with high-quality results and a reasonably fast response time. We propose a ""web information space"" metadata model for web information re sources, and a query language SQL-TC (Topic-Centric SQL) to query the model. The web information space model is composed of web-based information resources (XML or HTML documents on the web), expert advice repositories (domain ex pert specified metadata for information resources), and personalized information about users (user profiles and preferences, as XML documents). Expert advice is specified using topics and relationships among topics (called metalinks) in a particular domain of interest, along the lines of the recently proposed topic maps. Experts also attach importance values to topics and metalinks that they spec ify, and link them to actual information resources on the web whenever possible, creating a semantic index over the resources. User profiles keep track of user knowledge and navigation history in terms of these topics and their (visited) sources, whereas user preferences declare users' attitudes and confidence for the mIV choices of particular experts. The query language SQL-TC makes use of the metadata information provided in expert advice repositories and embedded in information resources, and employs user preferences to further refine the query output. Query output objects/tuples are ranked with respect to the (expert-judged and user-preference- revised) im portance values of requested topics/metalinks, and the query output is limited by either top ra-ranked objects/tuples, or objects/tuples with importance values above a given threshold, or both. Therefore, the query output of SQL-TC is expected to produce highly relevant and semantically related responses to user queries within short amounts of time. Keywords: metadata, XML, Topic Maps, web data modeling, web querying, se mantic indexing, user profile."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ARAZİ YÜKSEKLİK VERİLERİNİN STEREOSKOPİK BAKIŞA GÖRE HIZLI GÖRÜNTÜLENMESİ Türker Yılmaz Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Uğur Güdükbay Haziran, 2001 Geniş geometrik ortamların etkileşimli olarak görüntülenmesi bilgisayar grafik leri ile uğraşanlar için devamlı olarak bir problem olmuştur. Bu çalışmada geniş arazi verilerinin stereoskopik ve çok çözünürlüklü olarak görüntülenme si için bir yapı sunulmuştur. Arazi verilerinin temsili için kullanılan dörtlü ağaç yapısı, arazinin bakışa göre değişken olarak çok çözünürlüklü modeli nin elde edilmesi için sorgulanmıştır. Derinlik bilgisini kaybetmeyen mesafe temelli açısal sadeleştirme kriteri ile birlikte, stereoskopik olarak görüntülemeyi hızlandıran ""Üçgenlerin Aynı Anda Üretilmesi"" adı verilen yeni bir yöntem geliştirilmiştir. Bu yöntemde, birinci göz görüntüsü için çizilecek üçgenler üretildikten sonra, ikinci göz görüntüsünü elde etmek için çeşitli dönüşümlere tabi tutulmakta, böylece işlemci zamanını alan üçgenlerin bakış piramidinin dışında kalanlarının ayıklanması ve köşelerin aktif hale getirilmesi işlemleri iki göz görüntüsü için bir defa yapılmaktadır. Arazi üzerindeki kırılma problemi bağımlılık ilişkileri kullanılarak çözülmüştür. Farklı çözünürlükler arasındaki atlama etkileri başkalaşım yöntemi kullanılarak giderilmiştir. Önerilen algorit malar bir görüntüleme sistemi haline getirilmiş ve kişisel bilgisayarlar ile grafik iş istasyonlarında uygulanmıştır. Performans ölçümleri sonucunda, arazinin düzgün ve çok çözünürlüklü stereoskopik görüntüleri, her iki gözü ayrı ayrı üretmeye göre % 45 daha hızlı olarak elde edilmiştir. Anahtar Sözcükler: Stereoskopik görüntüleme, arazi yükseklik verisi, çok çözünürlüklü modelleme, dörtlü ağaçlar. iv","ABSTRACT FAST STEREOSCOPIC VIEW-DEPENDENT VISUALIZATION OF TERRAIN HEIGHT FIELDS Türker Yılmaz M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay September, 2001 Visualization of large geometric environments has always been an important problem of computer graphics. In this thesis, we present a framework for the stereoscopic view-dependent visualization of large scale terrain models. We use a quadtree based multiresolution representation for the terrain data. This structure is queried to obtain the view-dependent approximations of the terrain model at different levels of detail. In order not to loose depth information, which is crucial for the stereoscopic visualization, we make use of a different simplification criterion, namely distance-based angular error threshold. We also present an algorithm for the construction of stereo pairs in order to speed up the view-dependent stereoscopic visualization. The approach we use is the simultaneous generation of the triangles for two stereo images using a single draw-list so that the view frustum culling and vertex activation is done only once for each frame. The cracking problem is solved using the dependency information stored for each vertex. We eliminate the popping artifacts that can occur while switching between different resolutions of the data using morphing. We implemented the proposed algorithms on personal computers and graphics workstations. Performance experiments show that the second eye image can be produced approximately 45 % faster than drawing the two images separately and a smooth stereoscopic visualization can be achieved at interactive frame rates using continuous multi-resolution representation of height fields. Keywords: Stereoscopic visualization, terrain height fields, multiresolution rendering, quadtrees. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KONUMA DAYALI SÜREKLİ SORGULARIN İŞLENMESİNDE KULLANILAN ETKİN BİR KONUM GÜNCELLEME METODU İlker Yoncacı Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Eylül, 2001 Mobil haberleşme ağları ve taşınabilir bilgisayarlardaki son teknolojik gelişmeler mobil bilgi servislerinin ve mobil uygulamaların fonksiyonlarını önemli derecede arttırmıştır. Bu uygulamaların pek çoğunun yakın gelecekte konuma dayalı sürekli sorguları desteklemesi beklenmektedir. Bir konuma dayalı sürekli sorgu belli bir zaman dilimi içerisinde arka arkaya işlenir ve değişen sonuçlar sorguyu yapan mobil kullanıcıya iletilir. Mobil bir kullanıcı tarafından sisteme verilen konuma dayalı sorguların sonucu hem sorgulanan mobil nesnelerin konumuna hem de sorguyu yapan mobil kullanıcının konumuna bağlıdır. Konuma dayalı sürekli bir sorguda S nesnesi başlangıç zamanından bitiş zamanına kadar bu sor gunun yanıtına dahil ise, bunun gösterimi < S, başlangıç, bitiş > şeklinde ola bilir. Sürekli sorguların doğruluk ve zamanlamaları hareketli nesnelerin konu munu yönlendiren Değişken İzleme Metodu (DİM) ile daha önce incelenmiştir. Bu tezde, mobil bilgi sistemlerinde sorgulara değişik kategoriler verilebilme sine olanak sağlayan Kategorize Edilmiş İzleme Metodu (KEİM) önerilmiştir. KEİM metodunun amacı, mobil sistemlerde kritiklik kategorisi arttıkça, sis temin yükünü çok arttırmadan sorgu sonuçlarının doğruluğunu arttırmaktır. Bu tezde çok sayıdaki kritiklik seviyesi olan bir simülasyon modeli kullanılmış ve bu modelle önerilen metodun geniş bir yelpazede deneyleri yapılmıştır. Anahtar sözcükler: Mobil sistemler, konum yönetimi, konum güncelleme, konuma dayalı sürekli sorgu. iv","ABSTRACT An Efficient Method for Generating Location Updates in Processing Location Dependent Continuous Queries İlker Yoncacı M.S. in Computer Engineering Supervisor: Assoc. Prof. Özgür Ulusoy September, 2001 Recent advances in wireless communication networks and portable computers have greatly increased the functionality of mobile information services and mobile computing applications. Most of those applications are expected to support location-dependent continuous queries (LDCQs) in the near future. A LDCQ is evaluated continuously in the system for a period of time and the changing results can be transmitted to the requesting client. The result of a LDCQ depends on both the locations of mobile objects on which the query has been issued and the location of the mobile client which has submitted the query. The result of a LDCQ can be provided to the requesting client as a set of tuples < S, begin, end > indicating that object S satisfies the query from time begin to time end. The correctness and timeliness of query results of LDCQs have been studied by employing an adaptive monitoring method (AMM) that manages the locations of moving objects. In this thesis, we propose a Categorized Adaptive Monitoring Method (CAMM) which allows to specify various criticality levels for LDCQs. The aim of CAMM is to provide higher levels of correctness for query results as the criticality of the query increases, without significantly increasing the wireless bandwidth requirements. We provide a simulation model with multiple criticality levels and evaluate the performance of the mobile system and the proposed method with extensive simulation experiments. Keywords: Mobile computing, location management, location update gener ation, location dependent continuous queries. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET EN İYİ ÖZNİTELİKLERİ SEÇME İLE REGRESYON Tolga Aydın Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Halil Altay Güvenir Eylül, 2000 Regresyon problemleri için, En İyi Öznitelik izdüşümlerini Seçerek Regresyon (RSBFP) ve En İyi Öznitelikleri Seçerek Regresyon (RSBF) adında iki yeni makine öğrenmesi metodu sunulmuştur. Bu metodlar minimum kareler re- gresyonunun ağırlıklı kullanımı ile çalışkan, parametrik ve adaptif modeller oluştururlar. Makine öğrenmesi ve istatistik literatürünün DART, MARS, RULE ve kNN gibi ünlü metodları hem tahmin gücü yüksek, hem de hızlı öğrenme ve/veya sorgulama yapan modeller üretememektedirler. RSBFP ve RSBF, literatürdeki bu boşluğu doldurmak için geliştirilmiştir. RSBFP, lineer özniteliklere ve/veya kategorik öznitelik parçalarına ait olan basit lineer re gresyon doğrularından bir karar listesi (model) oluşturur. RSBF, RSBFP 'nin gelişmiş versiyonu olup, karar listesi hem lineer özniteliklere ait çoklu hem de kategorik öznitelik parçalarına ait basit lineer regresyon doğrularından oluşur. Çoklu regresyon doğrularında yer alan öznitelikler geliştirilen bir uygunluk sezgisi ile bulunur. RSBFP ve RSBF'in, gereksiz özniteliklere, bilinmeyen değerlere ve gürültülü hedef öznitelik değerlerine karşı dayanıklı bir perfor mansa sahip olduğu gösterilmiştir. Böyle durumlarda diğer metodlara nazaran daha iyi sonuçlar vermeleri, gerçek veri kümeleri için uygun birer tahmin aracı olduklarını gösterir. Anahtar Sözcükler: Regresyon, fonksiyon yaklaştırımı, öznitelik izdüşümü. iv","ABSTRACT REGRESSION BY SELECTING BEST FEATURE(S) Tolga Aydın M.S. in Computer Engineering Supervisor: Assoc. Prof. Halil Altay Güvenir September, 2000 Two new machine learning methods, Regression by Selecting Best Feature Projections (RSBFP) and Regression by Selecting Best Features (RSBF), are presented for regression problems. These methods heavily make use of least squares regression to induce eager, parametric and context-sensitive models. Famous regression approaches of machine learning and statistics literature such as DART, MARS, RULE and kNN can not construct models that are both predictive and having reasonable training and/or querying time dura tions. We developed RSBFP and RSBF to fill the gap in the literature for a regression method having higher predictive accuracy and faster training and querying time durations. RSBFP constructs a decision list consisting of simple linear regression lines belonging to linear features and/or categorical feature segments. RSBF is the extended version of RSBFP such that the decision list consists of both simple, belonging to categorical feature segments, and/or mul tiple, belonging to linear features, linear regression lines. A relevancy heuristic has been developed to determine the features involving in the multiple regres sion lines. It is shown that the proposed methods are robust to irrelevant features, missing feature values and target feature noise, which make them suitable prediction tools for real-world databases. In terms of robustness, RS BFP and RSBF give better results when compared to other famous regression methods. Keywords: Regression, function approximation, feature projections. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET ÇOK DİSKLİ VERİTABANLARI İÇİN HİPERÇİZGE TABANLI AYRIŞTIRMA Mehmet Koyutürk Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Cevdet Aykanat Eylül, 2000 Çok büyük dağınık veritabanlarmda, sorguların işlenmesini paralelleştirmek için veri disklere ayrıştırılmaktadır. Ayrıştırma, verinin her ilişkide yer alan öğelerin disklere eşit dağılacakları şekilde yerleştirilmesi anlamına gelir. Lit eratürde birçok ayrıştırma yöntemi önerilmiş olmasina karşın önerilen yöntemler ya alana özel ya da bazı dezavantajları olan yöntemlerdir. Bu çalışmada prob leme tam olarak uyan bir model önerilmiş ve yinelemeli iyileştirme yöntemlerinin her ilişkiyi detaylı olarak değerlendirerek ayrıştırma hedefini gerçekleştirme yetisine sahip olduklarını gösterilmiştir. Ayrıştırma probleminin çözümü için iki aşamalı bir yinelemeli iyileştirme algoritması ve bu probleme uygun kazanç fonksiyonları önerilmiştir. Yapılan deneyler, önerilen algoritmanın en gelişkin ayrıştırma yöntemi olan çizge parçalama yönteminden daha üstün performans sergilediğini göstermektedir. Anahtar kelimeler: Dağınık veritabanları, ayrıştırma, hiperçizge parçalama, çizge parçalama.","Ill ABSTRACT HYPERGRAPH BASED DECLUSTERING FOR MULTI-DISK DATABASES Mehmet Koyutürk M.S. in Computer Engineering Supervisor: Assoc. Prof. Dr. Cevdet Aykanat September, 2000 In very large distributed database systems, the data is declustered in order to exploit parallelism while processing a query. Declustering refers to allocat ing the data into multiple disks in such a way that the tuples belonging to a relation are distributed evenly across disks. There are many declustering strategies proposed in the literature, however these strategies are domain spe cific or have deficiencies. We propose a model that exactly fits the problem and show that iterative improvement schemes can capture detailed per-relation ba sis declustering objective. We provide a two phase iterative improvement based algorithm and appropriate gain functions for these algorithms. The experimen tal results show that the proposed algorithm provides a significant performance improvement compared to the state-of-the-art graph-partitioning based declus tering strategy. Key words: Distributed Databases, Declustering, Hypergraph Partitioning, Max-cut Graph Partitioning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÖZNİTELİK İZDÜŞÜMLERİNİN PARÇALANMASI İLE ÖRNEKLERE DAYALI REGRESYON ilhan Uysal Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Halil Altay Güvenir Ocak, 2000 Yüksek öznitelik sayılarına sahip verilerin regresyon çözümleri için örneklere dayalı yeni bir öğrenme metodu sunulmuştur. Örneklere dayalı bir yaklaşım olarak geleneksel K- Yakın Komşu (KNN) yöntemi hem sınıflandırma hem de regresyon problemleri için uygulanmıştır. KNN sınıflandırma işlemleri için iyi bir performans sergilerken, regresyon için benzer bir performansa sahip değildir. Biz literatürdeki bu boşluğu doldurmak üzere, tembel öğrenme yaparak yüksek başarı sağlayan örneklere dayalı yeni bir regresyon yöntemi olan, Öznitelik izdüşümlerinin Parçalanması ile Regresyon (RPFP) isimli yöntemi geliştirdik. RPFP makina öğrenmasi ve istatistik literatüründe yer alan MARS, kurallara dayalı regresyon ve regresyon ağacı öğrenen sistemler gibi önemli çalışkan al goritmalarda dahi bulunmayan bazı özelliklere ve hatta daha iyi performansa sahiptir. RPFP'nin bu özelliklerinden en önemli olanı verilerde eksik değerler olduğu durumlarda pek çok uygulama için diğer tüm çalışkan veya tembel yöntemlerden daha çok başarı sağlamasıdır. Günümüzde, çok sayıda alanları bulunan veri tabanlarını dikkate aldığımız zaman, böyle ortamlara sıklıkla rast lanır. RPFP veri seti içindeki tüm öznitelik değerlerinin doldurulmuş olmasını gerektirmediği için eksik olan değerleri doğal bir şekilde çözümler. Anahtar Sözcükler: Makina öğrenmesi, örneklere dayalı öğrenme, regresyon. iv","ABSTRACT INSTANCE-BASED REGRESSION BY PARTITIONING FEATURE PROJECTIONS İlhan Uysal M.S. in Computer Engineering Supervisor: Assoc. Prof. Halil Altay Güvenir January, 2000 A new instance-based learning method is presented for regression problems with high-dimensional data. As an instance-based approach, the conventional K-Nearest Neighbor (KNN) method has been applied to both classification and regression problems. Although KNN performs well for classification tasks, it does not perform similarly for regression problems. We have developed a new instance-based method, called Regression by Partitioning Feature Projections (RPFP), to fill the gap in the literature for a lazy method that achieves a higher accuracy for regression problems. We also present some additional properties and even better performance when compared to famous eager approaches of machine learning and statistics literature such as MARS, rule-based regression, and regression tree induction systems. The most important property of RPFP is that it performs much better than all other eager or lazy approaches on many domains that have missing values. If we consider databases today, where there are.generally large number of attributes, such sparse domains are very frequent. RPFP handles such missing values in a very natural way, since it does not require all the attribute values to be present in the data set. Keywords: Machine learning, instance-based learning, regression. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET Çekmeye Dayalı Mobil Ortamlarda Etkin Bir Yayım Programlama Algoritması K. Murat Karakaya Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos 2000 Haberleşme ve bilgisayar teknolojilerindeki gelişmeler sayesinde bugün mo- bil sistemler hayatın her alanında önemli bir araç olmuştur. Şimdilerde, bir çok insan dizüstü bilgisayar, kişisel sayısal yardımcı ve cep telefonu gibi taşınabilir araçlar kullanmaktadır. Bu tür mobil araçlar hızla gelişen haberleşme teknolo jisi tarafından desteklenmektedir. Hücresel haberleşme, kablosuz Yerel Alan Şebekeleri (YAŞ) ve Geniş Alan Şebekeleri (GAŞ) ile uydu hizmetleri günlük hayatta kullanılan uygulamalar için mevcuttur ve taşınabilir araçlar bilgi sağla yıcılara bu gibi kablosuz bağlantıları kullanarak ulaşabilmektedir. Böylece, bir kullanıcının ağ üzerinde sabit bir bağlantı sürdürmesine gerek kalmamıştır. Artık kullanıcılar hemen hemen sınırsız hareketlilik imkanına kavuşmuşlardır. Yeni ve değişik mobil altyapılar oluştukça, kullanıcılar bu ortamda çalışacak yeni uygulamalar talep etmektedirler. Ancak, telsiz kanallarının dar olan bant genişliği, taşınabilir araçların batarya sürelerinin göreceli olarak az olması ve kullanıcıların hareket halinde bulunmaları, kablolu ağlara göre veri iletişimini daha zor bir hale getirmektedir. Bu nedenle, bilgiyi çok geniş mobil kitlelere etkin olarak ulaştırabilecek mekanizmalar büyük önem arz etmektedir. Veri yayımı, mobil ortamlarda veri iletişimini sağlamakta en çok gelecek vaad eden yöntemlerden birisi olarak değerlendirilmektedir. Genel olarak, veri yayımında iki ana yaklaşım mevcuttur: itme ve çekme, itmeye dayalı veri yayımında, veri mobil kullanıcılara, kullanıcıların profiline veya üyelik bilgilerine göre yayımlanmaktadır. Diğer taraftan, çekmeye dayalı veri yayımında ise verilerin aktarımı kullanıcıların açıkça talep etmesi suretiyle gerçekleşmektedir. ivBu tezde, çekmeye dayalı ortamlarda, veri yayımının programlanması prob lemi üzerinde yoğunlaşılmış ve veri yayımını programlayan etkin bir algoritma geliştirilmiştir. Diğer bilinen algoritmalarla başarım sonuçları karşılaştırıldığın da, algoritmanın, şimdiye kadar önerilen en iyi algoritmalardan biri olduğuna kanaat getirilmiştir. Anahtar sözcükler: Mobil sistemler, mobil veritabanı, veri yayımı, çekmeye dayalı, programlama algoritmaları.","ABSTRACT An Efficient Broadcast Scheduling Algorithm for Pull-Based Mobile Environments K. Murat Karakaya M.S. in Computer Engineering Supervisor: Assoc. Prof. Özgür Ulusoy August 2000 Thanks to the advances in telecommunications and computers, today mo bile computing becomes a significant means in every pace of life. Many people are now carrying portable devices such as laptop computers, Personal Digital Assistants (PDAs), and cellular phones. These mobile computing devices are supported by rapidly expanding telecommunication technology. Cellular com munication, wireless LAN and WAN, and satellite services are available for daily life applications, and portable devices make use of these wireless connec tions to contact with the information providers. Thus, a user does not need to maintain a fixed connection in the network and may enjoy almost unrestricted user mobility. As the new and various mobile infrastructures emerge, users demand a new class of applications running in this environment. However, the narrow band width of the wireless communication channels, the relatively short active life of the power supplies of mobile units, and the mobility of clients make the problem of data retrieval more difficult than that in wired networks. There fore, mechanisms to efficiently transmit information to vast numbers of mobile users are of significant interest. Data broadcasting has been considered one of the most promising ways of data dissemination in mobile environments. There are two basic data broadcasting approaches available: push and pull. In push- based broadcasting approach, data is broadcast to mobile users according to users' profiles or subscriptions, whereas in pull-based broadcasting approach, transmission of data is initiated by the explicit request of users. In this thesis, we have focused on the problem of scheduling data items iiiIV to broadcast in a pull-based environment. We have developed an efficient broadcast scheduling algorithm, and comparing its performance against several well-known algorithms, we have observed that our algorithm appears to be one of the best algorithms proposed so far. Key words: Mobile computing, mobile database, data broadcast, broadcast delivery, pull-based, scheduling algorithm."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE İÇİN İSTAriSTİKSEL BİR BİLGİ ÇIKARIM SİSTEMİ Gökhan Tür Bilgisayar Mühendisliği. Doktora Tez Yöneticisi: Doç. Dr. Kemal Oflazer Ağustos. 2000 Bu tezde, istatistiksel dil işleme yöntemleri kullanarak Türkçe metinlerden bilgi çıkarımı üzerine yapılan bir dizi çalışmanın sonuçları sunulmaktadır. Sözcüksel (lexical) ve biçimbirimsel (morphological) bilgiler kullanan istatistiksel yöntemler aşağıdaki problemlerde başarıyla uyguLuımışttr:. Türkçe Metin Düzeltme sistemi. ASCII karakter kümesinde olmayan Türkçe karakterlerin ASCII karşılıklarıyla (ör: ""ı"" yerine 'T"") yazıldıkları metinleri düzeltme amacını taşır.. Sözcüklere Ayırma sistemi, içinde boşluk ya da noktalama işaretleri olmayan bir dizi karakter verildiğinde, bunları sözcüklerine ayırmaya çalışır.. Ünlüleri Yerin'--: Koyma sistemi, ünlü karakterleri olmayan bir metin ver ildiğinde bunları tekrar yerine koymayı amaçlar.. Cümlelere Ayırma sistemi, bir dizi sözcük verildiğinde bunları sözdizimsel cümlelere bölmeyi amaçlar.. Konulara Ayırma sistemi, bir metinde konuların değiştiği yerleri bulmayı amaçlar.. isim işaretleme sistemi, bir metindeki özel isimleri (insan, yer. ve. kurum isimleri) işaretlemeyi amaçlar. Türkçe Metin Düzeltme. Sözcüklere Ayırma, ve Ünlüleri Yerine Koyma gibi görece basit sistemler için sözcüksel bilginin yeterli olduğu görüldü. Ancak Cümlelere Ayırma, Konulara Ayırma, ve isim işaretleme gibi daha karmaşık vıvıı problemler için, ek olarak biçimbirimsel ve çevresel (contextual) bilgi de kul lanıldı. Cümlelere ayırma problemi için, sözcüklerin son çekim eki grubunu (in flectional group) istatistiksel modelleyip sözbirimsel modelle birleştirerek hata oranını 4.34%'e düşürmeyi başardık. İsim işaretleme sisteminde, sözbirimsel ve biçimbirimsel modellerin yanı sıra, çevresel ve işaret (tag) modellerini de kul landık ve 91.56% oranında doğruluğa ulaştık. Konulara ayırma problemi için ise, sözcüklerin köklerini kullanmak, asıl hallerini kullanmaktan daha iyi sonuçlar verdi, ve hata oram 10.90% oldu. Anahtar sözcükler: Bilgi Çıkarımı, İstatistiksel Doğal Dil işleme, Türkçe, İsim İşaretleme, Konulara Ayırma, Cümlelere Ayırma, Ünlüleri Yerine Koyma, Sözcüklere Ayırma, Türkçe Metin Düzeltme.","ABSTRACT A STATISTICAL INFORMATION EXTRACTION SYSTEM FOR TURKISH Gökhan T;ir Ph.D. in Computer Engineering Supervisor: As:ioc. Prof. Kemal Oflazer August, 2000 This thesis presents the results of a study on information extraction from un restricted Turkish text using statistical language processing methods. We have successfully applied statistical methods using both the lexical and morphological information to the following tasks: ?"".o » The Turkish Text Deasciifier task aims to convert the ASCII characters in a Turkish text, into the corresponding non-ASCII Turkish characters (i.e., ""ir, ''ö"". ""ç""'. ""ş"". ""ğ"". ""f. and their upper cases).. The Word Segmentation task aims to detect word boundaries, given we have a sequence of characters, without space or punctuation. e The Vowel Restoration task aims to restore the vowels of an input stream, whose vowels are deleted.. The Sentence Segmentation task aims to divide a stream of text or speech into grammatical sentences. Given a sequence of (written or spoken) words, the aim of sentence segmentation is to find the boundaries of the sentences.. The Topic Segmentation task aims to divide a stream of text or speech into topically homogeneous blocks. Given a sequence of (written or spoken) words, the aim of topic segmentation is to find the boundaries where topics change.. The Name Tagging task aims to mark the names (persons, locations, and organizations) in a text. For relatively simpler tasks, such as Turkish Text Deasciifier, Word Segmentation. and Vowel Restoration, only lexical information is enough, but in order to obtain ivbetter performance in more complex tasks, such as Sentence Segmentation, Topic Segmentation, and Name Tagging, we not only use lexical information, but also exploit morphological, and contextual information. For sentence segmentation, we have modeled the final inflectional groups of the words and combined it with the lexical model, and decreased the error rate to 4.34%. For name tagging, in ad dition to the lexical and morphological models, we have also employed contextual and tag models, and reached an F-measure of 91.56%. For topic segmentation, stems of the words (nouns) have been found to be more effective than using the surface forms of the words and we have achieved 10.90% segmentation error rate on our test set. Keywords: Information Extraction, Statistical Natural Language Processing, Turkish, Named Entity Extraction, Topic Segmentation. Sentence Segmentation. Vowel Restoration, Word Segmentation, Text Deasciification."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TRAFFIC DÜNYASININ C EYLEM DİLİNDE BİÇİMSELLEŞTİRİLMESİ Selim T. Erdoğan Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Varol Akman Temmuz, 2000 Eylemler ve sonuçları hakkında akıl yürütme Yapay Zekâ açısından bilgi gösterimi ve planlama ile bağlantıları olan önemli bir iştir. Geçmiş yıllarda eylemlerin gösterimi ve sonuçlarının çıkarımı için pek çok biçimsel yöntem geliştirilmiştir (örneğin, eylem dilleri, akar hesabı, durum hesabı). Ne var ki, şimdiye kadar modellenmiş bütün örnekler çok küçük boyutlarda olan ""oyuncak"" dünyalardır. Bahsedilen yöntemlerin gerçek uygulamalar için uygun olduklarını göstermek ve değişik yöntemlerin güçlü ve zayıf yanlarını saptayabilmek için basit ol mayan boyutlardaki senaryoların başarılı biçimselleştirmelerine gerek vardır. C eylem dili eylemlerin akarlar üzerindeki etkilerini göstermek için tasar lanmış bir mantık programlama dilidir. Bu tezde TRAFFIC senaryo dünyası - http://www.ida.liu.se/ext/etai/lmw/ adresindeki Logic Modelling Workshop tarafından tanımlanmış, orta büyüklükte bir dünya - C eylem dili kullanılarak biçimselleştirilmektedir. Örnek planlama problemleri eylem dünyalarında plan lamaya ve sorgulamaya yarayan Causal Calculator programını - bu program http://www.cs.utexas.edu.users/tag/cc/ adresinden temin edilebilir - kulla narak başarılı bir biçimde çözülmektedir. Biçimselleştirme, TRAFFIC dünyasını biçimselleştirmek için daha önce A. Henschel ve M. Thielscher'in akar hesabı kullanarak yaptıkları çalışmayla karşılaştırılmaktadır. Anahtar sözcükler: Eylem dilleri, planlama, nedensel akıl yürütme, bilgi gösterimi, C, Causal Calculator, TRAFFIC, Logic Modelling Workshop.","ABSTRACT FORMALIZATION OF THE TRAFFIC WORLD IN THE C ACTION LANGUAGE Selim T. Erdoğan MS in Computer Engineering Supervisor: Prof. Varol Akman July, 2000 Reasoning about actions and effects of actions is an important task in Arti ficial Intelligence, with connections to knowledge representation and planning. Many formal methods for representing actions and inferring their effects have been developed over the years (e.g. action languages, fluent calculus, situation calculus). However, the examples formalized so far have been ""toy"" domains of very small sizes. Successful formalizations of scenarios of nontrivial size are needed in order to show that these methods are suitable for real applications and to assess the strong and weak sides of different methods. The C action language is a logic programming language designed to represent the effects of actions on fluents. In this thesis we formalize the TRAFFIC scenario world - a domain of moderate size, specified at the Logic Modelling Workshop at http://www.ida.liu.se/ext/etai/lmw/ - using the C action language. Example planning problems using the formalization are successfully solved using the Causal Calculator - available at http://www.cs.utexas.edu/users/tag/cc/ -, a program for planning and querying in action domains. The formalization is contrasted with previous work on the TRAFFIC world, namely the formalization of A. Henschel and M. Thielscher using the fluent calculus. Keywords: Action languages, planning, causal reasoning, knowledge representa tion, C, Causal Calculator, TRAFFIC, Logic Modelling Workshop. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GÖRÜNTÜ-UZAYI PARALEL HACİM GÖRÜNTÜLEME İÇİN HİPERÇİZGE BÖLÜMLEMEYE DAYALI YENİDEN EŞLEME MODELİ Berkant Barla Cambazoğlu Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç Dr. Cevdet Aykanat Şubat, 2000 Işın izleme, üç boyutlu verilerin incelenmesinde kullanılan, popüler bir doğ rudan hacim görüntüleme tekniğidir. Bu teknik yüksek kalitede görüntüler üretebilecek kapasitede olmasına rağmen yavaşlığı birebir etkileşimli kulla nımını engellemektedir. Bu hız sınırlamasını aşmanın en önemli yolu pa ralelleştirmedir. Bu çalışmada, ışın izlemenin dağıtık bellekli mimarilerdeki görüntü-uzayı paralelleştirmesi araştırılmıştır. Görüntü-uzayı paralelleştirme deki en önemli konular yük dengeleme ve takip eden görüntüleme örneklerinde ortaya çıkan veri yeniden dağıtım yükünün en aza indirilmesidir. Hacim gö rüntülemedeki yük dengeleme, ekran iş yükünün doğru olarak tahminini gerek tirmektedir. Bu amaçla, üç değişik yük tahsis etme planı denenmiştir. Bu çalışmada kullanılan veriler düzensiz tetrahedral ızgaralardan oluştuğu için, verimlilik amacıyla bireysel veri hücreleri yerine veri grupları kullanılmıştır, ilk veri dağılımının etkilerini görmek için iki farklı veri grubu-işlemci dağılım planı kullanılmıştır. Çalışmanın en önemli katkısı yeniden eşleme problemi ne bir çözüm olarak önerilen hiperçizge bölümleme modelidir. Bu amaçla, var olan hiperçizge parçalama aracı PaToH değiştirilerek tek safhalı yeniden eşleme aracı olarak kullanılmıştır. Model Parsytec CC sisteminde denenmiş ve tatmin edici sonuçlar elde edilmiştir. Önerilen yöntem iki safhalı kesikli bölümleme modeline göre, daha az ön hazırlık yükü yaratmaktadır. Kıyaslanabilir yük dengesizliklerinde, önerilen hiperçizge modeli kesikli bölümleme modelinden ortalama %25 daha az toplam iletişim hacmi gerektirmektedir. Anahtar Kelimeler: görüntü-uzayı paralelleştirme, ışın izleme, düzensiz ız garalar, iş yükü tahsisi, hiperçizge bölümleme, yük dengeleme, yeniden eşleme. iv","ABSTRACT A HYPERGRAPH-PARTITIONING BASED REMAPPING MODEL FOR IMAGE-SPACE PARALLEL VOLUME RENDERING Berkant Barla Cambazoğlu M.S. in Computer Engineering Supervisor: Assoc. Prof. Cevdet Aykanat February, 2000 Ray-casting is a popular direct volume rendering technique, used to explore the content of 3D data. Although this technique is capable of producing high quality visualizations, its slowness prevents the interactive use. The major method to overcome this speed limitation is parallelization. In this work, we investigate the image-space parallelization of ray-casting for distributed mem ory architectures. The most important issues in image-space parallelization are load balancing and minimization of the data redistribution overhead introduced at successive visualization instances. Load balancing in volume rendering re quires the estimation of screen work load correctly. For this purpose, we tested three different load assignment schemes. Since the data used in this work is made up of unstructured tetrahedral grids, clusters of data were used instead of individual cells, for efficiency purposes. Two different cluster-processor dis tribution schemes are employed to see the effects of initial data distribution. The major contribution of the thesis comes at the hypergraph partitioning model proposed as a solution to the remapping problem. For this purpose, ex isting hypergraph partitioning tool PaToH is modified and used as a one-phase remapping tool. The model is tested on a Parsytec CC system and satisfactory results are obtained. Compared to the two-phase jagged partitioning model, our work incurs less preprocessing overhead. At comparable load imbalance values, our hypergraph partitioning model requires 25% less total volume of communication than jagged partitioning on the average. Keywords: image-space parallelization, ray-casting, unstructured grids, work load assignment, hypergraph partitioning, load balancing, remapping. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SONDAN EKLEMELİ DİLLERİN İSTATİSTİKSEL MODELLENMESİ Dilek Z. Hakkani-Tür Bilgisayar Mühendisliği, Doktora Tez Yöneticisi: Doç. Dr. Kemal Oflazer Ağustos, 2000 Bilgisayar donanımmdaki yeni gelişmeler ve çok büyük derlemlerin varlığı is tatistiksel tekniklerin doğal dil işlemeye uygulanmasını mümkün ve çok çekici bir araştırma alanı yapmıştır. Bu tekniklerin ingilizce ve benzeri dillerde cümle çözümleme (parsing), kelime anlamı tekleştirme (word sense disambiguation), sözcük sınıfı işaretleme (POS tagging) ve konuşma tanımaya uygulanmasıyla oldukça iyi sonuçlar elde edilmiştir. Ancak, Türkçe gibi, ingilizce ve benzeri diller den bir takım farklı özellikleri olan diller genellikle bu açıdan incelenmemişlerdir. Türkçe'nin istatistiksel modellenmesi ilginç bir problemdir. Verilen bir kökten az sayıda kelime üretilebilen ingilizce ve benzeri dillerin aksine Türkçe ve Fince gibi üretken eklemeli biçimbirimi olan dillerde, verilen bir kökten binlerce, hatta milyonlarca, yeni kelime üretmek mümkündür. Bu dil modelleme açısından çok ciddi bir veri yetersizliği problemine sebep olur. Bu doktora tezinde, Türkçe için istatistiksel dil modelleme tekniklerinin geliştirilmesi ve uygulanması ve bu tekniklerin biçimbirimsel tekleştirme, yazım hatalarının düzeltilmesi ve konuşma tanıma için aday (n-best) listesini yeniden değerlendirme gibi temel doğal dil ve konuşma işleme uygulamalarında denenmesi anlatılmaktadır. Bütün bu uygulamalarda veri yetersizliği probleminin etkisini azaltmak için kelimeden daha küçük birimler kullanıldı Biçimbirimsel tekleştirme için, çekim eki grupları (inflectional groups) modelleme birimi olarak kullanılarak n-birimli dil modelleri (n-gram language models) ve maksimum düzensizlik (max imum entropy) modelleri geliştirildi. Aldığımız sonuçlar, karmaşık biçimbirimsel yapıya sahip dilleri modellemek için sözcükten daha küçük birimler kullanmanın gerçekten de çok faydalı olduğunu gösterdi ve n-birimli dil modelleme yöntemi, maksimum düzensizlik yönteminden daha iyi sonuçlar verdi. Aday listesini yeniden değerlendirmek ve yazım hatalarının düzeltilmesi içinse biçimbirimsel tekleştirme için geliştirilen bu modeller ve bunların önek-sonek (prefix-suffix) vivıı modelleri gibi yakınsamaları kullandıldı. Önek-sonek modelleri, aday listesinin yeniden değerlendirilmesinde çok iyi sonuçlar verdi, ancak yazım hatalarının düzeltilmesinde doğruluk açısından sözcük tabanlı modellerden daha iyi sonuç vermedi. Anahtar sözcükler: Doğal Dil İşleme, İstatistiksel Dil Modelleme, Biçimbirimsel Tekleştirme, Konuşma Tanıma, Yazım Hatalarının Düzeltilmesi, n-birimli Dil Modelleri, Maksimum Düzensizlik Modelleri.","ABSTRACT STATISTICAL MODELING OF AGGLUTINATIVE LANGUAGES Dilek Z. Hakkani-Tür Ph.D. in Computer Engineering Supervisor: Assoc. Prof. Kemal Oflazer August, 2000 Recent advances in computer hardware and availability of very large corpora have made the application of statistical techniques to natural language process ing a possible, and a very appealing research area. Many good results have been obtained by applying these techniques to English (and similar languages) in pars ing, word sense disambiguation, part-of-speech tagging, and speech recognition. However, languages like Turkish, which have a number of characteristics that dif fer from English have mainly been left unstudied. Turkish presents an interesting problem for statistical modeling. In contrast to languages like English, for which there is a very small number of possible word forms with a given root word, for languages like Turkish or Finnish with very productive agglutinative morphology, it is possible to produce thousands of forms for a given root word. This causes a serious data sparseness problem for language modeling. This Ph.D. thesis presents the results of research and development of statisti cal language modeling techniques for Turkish, and tests such techniques on basic applications of natural language and speech processing like morphological dis ambiguation, spelling correction, and n-best list rescoring for speech recognition. For all tasks, the use of units smaller than a word for language modeling were tested in order to reduce the impact of data sparsity problem. For morphological disambiguation, we examined n-gram language models and maximum entropy models using inflectional groups as modeling units. Our results indicate that using smaller units is useful for modeling languages with complex morphology and n-gram language models perform better than maximum entropy models. For n-best list rescoring and spelling correction, the n-gram language models that were developed for morphological disambiguation, and their approximations, via prefix-suffix models were used. The prefix-suffix models performed very well for n-best list rescoring, but for spelling correction, they could not beat word-based iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NEREDEYSE TAMAMEN BÖLÜNEBİLİR MARKOV ZİNCİRLERİ ÜZERİNDE RASSAL KARŞILAŞTIRMA Denizhan N. Alparslan Bilgisayar Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Tuğrul Dayar Temmuz, 2000 Bu tezde neredeyse tamamen bölünebilir Markov zincirlerinin değişmez durum olasılık dağılımları için tek tek sınırlar veren bir sınırlandırma algoritmasının gelişmiş biçimi anlatılmaktadır. Sunulan bu iki seviyeli algoritma, birleştirmeye ve güçlü rassal (st) sıralama ile rassal karşılaştırmaya dayalıdır. Sonucun kesinliğinin arttırabilmesi için durumların yeniden sıralanması ve st bağıntısına göre üstten- ve alttan-smırlayan olasılık dağılımlarından tek tek sınırların elde edilmesini sağlayan daha iyi bir algoritma ortaya konmuştur. Sınırlandırma al goritmasının indirgeme açısından eksiksiz bir analizi yapılmıştır. Bu algoritma seyrek saklama düzeninde programlanmış ve bu programlamanın ayrıntıları ver ilmiştir. Farklı zamanlı aktarma biçimi üzerine kurulmuş olan kablosuz bir ağ sisteminden elde edilen sayısal sonuçlar bu algoritmanın bazı durumlarda ver ilen sistemin başarım değerleri üzerinde sınırlar bulmada yararlı olabileceğini göstermektedir. Başarım değerleri üzerinde verilen sınırların daha iyi olabilmesi için algoritmada yapılması gereken iyileştirme en sonda belirtilmiştir. Anahtar sözcükler: Markov zincirleri, neredeyse tamamen bölünebilirlik, rassal karşılaştırma, güçlü rassal sıralama, sıralama, birleştirme.","ABSTRACT STOCHASTIC COMPARISON ON NEARLY COMPLETELY DECOMPOSABLE MARKOV CHAINS Denizhan N. Alparslan M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Tuğrul Dayar July, 2000 This thesis presents an improved version of a componentwise bounding algorithm for the steady state probability vector of nearly completely decomposable Markov chains. The given two-level algorithm uses aggregation and stochastic comparison with the strong stochastic (st) order. In order to improve accuracy, it employs reordering of states and a better componentwise probability bounding algorithm given st upper- and lower-bounding probability vectors. A thorough analysis of the algorithm from the point of view of irreducibility is provided. The bounding algorithm is implemented in sparse storage and its implementation details are given. Numerical results on an application of wireless Asynchronous Transfer Mode network show that there are cases in which the given algorithm proves to be useful in computing bounds on the performance measures of the system. An improvement in the algorithm that must be considered to obtain better bounds on performance measures is also presented at the end. Keywords: Markov chains, near complete decomposability, stochastic comparison, st-order, reorderings, aggregation. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET TURING TESTİ VE KONUŞMA Ayşe Pmar Saygın Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. Ilyas Çiçekli Temmuz, 1999 Turing Testi Yapay Zeka, Dil Felsefesi ve Bilişsel Bilimler alanlarında çok tartışılan konulardan biridir. 50 yıl önce, makinelerin düşünüp düşünmediğini ölçmek için kullanılacak bir test olarak öne sürülmüştür. Bünyesinde, hem felsefe hem de bilgisayar bilimi açısından önemli olan kavramları barındırır. Ayrıca, kendine has özelliklerinden dolayı, disiplinler arası bir yaklaşım gerek tirmektedir. Turing Testi 'ne göre bir bilgisayara zeki diyebilmemiz için, onun insan konuşma davranışlarını gerçek bir insandan ayırdedilemeyecek kadar iyi taklit edebilmesi gerekir. Buradan da görülebileceği gibi, konuşma, Turing Testi'nin çok önemli bir parçasıdır. Ama şaşırtıcı bir şekilde, testle ilgili önceki yorumlar konuya bu açıdan yaklaşmamaktadır. Bu tez, öncelikle Tur ing Testi'nin geniş ve derin bir incelemesini sunmaktadır. Felsefi tartışmalara, pratik gelişmelere, ve konunun diğer bilimlerde yarattığı yankılara yer ve rilmiştir. Ayrıca, Turing Testi bir çeşit konuşma olarak ele alınmaktadır. Halen varolan konuşma teorileri ile bilgisayar-insan iletişimi arasındaki ilişki ince lenmiştir. Özellikle Grice'm işbirliği ilkesi ve konuşma ilkeleri üzerine yoğunla- şılmıştır. Turing Testi'ni bir çeşit konuşma olarak, ve bilgisayarları dil kul lanıcıları olarak görmek, hem Yapay Zeka'ya, hem de genel olarak iletişime bakış açımız üzerinde büyük etkiye sahiptir. Anahtar kelimeler: Turing Testi, Yapay Zeka, Konuşma İlkeleri, İşbirliği İlkesi, Edimbilim, Doğal Dil Konuşma Sistemleri, Otomatik Gevezeler, Konuşma İncelemesi, Bilişsel Bilimler, Dil Felsefesi, Bilgisayarlı Dilbilim","Ill ABSTRACT TURING TEST AND CONVERSATION Ayşe Pınar Saygın M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. Ilyas Çiçekli July, 1999 The Turing Test is one of the most disputed topics in Artificial Intelligence, Philosophy of Mind and Cognitive Science. It has been proposed 50 years ago, as a method to determine whether machines can think or not. It embodies important philosophical issues, as well as computational ones. Moreover, be cause of its characteristics, it requires interdisciplinary attention. The Turing Test posits that, to be granted intelligence, a computer should imitate human conversational behavior so well that it should be indistinguishable from a real human being. From this, it follows that conversation is a crucial concept in its study. Surprisingly, focusing on conversation in relation to the Turing Test has not been a prevailing approach in previous research. This thesis first provides a thorough and deep review of the 50 years of the Turing Test. Philosophical arguments, computational concerns, and repercussions in other disciplines are all discussed. Furthermore, this thesis studies the Turing Test as a special kind of conversation. In doing so, the relationship between existing theories of conversation and human-computer communication is explored. In particu lar, Grice's cooperative principle and conversational maxims are concentrated on. Viewing the Turing Test as conversation and computers as language users have significant effects on the way we look at Artificial Intelligence, and on communication in general. Key words: Turing Test, Artificial Intelligence, Conversational maxims, Cooperative Principle, Pragmatics, Natural Language Conversation Systems, Chatterbots, Conversation Analysis, Cognitive Science, Philosophy of Lan guage, Computational Linguistics"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DAĞITIK BELLEKLİ BİLGİSAYARLARDA SIRADÜZENSEL IŞIMA ALGORİTMALARININ PARALELLEŞTİRİLMESİ Ahmet Reşat Şireli Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Attila Gürsoy Ocak 1999 Işığın verilen ortam içerisinde dağılımını hesaplamak bilgisayar destekli gerçeğe uygun görüntü üretiminde önemli bir problemdir. Işıma metodu, bu aşırı bir miktarda hesap ve hafıza gerektiren problemin çözümü için önerilmiştir. Sıradüzensel ışıma metodu, bu işlemsel gereksinimleri dikkatli hata analizi sonucu azaltan nihai yaklaşımlardan biridir. Fikrini N-gövde probleminin çözüm metodlarmdan almıştır. Sıradüzensel yaklaşım işlemlerin miktarını büyük ölçüde azaltmış olmasına rağmen, zaman bakımından tatminkar sonuçlar hala elde edilememektedir. Paralellikten faydalanmak işlemsel sürenin daha da azaltılması için pratik bir metoddur. Bu tezde, dağıtık bellekli bilgisayarlar için bir paralel sıradüzensel ışıma algoritması tasarladık ve uyguladık. Aşırı düzensiz işlemsel yapısı yüzünden sıradüzensel ışıma algoritmaları dağıtık bellekli bilgisayarlar üzerinde paralelleştirilmesi kolay olmamaktadır. Algoritmanın dinamik olarak değişen işlemsel örüntüleri birçok yük dengesizliklerine sebep olmaktadır. Bu yüzden paralel sıradüzensel ışıma algoritmamız için bir dinamik yük dengeleme tekniği de geliştirdik. Anahtar sözcükler: Gerçeğe Uygun Görüntü Üretimi, Paralel Sıradüzensel Işıma, Dinamik Yük Dengeleme. iv","ABSTRACT PARALLELIZATION OF HIERARCHICAL RADIOSITY ALGORITHMS ON DISTRIBUTED MEMORY COMPUTERS Ahmet Reşat Şireli M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Attila Gürsoy January 1999 Computing distribution of light in a given environment is an important prob lem in computer-aided photo-realistic image generation. Radiosity method has been proposed to address this problem which requires an enormous amount of calculation and memory. Hierarchical radiosity method is a recent approach that reduces these computational requirements by careful error analysis. It has its idea from the solution methods of N-body problems. Although hier archical approach has greatly reduced the amount of calculations, satisfactory results still cannot be obtained in terms of processing time. Exploiting paral lelism is a practical way to reduce the computation time further. In this thesis, we have designed and implemented a parallel hierarchical radiosity algorithm for distributed memory computers. Due to its highly irregular computational structure, hierarchical radiosity algorithms do not yield easily to paralleliza- tion on distributed memory machines. Dynamically changing computational patterns of the algorithm cause severe load imbalances. Therefore, we have developed a dynamic load balancing technique for the parallel hierarchical ra diosity calculation. Keywords: Realistic Image Generation, Parallel Hierarchical Radiosity, Dy namic Load Balancing. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET SEYREK DİKDÖRTGENSEL MATRİSLERİN AATx'W PARALEL İŞLEMCİLERDE HESAPLANABİLMESİ İÇİN PARÇALANMASI Bora Uçar Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doc. Dr. Cevdet Aykanat Eylül, 1999 Birçok bilimsel uygulama, bir seyrek dikdörtgensel matrisin bir vektör ve de aynı matrisin transpozunun bir başka vektör ile çarpılmasını içermektedir. Şimdiye kadar kullanılan çizge ve hiperçizge parçalanması algoritmalarında toplam haberleşme hacmi azaltılırken işlemciler arasındaki işlemsel denge, ma trisin tek boyutlu ayrıştırılması ile yakalanmaya çalışılmıştır. Bu tezde, toplam haberleşme sayısı ve hacmi iki yaklaşımla azaltılmaya çalışılmıştır. Genel haberleşme yaklaşımındaki birleşik haberleşme hacminin azaltılması problemi, matrislerin tek sınırlı çarpraz bloklar haline getirilmesi problemine dönüştü rülmüştür. Bu yaklaşımdaki birleşik ve toplam haberleşme sayısı paralel iş lemcilerin bağlantısına bağlı olup değiştirilememektedir. Kişiselleştirilmiş ha berleşme yaklaşımındaki toplam haberleşme sayısı ve hacmi iki aşamada azal tılmıştır. Birinci aşamada, toplam haberleşme hacmi azaltılırken işlemsel denge sağlanılmıştır. ikinci aşamada, önerilen haberleşme hiperçizgesi yardımı ile toplam haberleşme sayısı azaltılırken işlemciler arasında haberleşme işi dengesi sağlanmaya çalışılmıştır. Sunulan yöntemler birçok matris üzerinde sınanmış ve iyi yönde sonuçlar elde edilmiştir. Anahtar kelimeler: Seyrek dikdörtgensel matrisler, hesap hiperçizgesi, haber leşme hiperçizgesi, hiperçizge parçalama.","Ill ABSTRACT PARTITIONING SPARSE RECTANGULAR MATRICES FOR PARALLEL COMPUTING OF AATx Bora Uçar M.S. in Computer Engineering and Information Science Supervisor: Asoc. Prof. Dr. Cevdet Aykanat Spetember, 1999 Many scientific applications involve repeated sparse matrix- vector and matrix- transpose- vector product computations. Graph and hypergraph partitioning based approaches used in the literature aim at minimizing the total commu nication volume while maintaining computational load balance through one dimensional partitioning of sparse matrices. In this thesis, we consider two approaches which consider minimizing both the total message count and com munication volume while maintaining balance on the communication loads of the processors. Two communication schemes are investigated for the fold and expand operations needed in the parallel algorithm. For the global communica tion scheme, we show that the problem of minimizing concurrent communica tion volume can be formulated as the problem of permuting the sparse matrix into a singly-bordered block-diagonal form, where the total and concurrent message count is determined by the interconnection topology. For the person alized communication scheme, a two stage approach is proposed. In the first stage, the total communication volume is minimized while maintaining balance on the computational loads of the processors. In the second stage, a novel com munication hypergraph model is proposed which enables the minimization of the total message count while maintaining balance on the communication loads of the processors through hypergraph-partitioning-like methods. The solution methods are tested on various matrices and results, which are quite attractive in terms of solution quality and running times, are obtained. Key words: Sparse Rectangular Matrices, Computational Hypergraph Model, Communication Hypergraph Model, Hypergraph Partitioning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Internet ve Web'in ve bu global ağdaki kaynakların popüleritesi gün geçtikçe daha da artmaktadır. Günümüzde Web eğlenceden iletişime, reklamdan araştırmalara kadar hemen hemen her konuda kullanılmaktadır. Araştırmacılar Web'i, Web'deki bilgiler daha yeni olduğu için, Web bir bilgisayar kadar yakında olduğu için ve Web çok büyük olduğu için tercih ederler. Fakat Web yapısal bir indeksleme ve/veya kategorizasyon mekanizmasından yoksun olduğu için Web'deki bilgi kaynaklarını arayıp bulmak zordur. insanlar Web'deki bilgiye şu dört yoldan birini veya birkaçını kullanarak ulaşırlar: arama motorları, konu dizinleri, kişisel bookmarkları, veya o bilginin nerede olduğunu bilen biriler ine sorarak. Arama motorları ve konu dizinleri Web'i indekslemeye çalışırlar, ama sorunsuz değildirler: çok fazla sayıda kaynağı sonuç olarak gönderirler ve bunların kalitesi belirli değildir. Kişisel bookmarklar da yeterince yardımcı değildir: çünkü yapısal olmayıp fazla kişiseldirler. Dağıtık indeksleme, Web'deki problemler için olası bir çözümdür. Bu konuda araştırmalar devam etmektedir, ancak sonuç sistemler henüz halka açık değildirler. Book mark yöneticileri aracılığı ile bookmarklarm paylaşımı kolay kullanılabilir olması, kaynaklarla ilgili yeterli bilgiyi saklayabilmesi ve bunları tarama imkânı verebilmesi şartıyla başka olası bir çözümdür. Bizler, hem bookmark paylaşım yöneticisi hem de arama aleti olan Dağıtık Bookmark Paylaşımını (Distribute Bookmark Sharing, DBS) geliştirdik. Araştırmamızda, bir DBS istemcisi ve bir DBS sunucusu olan DBS prototipini geliştirdik. Kaynak Tanımlama Şablonları ve Kaynak Tanımlama Yüklemleri kullanarak bookmark ek leme mekanizması sağladık. Kaynak Tanımlama Yüklemlerini sorgulamada kullandık. DBS sunucusunu aynı kaynağa ait farklı tanımlamaları kabul edecek şekilde geliştirdik. Kay nakların yükselmesi ve alçalması için geri bildirim mekanizmasını sağladık. DBS sistemi, kaynak tanımlama şablonları ve kaynak tanımlama yüklemlerini yardımı ile ekleme ve sorgu ları daha etkili hale getirdi. Anahtar kelimeler, dağıtık bookmark paylaşımı, bookmark paylaşımı, dağıtık indek sleme, bookmark yöneticileri, bookmark organizatörleri, kaynak tanımlama şablonları, kay nak tanımlama yüklemleri","The popularity of the Internet and the Web, along with the resources on this distributed global network is increasing exponentially. Today, the Web is used for almost every purpose from entertainment to communication, from advertisement to research. Researchers use the Web because the information is fresh, it is close to you as much as your computer, and it is huge. But it lacks structured indexing and/or categorization mechanisms that makes the search process difficult. People use one or more of four methods to access the information on the Web: search engines, directory services, personal bookmarks, or asking someone who knows where the information is. Search engines and global directory services try to index the Web, but they have their own problems such as too many returned results, low quality, etc. Peoples' bookmarks do not help as much as they should, because they are usually unstructured, being personal. Distributed indexing is one possible solution to this existing problem. Research is going on in distributed indexing, but the resulting systems are not usable by the public yet. Bookmark sharing through bookmark managers is another possible solution provided that the bookmark manager in use can be accessed easily, can store enough information about the resource, and can search its repository effectively. Finally, we propose a new approach called distributed bookmark sharing (DBS), which is both a bookmark manager and a search tool. It is like a Virtual Web on the top of the existing Web. In our research we built the prototype parts of a DBS, a stand-alone DBS client and a DBS server. We provide mechanisms to insert well-described bookmarks using Resource Description Templates and Resource Description Predicates. Resource Description Predi cates are also used in queries. The DBS server can handle different descriptions of the same resource. The system also provides a simple feedback mechanism to promote and demote the resources. The DBS system, with the help of description templates and description predicates, makes insertions and queries easy and efficient. Keywords: distributed bookmark sharing, bookmark sharing, distributed indexing, book mark managers, bookmark organizers, resource description templates, resource description predicates"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ReMemex: BİLGİ İNŞAASI ve DÜZENLEMESİ İÇİN BİR YAZILIM ARACI Özden Emek Erarslan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. David Davenport Kasım, 1999 Bilgi Çağı insanları, bilginin inşaasmda; toplamada, çözümlemede, sentezleme- de ve düzenlemede vasıflı olmalıdır. Bu vasfa sahip olmaya yönelik talepler eğitimsel teknoloji araştırmalarını yeni teoriler ve uygulamalar bulmaya yönlen dirmektedir. Gelişen bir öğrenme teorisi olarak Konstrüktivizm (construc tivism), yeni bakış açıları önermekte ve bunun yakın gelecekte eğitim üzerinde büyük etkisi olması beklenmektedir. Bugün, artan çabalara rağmen, kon- strüktivist fikirlerin eğitim yazılımlarına en iyi ne şekilde uygulanacağına karar vermek ve etkisinin olup olmadığını ispatlamak için yeterince araştırmaya ihtiyaç hala vardır. Bu tez, bilgi işçilerinin bilgiyi toplama ve düzenlemesine yardım eden ve konstrüktivist öğrenme teorisinin eğitimsel yazılımlara uygulanması konusunda araştırmalar için bir taban teşkil eden ReMemex sistemini, bir yazılım aracını sunmaktadır. ReMemex, öğrenen konumundaki kişilerin kavram lar ve/ veya nesneler arasındaki ilişkileri görselleştirmek için bilgi peyzajı oluştur dukları bilgi inşaa ve düzenleme ortamları için bir çatıdır. Diğer uygula malardan aktarılan kütük ve verilerin düğümler şeklinde gösterilmesi, detay ların gruplama yoluyla soyutlanması, haritalar ve düğümler için çoklu-görünüş desteği ve biçem desteği gibi özelliklerin tamamı ReMemex sistemini geleneksel araçlardan daha güçlü kılmaktadır. ReMemex sisteminin temel gerçekleşti rimi Java kullanılarak, esnek ve geliştirmeye açık, Nesneye Yönelik Uygulama. Pro gramı Arayüzü (API) olarak yapılmıştır. Anahtar sözcükler: Konstrüktivizm, Bilgi Düzenleme, Eğitimsel Yazılım, An lamsal Ağlar, Bilişsel Araçlar iv","ABSTRACT ReMemex: A SOFTWARE TOOL FOR KNOWLEDGE CONSTRUCTION and ORGANIZATION Özden Emek Erarslan M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. David Davenport November, 1999 Citizens of the Knowledge Age have to be skilled in the construction of mean ingful knowledge; in the collecting, analyzing, synthesizing, and organizing of information. These demands are driving educational technology research to find new theory and practices. Constructivism, an emerging theory of learning, offers new perspectives and is expected to have a major influence on education in the near future. Yet today, despite increasing efforts, considerable research is still needed to determine how best to apply constructivist ideas to educational software and to prove or disprove its effectiveness. This thesis presents the Re Memex system, a software tool, which both aids knowledge workers to collect and organize information, and provides a basis for research into the applica tion of constructivist learning theory in educational software. ReMemex is a framework for a knowledge construction and organization environment, where learners create information landscapes to visualize the inter-relationships be tween the concepts and/or objects the domain is composed of. Features such as importing files and data from other applications as nodes, abstraction of the details by grouping, multiple view support for maps and nodes, and style support make ReMemex more powerful than traditional tools. The base imple mentation of ReMemex has been developed in Java, as a flexible and extensible Object Oriented API. Keywords: Constructivism, Knowledge Organization, Educational Software, Cognitive Tools, Semantic Networks m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SEYREK MATRİS BÖLÜMLEME VE YENİDEN-DÜZENLEME İÇİN HİPERÇİZGE MODELLERİ Ümit V. Çatalyürek Bilgisayar ve Enformatik Mühendisliği, Doktora Tez Yöneticisi: Doç. Dr. Cevdet Aykanat Kasım, 1999 Çizgeler, koşut seyrek-matris vektör çarpımında (SpMxV) seyrek matrislerin ayrıştırılması ve az doluluk faktorizasyonu için kullanılan seyrek matrislerin yeniden düzenlenmesini içeren çeşitli bilimsel uygulamalarda seyrek matris lerin gösterimi için yaygın olarak kullanılmaktadır. Ancak seyrek matris lerin standart çizge-bölümlemeye dayalı tek-boyutlu ayrıştırılması koşut Sp MxV işlemi için gerekli iletişim hacmini yansıtamamaktadır. Çizge modelinin tek-boyutlu ayrıştırmadaki bu önemli eksikliğine karşılık benzer bir eksiği ol mayan iki bilişimsel hiperçizge modeli sunuyoruz. Önerdiğimiz modeller tek- boyutlu ayrıştırma problemim iyi bilinen hiperçizge bölümleme problemine in dirgemektedir. Literatürde koşut SpMxV hesaplamaları için iletişim gereksin imini doğrudan azaltan iki-boyutlu ayrıştırma yöntemi yoktur. İletişim hacmi gereksinimini azaltmak için seyrek matrislerin iki-boyutlu ayrıştırmasını sağlayan üç yeni hiperçizge modeli tanıtıyoruz. Bunlardan ilki koşut SpMxV işlemindeki seyrek matrislerin fine-grain iki-boyutlu ayrıştırması için önerildi. İki-boyutlu ayrıştırmada kullanılan ikinci hiperçizge modeli seyrek matrislerin çentikli-benzeri ayrıştırmalarının üretilmesi için önerildi. Literatürde dama tahtası tabanlı ayrıştırmaya dayanan koşut matris vektör çarpımı algoritmaları yaygınca bu lunmaktadır. Bununla birlikte bu çalışmalarda sadece yük dengeleme prob lemine işaret edilmiştir. Biz bu çalışmada iki-boyutlu ayrıştırmanın üçüncü modeli olarak dama tahtası tabanlı ayrıştırmada işlemciler arası yük dengesini korurken iletişim hacmini de azaltmayı hedefleyen yeni bir hiperçizge mod eli öneriyoruz. Önerdiğimiz model ayrıştırma problemini çoklu-kısıt hiperçizge bölümleme problemine indirgemektedir. Çoklu-kısıt bölümleme fikri çizge bölümleme alanında yakın zamanda popüler olmuştur. Biz de dama tahtası bölümleme problemini çözmek için bu çoklu-kısıt bölümleme fikrini hiperçizge parçalama yöntemine uyguladık. Düğüm ayırıcıları ile çizge bölümleme yöntemi vı","ABSTRACT HYPERGRAPH MODELS FOR SPARSE MATRIX PARTITIONING AND REORDERING Ümit V. Çatalyürek Ph.D. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat November, 1999 Graphs have been widely used to represent sparse matrices for various scientific applications including one-dimensional (ID) decomposition of sparse matrices for parallel sparse-matrix vector multiplication (SpMxV) and sparse matrix re ordering for low fill factorization. The standard graph-partitioning based ID de composition of sparse matrices does not reflect the actual communication volume requirement for parallel SpMxV. We propose two computational hypergraph mod els which avoid this crucial deficiency of the graph model on ID decomposition. The proposed models reduce the ID decomposition problem to the well-known hypergraph partitioning problem. In the literature, there is a lack of 2D decom position heuristic which directly minimizes the communication requirements for parallel SpMxV computations. Three novel hypergraph models are introduced for 2D decomposition of sparse matrices for minimizing the communication vol ume requirement. The first hypergraph model is proposed for fine-grain 2D de composition of the sparse matrices for parallel SpMxV. The second hypergraph model for 2D decomposition is proposed to produce jagged-like decomposition of the sparse matrix. The checkerboard decomposition based parallel matrix-vector multiplication algorithms are widely encountered in the literature. However, only the load balancing problem is addressed in those works. Here, we propose a new hypergraph model which aims the minimization of communication volume while maintaining the load balance among the processors for checkerboard decomposi tion, as the third model for 2D decomposition. The proposed model reduces the decomposition problem to the multi-constraint hypergraph partitioning problem. The notion of multi-constraint partitioning has recently become popular in graph partitioning. We applied the multi-constraint partitioning to the hypergraph par titioning problem for solving checkerboard partitioning. Graph partitioning by vertex separator (GPVS) is widely used for nested dissection based low fill or dering of sparse matrices for direct solution of linear systems. In this work, we IValso show that the GPVS problem can be formulated as hypergraph partition ing. We exploit this finding to develop a novel hypergraph partitioning-based nested dissection ordering. The recently proposed successful multilevel frame work is exploited to develop a multilevel hypergraph partitioning tool PaToH for the experimental verification of our proposed hypergraph models. Experimental results on a wide range of realistic sparse test matrices confirm the validity of the proposed hypergraph models. In terms of communication volume, the pro posed hypergraph models produce 30% and 59% better decompositions than the graph model in ID and 2D decompositions of sparse matrices for parallel SpMxV computations, respectively. The proposed hypergraph partitioning-based nested dissection produces 25% to 45% better orderings than the widely used multiple mimimum degree ordering in the ordering of various test matrices arising from different applications. Keywords: Sparse matrices, parallel matrix-vector multiplication, parallel pro cessing, matrix decomposition, computational graph model, graph partitioning, computational hypergraph model, hypergraph partitioning, fill reducing ordering, nested dissection."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE İÇİN GENİŞ SÖZCÜK DAĞ ARCIKLI KONUŞMA TANIMA SİSTEMİ Cemal Yılmaz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticileri: Doç. Dr. Kemal Oflazer ve Prof. Dr. A. Enis Çetin Ağustos, 1999 Bu tezde Türkçe için konuşmacıya bağımlı, geniş sözcük hazneli konuşma tanıma sistemi sunulmaktadır. Bu sistemde sözcükler üçlü-fon temelli Saklı Markov Modeller ile model lenir. Bu üçlü-fonlar sistemdeki en küçük birimlerdir. Her sözcük için bu üçlü-fonların modellerinin yardımı ile sözcük modeli oluşturulur. Öğrenme safhasında, bu model bir bütün olarak eğitildikten sonra herbir üçlü-fon modeli ayrı olarak saklanır. Tanıma safhasında ise bir sözcük için gerekli olan üçlü-fon modelleri sırasıyla birbirlerine eklenerek o sözcük için gerekli model oluşturulur. Böylece öğrenme safhasında herhangi bir üçlü-fon için oluşturulan model tanıma safhasında birden fazla sözcüğün tanınması için kullanılabilir. Diğer bir deyişle, öğrenme safhasında sisteme öğretilen sözcüklerden daha fazla sözcük tanıma safhasında tanınabilir. Bu tezde ayrıca Türkçe için ""trie"" yapılı sözlük modeli geliştirilmiştir. Sözlük modelinde kullanılmak üzere ""ütrie"" yapısının her düzeyinde konuşma işaretinin en uygun kısmını kullanan bir arama stratejisi geliştirilmiştir. Aynı zamanda, bu arama stratejisi sistemin tepki süresini azaltmak için arama uzayını azaltan bir strateji ile birleştirilmiştir. Anahtar Sözcükler: Konuşma tanıma, üçlü-fonlar, Saklı Markov Modeli, ""trie"" yapılı sözlük modeli, ""trie"" yapılı arama stratejisi iv","ABSTRACT A LARGE VOCABULARY SPEECH RECOGNITION SYSTEM FOR TURKISH Cemal Yılmaz M.S. in Computer Engineering and Information Science Supervisors: Assoc. Prof. Dr. Kemal Oflazer and Prof. Dr. A. Enis Çetin August, 1999 This thesis presents a large vocabulary isolated word speech recognition system for Turkish. The triphones modeled by three-state Hidden Markov Models (HMM) are used as the smallest unit for the recognition. The HMM model of a word is constructed by using the HMM models of the triphones which make up the word. In the training stage, the word model is trained as a whole and then each HMM model of the triphones is extracted from the word model and it is stored individually. In the recognition stage, HMM models of triphones are used to construct the HMM models of the words in the dictionary. In this way, the words that are not trained can be recognized in the recognition stage. A new dictionary model based on trie structure is introduced for Turkish with a new search strategy for a given word. This search strategy performs breadth-first traversal on the' trie and uses the appropriate region of the speech signal at each level of the trie. Moreover, it is integrated with a pruning strategy to improve both the system response time and recognition rate. Keywords: Speech recognition, triphones, Hidden Markov Model (HMM), trie-based dictionary model, trie-based search strategy iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET MOBİL İLETİŞİM ORTAMLARINDA HAREKETLİ KULLANICILARDAN GÖNDERİLEN SÜREKLİ SORGULARIN İŞLENMESİ Hüseyin Gökmen Gök Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ocak, 1999 Bilgisayar donanımı ve telsiz iletişim ağı teknolojilerindeki gelişmeler mo- bil iletişim ortamlarının gelişmesine yolaçtı. Mobil iletişim ortamlarında, bilgi ağına telsiz bağlantısı olan kullanıcılar, veri sunucusuna sorgular göndererek veriye ulaşırlar. Mobil iletişim ortamlarında kullanıcıların hareketli olması ne deniyle, kullanıcıların konumları konuma-dayah sorgular açısından önemli bir bilgidir. Konuma-dayalı sorgular sürekli sorgular haline getirildiğinde daha da karmaşıkladırlar, çünkü sorgunun cevabı mobil kullanıcının hareket etmesi nedeniyle sürekli değişir. Konuma-dayalı sürekli bir sorgunun cevap kümesi < S, başlangıç, bitiş > gibi elemanlardan oluşur ve her bir eleman S nes nesinin başlangıç ve bitiş süreleri arasında sorgunun cevabı olduğu anlamına gelir. Cevap kümesindeki elemanların belirlenmesinden bir sonraki aşama bu elemanların mobil kullanıcıya ne zaman gönderileceğidir. Bu zamanlama tel siz ağ üzerindeki iletişim yükünü ve bağlantı kopukluğu durumunda sorgu cevabının ne kadarının kullanıcıya gönderilmiş olduğunu etkilemesi açısından kritiktir. Bu tezde konuma-dayalı sürekli sorguların cevap kümesindeki ele manların mobil kullanıcılara gönderiliş zamanını belirleyen üç değişik metot önerilmektedir. Bunun yanında, önerilen metotların değişik ortamlardaki per formanslarını karşılaştırabilmek amacıyla bir simülasyon modeli tasarlanmış ve gerçekleştirilmiştir. Anahtar kelimeler. Mobil İletişim, Mobil Veritabanı Sistemleri, Konuma Dayalı Sorgular, Sürekli Sorgular, Simülasyon.","Ill ABSTRACT PROCESSING OF CONTINUOUS QUERIES FROM MOVING OBJECTS IN MOBILE COMPUTING SYSTEMS Hüseyin Gökmen Gök M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Dr. Özgür Ulusoy January, 1999 Recent advances in computer hardware technology and wireless communi cation networks have led to the emergence of mobile computing systems. In a mobile computing environment, a user with a wireless connection to the in formation network can access data via submitting queries to the data server. Since the mobility is the most distinguishing feature of the mobile computing paradigm, location becomes an important piece of information for the so called location-dependent queries where the answer to a query depends on the current location of the user who issued the query. A location-dependent query submit ted by a mobile user can become more difficult to process when it is submitted as a continuous query for which the answer changes as the user moves. The answer to a location-dependent continuous query is a set that consists of tuples < S, begin, end > indicating that object S is the answer of the query from time begin to time end. Once the tuples in the answer set are determined, the next step is to determine when to send these tuples to the user. The transmission time of the tuples is critical in the sense that it can affect the communica tion overhead imposed on the wireless network and the availability of tuples in case of disconnections. In this thesis, we propose three tuple transmission approaches that determine the transmission time of a tuple in the answer set of a location-dependent continuous query. We also design and implement a sim ulation model to compare the performance of the proposed tuple transmission approaches under different settings of environmental parameters. Key words: Mobile Computing, Mobile Database Systems, Location-Dependent Queries, Continuous Queries, Simulation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KENDİNDEN DÜZENLENEN HARİTALAR ALGORİTMASI KULLANAN YENİ BİR SEZGİSEL YÜK DENGELEME ALGORİTMASI Murat Atun Bilgisayar ve Enformatik Mühendisliği Bölümü Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Attila Gürsoy Eylül, 1999 Koşut bir programın çalışması sırasında en iyi başarımın elde edilebilmesi için, görevlerin yüklerinin mümkün olduğu kadar eşit dağıtılması aynı zamanda yüksek oranlı iletişimde bulunan görevlerin de göreceli olarak yakın işlemcilere atanmaları gerekmektedir. Bu çalışmada, durgun yük dağıtımı için ""Kendin den Düzenlenen Haritalar"" algoritmasının yeni bir gerçekleştirimini sunduk. Kohonen'nin bu algoritması bilindiği üzere topolojik özellikleri koruyan bir al goritmadır. Biz girdi uzayını, birim kare olarak tanımladık ve bu uzayı, işlemci sayısı kadar parçaya böldük. Her seferinde Kohonen algoritmasının girdisini en az yüke sahip işlemcinin bölgesinden seçerek, algoritmaya eşit yük dağıtım özelliğini entegre ettik. Girdi seçimi, komşuluk çapı işlevi ve çalışma adım sayısı işlevlerinin varyasyonları incelenerek en iyi sonucu verenler tespit edildi. Sonuçlar, algoritmamızın, Kendinden Düzenlenen Haritalar algoritması kul lanılarak gerçekleştirilmiş diğer algoritmalardan çok daha iyi sonuçlar verdiğini göstermektedir. Anahtar Kelimeler. Sinir ağları, Kendinden Düzenlenen Haritalar, Koho nen, görev atama, yük dengeleme.","ABSTRACT A NEW LOAD BALANCING HEURISTIC USING SELF-ORGANIZING MAPS Murat Atun M. S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Attila Gürsoy September, 1999 In order to have an optimal performance during an execution of a parallel program, the tasks of the parallel computation must be mapped to processors such that the computational load is distributed as evenly as possible while highly communicating tasks are placed closely. We describe a new algorithm for static load balancing problem based on Kohonen Self- Organizing Maps (SOM) which preserves the neighborhood relationship of tasks. We define the input space of the SOM algorithm to be a unit square and divide it into ""number of processors"" regions. The tasks are represented by the neurons which are mapped to the regions randomly. We enforce load balancing by selecting training input from the region of the least loaded processor. We examine the impact of various input selection strategies and neighborhood function's on the accuracy of the mapping. The results show that our algorithm outperforms the other task mapping algorithms implemented with SOMs. Key words: Neural networks, Self- Organizing Maps, Kohonen, task map ping, load balancing. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET ERKEN ELİMİNASYON İLE YOĞUN NESNE KÜMELERİNİN GÜNCELLENMESİ Necip Fazıl Ayan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkun Temmuz, 1999 Bilişim uygulamalarının yaygınlaşması ile, bilgisayarlarda büyük miktarlarda veri depolanmasına başlanmıştır. Günümüz veri tabanı sistemleri, kullanıcıya depolanan bütün bilgilere kolayca ulaşabileceği araçları ve fonksiyonları sun mamaktadır. Büyük veri tabanlarında saklı olan bu bilgilere ulaşmak ve bu bilgileri kullanmak üzere, otomatik bilgi keşfetmeye yarayan teknikler geliştiril mektedir. Bu tekniklerden biri olan bağıntı kuralları bulma, depolanan veriler den, ilginç ve sıklıkla rastlanan şemaları tanıma işlevinin, yani veri araştırması nın çok önemli bir dalıdır. Bağıntı kuralları, nesnelerin bir arada olma du rumlarını belirlemeyi amaçlar ve bir çok alanda geniş kullanılabilirliğe sahip tir. Bağıntı kuralları bulma, yoğun nesne kümelerinin (verilerde sıkça bir arada görülen nesnelerin) hesaplanması esasına dayanır ve büyük veri taban larında hesaplanması oldukça pahalı bir işlemdir. Bu yüzden, daha önce belir lenmiş bağıntı kurallarının korunması oldukça önemli bir konudur. Bu tezde, daha önceden bulunmuş olan nesne kümelerini göz önüne alarak, yoğun nesne kümelerini güncellemekte kullanılan hızlı bir algoritma sunulmaktadır. Algo ritmanın temel fikri, herhangi bir nesne kümesini güncellenen veri tabanında yoğun olmadığı anlaşılır anlaşılmaz elemek ve böylece yoğun olması muhtemel nesne kümelerinin sayısını olabildiğince küçük tutmaktır. Sunulan algoritma, veri tabanı üzerindeki tarama sayısı ile üretilen ve sayılan nesne kümelerinin sayısı bakımından daha önce önerilen bütün güncelleme algoritmalarından daha iyidir. Ayrıca, sunulan algoritma yoğun nesne kümelerinin hesaplanması esası na dayanan diğer veri araştırması işlerine de kolayca uyarlanabilir. Anahtar kelimeler: Veri araştırması, bağıntı kuralları, yoğun nesne kümeleri, yoğun nesne kümelerinin güncellenmesi, erken eliminasyon.","Ill ABSTRACT UPDATING LARGE ITEMSETS WITH EARLY PRUNING Necip Fazıl Ayan M.S. in Computer Engineering and Information Science Supervisor: Prof. Dr. Erol Arkun July, 1999 With the computerization of many business and government transactions, huge amounts of data have been stored in computers. The existing database systems do not provide the users with the necessary tools and functionalities to cap ture all stored information easily. Therefore, automatic knowledge discovery techniques have been developed to capture and use the voluminous informa tion hidden in large databases. Discovery of association rules is an important class of data mining, which is the process of extracting interesting and frequent patterns from the data. Association rules aim to capture the co-occurrences of items, and have wide applicability in many areas. Discovering association rules is based on the computation of large itemsets (set of items that occur frequently in the database) efficiently, and is a computationally expensive operation in large databases. Thus, maintenance of them in large dynamic databases is an important issue. In this thesis, we propose an efficient algorithm, to update large itemsets by considering the set of previously discovered itemsets. The main idea is to prune an itemset as soon as it is understood to be small in the updated database, and to keep the set of candidate large itemsets as small as possible. The proposed algorithm outperforms the existing update algorithms in terms of the number of scans over the databases, and the number of can didate large itemsets generated and counted. Moreover, it can be applied to other data mining tasks that are based on large itemset framework easily. Key words: Data mining, association rules, large itemsets, update of large itemsets, early pruning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BAKIŞIMLI ÇOKLU-İŞLEMCİ ÖBEKLERİNİ PROGRAMLAMAK: DÜĞÜM SEVİYESİNDE DALLI NESNELER ve SIRADÜZENSEL ÇOKLU-ETKİLEŞİM YÖNTEMLERİ İÇİN TASARLANAN BİR ÇATI İlker Cengiz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Attila Gürsoy Eylül 1999 Bakışımlı Çoklu işlemciye (SMP) sahip iş istasyonları üretmeye yönelik eğilim bu tür iş istasyonlarini hızlı ağlarla birbirine bağlayarak ucuz ama güçlü koşut programlama platformları oluşturma yönündeki araştırmaları arttırmaktadır. Bu tür platformları oluşturmanın yanı sıra, programcıların SMP öbeklerinin vaadettiği güçten yararlanmalarını sağlayacak farklı düzeylerde soyutlamalar, mekanizmalar ve yordam kütüphaneleri sunabilmek te başlıbaşma bir araştırma konusudur. Bir SMP mimarisini işlemciler dizisi olarak görmek yetersiz bir yaklaşım olacaktır, çünkü böyle bir model başarım açısından olası faydaları gözardı etmektedir. SMP öbekleri için yazılan koşut programlarda ortak olarak kullanılabilecek iletişim ve hesaplama örüntülerini içeren yeniden kullanılabilir yordam kütüphaneleri üzerinde çalıştık. Durağan yük dengelemede, bellek yönetiminde, dağıtık veri yapıları ve modüller arası arayüzler oluşturmada kullanılabilen dallı nesneleri SMP öbekleri için düğüm seviyesinde yeniden tanımladık. Bu çalışmada Charm++ koşut nesneye-yönelik programlama dili ile koşut sıradüzensel çoklu-etkileşim uygulamaları geliştirirken karşılaştığımız ortak kavramları tartıştık ve bu tür uygulamaları SMP öbeklerinden faydala- narak geliştirmek, deneysel amaçlarla kullanabilmek için biîSçatı tanımladık. Bu tür yöntemlerde ortak olarak etkileşime konu olan iki parçacık eğer ayni adres uzayında ise düğümler arası herhangi bir iletişim gerekmez. Ancak ak sine iki parçacık farklı adres uzaylarında ise etkileşimin hesaplanabilmesi için ivparçacıkların etkileşimle ilgili verilerinin birbirlerinin adres uzaylarına getir ilmesi gerekir ki bu da SMP düğümleri arası ağ üzerinden yapılan iletişim de mektir. Sunduğumuz çatı ve çoklu-etkileşim uygulama arayüzü koşut nesneye- yönelik tasarımı ile programcının iletişim ile ilgili detaylardan soyutlanarak deneysel amaçlı hızlı uygulama geliştirmesine yardımcı olacaktır. Anahtar sözcükler: SMP Öbekleri, Koşut Nesneye Yönelik Programlama, Çoklu- Etkileşim Yöntemleri.","ABSTRACT PROGRAMMING SMP CLUSTERS: NODE-LEVEL OBJECT GROUPS and THEIR USE IN A FRAMEWORK FOR NBODY APPLICATIONS İlker Cengiz M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Attila Gürsoy September 1999 Symmetric Multiprocessor (SMP) cluster architectures emerge as a cheaper but powerful way of building parallel programming platforms. Providing mecha nisms, layers of abstraction, or libraries gaining the power of SMP clusters is a challenging field of research. Viewing an SMP architecture as an array of processors would be insufficient, since such a model ignores essential possible gains over performance. We have stressed on reusable patterns or libraries for collective communication and computations that can be used commonly in parallel applications within a parallel programming environment utilized for SMP clusters. We introduce node-level replicated objects, since replicated objects provide a versatile abstraction that can be used to implement static load-balancing, local services such as memory management, distributed data structures, and inter-module interfaces. This work was motivated while we were developing parallel object-oriented hierarchical Nbody applications with Charm++. We discuss common paradigms that we came across in those appli cations and present a framework for their implementation on SMP clusters. If the bodies that an interaction needs are local then that interaction can be com pleted without any communication. Otherwise, the data of the remote bodies must be brought, and after the interaction calculation, the remote body data must be updated. Parallel object-oriented design of this framework hides com munication details of bringing remote bodies from programmer and presents an interface to develop and experiment with nbody algorithms. Keywords: SMP Clusters, Parallel Object-Oriented programming, Hierarchical NBody Methods. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET VİDEO VERİ TABANLARINDA HAREKET EDEN NESNELER İÇİN BİR İNDEKS YAPISI Tuba Yavuz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos, 1999 Video veri tabanı alanında, hareketli nesnelerin modellenmesi ve çeşitli hareket sorgularının cevaplanması oldukça ilgi çeken bir araştırma konusu olmuştur. Hareket sorgularının bir çeşidinde birden fazla nesnenin hareketleri birbirlerine göre olan yerlerindeki değişiklikle ifade edilmektedir. Birbirlerine göre uzay- sal ilişkileri belirtilmiş fakat kimlikleri belirtilmemiş nesnelerden oluşan bu tip sorguların cevaplanması özel bir indeks yapısının kullanılmasını gerektirir. Bunun nedeni, böyle bir sorgunun herhangi bir indeks yapısı kullanılmaksızın cevaplanmasının hesaplama karmaşıklığı 0(N\/(N - n)\) olmasıdır. Burada N veri tabanındaki nesne sayısını, n ise sorguda bulunan nesne sayısını gösterir. Biz bu çalışmada SMIST-indeks diye isimlendirdiğimiz uzaysal ve zamansal bir indeks yapısı geliştirdik. Bu indeks yapısının performansını incelediğimiz sorgu çeşidinin cevaplanması için önerilmiş bir yöntemle ([18]) karşılaştırdık. Deney sonuçları SMIST-indeks yapısının karşılaştırdığımız yöntemden daha iyi bir performans sergilediğini gösterdi. Ayrıca, yapılan deneylerde önerdiğimiz indeks yapısının artan çerçeve ve nesne sayısı karşısında disk ulaşım sayısında keskin artışlar göstermediği saptanmıştır. Anahtar kelimeler: Hareket, sorgu, video, veri tabanı, çoklu ortam, uzaysal, zamansal, uzaysal-zamansal indeksleme.","Ill ABSTRACT AN INDEX STRUCTURE FOR MOVING OBJECTS IN VIDEO DATABASES Tuba Yavuz M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Dr. Özgür Ulusoy August, 1999 Modeling moving objects and handling various types of motion queries are interesting topics to investigate in the area of video databases. In one type of motion queries, motion of multiple objects is specified by the changes in relative spatial positions of objects. Answering such kind of queries, that involve motion of multiple objects whose identifications are not specified, requires some type of indexing because the time complexity of processing such a query in the absence of an index structure is 0(N\/(N - n)!), where N is the number of objects in the database and n is the number of objects in the query. In this work, we propose a spatio-temporal index structure, which we call.SMZST-index, and compare its performance against a similar scheme proposed in [18]. The scheme presented in [18] consists of a constraint satisfaction algorithm, which is called Join Window Reduction (JWR), combined with a spatial index structure (R*- tree). Experimental results indicate that SMIST-index outperforms the JWR algorithm. Also, SMIST-'index is shown to be scalable to increasing number of frames and objects. Key words: Motion, query, video, database, multimedia, spatial, temporal, spatio-temporal indexing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET MOBİL GERÇEK ZAMANLI BİR VERİTABANI SİSTEMİNDE HAREKET PERFORMANSININ İNCELEMESİ Ersan Kayan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Özgür Ulusoy Ocak, 1998 Gerçek zamanlı veritabam sistemleri ve mobil işletim sistemleri kapsamındaki araştırma konularının entegrasyonu yeni bir araştırma dalı olup, veritabam uygulamalarının gerçek zamanlı ihtiyaçlarını karşılama amacını gütmektedir. Mobil işletim sistemlerinin belirli bazı sınırlamaları; kaynakça yetersiz mobil bilgisayarlar, düşük kapasiteli ve hata oranı yüksek telsiz hatlar, kullanıcıların ve/veya bilgisayarların hareketliliği gibi, bilinen gerçek zamanlı veritabam sis temlerindeki birçok araştırma konularının yeniden gözden geçirilmesini gerek tirmektedir. Bu tezde, mobil gerçek zamanlı bir veritabam sisteminin detaylı bir simulasyon programı kullanılarak bu konuların bazıları incelenmektedir. Sistemin performansı önerilen iki değişik işleme stratejisine sahip bir hareket işleme modeli kullanılarak ve değişik mobil sistem özellikleri dikkate alınarak hesaplanmaktadır. Performans sonuçları zaman sınırlamalarının karşılanabilme oranı baz alınarak verilmektedir. Anahtar kelimeler. Mobil İşletim, Gerçek Zamanlı Veritabanları, Hareket İşleme Modeli, Performans Analizi.To my family and Rabia","Ill ABSTRACT AN EVALUATION OF TRANSACTION MANAGEMENT ISSUES IN MOBILE REAL-TIME DATABASE SYSTEMS Ersan Kayan M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. Özgür Ulusoy January, 1998 The integration of issues from real-time database systems and mobile com puting systems is a new research area that aims to provide support for real time requirements of database applications in a mobile computing environ ment. Due to certain constraints of a mobile computing environment, such as resource-poor mobile computers, low-bandwidth and unreliable wireless links, and mobility of users/computers, various research issues in traditional real time database systems need to be reconsidered. In this thesis, we investigate some of these issues using a detailed simulation model of a mobile real-time database management system. We propose a transaction execution model with two alternative execution strategies and evaluate the performance of the sys tem considering various mobile system characteristics. Performance results are provided in terms of the fraction of transactions that satisfy their deadlines. Key words: Mobile Computing, Real-Time Databases, Transaction Execu tion Model, Performance Analysis."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Görev atama probleminin amacı bir dağıtık sistemdeki görevlerin işlemcilere yürütme ve iletişim giderlerinin toplamını en küçük yapacak biçimde atamaktır. Bu çalışmada, görevlerin iletişim zamanlarının yanı sıra yürütme zamanları arasındaki farkı da dikkate alan yeni bir topaklama yöntemi önerilmiştir. Bu topaklama yöntemi uygun atama yöntemleri ile birlikte her türlü görev atama problemine en iyiye yakın çözümler bulabilecek olan iki-evreli atama algo ritmaları oluşturmak için kullanılmıştır. Bunlara ek olarak, çizge/hiperçizge parçalamada kullanılan çok düzeyli çizenek görev atama problemine uyarlan mıştır. Çok düzeyli atama algoritmaları görevleri birleştirerek asıl problemi küçültür, en küçük problem için bir başlangıç ataması bulur, sonra bu ata mayı her düzeyde iyileştirerek asıl probleme doğru yansıtır. Bu çalışmada çok düzeyli atama algoritmaları için bir çok topaklama çizeneği önerilmiştir. Bütün önerilen algoritmalar iki güncel algoritma ile karşılaştırılmış ve başarımları bir deneysel çalışma ile değerlendirilmiştir. Deney sonuçları göstermiştir ki önerilen algoritmalar varolan iki algoritmadan da daha iyi çalışmaktadır. Anahtar kelimeler. Görev atama, dağıtık sistemler, görev topaklama, çok düzeyli görev atama yöntemleri, Kerninghan-Lin algoritması","Task assignment problem deals with assigning tasks to processors in order to minimize the sum of execution and communication costs in a distributed sys tem. In this work, we propose a novel task clustering scheme which considers the differences between the execution times of tasks to be clustered as well as the communication costs between them. We use this clustering approach with proper assignment schemes to implement two-phase assignment algorithms which can be used to find suboptimal solutions to any task assignment prob lem. In addition, we adapt the multilevel scheme used in graph/hypergraph partitioning to the task assignment. Multilevel assignment algorithms reduce the size of the original problem by collapsing tasks, find an initial assignment on the smaller problem, and then projects it towards the original problem by successively refining the assignment at each level. We propose several clus tering schemes for multilevel assignment algorithms. The performance of all proposed algorithms are evaluated through an experimental study where the as signment qualities are compared with two up-to-date heuristics. Experimental results show that our algorithms substantially outperform both of the existing heuristics. Key words: Task assignment, distributed systems, task clustering, multilevel task assignment methods, Kernighan-Lin Heuristic."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET İşakişi yönetimindeki metodolojik konuların bir değerlendirmesi Anastasia Sotnikova Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Özgür Ulusoy Ağustos, 1998 İşakışı yönetimi teknolojisi, endüstride pek çok alanda uygulanma imkanı bulmasına rağmen, işakışı yönetim sistemlerinin henüz belirgin bir olgunluğa eriştiği söylenemez. Bu tezde hedeflerimiz, işakışı yönetim sistemlerinin metodo lojik özelliklerini analiz etmek ve işakışı yönetim teorisine fonksiyonel ve yapısal açıdan katkılarda bulunmak olmuştur. Önerdiğimiz fikirlerin doğrulanması amacıyla bir işakışı uygulanması tasarımı yapılmış ve simülasyon tekniği kul lanılarak tasarlanan uygulama test edilmiştir. Tasarım ve geliştirme adımlan arasında simülasyon tekniğinin kullanılması, tasarımcıya işakışı sisteminin çeşit li yönlerden değerlendirilebilmesi imkanını verecektir. Ayrıca, simülasyon sonuç lan tasanmcınm işakışı sistemi için en uygun yapıyı seçmesini sağlayarak, ileriki aşamalarda olması muhtemel kayıplann önlenmesi için de yardımcı olacaktır. Anahtar sözcükler. İşakışı Sistemleri, Gelişmiş İşlem Modelleri, Performans Ölçümü.","m ABSTRACT AN EVALUATION OF METHODOLOGICAL ISSUES IN WORKFLOW MANAGEMENT Anastasia Sotnikova M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Dr. Özgür Ulusoy August, 1998 Workflow management is a diverse and rich technology being applied over an increasing number of industries. Despite this fact, workflow management systems (WFMSs) still have a long way to go before they can be regarded as mature technology. In this thesis, we try to analyze methodological aspects of WFMSs and contribute to the workflow management theory in terms of new functionality and structures of workflow schemas. A confirmation of our ideas is provided by simulation results of a workflow application which we have designed. Bringing the simulation stage in between design and implementation stages would let a schema designer assess a workflow system in terms of optimal system throughput, required facility capacities, and an efficient transactional representation of activities. Also, by allowing a schema designer to choose an effective structure of a workflow system, simulation results help to avoid possible future losses at the early stages of the workflow schema design. Key words: Workflow Systems, Advanced Transaction Models, Performance Evaluation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DAĞITIK BELLEK MİMARİLERİNDE BÜYÜK SEYREK LİNEER EŞİTSİZLİK SİSTEMLERİNİN ÇÖZÜMÜ İÇİN PARALLEL ALGORİTMALAR Esma Turna Bilgisayar ve Enformatik Mühendisliği Bölümü Yüksek Lisans Tez Yöneticisi: Doç. Mustafa Ç. Pınar Ağustos, 1998 Bu tezde birçok parallel algoritma önerilmiş ve bu algoritmalardan büyük seyrek lineer eşitsizlik sistemlerinin çözümü için yaralanılmıştır. Parallelleştirme şemala rı, aracı kısıtlar yöntemi için önerilmiş olan orta ölçekli parallel formülasyondan yararlanılarak geliştirilmiştir. Sözü edilen şemalar bir boyutlu ve iki boyutlu parçalama esasına dayalıdır. Bununla birlikte, bir boyutlu parçalama şemasında iletişim gereksiniminin azaltılması için hiperçizge parçalama yöntemini kullanan üçüncü bir paralleleştirme şeması önerilmiştir. Hiperçizge modelinden yarar lanılarak iletişim gereksiniminin düzenlenmesi genel ve bölgesel iletişim şemaları vasıtasıyla sağlanmıştır. Aynı zamanda, düzgün satırsal bölümlü ve kartezyen parçalama yöntemlerinin etkin biçimde kullanılabilmesi için parça yükleme yakla şımına dayalı yeni algoritmalar araştırılmıştır. Görüntü düzeltme problemi genel bir kapsamda lineer eşitsizlik sistemi olarak formüle edilmiştir. Geliştirilen par allel algoritmalarla görüntüleme araçlarının yanlış odaklaması, atmosferdeki dal galanmalar ve benzeri sebeplerden noktasal dağılım fonksiyonlarıyla bulanıklaşmış görüntülerin restorasyonu sağlanmıştır. Anahtar kelimeler, lineer fizibilite, blok projeksyonlar, aracı kısıtlar yöntemi, yük denkliği, hiperçizge parçalama modeli, görüntü düzeltme, görüntü restorasy onu, projeksiyon yöntemiyle görüntü yapılandırılması, parallel algoritmalar. iv","ABSTRACT PARALLEL ALGORITHMS FOR THE SOLUTION OF LARGE SPARSE INEQUALITY SYSTEMS ON DISTRIBUTED MEMORY ARCHITECTURES Esma Turna M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Mustafa Ç. Pmar August, 1998 In this thesis, several parallel algorithms are proposed and utilized for the so lution of large sparse linear inequality systems. The parallelization schemes are developed from the coarse-grain parallel formulation of the surrogate constraint method, based on the partitioning strategy: ID partitioning and 2D partition ing. Furthermore, a third parallelization scheme is developed for the explicit minimization of the communication overhead in ID partitioning, by using hyper- graph partitioning. Utilizing the hypergraph model, the communication overhead is maintained via a global communication scheme and a local communication scheme. In addition, new algorithms that use the bin packing heuristic are inves tigated for efficient load balancing in uniform rowwise stripped and checkerboard partitioning. A general class of image recovery problems is formulated as a linear inequality system. The restoration of images blurred by so called point spread functions arising from effects such as misfocus of the photographic device, at mospheric turbulence, etc. is successfully provided with the developed parallel algorithms. Key words: linear feasibility, block projections, surrogate constraints method, load balancing, hypergraph partitioning model, image recovery, image restora tion, image reconstruction from projections, parallel algorithms. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇEVİRİ KALIPLARINA GÜVEN FAKTÖRÜ ATANMASI Zeynep Orhan Bilgisayar ve Enformatik Mühendisliği Bölümü, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. İlyas Çiçekli Eylül, 1998 Çeviri Kalıpları Öğrenicisi (ÇKÖ) algoritması iki çeviri örneği arasındaki yapısal seviyedeki uy gunlukları analojik muhakemeyle öğrenir. Çeviri örneğinde kullanılan cümlelerin hedef dildeki benzer ve ayrı kısımlara karşılık gelmesi gereken kaynak dilde bulunan benzer ve ayrı kısımları olmalıdır. Bu yüzden bu uygunluklar çeviri kalıpları olarak öğrenilir. Öğrenilen çeviri kalıpları diğer cümlelerin çevirisinde kullanılır. Bununla beraber, daha önce elde edilmiş olan güven faktörlerini dikkate alarak çeviri neticelerini sıralamak için bu çeviri kalıplarına güven faktörleri vermemiz gerekir. Bu tez ÇKÖ algoritması ile öğrenilen çeviri kalıplarına güven faktörleri ver mek için bir algoritma ortaya koymaktadır. Bu işlemde her kalıba eğitme örneklerinden elde edilen istatistiksel bilgiye göre bir güven faktörü verilmiştir. Ayrıca, kötü çeviriye yol açacak belli kombinasyonları elemek için bazı kalıp kombinasyonlarına da güven faktörleri verilmiştir. iv","ABSTRACT CONFIDENCE FACTOR ASSIGNMENT TO TRANSLATION TEMPLATES Zeynep Orhan M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Ilyas Çiçekli September, 1998 TTL (Translation Template Learner) algorithm learns lexical level correspondences between two translation examples by using analogical reasoning. The sentences used as translation ex amples have similar and different parts in the source language which must correspond to the similar and different parts in the target language. Therefore, these correspondences are learned as translation templates. The learned translation templates are used in the translation of other sentences. However, we need to assign confidence factors to these translation templates to order translation results with respect to previously assigned confidence factors. This thesis proposes a method for assigning confidence factors to translation templates learned by the TTL algo rithm. In this process, each template is assigned a confidence factor according to the statistical information obtained from training data. Furthermore, some template combinations are also assigned confidence factors in order to eliminate certain combinations resulting bad translation. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NOKTALAMAYA ENFORMASYON TEMELLİ BİR YAKLAŞIM Bilge Say Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticisi: Prof. Dr. Varol Akman Kasım 1998 Yazılı dilin anlamsal ifadesinde noktalama işaretleri özel bir önem taşır. Geoffrey Nunberg'in yazılı cümlede noktalama işaretlerinin oluşturduğu metin grameri üzerine 1990 tarihli kitabı bu konudaki betimleyici ve buyurucu yaklaşımları birleştirmiştir. Bu yapıt yakın geçmişte Doğal Dil işleme (DDI) alanında noktalama işaretlerine yaklaşımların çoğuna esin kaynağı olmuştur. Daha sonra geliştirilen sözdizimsel ayrıştırıcılar çözümleme hata ve belirsizliklerinin noktalama işaretlerinin göz önüne alınmasıyla azaldığını göstermiştir. Keza Nunberg'in noktalama işaretlerinin (ve metin düzenleme araçlarının) sunumuna getirdiği yaklaşım doğal dil üretme dizgeleri tarafın dan değerlendirilmiştir. Ancak noktalama işaretlerinin anlamsal ve söylemsel etkileri ve bunların hesapsal kullanımı hakkında çok az çalışma yapılmıştır. Bu tezin amacı noktalama işaretlerinin anlamsal ve söylemsel yönlerini Hans Kamp ve Uwe Reyle""nin Söylem Gösterim Kuramını (SGK) (ve Nicholas Asher'in bunun üzerine geliştirdiği Bölümlü Söylem Gösterim Kuramını (BSGK)) kullanarak incelemek ve DDI dizgeleri için gerekli sonuçları çıkarmaktır. Uygulanan yöntem elektronik metinlerden dört yaygın noktalama işareti (uzun tire, noktalı virgül, iki nokta üstüste ve parentez) ile vıVII ilgili örüntüleri çıkararak, biçimsel bir model ve bilgisayarda küçük bir uygulama elde et mek olarak özetlenebilir. Gözlem ve sonuçlarımız anafora çözümleme ve varsayım gibi dil- bilimsel olguların noktalama işaretleri ile ilgisi hakkında ilginç bağlar ortaya çıkarmıştır. BSGK çerçevesinde bu örneklemeler genel söylem yapısına bağlanmıştır. Önerilen model DDİ için yazılım geliştirenlerin noktalama işaretlerini daha etkili kullanabilmesi için bir şablon olarak alınabilir. Tez genelde noktalamanın yazılı metin aracılığıyla okuyucuya aktarılan enformasyona yaptığı katkıyı betimlemektedir. Anahtar Sözcükler: Noktalama, Söylem, (Bölümlü) Söylem Gösterim Kuramı [(B)SGK]. Enformasyon Yapısı, Külliyat, Doğal Dil işleme (DDI)","ABSTRACT AN INFORMATION-BASED APPROACH TO PUNCTUATION Bilge Say Ph.D. in Computer Engineering and Information Science Supervisor: Prof. Varol Akman November 1998 Punctuation marks have special importance in bringing out the meaning of a text. Ge offrey Nunberg's 1990 monograph bridged the gap between descriptive treatments of punctuation and prescriptive accounts, by spelling out the features of a text-grammar for the orthographic sentence. His research inspired most of the recent work concentrat ing on punctuation marks in Natural Language Processing (NLP). Several grammars incorporating punctuation were then shown to reduce failures and ambiguities in pars ing. Nunberg's approach to punctuation (and other formatting devices) was partially incorporated into natural language generation systems. However, little has been done concerning how punctuation marks bring semantic and discourse cues to the text and whether these can be exploited computationally. The aim of this thesis is to analyse the semantic and discourse aspects of punctuation marks, within the framework of Hans Kamp and Uwe Reyle's Discourse Representation Theory (DRT) (and its extension by Nicholas Asher, Segmented Discourse Representa tion Theory (SDRT)), drawing implications for NLP systems. The method used is the extraction of patterns for four common punctuation marks (dashes, semicolons, colons. ivand parentheses) from corpora, followed by formal modeling and a modest computa tional prototype. Our observations and results have revealed interesting occurrences of linguistic phenomena, such as anaphora resolution and presupposition, in conjunction with punctuation marks. Within the framework of SDRT such occurrences are then tied with the overall discourse structure. The proposed model can be taken as a template for NLP software developers for making use of the punctuation marks more effectively. Overall, the thesis describes the contribution of punctuation at the orthographic sen tence level to the information passed on to the reader of a text. Keywords: Punctuation, Discourse, (Segmented) Discourse Representation Theory [(S)DRT]. Information Structure, Corpora, Natural Language Processing (NLP)"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BİLİŞSEL DİZGENİN ALT DÜZEY ÖĞELERİNİN TAHMİN ETME TEMELİNDE ORGANİZASYONU Armağan Yavuz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. David Davenport Ocak 1998 Bu tezde beyinin alt düzey duyusal-motor işlevlerinin tahmin etme temelinde organize olduğu öne sürülmektedir. Bu öneri duyusal-motor sistemler hakkındaki varolan kuramlardan oldukça farklıdır ve şu şekilde özetlenebilir: Beyindeki birtakım basit mekanizmalar diğerlerinin o anki ya da gelecekteki durumlarını tahmin ederler. Bu mekanizmalar dinamik olarak.oluşabilir ya da yok olabilirler. Tahminlerdeki doğruluk derecesi bu süreçte seçim kriteri olarak rol oynar. Basit mekanizmalar bu şekilde birbirlerine bağlanır ve hiyerarşik kompleksler oluştururlar. Bu kompleksler, ilginç olayları tanıma, yüksek düzey gösterimler oluşturma, ve bir hedefe yönelik etkinliklere yardımcı olma gibi bir dizi işlevi yerine getirirler. Dikkat ve bellek gibi diğer dizgeler bu işlemlere yardımcı olur ve bu sistemden yararlanırlar. Tahmin temelinde gerçekleşen böyle bir organizasyon alt-düzey bilişsel etkinliklerin temelini oluşturur. Bu tezde, tahmin temelinde çalışan bir duyusal-motor sistem modeli verilmekte ve bu modelin algıya ilişkin bazı ilginç problemleri nasıl çözümlediği ve diğer bilişsel etkinliklerle nasıl ilişkili olabileceği tartışılmaktadır. Anahtar Sözcükler: tahmin mekanizması, gösterimlerin oluşumları, konstrük- tivizm, algı, biliş, bilişsel bilim. iv","ABSTRACT PREDICTION AS THE BASIS OF LOW LEVEL COGNITIVE ORGANIZATION Armağan Yavuz M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. David Davenport January, 1998 I suggest that the brain's low-level sensory-motor systems are organized on the basis of prediction. This suggestion differs radically from existing theories of sensory-motor systems, and can be summarized as follows. Certain simple mechanisms in the brain predict the current or future states of other brain mecha nisms. These mechanisms can be established and disposed dynamically. Success ful prediction acts as a kind of selection criteria.and new structures are formed and others are disposed according to their predictive powers. Simple mechanisms become connected to each other on the basis of their predictive power, possibly establishing hierarchical structures, and forming large complexes. The complexes so formed, can implement a number of functionalities including detecting inter esting events, creating high-level representations, and helping with goal-directed activity. Faculties such as attention and memory contribute to such processes of internal predictions and they can be studied and understood within this setting. All of this does not rule out the existence of other mechanisms, but an organiza tion driven by prediction serves as the backbone of low-level cognitive activity. I develop a computational model of a sensory-motor system that works on this basis. I also show how this model explains certain interesting aspects of human perception and how it can be related to general cognitive capabilities. Keywords: prediction mechanism, emergent representations, constructivism, per ception, cognition, cognitive science. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET SENTETİK İNSAN YÜZÜ İÇİN GERÇEĞE UYGUN KONUŞMA CANLANDIRMASI Barış Uz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticileri: Prof. Dr. Bülent Özgüç ve Yrd. Doç. Dr. Uğur Güdükbay Haziran, 1998 Bu çalışmada, sentetik insan yüzlerinde gerçekçi konuşma animasyonu için fiziğe dayalı modelleme ve parametrik yaklaşım birleştirilmiştir. Yüzdeki kaslar için fiziksel modelleme kullanılmıştır. Kaslar, poligonlardan oluşan bir ağ yapısını deforme eden kuvvetler olarak modellenmiştir. Metnin her ""anlamlı"" parçası (bu çalışmada bir harf), belli bir ağız şekline karşı gelmektedir ve ağız şekilleri kasları ve çene hareketini etkileyen bir dizi parametrenin değiştirilmesi ile oluşturulmuştur. Ayrıca, yüz ifadelerini konuşma ile eşzamanlı kılacak bir yöntem geliştirilmiştir. Metinde bazı yerlere yüz ifadelerini ve derecelerini gösteren özel etiketler yerleştirilebilir ve böylece, yüz ifadesi oluşturulup konuşma animasyonu ile eşzamanlı yapılabilir. Anahtar kelimeler. Yüz animasyonu, konuşma animasyonu, kas tabanlı, fiziğe dayalı, yüz ifadesi.","Ill ABSTRACT REALISTIC SPEECH ANIMATION OF SYNTHETIC FACES Barış Uz M.S. in Computer Engineering and Information Science Supervisors: Prof. Dr. Bülent Ozgüç and Asst. Prof. Dr. Uğur Güdükbay June, 1998 In this study, physically-based modeling and parameterization are combined to generate realistic speech animation on synthetic faces. Physically-based modeling is used for muscles which are modeled as forces deforming the mesh of polygons. Parameterization technique is used for generating mouth shapes for speech animation. Each meaningful part of a text, a letter in our case, cor responds to a specific mouth shape, and this shape is generated by setting a set of parameters used for representing the muscles and jaw rotation. A mechanism has also been developed to generate and synchronize facial expressions while speaking. Tags specifying facial expressions are inserted into the input text together with the degree of the expression. In this way, the facial expression with the specified degree is generated and synchronized with speech animation. Key words: facial animation, speech animation, muscle-based, physically- based, facial expression."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET Ertuğrul Uysal Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Tuğrul Dayar Haziran, 1997 RASSAL ÖZDEVİNİMLİ AĞLAR İÇİN BÖLÜNME TABANLI İTERATİF YÖNTEMLER Bu tezde Rassal Özdevinimli Ağlar için bölünme tabanlı dolaylı yöntemler (Jacobi, Gauss-Seidel, Succesive Over Relaxation) ve bunların blok çeşitleri geliştirilmiştir. Bu yöntemlerin, yakın zamana kadar Rassal Özdevinimli Ağları çözmekte kullanılan power yönteminden daha iyi oklukları gösterilmiştir. Üç örnek yardımıyla, Rassal Özdevinimli Ağlar kullanılarak geliştirilmiş bir modelin çözülmesi için gerekli sürenin hala oldukça yüksek olduğunu, ve şu anki teknolojik imkanlarla, on milyonlar mertebesinde duruma (state) sahip bir modelin standart masaüstü bilgisayarlarla çözülmesinin pek mümkün gözükmediğini buluyoruz. Diğer taraftan Rassal Özdevinimli Ağlar yöntemi ile, tüm sis temi ifade eden matrisi bilgisayarın ana hafızasında seyrek şekilde saklayarak çözülebilecek modellerden çok daha büyük modellerin çözülebileceği görülmüş tür. Bu durum, özellikle tüm sistemi ifade eden matrisin yoğun olduğu durumda geçerlidir. Anahtar kelimeler. Markov süreçleri, Rassal Özdevinimli ağlar, Tensör cebri, Bölünmeler, Blok yöntemler.","Ill ABSTRACT ITERATIVE METHODS BASED ON SPLITTINGS FOR STOCHASTIC AUTOMATA NETWORKS M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. Tuğrul Dayar June, 1997 This thesis presents iterative methods based on splittings (Jacobi, Gauss- Seidel. Successive Over Relaxation) and their block versions for Stochastic Au tomata Networks (SANs). These methods prove to be better than the power method that has been used to solve SANs until recently. Through the help of three examples we show that the time it takes to solve a system modeled as a SAN is still substantial and it does not seem to be possible to solve sys tems with tens of millions of states on standard desktop workstations with the current state of technology. However, the SAN methodology enables one to solve much larger models than those could be solved by explicitly storing the global generator in the core of a target architecture especially if the generator is reasonably dense. Keywords: Markov processes; Stochastic automata networks; Tensor alge bra; Splittings; Block methods"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TASLAĞA DAYALI MANTIK PROGRAMI DÖNÜŞTÜRME Halime Büyükyıldız Bilgisayar ve Enformatik Mühendisliği. Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Pierre Flener Ağustos 1997 Geleneksel programlama metodolojisinde, doğru ve etkili program geliştirme iki aşamaya ayrılır: birinci aşamada, sentez aşaması denir, doğru, fakat yeterince etkili olmayabilen bir program yapılır, ve ikinci aşamada, dönüştürme aşaması denir, yapılan program daha etkili eşdeğer bir programa dönüştürülür. Eğer sentez aşaması belirli bir program ailesinin yapımını özetleyebilen algoritma plan bilgisini içeren program taslağı rehberliğindeyse, dönüştürme aşaması da giren program taslağından çıkan program taslağına tanımlanmış dönüşüm tekniklerini daha etkili eşdeğer bir program elde etmeyi sağlayacak gerekli koşulları tanımlayarak kodlayan dönüşüm taslakları kullanarak yapılabilir. Böl-ve-fethet ve genelleme metodlarını kullanarak sentezlenebilecek program ailelerini temsil eden yedi program taslağı sunuluyor. Sunulan dönüşüm taslakları ya içine birikeç sokmak ve yapısal genellemenin özel bir hali olan Çoğullama genellemesi gibi dönüşüm tekniklerinin otomasyonunu sağlar, ya da fonksiyonel programlamanın temel teoremlerinden birini (fold operatörlerinin ilk ikilik kuralını) mantıksal programlamaya geliştirerek uygular. Sunulan dönüşüm taslaklarını kullanarak program dönüştürebilen prototip bir sistem geliştirilmiştir. Anahtar Sözcükler: mantıksal programlama, program geliştirme, program dönüştürme, program taslağı, dönüşüm taslağı, genelleme, ikilik kuralları.","ABSTRACT SCHEMA-BASED LOGIC PROGRAM TRANSFORMATION Halime Büyükyıldız M.S. in Computer Engineering and Information Science Supervisor: Ass't Prof. Pierre Flener August 1997 In traditional programming methodology, developing a correct and efficient program is divided into two phases: in the first phase, called the synthesis phase, a correct, but maybe inefficient program is constructed, and in the sec ond phase, called the transformation phase, the constructed program is trans formed into a more efficient equivalent program. If the synthesis phase is guided by a schema that embodies the algorithm design knowledge abstracting the con struction of a particular family of programs, then the transformation phase can also be done in a schema-guided fashion using transformation schemas, which encode the transformation techniques from input program schemas to output program schemas by defining the conditions that have to be verified to have a more efficient equivalent program. Seven program schemas are proposed, which capture sub-families of divide- and-conquer programs and the programs that are constructed using some gen eralization methods. The proposed transformation schemas either automate transformation strategies, such as accumulator introduction and tupling gen eralization, which is a special case of structural generalization, or simulate and extend a basic theorem in functional programming (the first duality law of the fold operators) for logic programs. A prototype transformation system is presented that can transform programs, using the proposed transformation schemas. Keywords: logic programming, program development, program transforma tion, program schema, transformation schema, generalization, duality laws. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET OYLAYAN ÖZNİTELİK BÖLÜNTÜLERİNE DAYALI TOPLU SINIFLANDIRMA ÖĞRENME ALGORİTMALARI Gülsen Demiröz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Halil Altay Güvenir Ağustos, 1997 Öğrenmek akıllı bir bireyin en gerekli özelliklerinden biridir. Bu tezde çoklu kavram tanımlarını öznitelik aralıkları şeklinde öğrenen yeni algoritmalar önerilmektedir. Oylayan Öznitelik Aralıkları ( VFT) olarak isimlendirilen bu algoritmalar toplu sınıflandırma öğrenme algoritmalarıdırlar. Daha önceden sınıflandırılmış olan örneklerden sınıflandırma bilgisini çıkarmak için öznitelik izdüşümlerine dayalı bilgi gösterim yöntemini kullanırlar. Öğrenilen kavram tanımı her öznitelik için ayrı ayrı öğrenilen aralıklar şeklindedir. Her bir aralık bütün sınıflar için sınıflandırma bilgisi içerir. Yeni bir örneğin sınıflandırılması her özniteliğin oyunu bütün sınıflara dağıttığı bir oylama sistemine dayanır. Gerçek hayattan alınan veri kümeleri üzerinde yapılan deneylerde VFI algoritmaları daha önce geliştirilmiş öznitelik izdüşümlerine dayalı diğer metodlardan daha yüksek sınıflandırma doğruluğu elde etmişlerdir. Ayrıca sınıflandırma doğruluğunu daha çok arttırmak için optimum öznitelik ağırlıklarını öğrenen genetik algoritmalar geliştirilmiştir. Aynı zamanda bu genetik algoritmalarda kullanılmak üzere yeni bir çaprazlama operatörü de geliştirilmiştir. Bir öğrenme sisteminin açıklama yeteneği de en az doğruluğu kadar önemli olduğundan, YFI algoritmaları öğrendiklerini insanların anlayabileceği bir şe kilde gösterebilmektedirler. Anahtar Sözcükler: öğrenme, tümevarımsal öğrenme, sınıflandırma, toplu öğrenme, denetimli öğrenme, öznitelik izdüşümleri, oylama, genetik algoritmalar. iv","ABSTRACT NON-INCREMENTAL CLASSIFICATION LEARNING ALGORITHMS BASED ON VOTING FEATURE INTERVALS Gülsen Demiröz M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Halil Altay Güvenir August, 1997 Learning is one of the necessary abilities of an intelligent agent. This thesis proposes several learning algorithms for multi-concept descriptions in the form of feature intervals, called Voting Feature Intervals (VFI) algorithms. These algorithms are non-incremental classification learning algorithms, and use fea ture projection based knowledge representation for the classification knowledge induced from a set of preclassified examples. The concept description learned is a set of intervals constructed separately for each feature. Each interval car ries classification information for all classes. The classification of an unseen instance is based on a voting scheme, where each feature distributes its vote among all classes. Empirical evaluation of the VFI algorithms have shown that they are the best performing algorithms among other previously developed fea ture projection based methods in terms of classification accuracy. In order to further improve the accuracy, genetic algorithms are developed to learn the op timum feature weights for any given classifier. Also a new crossover operator, called continuous uniform crossover, to be used in this weight learning genetic algorithm is proposed and developed during this thesis. Since the explanation ability of a learning system is as much important as its accuracy, VFI classi fiers are supplemented with a facility to convey what they have learned in a comprehensible way to humans. Keywords: machine learning, supervised learning, classification, inductive learning, non-incremental learning, feature intervals, voting, genetic algorithms. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET DÜZENSİZ IZGARALARIN OBJE UZAYI BÖLÜNMESİNE DAYANAN PARALEL HACİM GÖRÜNTÜLENMESİ Ferit Fındık Bilgisayar ve Enformatik Mühendisliği. Yüksek Lisans Tez Yöneticisi: Doç. Dr. Cevdet Aykanat Ekim, 1997 Bu çalışma düzensiz ızgaraların ışın izlemeye dayanan verimli doğrudan hacim görüntüleme (DHG) algoritmasının çok-işlemcili dağıtık bellekli bilgisayarlarda obje-uzayı (OU) paralelleştirilmesini araştırmaktadır. Başarılı bir paralelleştirmenin sırrı, OU benzerliğini koruyan ve mümkün olduğunca hesap- sal yük dengesini sağlayan OU bölümünü bulmaktır. OU bölümü problemi çizge bölünmesi (ÇB) problemi olarak bakış açısına bağımlı doğru düğüm ve kenar ağırlığı verilmesiyle modellendi. Paralel mühendislik simulasyonlarının sonuçlan aynı makinada paralel görüntülendiği için, ardışık paralel adımlarda oluşan hesapsal yapıdaki değişiklik, OU bölünmesini gerektirir ve bu bölünme genel yeniden eşleme problemine örnek teşkil eder. Genel yeniden eşleme probleminin çözümü için eklentili yeniden eşleme çizgesi oluşturarak, ÇB'ne dayalı bir model sunuldu. MeTiS ÇB aracını değiştirerek, yeniden eşleme aracı RM-MeTiS geliştirildi ve bu araç sunulan paralel DHG algoritmasında başarıyla kullanıldı. Ardışık görüntülemeler için bakış açısına bağımlı olmayan hücre gruplamasına gidilerek daha hızlı bölünebilen yeniden eşleme çizgesi oluşturuldu. Yeniden eşleme çizgesindeki düğüm ve kenarlarının ağırlık hesaplamaları için verimli ve hassas bir tahmin yöntemi geliştirildi. 24 işlemcili Parsytec CC sisteminde 22'ye varan hızlanma değerleri elde edildi. Deney sel sonuçlar, sunulan DHG algoritmasının doğrusal ölçeklenebilir olduğunu gösterdi. Anahtar Kelimeler. Paralel Doğrudan Hacim Görüntüleme, Düzensiz Izgaralar, Obje Uzayı Bölünmesi, Çizge Bölünmesi, Yeniden Eşleme, Ölçeklenebilirlik.","III ABSTRACT PARALLEL DIRECT VOLUME RENDERING OF UNSTRUCTURED GRIDS BASED ON OBJECT-SPACE DECOMPOSITION Ferit Fındık M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat October, 1997 This work investigates object-space (OS) parallelization of an efficient ray- casting based direct volume rendering algorithm (DVR) for unstructured grids on distributed-memory architectures. The key point for a successful paral lelization is to find an OS decomposition which maintains the OS coherency and computational load balance as much as possible. The OS decomposition problem is modeled as a graph partitioning (GP) problem with correct view- dependent node and edge weighting. As the parallel visualizations of the results of parallel engineering simulations are performed on the same machine. OS de composition, which is necessary for each visualization instance because of the changes in the computational structures of the successive parallel steps, con stitutes a typical case of the general remapping problem. A GP-based model is proposed for the solution of the general remapping problem by constructing an augmented remapping graph. The remapping tool RM-MeTiS, developed by modifying and enhancing the original MeTiS package for partitioning the remapping graph, is successfully used in the proposed parallel DVR algorithm. An effective view-independent cell-clustering scheme is introduced to induce more tractable contracted view-dependent remapping graphs for successive vi sualizations. An efficient estimation scheme with high accuracy is proposed for view- dependent node and edge weighting of the remapping graph. Speedup values as high as 22 are obtained on a Parsytec CC system with 24 processors in the visualization of benchmark volumetric datasets and the proposed DVR algorithm seems to be linearly scalable according to the experimental results. Key words: Parallel Direct Volume Rendering, Unstructured Grids, Object- Space Decomposition, Graph Partitioning, Remapping, Scalability."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET KARMAŞIK GEOMETRİK ORTAMLARDA GERÇEK ZAMANDA GEZİNTİ Alper Selçuk Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Bülent Özgüç Eylül, 1997 Bilgisayar Grafiği alanındaki en büyük problemlerden birisi dev boyuttaki karmaşık geometrik modelleri etkileşimli ve hızlı olarak göstermektir. Bu boyut taki modeller en iyi grafik iş istasyonlarının bile kapasitesini aşmaktadır. Mi marlık, simulasyon, bilgisayar destekli tasarım ve eğlence alanlarındaki uygula malarda, gereken etkileşimi sağlayabilmek için pek çok çalışma yapılmıştır. Bu tezde, dev geometrik modellerde gerçek zamanlı gezintiyi sağlayan yöntemler incelenmiş ve bu amaca hizmet eden bir sistem geliştirilmiştir. Sistem üçgenler den oluşan, hiyerarşik geometrik modeller üzerinde çalışmaktadır. Önce or tamdaki nesnelerin basitleştirilmiş halleri, çokgen sadeleştirme teknikleri kul lanılarak oluşturulmakta: gezinti sırasında, görüş alanı dışında kalan nesneler, kapsama kutuları kullanılarak hızlı bir biçimde ayıklanmaktadır. Nesnelerin orjinal ya da basitleştirilmiş halleri, nesnenin kameraya olan uzaklığına göre gösterilmektedir. Çok uzaktaki nesneler için sadece doku kaplanmış kapsama kutusu gösterilmektedir. Sistem orta karmaşıklıktaki modeller için gereken et kileşimli hızları sağlamakla birlikte, dev boyuttaki modeller için yavaş kalmak tadır. Anahtar kelimeler, basamaklı gösterim, görünürlük testi, geometrik ba sitleştirme, gerçek zamanlı görüntüleme.","Ill ABSTRACT REAL-TIME WALKTHROUGH OF COMPLEX ENVIRONMENTS Alper Selçuk M.S. in Computer Engineering and Information Science Supervisor: Prof. Dr. Bülent Özgûç September, 1997 One of the biggest problems in computer graphics is displaying huge geo metric models in interactive frame-rates. Such models exceed limits of best graphics workstations. A lot of work has been done for achieving the required frame-rates in architecture, simulation, computer-aided design and entertain ment applications. In this thesis, a survey of methods that enable walkthrough of huge geometric models is done and a system for walkthrough is developed. The system uses hierarchical triangulated geometric models as input. In pre processing phase, multiresolution models of objects in the scene are created using polygonal simplification techniques. During walkthrough, fast frustum culling based on bounding boxes is performed which eliminates branches of hi erarchy that are not visible to camera efficiently. Appropriate level of detail of objects are selected and displayed depending on the distance of the objects to the camera. For far nodes of hierarchy, geometric data in lower levels is ignored and textured bounding box is displayed. The system achieves interactive frame rates for moderate level models, however it is far from being interactive with huge models. Key words: level-of-detail, visibility culling, geometric simplification, real time rendering."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET TÜRKÇE İÇİN BİR HESAPSAL SÖZLÜĞÜN TASARIMI VE GERÇEKLEŞTİRİLMESİ Abdullah Kurtuluş Yorulmaz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Kemal Oflazer Şubat, 1997 Bütün doğal dil işleme sistemleri (örneğin çözümleyiciler, üreticiler, metin işaretleyiciler) dildeki kelimeler hakkında, bir sözlüğe erişmeye ihtiyaç duyarlar. Bu tezde, Türkçe'de doğal dil işleme için bir sözlük mimarisi sunulmuştur. Bir kelimenin yüzeysel hali ve kısıtlayıcı diğer özellikler içeren sorguya karşılık, sözlük, verilen kelimenin yüzeysel halinin, bu kısıtlayıcı özellikleri sağlayan her çözümü için biçimbirimsel/sözdizinsel, şekilsel ve anlamsal özellikler içeren bir özellik yapısı üretir. Sözlük, özellik temelli temsil, kalıtım ve birleştirme gibi çağdaş yaklaşımlara dayanır. İki bilgi kaynağı kullanır: bir sözcükyapısal işleyici ve Türkçe'nin bütün açık ve kapalı kelime gruplarını içeren bir kelime veritabanı. Sistem, SICStus Prolog'da kendi başına çalışabilecek ve doğal dil işleme uygulamalarında kullanılabilecek şekilde gerçekleştirilmiştir. Anahtar sözcükler: Doğal Dil İşleme, Sözlük","İİİ ABSTRACT DESIGN AND IMPLEMENTATION OF A COMPUTATIONAL LEXICON FOR TURKISH Abdullah Kurtuluş Yorulmaz M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Kemal Oflazer February, 1997 All natural language processing systems (such as parsers, generators, taggers) need to have access to a lexicon about the words in the language. This thesis presents a lexicon architecture for natural language processing in Turkish. Given a query form consisting of a surface form and other features acting as restrictions, the lexicon produces feature structures containing morphosyntactic, syntactic, and semantic information for all possible interpretations of the surface form sat isfying those restrictions. The lexicon is based on contemporary approaches like feature-based representation, inheritance, and unification. It makes use of two information sources: a morphological processor and a lexical database containing all the open and closed-class words of Turkish. The system has been implemented in SICStus Prolog as a standalone module for use in natural language processing applications. Key words: Natural Language Processing, Lexicon"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV Özet NESNESEL YAPILI BİR SORGULAMA DILI VE BİÇİMSEL BİR CEBİRE ÇEVİRİSİ Ali Gürhan Gür Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. M. Erol Arkun Eylül, 1997 İfadesel bir sorgulama yeteneği, herhangi bir veri tabanı sisteminin temel bir özelliği olarak kabul edilmiştir. Bu tezde, standart sorgulama dili SQL'in bir uzantısı, SQL/OO, nesnesel veri tabanlarını sorgulamak amacıyla önerilmiştir. Bu dil nesnesel yaklaşımın getirdiği zengin veri modeliyle ilgilenmek için ek yapılara sahiptir. SQL/OO, ifade gücü yüksek, optimizasyona açık, nesnesel bir sorgulama cebirine dayanır. Dilin sözdizimsel ve anlamsal tanımları sunul muştur. SQL/OO sorgularının nesnesel cebire eşlemesi sözdizimine dayalı bir çeviri düzeninde verilmiştir. SQL/OO sorgularını değerlendiren bir prototip sistem tasarlanmıştır. Sistem bir SQL/OO sorgusunu karşılık gelen cebirsel ifadeye çeviren bir çevirmenle başlar. Bu cebirsel ifade çözümlenerek içsel temsilci olarak kullanılacak olan nesnesel cebir ağacı (OAT) oluşturulur. OAT ağaçları sorgu iyileştiren bir bölüm için girdi ve çıktı olarak kullanılabilir. Ağaç üzerinde dolaşarak her düğüm için uygun olan cebirsel işlemi gerçekleştirecek fonksiyon çalıştırılmak suretiyle sorgu sonucu hesaplanır. Ayrıca literatürde bulunan nesnesel sorgu dillerinin genel bir özeti verilmiştir. Gerekli zemini sunmak amacıyla, bu dillerin özellikleri belirlenmiş, kıyaslanmış ve karşılaş- tırılmıştır. Anahtar kelimeler: nesnesel veri tabanı, sorgulama dili, sorgulama cebiri, SQL, çeviri.To my family, who make this, and all things, possible...","Ill Abstract AN OBJECT-ORIENTED STRUCTURED QUERY LANGUAGE AND ITS TRANSLATION TO A FORMAL ALGEBRA Ali Gürhan Gür M.S. in Computer Engineering and Information Science Supervisor: Prof. Dr. M. Erol Arkun September, 1997 A declarative query capability has been accepted as a fundamental feature of any database management system. This thesis proposes an extension of the standard query language SQL, SQL/OO, designed for querying object-oriented databases. It has additional constructs to deal with the rich data model intro duced by object-orientation. SQL/OO rests on a formal object-oriented query algebra that is highly expressive and open to optimization. Formal definitions of syntax and semantics are presented. The mapping of SQL/OO queries into object algebra is provided by a syntax-directed translation scheme. A proto type system that evaluates SQL/OO queries is designed. The system starts with a translator that translates an SQL/OO query into an equivalent object algebra expression. This algebra expression is parsed and an Object Algebra Tree (OAT) is generated which will be used as the internal representation. OAT Trees can be used as the input and output of a query optimizer module. The result of the query will be evaluated by traversing the tree and evaluating each node using proper functions that execute object algebra operations. A survey of existing object-oriented query languages in the literature is also pro vided. Their characteristics are identified, compared and contrasted, in order to present the necessary background. Keywords: object-oriented database, query language, query algebra, SQL, translation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ELEKTROMANYETİK SAÇILIM PROBLEMİNİN HIZLI MULTIPOLE ÇÖZÜMÜ PARALELİSTİRİLMESİ Ali Ayub M. Kalufya Bilgisayar ve Enformatik Mühendisliği Bölümü Yüksek Lisans Tez Yöneticisi: Doç. Cevdet Ay kanat Eylül, 1997 Elektromanyetik saçılım probleminin çözümü iV-body problemi ile modellenebilir. Bu çalışmada bu modeli kullanarak V. Rokhlin [17] tarafından önerin ve An- derson [3] tarafından geliştirilen Hızlı Multipole Algoritmasının bir uyarlama olan bir çözüm üretir : Multipolesuz Hızlı Multipole Metodu. Inelemeli bir yöntem kullanıldığından, özellikle Af-body kavram kullanilarak modellenebilen problemlerin çözümü tüm üretilmiş bir ön-şartkandirma algoritması geliştridik. Bu çalışmada, paralel hesaplama, daha enteresan ve büyük gerçek hayat problemlerini mâkûl bir zamanda ve olasi minimum hafıza alanı kullanarak çözebilmek için yukarda bahsedilen hızlımultipole metodu paralelistiren bir program geliştererek çözümü daha da ilerletmek için kullanıldı. Hızlı Multipole Metodunun paralel bir uyarlama iki boyutta elektromayetik saçılma problemini çözmek için, tekli program çoklu data yöntemi kullanilarak Parytec Coignitive Computer 24 düğümü da geliştirdi ve uygulandı. Anahtar sözcükler: AT-body kavramı, Hızlı Multipole Metodu, Seyrek Blok On-sartlandırması. iv","ABSTRACT PARALLELIZATION OF THE FAST MULTIPOLE SOLUTION OF THE ELECTROMAGNETIC SCATTERING PROBLEM Ali Ayub M. Kalufya M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat September, 1997 The solution to the electromagnetic scattering problem may be modelled as an iV-body problem. Using this model this work develops a solution that is based on a specific variant of the Fast Multipole algorithm that was proposed by V. Rokhlin[17] and modified further by Anderson[3], that is the Fast Multipole Method without multipoles. Because an iterative scheme is used, we also de- velop an preconditioning algorithm that is especially tailored for the solution of problems that may be modelled using iV-body concept. Moreover, in this work parallel computing is employed to improve the solu- tion even further by developing a program that will utilize the above mentioned fast multipole method concept in parallel so as to be able to solve even larger and more interesting real-life problems in a reasonable amount of time and using minimum possible memory space. A parallel version of the fast multipole method is developed and imple- mented on the Parystec Coignitive Computer 24 node multicomputer using the single program multiple data paradigm for solving the electromagnetic scattering problem in 2 dimensions. Key words: A^-body Concept, Fast Multipole Method, Blockwise Sparse Preconditioning m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET AKTİF BİR VERİTABANINDA TETİKLEME VE BA?LILIK ÇİZGELERİNİN EYLEM VE KURAL TANIMLAMALARI KULLANILARAK OLUŞTURULMASI Ali Şaman Tosun Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Özgür Ulusoy Temmuz, 1997 Klasik veri tabanları pasiftir ve sadece kullanıcılar veya uygulamalar tarafın dan yapılan açık isteklere cevap verebilirler. Aktif veri tabanları ise, belli eylemler gerçekleştiğinde ve belli koşullar sağlandığında işlemleri otomatik ola rak işleme koyar. Veri tabanları kuralların eklenmesi yoluyla aktif olur. Ku ral uygulamaları geliştirilmesindeki en büyük problem tasarım yöntemlerinin ve uygun tasarım araçlarının eksikliğidir. Birleşme ve bitim uygulamaların doğru geliştirmesi için gerekli iki önemli özelliktir. Bu tezde, tetikleme ve bağlılık çizgelerinin aktif bir veritabanının sınıf ve kural tanımlamalarından oluşturulması tartışılmaktadır. Bu çizgeler bitim'i kontrol edebilmemizi ve birleşme kural işlemeyi sağlar. Bu çizgeleri oluşturacak bir önişleyicinin geliş tirmesi de tezimizde gerçekleştirilmiştir. Anahtar kelimeler. Aktif Veri Tabanları, Veri Tabanlarında Kural İşleme, Statik Analiz, Birleşme, Bitim.","m ABSTRACT CONSTRUCTION OF TRIGGER AND DEPENDENCY GRAPHS USING EVENT AND RULE DECLARATIONS OF AN ACTIVE OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEM Ali Şaman Tosun M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. Özgür Ulusoy July, 1997 Traditional database systems are passive, meaning that they only react to explicit requests by users or applications. An active database system on the other hand, executes operations automatically when certain events occur and certain conditions are met. A database management system becomes active through the addition of rules. The main difficulties in the development of rule applications is the lack of design methods and suitable design tools. Conflu ence and termination are two important properties to be able to implement rule applications correctly. In this thesis, the construction of trigger and de pendency graphs using class and rule declarations of an active object-oriented database system is described. The construction of these graphs provides that termination can be checked and a confluent rule execution can be achieved. Im plementation of a preprocessor that constructs trigger and dependency graphs is also provided. Key words: Active Database Systems, Database Rule Processing, Static Analysis, Confluence, Termination."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÖZYINELI MANTIK PROGRAMLARININ TÜMEVARMSAL YOLLA SENTEZİ Serap Yılmaz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Pierre Flener Ağustos 1997 Özyineli mantık programlarının (en azından bir yantümcesi özyineli olan) tam olma yan bilgiden yola çıkılarak, mesela, girdi/çıktı örneklerinden, otomatik sentezi oldukça zor bir iştir. Ve bu iş tümevarımsal mantık programlama ile otomatik program sente zinin bir alt çalışma alanıdır. Bu tür programlar mantık programlarının çok önemli bir sınıfım oluştururlar. Yapıcı tümevarım çalışmaları göstermiştir ki özyineli program ların sentezi özyineli olmayan programların sentezinden çok daha zordur. Bu çalışma alanı ""tümevarımsal program sentezi"" diye anılır. DiALOGS-II adıyla geliştirdiğimiz sistem (bu sistemin bir önceki versiyonu Dialogs adlı sistemdir) taslak-yönetimli, interaktif ve artımsızdır. Sistem insiyatifi alıp kullanıcıyı kullanıcının dilinde sorgulayarak özyineli mantık programlan sentezler. Sistem kendisi tarafindan özyineli olarak ya da başka bir sistem tarafindan, sistem özyineli bir programın sentez inin gerekliliğini farkettiği zaman kullanılabilir. ""Özyineleme Teorisi"" sistemin içinde taslaklar tarafından etkili bir şekilde kodlandığı için sistem çok az bilgiye gerek duyar ve çok hızlı çalışır. Anahtar Sözcükler: program geliştirme, tümevarımsal mantık programlama, otomatik program sentezi, taslak yönetimli program sentezi. IV","ABSTRACT INDUCTIVE SYNTHESIS OF RECURSIVE LOGIC PROGRAMS Serap Yılmaz M.S. in Computer Engineering and Information Science Supervisor: Ass't Prof. Pierre Flener August 1997 The learning of recursive logic programs (i.e. the class of logic programs where at least one clause is recursive) from incomplete information, such as input/output examples, is a challenging subfield both of ILP (Inductive Logic Programming) and of the syn thesis (in general) of logic programs from formal specifications. This is an extremely important class of logic programs, as the recent work on constructive induction shows that necessarily invented predicates have recursive programs, and it even turns out that their induction is much harder than the one of non-recursive programs. We call this in ductive program synthesis. We introduce a system called DiALOGS-II (Dialogue-based Inductive and Abductive LOgic Program Synthesizer-II) whose ancestor is DIALOGS. It is a schema-guided, interactive, and non-incremental synthesizer of recursive logic programs that takes the initiative and queries a (possibly naive) specifier for evidence in her/his conceptual language. It can be used by any learner (including itself) that de tects, or merely conjectures, the necessity of invention of a new predicate. Moreover, due to its powerful codification of ""recursion-theory"" into program schemata and sche matic constraints, it needs very little evidence and is very fast. Keywords: program development, inductive logic programming, automatic program synthesis, schema-guided program synthesis. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET çok İşlemcili dagitik-hafizali bilgisayarlarda paralel görüntüleme algoritmaları Tahsin Mertefe Kurç Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticileri: Doç. Dr. Cevdet Aykanat and Prof. Bülent Özgüç Haziran 1997 Bu tezde dağıtık hafızalı çok işlemcili bilgisayarların ışıma yönteminin toplama metodunda, poligon görüntülemede ve hacim görüntülemede kullanımı araştırılmıştır. Toplama metodunda ele alman temel konular durum- katsayı matrisinin hesaplanması ve çözüm adımının hiperküp bağlantılı çoklu bilgisayarlarda paralel olarak yapılmasıdır. Durum-katsayı matrisinin hesaplanmasında işlemciler arası veri aktarımı her işlemcideki hafızanın durum-katsayı matrisi ve ışıma metoduyla görüntülenen ortamı oluşturan ver iler arasında paylaştırılması ile azaltılmıştır. İşlemcilerin daha verimli kullanılabilmesi için dinamik paylaştırma yöntemi uygulanmıştır. Çözüm aşamasında Scaled Conjugate- Gradient metodu başarılı bir şekilde uygulanmıştır. Gauss-Jacobi ve Scaled Conjugate- Gradient metodları için verimli paralel algoritmalar geliştirilmiştir. Durum-katsayı matrisinin hesaplanmasından sonra her işlemcide kalan sıfırdan farklı durum-katsayı değerlerinin işlemciler arasında tekrar dağıtılması ile hemen hemen ideal yük dağılımı sağlanmıştır.İ Poligon görüntüleme konusunda yapılan çalışmalarda parça uzayında paralelleştirme yaklaşımı ele alınmıştır. Parça uzayında paralelleştirmede ortamı oluşturan parçalar işlemciler arasında dağıtılır. Her işlemci kendi parçalarının üzerinde görüntüleme algo ritmalarını çalıştırır. Daha sonra her işlemcideki resimler birleştirilerek son resim ortaya çıkarılır. Bu çalışmada hiperküp bilgisayarında parça uzayında paralelleştirme algoritmaları geliştirilmiştir. Resimlerin birleştirilmesi sırasında işlemciler arasında iletişim hacmini azaltan verimli algoritmalar önerilmiştir, işlemciler arasındaki mesajların kopuk kopuk olmasını önlemek için değiştirilmiş bir görüntüleme algoritması önerilmiştir. Hacim görüntülemede ise ekran uzayında paralelleştirme yaklaşımı araştırılmıştır. Bu yaklaşımda ekran uzayı işlemciler arasında bölünür. Her işlemci kendisine ait olan ekran parçası üzerinde görüntüleme algoritmasını çalıştırtır. Ekranın bölünmesine göre hacim elemanlarıda işlemciler arasında dağıtılır. Bu çalışmada, çeşitli ekran uzayında bölme yöntemleri incelendi ve geliştirildi. Bu yöntemler ekranı hacim elemanlarının ekrandaki dağılımlarına göre bölerek daha iyi yük dağılımı sağlar. Bu yöntemlerden çizge parçalamaya dayalı bölme ve Hubert eğrisine dayalı bölme yeni yöntemlerdir. Bu yöntemler deneysel olarak karşılaştırılmıştır. Ayrıca, bu çalışmada incelenen ve geliştirilen yöntemler poligon görüntülemeye dayalı bir hacim görüntüleme algoritmasına başarı ile uygulanmışladır.","ABSTRACT PARALLEL RENDERING ALGORITHMS FOR DISTRIBUTED-MEMORY MULTICOMPUTERS Tahsin Mertefe Kurç Ph.D. in Computer Engineering and Information Science Supervisors: Assoc. Prof. Cevdet Ay kanat and Prof. Bülent Özgüç June 1997 In this thesis, utilization of distributed memory multicomputers in gathering radios- ity, polygon rendering and volume rendering is investigated. In parallel gathering radiosity, the target issues are the parallelization of the com putation of the form-factor matrix and solution phases on hypercube-connected multi- computers. Interprocessor communication in matrix computation phase is decreased by sharing the memory space between matrix elements and the scene data. A demand- driven algorithm is proposed for better computational load balance during calculation of form- factors. Gauss- Jacobi (GJ) iterative algorithm is used by all of the previous works in the solution phase. We apply more efficient Scaled Conjugate- Gradient (SCG) algorithm in the solution phase. Parallel algorithms were developed for GJ and SCG algorithms for hypercube-connected multicomputers. In addition, load balancing in the solution phase is investigated. An efficient data redistribution scheme is proposed. Thisscheme achieves perfect load balance in matrix- vector product operations in the solution phase. Object-space parallelism is investigated for parallel polygon rendering on hypercube- connected multicomputers. Briefly, in object-space parallelism, scene data is partitioned into disjoint sets among processors. Each processor performs the rendering of its local partition of primitives. After this local rendering phase, full screen partial images in each processor are merged to obtain the final image. This phase is called pixel merging phase. Pixel merging phase requires interprocessor communication to merge partial images. In this work, hypercube interconnection topology and message passing structure are exploited to merge partial images efficiently. Volume of communication in pixel merging phase is decreased by only exchanging local foremost pixels in each processor after local rendering phase. For this purpose, a modified scanline z-buffer algorithm is proposed for the local rendering phase. This algorithm avoids message fragmentation by storing local foremost pixels in consecutive memory locations. In addition, it eliminates initialization of z-buffer, which is a sequential overhead to parallel execution. For pixel merging phase, we propose two schemes referred to here as pairwise exchange scheme and all-to- all personalized communication scheme, which are suited to the hypercube topology. We investigate load balancing in pixel merging phase. Two heuristics, recursive subdivision and heuristic bin packing, were proposed to achieve better load balancing in pixel merging phase. These heuristics are adaptive such that they utilize the distribution of foremost pixels on the screen to subdivide the screen in the pixel merging phase. Image-space parallelism is investigated for parallel volume rendering of unstructured grids. In image-space parallelism, the screen is subdivided into regions. Each processor is assigned one or more subregions. The primitives (e.g., tetrahedrals) in the volume data are distributed among processors according to screen subdivision and processor-subregion assignments. Then, each processor renders its local subregions. The target topic in this work is the adaptive subdivision of the screen. Adaptive subdivision issue has not been investigated in parallel volume rendering of unstructured grids before. Only some re searchers utilized adaptive subdivision in parallel polygon rendering and ray tracing. In this work, several algorithms are proposed to subdivide the screen adaptively. The algorithms presented in this work can be grouped into two classes: 1-dimensional arraybased algorithms and 2-diraensional mesh based algorithms. Among the 2-dimensional mesh based algorithms, graph partitioning based subdivision and Hubert curve based subdivision algorithms are new approaches in parallel rendering field. An experimental comparison of the subdivision algorithms are performed on a common frame work. The subdivision algorithms were employed in the parallelization of a volume rendering algo rithm, which is a polygon rendering based algorithm. In the previous works on parallel polygon rendering, only the number of primitives in a subregion were used to approxi mate the work load of the subregion. We experimentally show that this approximation is not enough. Better speedup values can be obtained by utilizing other criteria such as number of pixels, number of spans in a region. By utilizing these additional criteria, the speedup values are almost doubled."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET METİN ANLAMSAL GÖSTERİMLERİNİN TÜRKÇE CÜMLE YAPILARINA DÖNÜŞTÜREN BİR SİSTEMİN TASARIMI VE UYGULAMASI Selman Murat Temizsoy Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. İlyas Çiçekli Ağustos, 1997 Bilgisayarla Çeviri problemine Interlingua yaklaşımı çeviri sorununu birbirinden bağımsız iki aşamada gerçekleştirmeyi amaçlar. Önce, kaynak dildeki cümlelerin anlamları doğal dilden bağımsız, yapay bir dilde temsil edilir. Sonra, hedef dildeki cümleler bu anlamsal gösterimlerden üretilir. Metin üretim görevi bu yaklaşımda üç ana aşamada gerçekleştirilir ve ikinci basamakta anlamsal gösterimden cümlenin yapısal özellikleri çıkartılır ve cümlede kullanılacak sözcükler seçilir. Bu tezde bu ikinci basamağı gerçekleştirebilecek prototip bir sistemin tasarımı ve uygulaması amaçlanmaktadır. Bu çalışmada kullanılan anlamsal gösterim olayları ve varlıkları temsil edebilmek için dünyanın sıradüzensel bir gösterimi olan ontolojiden yararlanmaktadır ve ayrıca bu gösterim anlamsal ve pragmatik özellikler için farklı yapılar kullanmaktadır. Geliştirilen sistem dilden bağımsızdır ve dile ait bilgileri üç ayrı bilgi kaynağından alır: sözlük (anlamsal ve yapısal sözcük bilgisi), dönüştürme-kuralları (anlamsal gösterimle cümle yapıları arasındaki bağlantı), ve hedef dilin yapısal özelliklerinin gösterimi. Sistem anlamsal gösterimi işlerken iki ana görevi yerine getirir: sözcük seçimi ve cümlenin iki gösterimi arasında dönüşümü. Uygulanan sistem Türkçe için geliştirilmiş küçük-ölçekli bilgi kaynaklarıyla test edildi. Bu sistemin çıktısı Türkçe için geliştirilmiş bir yüzeysel üreticinin yardımıyla amaçlanan Türkçe cümlelerin üretilmesinde kullanılabilir. Anahtar Sözcükler: Bilgisayarla Çeviri, Interlingua Yaklaşımı, Doğal Dil Üretimi, Metin Anlamsal Gösterimi, Sözdizim Yapısal Gösterimi, Ontoloji, Sözlük iv","ABSTRACT DESIGN AND IMPLEMENTATION OF A SYSTEM FOR MAPPING TEXT MEANING REPRESENTATIONS TO F-STRUCTURES OF TURKISH SENTENCES Selman Murat Temizsoy M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Ilyas Çiçekli August, 1997 Interlingua approach to Machine Translation (MT) aims to achieve the translation task in two independent steps. First, the meanings of source language sentences are represented in a language-independent artificial language. Then, sentences of the target language are generated from those meaning representations. Generation task in this approach is performed in three major steps among which the second step creates the syntactic structure of a sentence from its meaning representation and selects the words to be used in that sentence. This thesis focuses on the design and the implementation of a prototype system that performs this second task. The meaning representation used in this work utilizes a hierarchical world representation, ontology, to denote events and entities, and embeds semantic and pragmatic issues with special frames. The developed system is language-independent and it takes information about the target language from three knowledge resources: lexicon (word knowledge), map-rules (the relation between the meaning representation and the syntactic structure), and target language's syntactic structure representation. It performs two major tasks in processing the meaning representation: lexical selection and mapping the two representations of a sentence. The implemented system is tested on Turkish using small-sized knowledge resources developed for Turkish. The output of the system can be fed as input to a tactical generator, which is developed for Turkish, to produce the final Turkish sentences. Keywords: Machine Translation, Interlingua Approach, Natural Language Generation, Text Meaning Representation, Syntactic Structure Representation, Ontology, Lexicon III"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HAREKETLİ VERİ TABANI SİSTEMLERİNDE DİNAMİK ÖZNİTELİKLER İÇİN İNDEKSLEME YÖNTEMLERİ TASARIMI VE PERFORMANS ÖLÇÜMÜ Jamel Tayeb Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yard. Doç. Dr. Özgür Ulusoy Mayıs 1997 Genel amaçlı veri tabanlarını oluşturan öznitelikler, nadiren ve bazı işlemlerin uygulanması sonucu değer değişikliklerine uğradıkları için, durağan öznitelikler olarak adlandırılmaktadırlar. Zamansal veri tabanlarında, özniteliklerin deği şikliğe uğrama zamanlan da önemlidir ve her değişiklikte özniteliğin yeni bir versiyonu yaratılır. Bu tip veri tabanlarında öznitelikler durağan özniteliklere oranla genelde daha sık değişikliğe uğrarlar. Özniteliklerin, dinamik öznitelikler olarak adlandırılan bir diğer kategorisi ise, zamana bağlı olarak devamlı değişme özelliğine sahiptir ve bu nedenle, her değer değişiminin bir işlem aracılığıyla veri tabanına yansıtılması sisteme çok büyük bir yük getirecektir. Bu tezde, dinamik öznitelikler üzerinde iki değişik indeksleme metodunun performans analizi gerçekleştirilmektedir. Bu yöntemlerdeki temel fikir, doğrusal bir işlev aracılığıyla, özniteliklerin geçmişteki değerlerini de kullanarak, gelecek için ala bilecekleri değerlerin belirlenmesidir. Bu yöntemlerin araştırılması, hareketli veri tabanı yönetimi alanı temel alınarak gerçekleştirilirken, zamansal ve uzay- sal indeksleme konularından da gerekli yerlerde faydalanılmıştır. Tezin başlıca katkıları, çeşitli deneysel araştırma sonuçlarının sunulması yanında bazı mate matiksel analizlerin gerçekleştirilmesi ve elde edilen sonuçlar ışığında FP-Index olarak adlandırdığımız yeni bir indeksleme yöntemin geliştirilmesi olmuştur. Yeni yöntemin diğer indeksleme yöntemlerine olan üstünlüğü matematiksel analiz yoluyla kanıtlanmıştır. Anahtar sözcükler: Hareketli Veri Tabanı Yönetimi, Uzaysal İndeksleme, Di namik Öznitelikler. iv","ABSTRACT DESIGN AND PERFORMANCE EVALUATION OF INDEXING METHODS FOR DYNAMIC ATTRIBUTES IN MOBILE DATABASE MANAGEMENT SYSTEMS Jamel Tayeb M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Özgür Ulusoy May 1997 In traditional databases, we deal with static attributes which change very in frequently over time and their change is handled with an explicit update opera tion. In temporal databases, the time of change of attributes is also important and every update creates a new version. Attributes, typically change more frequently over time. A more agitated category of attributes are the so-called dynamic attributes whose value changes continuously over time, thus making it impractical to explicitly update them as they change. In this thesis, we conduct a performance evaluation study of two indexing methods for dynamic attributes. These are based on the key idea of using a linear function that describes the way the attribute changes over time and allows us to predict its value in the future based on any value of it in the past. The problem is rooted in the context of mobile data management and draws upon the fields of spatial and temporal indexing. We contribute various experimental results, mathemat ical analyses, and improvement and optimization algorithms. Finally, inspired by a few of the observed shortcomings of both of the studied techniques, we propose a novel indexing method which we call the FP-Index which is shown analytically to have promising prospects and to beat both methods over most performance parameters. Keywords: Mobile Data Management, Spatial Indexing, Dynamic attributes. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET WIZ - dinamik A? TABANLI DERS SUNUM SİSTEMİ Ozan Özhan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. David Davenport Ağustos, 1997 Dünya Çapındaki Bilgisayar Ağı (WWW) eğitim açısından büyük bir potansiyele sahiptir. Bu ağ sadece haberleşme imkanı sağlamakla kalmayıp, bilgilerin saklanması ve arzu edildiğinde bunlara kolayca erişilmesine de imkan vermektedir. Bu özellikleri, Dünya Çapındaki Bilgisayar Ağı'nı uzaktan eğitim için ideal hale getirmektedir. Mevcut sorunlar bilgisayar ağının sabit doğasından kaynaklanmaktadır. Bu sabit yapı kişiye uygun bireysel sunumu çok zor hale getirmektedir. Bu tez bu sorunun üstesinden gelmek için geliştirilmiş bir projeyi; WIZ sistemini sunmaktadır. WIZ sistemi Internet üzerinde etkileşimli ders sunan bir dinamik hipermedya sistemidir. Bu sistem bireye en uygun ders sunumu sağlayabilmek için her bir öğrencinin dersteki durumunu sürekli olarak takip etmektedir. Öğrenciler ders materyali içerisinde istedikleri şekilde dolaşma özgürlüğüne sahip olmakla beraber, bir tuşa basmaları tekrar birey sel (kılavuzlu) sunuma dönüşü sağlamaktadır. Bununla birlikte, WIZ sistemi, derslerde, birlikte çalışma ve tartışma ortamı sağlamak için Internet 'in iletişim olanaklarını sonuna kadar kullanmaktadır. WIZ sistemi yer ve zamandan bağımsız, hesaplı, bireysel, etkileşimli uzaktan eğitimi sağlamak için somut eğitimsel tasarım ve yazılım mühendisliği prensiplerini kullanmaktadır. Anahtar kelimeler: Uzaktan Eğitim, Internet, WWW, Akıllı Eğitim Sistemleri, Dinamik Hipermetin, Hipermedya, Kullanıcı Modelleme","III ABSTRACT WIZ - DYNAMIC WEB-BASED COURSE PRESENTATION SYSTEM Ozan Ozhan M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. David Davenport August, 1997 The educational potential of the World- Wide- Web is clear. It provides not only a means of communication, but, just as importantly, a means for storing and conveniently accessing a massive amount of information. Such character istics make it an ideal platform for distance education. One limitation however is the fixed nature of the web itself, making guidance and individualized pre sentation difficult. This thesis presents a project, the WIZ system, aimed at overcoming this problem. The WIZ system is a dynamic hypermedia system that presents interactive courses over the Internet. It monitors the progress of each student in order to provide a presentation that best matches the individ ual. Students have the freedom to browse the course material but can return back to the guided presentation by simply clicking on a button. In addition, the WIZ system exploits the communication (newsgroup and email) facilities of the Internet in order to provide a collaborative discussion environment for courses. WIZ employs sound educational design and software engineering principles to generate affordable, individualized, interactive distance education, indepen dent of time and location. Key words: Distance Education, Internet, WWW, Intelligent Tutoring Sys tems, Dynamic Hypertext, Hypermedia, User Modeling"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET İKİ SEVİYELİ DOLAYLI ÇÖZÜCÜLER VE İYİLEŞTİRİLMİŞ KRYLOV ALTUZAY YÖNTEMLERİ İLE NEREDEYSE BÖLÜNEBİLİR MARKOV ZİNCİRLERİ ÜZERİNDE DENEYLER Wail Gueaieb Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Tuğrul Dayar Haziran, 1997 iyileştirilmiş Krylov altuzay yöntemleri çoğunlukla son onbeş yılda geliş tirilmiş, başka şeyler yanında, Markov zincirlerinin durağan dağılımlarını elde etmede kullanman en son dolaylı çözücülerdir. İlgilenilen Markov zincirlerinin indirgenemez olduğu varsayılırsa, problem tekil bir katsayı matrisine sahip bir türdeş lineer cebirsel denklemler takımına bir normalleştirme şartı altında pozitif bir çözüm vektörü hesaplamaktan ibarettir. Yani, Ax = 0, |[a?||a = 1 (0.1) deki (n X l)'lik bilinmeyen durağan vektör x aranmaktadır. Burada A = I - PT n x n tekil bir M-matrisi ve P bir-adımlık rassal geçiş olasılık matrisidir. Son gelişmelere rağmen, mesleklerini icra eden başarım çözümleyicileri, yeni tasarlanmış algoritmaların başarımını var olanlarla kıyaslamak istediklerinde, veya eldeki bir sistem modelinin başarımını değerlendirmek için aday çözücülere gerek duyduklarında, hala çoğunlukla bölmeye dayanan dolaylı yöntemleri tercih etmektedirler. Esasında, Markov zincirleri, özellikle de hastalıklı neredeyse bölünebilir olanları üzerinde Krylov altuzay yöntemleri ile deneysel sonuçlar pek azdır. Biz bu alanda, özellikle de neredeyse bölünebilir Markov zincir lerinin bağlanma derecelerinin ve sıfırdan farklı yapılarının iyileştirilmiş Krylov altuzay yöntemlerinin yakınsama özellikleri ve yer gerekleri üzerindeki etkilerini anlamamıza yardım edecek araştırmalar için yer olduğuna inanıyoruz. Bazı araştırmacıların çalışmaları başka fakat ilintili yönde araştırmaları ne den olan önemli ve ilginç sorular ortaya çıkardı. Bu sorular şunlardır: ""EğerVI sistem neredeyse bölünebilirse ve (blok ardarda üst yumuşatma - SOR gibi) iki seviyeli bir dolaylı çözücü kullanılacaksa, (0.1) denklemindeki global kat sayı matrisi A 'yi nasıl parçalara ayırmalı P'nin neredeyse bölünebilir normal yapısının zorunlu kıldığı blok ayrıştırmalar diğerlerine oranla mutlaka daha mı üstündür? Alternatif ayrıştırmalara yatırım yapmaya değer mi? Hatta, durumlar sabit adlandırılıp ayrıştırıldığmda blok ardarda üst yumuşatmanın (hatta nokta ardarda üst yumuşatmanın) başarımı dolaylı birleştirme-ayrıştırma algoritmasının başarımı ile nasıl kıyaslar? Son olarak, iyileştirilmiş Krylov altuzay yöntemleri varken iki seviyeli dolaylı çözücüleri kullanmanın bir değeri var mıdır?"" Deneysel sonuçlar pek çok test vakasında iki seviyeli dolaylı çözücülerin seçilmiş iyileştiriciler için Krylov altuzay yöntemlerine göre neredeyse bölünebilir Markov zincirlerinden daha üstün olduklarını göstermektedir. İki seviyeli dolaylı çözücüler için, katsayı matrisinin basit bir ayrıştırılmasının neredeyse bölünebilir yapısının kullanılarak bulunacak bir taneden daha hızlı çözüm verdiği vakalar vardır. Anahtar kelimeler. Markov zincirleri, neredeyse bölünebilirlik, durağan dolaylı yöntemler, projeksiyon yöntemleri, blok dolaylı yöntemler, iyileştirme, hastalıklılık.","III ABSTRACT EXPERIMENTS WITH TWO-STAGE ITERATIVE SOLVERS AND PRECONDITIONED KRYLOV SUBSPACE METHODS ON NEARLY COMPLETELY DECOMPOSABLE MARKOV CHAINS Wail Gueaieb M.S. in Computer Engineering and Information Science Supervisor: Assistant Professor Dr. Tuğrul Dayar June, 1997 Preconditioned Krylov subspace methods are state-of-the-art iterative solvers developed mostly in the last fifteen years that may be used, among other things, to solve for the stationary distribution of Markov chains. Assuming Markov chains of interest are irreducible, the problem amounts to computing a pos- itive solution vector to a homogeneous system of linear algebraic equations with a singular coefficient matrix under a normalization constraint. That is, the (n x 1) unknown stationary vector x in Ax = Q, llarllj = 1(0.1) is sought. Here A - I - PT, an n x n singular M-matrix, and P is the one-step stochastic transition probability matrix. Albeit the recent advances, practicing performance analysts stili widely pre- fer iterative methods based on splittings when they want to compare the per¬ formance of newly devised algorithms against existing ones, ör when they need candidate solvers to evaluate the performance of a system model at hand. in fact, experimental results with Krylov subspace methods on Markov chains, especially the ill-conditioned nearly completely decomposable (NCD) ones, are few. We believe there is room for research in this area specifically to help us understand the effect of the degree of coupling of NCD Markov chains and their nonzero structure on the convergence characteristics and space requirements of preconditioned Krylov subspace methods.IV The work of several researchers have raised important and interesting ques- tions that led to research in anotlıer, yet related direction. These questions are the following: ""How must öne go about partitioning the global coefficient matrix A in equation (0.1) into blocks if the system is NCD and a two-stage iterative solver (such as block successive overrelaxation-SOR) is to be em- ployed? Are block partitionings dictated by the NCD normal form of P neces- sarily superior to others? Is it worth investing alternative partitionings? Better yet, for a fixed labelling and partitioning of the states, how does the perfor- mance of block SOR (ör even that of point SOR) compare to the performance of the iterative aggregation-disaggregation (IAD) algorithm? Finally, is there any merit in using two-stage iterative solvers when preconditioned Krylov subspace methods are available?"" Experimental results show that in most of the test cases two-stage iterative solvers are superior to Krylov subspace methods with the chosen precondition- ers, on NCD Markov chains. For two-stage iterative solvers, there are cases in which a straightforward partitioning of the coefficient matrix gives a faster solution than can be obtained using the NCD normal form. Key words: Markov chains, near complete decomposability, stationary iter¬ ative methods, projection methods, block iterative methods, preconditioning, ill-conditioning."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DÜZENSİZ IZGARALARIN HIZLI DİREK HACİM GÖRÜNTÜLENMESİ Hakan Berk. Bilgisayar ve Enformatik Mühendisliği. Yüksek Lisans Tez Yöneticisi: Doç. Dr. Cevdet Aykanat Eylül. 1997 Bilimsel Hesaplama her geçen gün gelişen teknoloji ile daha da önem kapan maktadır. Bilimsel uygulamalar tarafından üretilen çok miktardaki verilerin bilimadamlarınca işlenmesi ve daha kolay anlaşılabilmesi için yeni yöntemlere ihtiyaç duyulmaktadır. Üretilen verilerin yüksek miktarda olmasından dolayı bilimadamlarının bu verilerden anlamlı ve işe yarar bilgileri çıkarmaları zorlaşmaktadır. Bu yüzden bu sayısal verilerin görüntülenmesi bilimadamları için vazgeçilemez bir araçtır, ve bilgisayar grafiklerinin bu konuyla uğraşan dalma da Bilimsel Görüntüleme adı verilir. Amacı 3-boyutlu hacimsel verilerin görüntülenmesi olan Hacim Görüntüleme ise Bilimsel Görüntüleme'nin en önemli alt dallarından birisidir. Hacim Görüntüleme yöntemleri iki sınıfa ayırılabilir; Dolaylı ve Direk. Dolaylı yöntemler daha hızlıdır, fakat Direk yöntemler daha esnektir ve daha doğru sonuçlar verirler. Direk hacim görüntüleme yöntemleri de kendi içinde üçe ayrılırlar: ekran-uzayı (ışın izleme), cisim-uzayı ve karma yöntemler. Direk Hacim Görüntüleme (DHG) yöntemlerinin verimliliği daha çok noA-fa-veri tespiti ve görüntü-sıralama problemlerini ne şekilde çözdüğüne bağlıdır. Bu problemlerin çözümü düzenli ızgaralarda basit olmasına rağmen. düzensiz ızgaralarda daha zordur. Araştırmacılar düzensiz ızgaraların hacim görüntülenmesini özel grafik donanımı, yada paralel mimariler kullanarak hızlandırmaya çalışmaktadırlar, ama bu alanda yazılım çözümlerine her zaman ihtiyaç duyulacaktır. Bu tez düzensiz ızgaraların hacim görüntülenmesindeki problemlerin yazılım yöntemleri ile çözülmesi üzerinedir. Düzensiz ızgaraların hacim görüntülenmesi için olan üç ayrı kategorideki yöntemleri inceler. Tezin en önemli amaçlarından birisi değişik kategorilerdeki yöntemlerin avantaj ve dezavantajlarını saptamaktır. Bu konudaki önemli yöntemlerin tartışması.bunların dezavantajlarının ve avantajlarının belirtilmesi ile zenginleştirilmeye çalışılmıştır. Bu problemleri çözmeye yönelik üç yeni ve hızlı yöntem geliştirilmiş ve daha iyi bir karşılaştırma için olan bir yöntem detaylı bir şekilde incelenmiştir. Tezde öne sürülen tüm yöntemler doğru ve yüksek kalite resim üretmeyi amaç edinmişlerdir. Bunlardan iki tanesi tamamen ışın-izleme üzerine geliştirilmiş yöntemler olup, erken ışın sonlanmasını desteklemekte ve döngüsel ızgaraları görüntüleyebilmektedirler. Bu yöntemlerin göreceli performansları geniş bir veri kümesi üzerinde deneysel olarak ölçülmüş ve olan en iyi DHG yöntemlerinden daha hızlı oldukları sonucuna varılmıştır. Anahtar Kelimeler. Hacim Görüntüleme. Direk Hacim Görüntüleme. Düzensiz Izgara, Işın izleme.","Ill ABSTRACT FAST DIRECT VOLUME RENDERING OF UNSTRUCTURED GRIDS Hakan Berk M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Ay kan at September, 1997 Scientific Computing has become more and more important with the evolv ing technology. The vast amount of data that the scientific computing applica tions produce need new ways to be processed and be interpreted by scientists. The large amount of data makes it very difficult for scientists to extract useful information from the data, and interpret it to reach a useful conclusion. Thus. visualization of such numerical data as an image, which is named as Scientific Visualization, is an indispensable tool for researchers. Volume Rendering is a very important branch of Scientific Visualization and makes it possible for scientists to visualize the 3-dimensional (3D) volumetric datasets. \ olume Rendering algorithms can be classified into two categories: Indirect and Direct methods. Indirect methods are faster, but direct methods are more flexible, and accurate. Direct methods can be classified into three categories: image-space (ray-casting), object-space (projection) and hybrid. The efficiency of a direct volume rendering (DVR) algorithm is strongly related to the way that it solves the underlying point location and view sort problems. Although these problems are almost trivial ones to solve in structured grids, they be come more complex ones to deal with for unstructured grids. Researchers have tried to speed up the volume rendering of unstructured grids by using spe cial graphics hardware, and parallel architectures, but the need for software solutions to these problems will always exist. This thesis is involved in solv ing those problems in unstructured grids via software methods. It investigates three distinct categories, namely image-space methods, object-space methods and hybrid methods for fast direct volume rendering of unstructured grids.IV The main objective of the thesis is to identify the relative superiorities and inferiorities of the algorithms in these three categories. A survey of existing methods is enriched by a discussion of their merits and shortcomings. Three new and fast algorithms to overcome the existing inefficiencies are proposed, and one existing algorithm is investigated in detail for a better comparison. All of the proposed algorithms are aimed at producing correct, high quality images. Two of the proposed algorithms are pure ray-casting based solutions that support early ray termination and can handle cyclic grids. The relative performances of the proposed algorithms are experimented on a wide range of benchmark grids in a common framework for software methods and they are found to be faster than the existing best DVR algorithms. Key words: Volume Rendering, Direct Volume Rendering (DVR). Unstruc tured Grid, Volume Visualization. Ray Casting."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DÜZENSİZ IZGARALARIN ÖNCE-SIRALA ALGORİTMASI KULLANARAK PARALEL HACİM GÖRÜNTÜLENMESİ İÇİN EKRAN UZAYI BÖLÜMLEME ALGORİTMALARI Hüseyin Kutluca Bilgisayar ve Enformatik Mühendisliği Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Cevdet Aykanat Ağustos, 1997 Bu tezde görüntü uzayı bölümleme algoritmaları önerilmiş ve bu algoritmalar dan paralel doğrudan hacim görüntüleme algoritması için yararlanılmıştır. Hacim elemanlarının kapsama kutluları onların ekrandaki kapladığı alanı yaklaşık olarak belirlemek için kullanılır. Bir bölgedeki kapsama kutusu sayısı o bölgenin iş yükü olarak kullanılmıştır. Kesin model adında yeni bir iş yükü yöntemi önerilmiştir. Bu yöntem dikdörtkensel bir bölgedeki kapsama kutusu sayısını 0(1) zamanında bulmak için kullanılır. Zincir üzerinde zincir parçalama algoritmasından önerilen bazı bölümleme algoritmalarının yük denkliği için yararlanılmıştır. Toplanmış alan tablosu yönteminden daha etkin eniyi kesikli (jagged) bölümleme ve yineli doğrusal bölümleme algoritmaları için yararlanılmıştır. Bu iki 2-boyutlu bölümleme algoritmasından kesin model yöntemi kullanarak görüntü uzayı bölümlemesi için yararlanılmıştır. Aynı zamanda, ters alan sezgisel algoritması kulanan yeni ekran-uzayı bölümleme algoritmaları önerilmiştir. Ortancamn-ortajacası yöntemini kullanan dikey özyineli bölme algoritması ekran üzerine yerleştirilmiş düzenli ızgara ve dörtlü ağaca uygulanmıştır. Hubert uzay doldurma eğriside görüntü uzayı bölümleme için kullanılmıştır. 12 görüntü uzayı algoritması deneysel olarak aynı ortamda yük denkliği, paylaşılan hacim eleman ları sayısı ve algoritmaların çalışma zamanı açısından irdelenmiştir. Anahtar kelimeler, paralel bilgisayar grafiği uygulamaları, hacim görüntüleme, önce-sırala türü görüntüleme, görüntü uzayı paralel hacim görüntüleme, görüntü uzayı bölümleme, yük denkliği. iv","ABSTRACT IMAGE-SPACE DECOMPOSITION ALGORITHMS FOR SORT-FIRST PARALLEL VOLUME RENDERING OF UNSTRUCTURED GRIDS Hüseyin Kutluca M. S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat August, 1997 In this thesis, image-space decomposition algorithms are proposed and utilized for parallel implementation of a direct volume rendering algorithm. Screen space bounding box of a primitive is used to approximate the coverage of the primitive on the screen. Number of bounding boxes in a region is used as a workload of the region. Exact model is proposed as a new workload array scheme to find exact number of bounding boxes in a rectangular region in 0(1) time. Chains-on-chains partitioning algorithms are exploited for load balancing in some of the proposed decomposition schemes. Summed area table scheme is utilized to achieve more efficient optimal jagged decomposition and iterative rectilinear decomposition al gorithms. These two 2D decomposition algorithms are utilized for image-space decomposition using the exact model. Also, new algorithms that use inverse area heuristic are implemented for image-space decomposition. Orthogonal recursive bisection algorithm with medians of medians scheme is applied on regular mesh and quadtree superimposed on the screen. Hubert space filling curve is also ex ploited for image-space decomposition. 12 image-space decomposition algorithms are experimentally evaluated on a common framework with respect to the load balance performance, the number of shared primitives, and execution time of the decomposition algorithms. Key words: parallel computer graphics application, volume rendering, sort-first rendering, image-space parallel volume rendering, image-space decomposition, load balancing. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET İMZA FİLTRELEME ve ÇOK BOYUTLU İNDEKSLEMEYLE ŞEKİL ARAMA Çağlar Günyaktı Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Prof. Dr. M.Erol ARKUN Temmuz, 1997 Çoklu ortam bilgilerinin içeriği değişken ve öznel yorumlamaya yatkındır. Bu durum indeksleme ve içeriğe dayalı aramayı zorlaştırır. Bu tezde, ar tan boyut sayısıyla düşen performans sorunu, yeterli ve etkili sorgulama için sorgu arayüzleri ve içerik özelliklerinin sınıflandırılması gibi şekil içerikli veri tabanlarının sorunları araştırılmıştır. Özellikle, şekillerinin özelliklerinin ifadelendirilmesi, içeriğe dayalı şekil sorgulanması ve etkili arama için çok boyutlu indeksleme yapıları araştırılmıştır, içeriğe dayalı sorgulama için farklı bir bakış açısı önerilmiş ve İnternetten ulaşılabilinen İŞA (imzaya dayalı Şekil filtrelenmesi ve Araması/Signature based image filtering and search) adlı prototip şekil arama mekanizması önerilen çözümlerin bir kışımı kullanılarak gerçekleştirilmiştir. İŞA'da şekil imzaları, temel şekil özellikleri (renk, biçim, ve dokum) kullanılarak hesaplanır. Bu imzalar sadece şekil içeriğini bir bütün olarak değil alt nesneler ve onların şekil içindeki konumlarını da tanımlar, imzalar, TV-ağacı diye bilinen çok boyutlu bir indeksleme yapısı kullanılarak arama uzayının filtrelenmesi ve taranması için kullanılır. İŞA şekillerin temel özelliklerinin sorgulanmasını sağlar ve istenen görüntüsel bilgilere erişimi hızlandırmaya yardım etmek için eşlenen özellikleri kullanıcıya geri bildirir. İŞA'nın kullanıcı arayüzü kullanılarak yapılan içeriğe dayalı arama yeteneği, şekil özelliklerinin tek başlarına kesinlikle belirtmelerinin imkansız olduğu kriterlere göre de aranmasını sağlamak için ek olarak anahtar kelime tabanlı sorgulama desteği ile artırılmıştır. Anahtar kelimeler. Şekil Veri Tabanları, içeriğe dayalı sorgulama, çok boyutlu indeksleme, şekil imzalan, Bilgi Erişimi.","m ABSTRACT IMAGE SEARCHING WITH SIGNATURE FILTERING AND MULTIDIMENSIONAL INDEXING Çağlar Günyaktı M.S. in Computer Engineering and Information Science Supervisor: Prof. Dr. M.Erol ARKUN July, 1997 The content of multimedia information is conducive to variable and subjec tive interpretation which makes indexing and content-based searching a difficult task. This thesis addresses such image database issues as performance degrada tion problem in indexing with the increase in the number of dimensions, query interfaces for efficient and effective querying and content-based feature cate gorization. In particular, image feature representation, content-based image retrieval and multi-dimensional indexing for efficient searching are surveyed. A different approach for content-based querying is proposed and a prototype of an image search engine, called SIS (Signature based Image Filtering and Search), that is accessible via Internet, is implemented using the subset of the proposed solutions. In SIS, image signatures are calculated using basic image features (color, shape and texture). These signatures describe not only the im age content as a whole, but also the subobjects and their orientations residing in the image. Signatures are used for filtering the search space, by employing a multidimensional indexing structure known as TV-tree. SIS utilizes basic image feature queries and reports back the matching features to help acceler ate the navigation towards the required visual information. A content-based search via SIS's user interface is additionally augmented with keyword-based queries to facilitate searching by criteria which are impossible to specify by image features alone. Key words: Image databases, content-based querying, multi-dimensional indexing, image signatures, Information Retrieval"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DO?RUSAL PROGRAMLARIN PARALEL ÇÖZÜMLEME İÇİN BÖLÜNMESİ Ali Pınar Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Cevdet Aykanat Temmuz, 1996 Birçok güncel araştırma büyük ölçekli problemlerin matrislerinde sıkça rast lanan seyreklikten daha iyi yararlanmaya dayalıdır. Bu araştırma, seyrek bir ma trisi belli sayıda eşit büyüklükte bloklardan oluşan blok açısal duruma çevirmek için değişik metodlar önermektedir. Bu problemin önemli bir uygulaması doğrusal programlamadadır. Doğrusal programlamada, varolan blok açısal yapılan kul lanan birçok çözüm yöntemi önerilmiştir. Ama bu yöntemler yalnızca varolan blok açısal duruma dayandıkları için ölçeklendirme sorunuyla karşı karşıyadırlar. Bu çalışma bölünme için iki hiperçizge modeli öneriyor, ve bu modeller prob lemi iyi bilinen hiperçizge parçalama problemine indirgiyor. Önerilen bir diğer model ise çizge modeli, ve bu model de problemi düğüm ayıracıyla çizge parçalama problemine indirgiyor. Önerilen modeller, çok sayıda çok büyük ölçekli matris leri bölmede denendi. Hem çözüm kalitesi, hem de zaman açısından çok çekici sonuçlar elde edildi. Anahtar sözcükler: Seyrek Matris, Block Açısal Durum, Hiperçizge Parçalama, Düğüm Ayıracıyla Çizge Parçalama iv","ABSTRACT DECOMPOSING LINEAR PROGRAMS FOR PARALLEL SOLUTION Ali Pmar M. S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat July, 1996 Many current research efforts axe based on better exploitation of sparsity - common in most large scaled problems - for computational efficiency. This work proposes different methods for permuting sparse matrices to block angular form with specified number of equal sized blocks for efficient parallelism. The problem has applications in linear programming, where there is a lot of work on the so lution of problems with existing block angular structure. However, these works depend on the existing block angular structure of the matrix, and hence suf fer from unscalability. We propose two hypergraph models for decomposition, and these models reduce the problem to the well-known hypergraph partitioning problem. We also propose a graph model, which reduces the problem to the graph partitioning by node separator problem. We were able to decompose very large problems, the results are quite attractive both in terms solution quality and running times. Key words: Sparse Matrices, Block Angular Form, Hypergraph Partitioning, Graph Partitioning by Node Separator in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Çoklu veritabanı alanında yapılan çalışmalar genellikle hareketlerin yönetimi ve kontrolü üzerine yoğunlaşmıştır. Klasik hareket yönetimi tekniklerini çoklu veritabanı sistemlerinde uygulamak, sisteme katılan yerel veritabanlarının dışarıdan kontrol edilemeyişi ve heterojenliğinden dolayı çok zordur. Bu tez çalışmasında, çoklu veritabanları ile ilgili, birçok genişletilmiş hareket modellerinin anlamsal özelliklerini ve formatını içeren yeni bir hareket modeli sunul maktadır. Önerilen model iç içe geçmiş hareket modellerini, alt hareketlerin bağımlılıklarını ve bağımsız sona erebilen hareket biçimlerini kapsamaktadır. Sunulan işletim modeli yerel veritabanları hakkında herhangi bir öngörü gerektirmemektedir. Önerilen hareket modelinin performans değerlendirilmesi için ayrıca detaylı bir simulasyon modeli de sunulmuştur. Simulasyon modeli kullanılarak, klasik hareket modeli ile beraber önerilen modelin performans değerlendirmeleri yapıl mıştır. Yeterli sistem kaynakları olduğunda, önerilen hareket modelinin, klasik hareket modelinden çok daha iyi sonuçlar verdiği gözlemlenmiştir. Yapılan deneylerde, çoklu veritabanı hareketlerinin yerel hareketler üzerindeki etkisi de incelenmiştir. Anahtar sözcükler. Çoklu veritabanı sistemleri, dağıtık veritabanları, hare ket modelleri, performans değerlendirmeleri. iv","Most work in the multidatabase systems (MDBSs) area has focused on the issues of transaction management and concurrency control. It is difficult to implement traditional transaction management techniques in a MDBS due to the heterogeneity and autonomy of the connected local sites. In this thesis, we present a new transaction execution model that captures the formalism and semantics of various extended transaction models and adopts them to a MDBS environment. The proposed model covers nested transactions, various dependency types among sub-transactions, and commit-independent transac tions. The execution model does not make any assumption regarding the con currency control protocols executed at the local sites connected to the MDBS. We also present a detailed simulation model of a MDBS to analyze the performance of the proposed model. The performances of both the traditional transaction model and the proposed transaction model are evaluated under a range of workloads and system configurations. The performance impact of global transactions' behavior on local transactions is also discussed. Keywords: Multi databases systems, distributed databases, transaction mo dels, performance evaluation. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Aktif veri tabanı yönetim sistemleri alanında pek çok çalışma yapılmış olmasına karşın, bu sistemlerin performanslarının değerlendirilmesine dair sadece bir kaç çalışma vardır, ve halen bir aktif veri tabanı yönetim sistemi performansının değerlendirmesinin sistematik olarak nasıl yapılacağı açık değildir. Bu tezde nesne yönelimli aktif veri tabam yönetim sistemleri için OBJEC TIVE Değerlendirmesi 'ni tanımlıyoruz, ve bu değerlendirmenin bir aktif veri tabanı sisteminde gerçekleştirilmesinden elde edilen sonuçları sunuyoruz. OB JECTIVE bir aktif veri tabanı yönetim sisteminin performans dar boğazları nı ve aktif fonksiyonlarını belirlemek, ve birden çok aktif veri tabanı yöne tim sisteminin performanslarını karşılaştırmak için kullanılabilir. OBJEC- TIVE'in amacı aktif fonksiyonları sağlayan parçaları ayırmak, ve diğer faktörlerin etkilerini en aza indirgeyerek sadece bu parçaların performansları üzerinde yoğunlaşmaktır. Anahtar sözcükler: Aktif veri tabanı sistemleri, veri tabanı değerlendirmesi, nesne yönelimli veri tabanı sistemleri. iv","Although much work in the area of Active Database Management Systems (ADBMSs) has been done, there have been only a few attempts to evaluate the performance of these systems, and it is not yet clear how the performance of an active DBMS can be evaluated systematically. In this thesis, we describe the OBJECTIVE Benchmark for object-oriented ADBMSs, and present experimental results from its implementation in an ac tive database system prototype. OBJECTIVE can be used to identify per formance bottlenecks and active functionalities of an ADBMS, and compare the performance of multiple ADBMSs. The philosophy of OBJECTIVE is to isolate components providing active functionalities, and concentrate only on the performance of these components while attempting to minimize the effects of other factors. Key words: Active database systems, database benchmarks, object-oriented database systems. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SİSTEMİK-FONKSİYONEL GRAMER YAKLAŞIMI İLE TÜRKÇE METİN ÜRETİMİ Turgay Korkmaz Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. İlyas Çiçekli Haziran, 1996Doğal Dil Üretimi (DDÜ) kabaca iki kışıma ayrılır: metin planlama ve metin üretme. Metin planlama kısımmda, kavramsal girdilerden metinin anlamsal tanımı üretilir. Sonra, metin. üretme sistemi bu anlamsal tanımları gerçek bir metine dönüştürür. Bu tez metin planlamadan ziyade Türkçe metin üretecek bir sisteminin tasarım ve gerçekleştirimi üzerinde durmaktadır. Bir metin üretici geliştirmek için, doğal dilin kaynaklarını tanımlayacak bir dilbilim teori sine, ve bu kaynaları bilgisayar ortamında gösterecek ve işleyecek bir yazılım aracına ihtiyacımız vardır. Bu tezde, Sistemik-Fonksiyonel Gramer (SFG) olarak bilinen fonksiyonel dilbilim teorisini, ve yazılım aracı olarak da FUF metin üretme sistemini kullandık. Gerçekleştirilen metin üretim sistemi metinin anlamsal tanımını cümle cümle alıyor, ve cümledeki her bir sözcüksel öğenin şekil bilgisini üretiyor ki bunlar Türkçe sözcüklerin şekil bilgilerinden kelimeler üreten bir program tarafından kelimeleştirilmektedir. Metin üretimi üzerinde yoğunlaşmamızdan ötürü, metin planlama kısmını ayrıntılı olarak incelemedik. Bu yüzden, metinin anlamsal tanımının bir uygulama tarafından üretildiğini ve sözcüklendirildiğini kabul ediyoruz (şu an elle veriliyor). Anahtar sözcükler. Doğal Dil İşleme, Doğal Dil Üretimi, Bilgisayarlı Dilbil imi, Sistemik-Fonksiyonel Gramer, Fonksiyonel Birleştirme Grameri. iv","ABSTRACT TURKISH TEXT GENERATION WITH SYSTEMIC-FUNCTIONAL GRAMMAR Turgay Korkmaz M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Dr. Ilyas Çiçekli June, 1996 Natural Language Generation (NLG) is roughly decomposed into two stages: text planning, and text generation. In the text planning stage, the semantic description of the text is produced from the. conceptual inputs. Then, the text generation system transforms this semantic description into an actual text. This thesis focuses on the design and implementation of a Turkish text generation system rather than text planning. To develop a text generator, we need a linguistic theory that describes the resources of the desired natural language, and also a software tool that represents and performs these linguistic resources in a computational environment. In this thesis, in order to carry out the mentioned requirements, we have used a functional linguistic theory called Systemic-Functional Grammar (SFG), and the FUF text generation system as a software tool. The ultimate text generation system takes the semantic description of the text sentence by sentence, and then produces a morphological description for each lexical constituent of the sentence. The morphological descriptions are worded by a Turkish morphological generator. Because of our concentration on the text generation, we have not considered the details of the text planning. Hence, we assume that the semantic description of the text is produced and lexicalized by an application (currently given by hand). Keywords: Natural Language Processing, Natural Language Generation, Computational Linguistic, Systemic-Functional Grammar, Functional Unifica tion Grammar. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tezde, gerçekleştirimi serbest öğe düzenine sahip bir dil olan Türkçe için bir yüzeysel üretici sunulmaktadır. Bir cümlenin bilgi yapısıyla ilgili herhangi bir bilginin (başlık, odak, arka plan, v.b.g.) olmaması durumunda, tümce öğeleri öngörülen bir sıraya uyarlar. Ancak bu sıra, tümcenin akışı veya konuşmanın sınırlamalarına göre şerbetçe değişebilir. Öğelerin sırasındaki değişiklikleri üretmek için sağ doğrusal gramer ile gerçekleştirilmiş bir öz yinelemeli sonlu durum makinesi kullanılmıştır. Gerçekleştirme ortamımız, Carnegie Mellon Üniversitesi - Center for Machine Translation'da (CMU - CMT) geliştirilen GenKit sistemidir. Biçimbirimsel gerçekleştirme, dışarıdan çağırılan, somut biçimbirim seçimini ve biçimbirimsel değişmeleri sağlayan biçimbirimsel bir üretim sistemi kullanılarak gerçekleştirilmiştir. Anahtar sözcükler: Doğal Dil Üretimi, Serbest Öğe Sıralı Diller, Gerçekleştirme, Gramer Teorisi.","This thesis describes a tactical generator for Turkish, a free constituent or der language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state ma chine for handling the changes in constituent order, implemented as a right- linear grammar backbone. Our implementation environment is the GenKit sys tem, developed at Carnegie Mellon University-Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes. Key words: Natural Language Generation, Free Constituent Order Language, Realization, Grammar Theory."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET AYRIK ÖZNİTELİK BÖLÜNTÜLERİNİ TOPLU Ö?RENME Aynur Akkuş Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Doç. Dr. Halil Altay Güvenir Eylül, 1996 Bu tezde öznitelik izdüşümlerine dayalı yeni öğrenme algoritmaları sunulmuş tur. Öznitelik Bölüntülerini Öğrenme (FİL) olarak isimlendirilen bu algorit malar toplu, denetimli ve tümevarımsal öğrenme yöntemlerini kullanırlar ve öğrenme örneklerinin öznitelik izdüşümlerini sınıflama bilgisini çıkarmak için kullanırlar. Bu izdüşümler ayrık öznitelik bölüntülerine genellenir. Böylece, öğrenilen kavram tanımları her öznitelik için ayrık öznitelik bölüntüleri şeklinde gösterilir. Daha önce görülmemiş bir örneğin sınıflandırması için her öznitelik tarafından bir ön sınıflandırma yapılır ve son sınıflama bu ön sınıflandırmaların ağırlıklı çoğunluk oylamasıyla belirlenir. Hatalı örnekleri tespit edebilmek için bölüntülere ağırlık verilerek bazı değişiklikler önerilmiştir. FİL algoritmalarının benzer sistemlerle uygulama sonuçları doğal ve yapay veri kümeleri üzerinde karşılaştırılrmştır. Bu algoritmaların doğruluk oranları daha öncekilere yakın olmasına rağmen ortalama çalışma süreleri çok daha azdır. Bu tezde literatürde yaygın olarak bilinen k en yakın komşu sınıflandırma algoritması (&-NN) yeniden tanımlanmıştır ve &-NNFP, öznitelik izdüşümleri üzerinde k en yakın komşu sınıflandırması, olarak isimlendirilmiştir. k-NNFP algoritmasında sınıflandırma her öznitelikten gelecek olan tahminler arasından çoğunluk oylaması yapılarak belirlenir. &-NNFP ve fe-NN algoritmalarının karşılaştırılması doğal ve yapay veri kümeleri üzerinde yapılmıştır. Anahtar Sözcükler: öğrenme, tümevarımsal öğrenme, toplu öğrenme, dene timli öğrenme, öznitelik izdüşümleri, oylama. iv","ABSTRACT BATCH LEARNING OF DISJOINT FEATURE INTERVALS Aynur Akkuş M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Halil Altay Güvenir September, 1996 This thesis presents several learning algorithms for multi-concept descriptions in the form of disjoint feature intervals, called Feature Interval Learning algo rithms (FIL). These algorithms are batch supervised inductive learning algo rithms, and use feature projections of the training instances for the representa tion of the classification knowledge induced. These projections can be general ized into disjoint feature intervals. Therefore, the concept description learned is a set of disjoint intervals separately for each feature. The classification of an unseen instance is based on the weighted majority voting among the local predictions of features. In order to handle noisy instances, several extensions are developed by placing weights to intervals rather than features. Empirical evaluation of the FIL algorithms is presented and compared with some other similar classification algorithms. Although the FIL algorithms achieve compa rable accuracies with other algorithms, their average running times are much more less than the others. This thesis also presents a new adaptation of the well-known &-NN clas sification algorithm to the feature projections approach, called &-NNFP for k-Nearest Neighbor on Feature Projections, based on a majority voting on in dividual classifications made by the projections of the training set on each feature and compares with the fc-NN algorithm on some real-world and artifi cial datasets. Keywords: machine learning, supervised learning, inductive learning, batch learning, feature projections, voting. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET DE?İŞİK BİLGİ KAYNAKLARI KULLANARAK BİÇİMBİRİMSEL BİRİKLEŞTİRME Gökhan Tür Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Kemal Oflazer Temmuz, 1996 Bu tezde, karmaşık biçimbirimli dillerde (özellikle üretken yapım ve çekim eklerine sahip çekimli ve bitişken dillerde) uygulanabilecek, kurallara dayanan bir biçimbirimsel birikleştirme yaklaşımı sunulmaktadır. Türkçe gibi karmaşık biçimbirimsel yapıya sahip dillerde, otomatik biçimbirimsel birikleştirme, kelimelerin, doğru yapım ve çekim eklerini içeren biçimbirimsel çözümlerini seçmeyi amaçlar. Bu çalışmada gerçekleştirilen sistem, metinlerden bağımsız olarak elle oluşturulmuş kuralları, öğrenilmiş kuralları, ve birikleştirilecek metinden elde edilen ek istatistiksel bilgileri kullanarak biçimbirimsel birikleştirme işlevini ger çekleştirmektedir. Elle oluşturulmuş kurallar, anma'dan (recall) fedakarlık et meden duyarlılığı (precision) artıracak şekilde düzenlenen dilbilimsel kurallardan meydana gelmiştir. Sistemin tasarımının çıkış noktası, Brill'in dönüşüm- sel yaklaşımının Türkçe gibi dillerde direkt olarak uygulanamayacağı gözlemi olmuştur. Ayrıca bilinmeyen kelimelerin çözümlenmesinde, ikinci bir biçimbirimsel işlemci kullanılarak ve kelimelerdeki olası yapım ve çekim ekleri belirlenerek çözümlemesi yapılmıştır. Bu yaklaşım sayesinde, deneylerde kullanılan metinlerdeki kelimelerin %1'inden çok daha azı çözümsüz kalmıştır. Elle oluşturulmuş ve öğrenilmiş kurallar ile istatistiki bilgilerin birleştirilmesi sayesinde üzerinde deney yaptığımız metinlerde kelime başına 1.02-1.03 çözüm düşerken %96-%97 anma ve buna karşılık %93-%94 duyarlılık sağlanmıştır. Anahtar sözcükler: Doğal Dil İşleme, Biçimbirimsel Birikleştirme, İşaretleme, Metinsel Dilbilimi, Otomatik Öğrenme","Ill ABSTRACT USING MULTIPLE SOURCES OF INFORMATION FOR CONSTRAINT-BASED MORPHOLOGICAL DISAMBIGUATION Gökhan Tür M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Kemal Oflazer July, 1996 This thesis presents a constraint-based morphological disambiguation approach that is applicable to languages with complex morphology-specifically agglutina tive languages with productive inflectional and derivational morphological phe nomena. For morphologically complex languages like Turkish, automatic morpho logical disambiguation involves selecting for each token morphological parse(s), with the right set of inflectional and derivational markers. Our system com bines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statisti cal information obtained from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve pre cision without sacrificing recall. In certain respects, our approach has been motivated by Brill's recent work [6], but with the observation that his trans formational approach is not directly applicable to languages like Turkish. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown. With this approach, well below 1% of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted, statistical and learned information sources, we can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%, and ambiguity of 1.02 to 1.03 parses per token. Key u?onZs:Natural Language Processing, Morphological Disambiguation, Tag ging, Corpus Linguistics, Machine Learning"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET VERİTABANI SİSTEMLERİNDE SORGULARARASI OPTİMİZASYON METODLARININ GELİŞTİRİLMESİ VE İNCELENMESİ Yiğit Kulabaş Bilgisayar ve Enformatik Mühendisliği Bölümü - Yüksek Lisans Danışman : Yrd. Doç. Özgür Ulusoy Ocak 1996 Çok kullanıcılı veri tabanı sistemlerinde, yakın zamanlı olarak birden fazla sorgunun, değişik kullanıcılar tarafından sisteme yüklenmesi sıkça rastlanan bir durumdur. Bu sorgular ortak işlem ve/veya verilere sahip olabilirler. Çalışmamızda, bu ortak işlem ya da verilerin, sorgular tarafından paylaşılması amacıyla bazı sorgulararası en iyileme (optimizasyon) teknikleri geliştirmiş bulunmaktayız. Bu çalışmalar sırasında daha çok birleştirme (join) işlemine odaklandık. Birleştirme işleminde ise şekil olarak dağıtım kodlaması (hash) tekniğini kullandık. Geliştirilen metodların bir kısmı dağıtım kod tablolarının oluşturulması, bir kısmı bu tabloların sorgulanması, bir kısmı ise belleğin verimli olarak kullanılmasını sağlamaktadır. Çalışma kapsamında aynca bu metodların performansları bir benzetim modelinin üzerinde karşılaştırılmıştır. Karşılaştırmalar sonucunda en çok dikkat çeken nokta ise, geliştirilen bütün tekniklerin, sorguların birer birer ele alınması tekniğine göre oldukça önemli üstünlükler göstermesi olmuştur. Anahtar Kelimeler: Sorgulararası en iyileme, birleştirme işlemi, inşa-etme aşaması, sorgulama aşaması, bellek temizleme aşaması. iv","ABSTRACT DEVELOPMENT AND EVALUATION OF INTER-QUERY OPTIMIZATION HEURISTICS IN DATABASE SYSTEMS Yiğit Kulabaş M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Özgür Ulusoy January, 1996 In a multi-user database system multiple queries can be issued by different users at about the same time. These queries may have some common operations and/or common relations to process. In our work, we have developed some inter-query optimization heuristics for improving the performance by exploiting the common relations within the queries. We have focused mostly on the join operation, with the build and probe phases. Some of the proposed heuristics are for the build phase, some for the probe phase, and finally some for the memory flush operation. The performance of the proposed heuristics is studied using a simple simulation model. We show that the heuristics can provide significant performance improvements compared to conventional scheduling methods for different workloads. Keywords: Inter-query optimization, join operation with build and probe phases, memory flush operation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"IV ÖZET PARALEL İÇ İÇE YUVALANMIŞ HAREKETLERİN AKTİF VERİ TABANI KURALLARININ İŞLEME KONMASINDA UYGULANMASI Yücel Saygın Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Özgür Ulusoy Eylül, 1996 Klasik veri tabanlarında, hareketler veya sorgular kullanıcının talebine karşılık işleme konur; buna karşılık, aktif veri tabanları, belli olayların sinyal edilmesi sonucu işleme konacak eylemlerin kullanıcı tarafından belirlenmesine izin verir. Aktif veri tabanları, aktif özelliklerini kurallar sayesinde gösterir. Kuralların işleme konması aktif veri tabanının önemli bir parçasıdır, ve tüm sistemin performansını etkileyebilir. İç içe yuvalanmış hareket modeli, bir kural işleme modeli olarak önceden sunulmuştur, iç içe yuvalanmış hareket modeli, normal hareket modelinden farklı olarak, hareketlerin içinde başka hareketler başlatılmasına izin verir, böylece bir hareket hiyerarşisi oluşur. Bu tezde paralel iç içe yuvalanmış hareketlerin uygulanmasından bahsedilmektedir. Paralel iç içe yuvalanmış hareketlerde, hiyerarşinin içindeki bütün hareketler paralel olarak çalışabilir ve bu şekilde sistemin verimliliği arttırılmış olur. Paralel iç içe yuvalanmış hareketlerin uygulanması OpenOODB'nin düz hareket modeli genişletilerek gerçekleştirilmiştir. Solaris thread'leri, hareketlerin paralel çalışması amacıyla kullanılmıştır. ACTA adlı formal çerçeve yapısı kullanılarak, sunulan çalışma modeli formal olarak açıklanmıştır. Anahtar kelimeler. Aktif Veri Tabanları, İç İçe Yuvalanmış Hareketler, işleme koyma modeli, Solaris Thread'leri, kural işleme, ACTA.","Ill ABSTRACT IMPLEMENTATION OF PARALLEL NESTED TRANSACTIONS FOR NESTED RULE EXECUTION IN ACTIVE DATABASES Yücel Saygın M.S. in Computer Engineering and Information Science Supervisor: Asst. Prof. Dr. Özgür Ulusoy September, 1996 Conventional, passive databases, execute transactions or queries in response to the requests from a user or an application program. In contrast, an Active Database Management System (ADBMS) allows users to specify actions to be executed when some specific events are signaled. ADBMSs achieve this fea ture by means of rules. Execution of rules is an important part of an ADBMS which may affect the overall performance of the system. Nested transactions are proposed as a rule execution model for ADBMSs. The nested transaction model, in contrast to fiat transactions, allows transactions to be started inside some other transactions forming a transaction hierarchy. In this thesis, imple mentation issues of parallel nested transactions, where all the transactions in the hierarchy may run in parallel, are discussed for parallel rule execution in ADBMSs. Implementation of nested transactions has been performed by ex tending the flat transaction semantics of OpenOODB using Solaris threads. A formal specification of the proposed execution model using ACTA framework is also provided. Key words: Active Databases, Nested Transactions, execution model, So laris Threads, rule execution, ACTA."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET YÜZ GÖRÜNTÜLERİNİ ÇİZİMLER YARDIMIYLA CANLANDIRMA Gamze Dilek Tunalı Bilgisayar ve Enformatik Mühendisliği Yüksek Lisans Tez Yöneticisi: Prof. Bülent Özgüç Temmuz, 1996 Bu çalışma, çizimlerin denetiminde doku kaplama yöntemini kullanarak, iki boyutlu animasyonların gerçekleştirilmesini içerir. Çalışma özellikle yüz ani masyonunu amaçlamaktadır ve yüzler insan yüzü olmak zorunda değildir. İlk aşamada, aynı yüze ait bir video dizgisinden alınmış yüz görüntüleri ve canlandıralacak başka bir yüze ait bir görüntü kullanılır. Amacımız, verilen tek kare yüz görüntüsünü, video dizgisindeki yüz ifadeleriyle canlandırmaktır. Bu çalışma sayesinde, canlandırılmış sıralı yüz hareketleri bir veri tabanında toplanarak, verilen herhangi bir yüz için istenilen hareket sırası seçilerek, can landırma gerçekleştirilebilir. Daha da önemlisi, insan yüzüne ait sıralı yüz hareketleri kullanılarak, insana ait olmayan yüzler gerçeğe çok yakm bir biçimde canlandırılabilir. Aynı zamanda, karmaşık karakterler daha basit karakterlerin hareket sıraları yardımıyla canlandırılabilirler. Anahtar sözcükler: yüz animasyonu, yüz ifadesi, snakes, aktif ayırıcı çizgi modelleri, çok katlı ızgara interpolasyonu, çok seviyeli B-Spline interpolasy- onu. iv","IV ABSTRACT ANIMATING FACIAL IMAGES WITH DRAWINGS Gamze Dilek Ttmalı M.S. in Computer Engineering and Information Science Supervisor: Prof. Bülent Özgüç July, 1996 The work presented here describes the power of 2D animation with texture mapping controlled by line drawings. Animation is specifically intended for facial animation and not restricted by the human face. We initially have a sequence of facial images which are taken from a video sequence of the same face and an image of another face to be animated. The aim is to animate the face image with the same expressions as those of the given video sequence. To realize the animation, a set of frames are taken from a video sequence. Key features of the first frame are rotoscoped and the other frames are automat ically rotoscoped using the first frame. Similarly, the corresponding features of the image which will be animated are rotoscoped. The key features of the first frame of the sequence and the image to be animated are mapped and using cross-synthesis procedure, other drawings for the given image are produced. Using these animated line drawings and the original image, the corresponding frame sequence is produced by image warping. The resulting sequence has the same expressions as those of the video sequence. This work encourages the reuse of animated motion by gathering facial mo tion sequences into a database. Furthermore, by using motion sequences of a human face, non-human characters can be animated realistically or com plex characters can be animated by the help of motion sequences of simpler characters. Key words: facial animation, facial expression, snakes, active contour mod els, multigrid relaxation, multilevel B-Spline interpolation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Dilin yazımsal bir öğesi olan noktalama, bilgisayarlı dilbilim alanındaki araş tırmalarda yıllar boyu ihmal edilegelmiştir. Bunun bir nedeni konunun genel zorluğu, diğer bir nedeni de dayanak noktası olabilecek sağlam bir teorinin eksikliğidir. Öte yandan, son yıllarda gerek 'geleneksel' gerekse bilgisayarlı dilbilim alanlarının noktalamaya ilgisi giderek artmıştır; çünkü, noktalama işaretlerini dikkate almadan yazılı dili gerçekten anlayıp işlemenin neredeyse imkansız olduğu ortaya çıkmıştır. Biçim kılavuzları ve genel dilbilgisi kitaplarında verilen kural listeleri dışında noktalama hakkında az bilgiye sahibiz. Bu tür kitaplar noktalama işaretlerinin nasıl kullanılacağına dair bilgiler verirken, bunların uygulamada nasıl kullanıldığı konusunda genelde sessiz kalmaktadırlar. Bu tez, İngilizce'de noktalama uygulamasının, virgülün (noktalama işaretlerinin en önemlisi) özel durumu için, cümle yapısına göre notlanmış bir metin veritabanında incelenmesi amacıyla yapılmış bilgisayar destekli bir deneyin ayrıntılarını içermektedir. Bu deneyde, virgülün değişik kullanımlarını cümlede ortaya çıktığı değişik sözdizimi şablonlarına göre sınıflandırmaya çalıştık. Kullanılan metin veri- tabanı (Penn Treebank) sadece sözdizimi yapısına göre notlanmış cümlelerden oluşup başka hiçbir bilgi içermemekte, bu ise yapısal noktalama işaretlerinin sınıflandırılması için ideal olarak yeterli görünmektedir. Anahtar sözcükler: Bilgisayarlı Dilbilim, Doğal Dil İşleme, Noktalama, İngilizce, Metin-tabanlı Analiz, Virgül. iv","Punctuation, an orthographical component of language, has usually been ig nored by most research in computational linguistics over the years. One reason for this is the overall difficulty of the subject, and another is the absence of a good theory. On the other hand, both 'conventional' and computational lin guistics have increased their attention to punctuation in recent years because it has been realized that true understanding and processing of written language will be almost impossible if punctuation marks are not taken into account. Except the lists of rules given in style manuals or usage books, we know little about punctuation. These books give us information about how we should punctuate, but they are generally silent about the actual punctuation practice. This thesis contains the details of a computer-aided experiment to investigate English punctuation practice, for the special case of comma (the most sig nificant punctuation mark) in a parsed corpus. The experiment attempts to classify the various uses of comma according to the syntax-patterns in which comma occurs. The corpus (Penn Treebank) consists of syntactically annotated sentences with no part-of-speech tag information about individual words, and this ideally seems to be enough to classify 'structural' punctuation marks. Keywords: Computational Linguistics, Natural Language Processing, Punctu ation, English, Corpus-based Analysis, Comma. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇOK İŞLEMCİLİ BİLGİSAYARLARDA UYARLANABİLİR KAYNAK DA?ITIMI VE YOL ÜRETİMİ Yücel Aydoğan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Cevdet Aykanat Temmuz, 1995 Ölçeklenebilir çokişlemcili bilgisayarlar herhangi iki işlemci arasında birden fazla haberleşme yolu sağlayan bağlantı ağları üzerine kurulan sistemlerdir. Bu tür ağlarda yol seçimi haberleşme performansım etkileyen önemli bir etk endir. Uyarlanabilir Kaynak Dağıtımı (UKD), uyarlanabilir dağıtım ve kaynak dağıtımı yöntemlerini birleştiren ve her ikisinin de avantaj lanna sahip olan bir dağıtım yöntemi olarak önerilmiştir. Her paket tam uyarlanabilir, kısmi uyarlanabilir yada uyarlamasız şekilde yöneltilir. UKD yöntemi kilitlenme sınırlamalarının sağlandığı herhangi bir ağ topolojisi kullanımına izin verir. Uyarlanabilir kaynak dağıtımı ve uyarlamasız rastlantısal dağıtım yöntemleri benzetim yapılarak karşılaştırılmıştır. Ayrıca çokişlemcili bilgisayar ağlarında işlemciler arasında uyarlanabilir yollar üreten bir yöntem önerilmiştir. Üretilen uyarlanabilir yollar her işlemcinin belleğindeki yol çizelgelerinde saklanır. Bu yöntem yüksek veri iletişim kapasitesi ve işlemciler arası güvenilir iletişimi sağlar. Önerilen yöntem ile IBM SP2 çokişlemcisi ağları kullanılarak deneyler yapılmış ve sağlanan veri iletişim kapasitesi ve işlemcilerde yol çizelgesi oluşturma zamanları ölçülmüştür. Yöntemin çokişlemcili bilgisayarlarda par alel işlemesi ile elde edilen verim de deneysel olarak sunulmuştur. Anahtar Sözcükler: Uyarlanabilir Dağıtım, Çokişlemcili Bilgisayarlar, Bağlantı Ağlan, Paralel İşleme iv","ABSTRACT ADAPTIVE SOURCE ROUTING AND ROUTE GENERATION FOR MULTICOMPUTERS Yücel Aydoğan M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Cevdet Aykanat July, 1995 Scalable multicomputer are based upon interconnection networks that typi cally provide multiple communication routes between any given pair of proces sor nodes. In such networks, the selection of the routes is an important prob lem because of its impact on the communication performance. We propose the adaptive source routing (ASR) scheme which combines adaptive routing and source routing into one which has the advantages of both schemes. In ASR, the degree of adaptivity of each packet is determined at the source processor. Every packet can be routed in a fully adaptive or partially adaptive or non- adaptive manner, all within the same network at the same time. The ASR scheme permits any network topology to be used provided that deadlock con straints are satisfied. We evaluate and compare performance of the adaptive source routing and non-adaptive randomized routing by simulations. Also we propose an algorithm to generate adaptive routes for all pairs of processors in any multistage interconnection network. Adaptive routes are stored in a route table in each processor's memory and provide high bandwidth and reliable in- terprocessor communication. We evaluate the performance of the algorithm on IBM SP2 networks in terms of obtained bandwidth, time to fill in the route tables, and efficiency exploited by the parallel execution of the algorithm. Keywords: Adaptive Routing, Multicomputers, Interconnection Networks, Par allel Processing m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özet PARALEL IŞIN DUŞURME/IZLEME İÇİN UZAYSAL BÖLÜMLEME Veysi İşler Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticileri: Prof. Dr. Bülent Özgüç ve Doç. Dr. Cevdet Aykanat Şubat 1995 Bu çalışma uzaysal bölme yönteminin paralel bir bilgisayarda gerçeğe uygun görüntü üretmek için kullanılması üzerinde yoğunlaşmaktadır. Işm izleme çok yararlı olmasına karşın oldukça fazla işlem gerektiren bir yöntemdir. Bu nedenle bir çok araştırıcı, bu yöntemin sorunlarına çözüm bulmak için çalışmaktadır. Bu çalışmalar sonucunda ortaya çıkan parallel ışın izleme yöntemlerinin sınıflandırılması bu tezde yapılmakta, önemli paralel ışın izleme yöntemleri yine bu tezde tartışılmakta ve eleştirilmektedir. Uzaysal bölümleme yöntemi, bir işlemcinin yerel belleğine sığamayan üç-boyutlu karmaşık sahnelerin ayrıştırılmasına dayanan paralel ışm izleme algoritmasına uygu lanmıştır. Geliştirilen ayrıştırma yöntemi, sahnedeki nesnelerin işlemcilere eşit bir şekilde dağıtılmasını sağlamakla birlikte grafiksel tutarlılığın (coherence) kullanılmasına da olanak sağlamaktadır. Uzaysal bölümlemeyi kullanan ayrıştırma yöntemi, ayıran düzlemleri etkin veri yapıları ile oldukça kısa sürede bulmaktadır. Ayrıca, ortaya mçıkan hacimlere bağlı nesnelerin işlemcilere, işlemciler arasındaki iletişimi azaltacak şekilde eşlenmesi de ayrıştırma yöntemi ile eşzamanlı olarak kısa sürede yapılmaktadır. Ayrıca, önişlemde harcanan zamanı azaltmak için, önerilen ayrıştırma ve eşleme işleri de paralelleştirilmişt ir. Son olarak, işlemcilere ait yerel belleklerin tamamını kullanmaya olanak sağlayan yeni bir ayırma düzlemi (çıkıntılı ayırma düzlemi) önerilmektedir. Önerilen çıkıntılı ayırma düzlemi paylaşılan nesnelerin birden fazla işlemcinin yerel belleğinde bulunmasına izin vermeyerek paralel bilgisayarın verimli kullanılmasını sağlar. Önerilen paralel algoritmalar Intel iPSC/2 hiperküp bilgisayarında gerçekleştirilmiştir. Anahtar Sözcükler: Işın Düşürme, Işın İzleme, Uzaysal Bölümleme, İkili Uzaysal Bölümleme, Ayırma Düzlemi, Hiperküp Topolojisi, Paralel İşleme. iv","Abstract SPATIAL SUBDIVISION FOR PARALLEL RAY CASTING/TRACING Veysi İşler Ph.D. in Computer Engineering and Information Science Supervisors: Prof. Bülent Özgüç and Assoc. Prof. Cevdet Aykanat February 1995 Ray casting/tracing has been extensively studied for a long time, since it is an elegant way of producing realistic images. However, it is a computationally intensive algorithm. In this study, a taxonomy of parallel ray casting/tracing algorithms is presented and the primary parallel ray casting/tracing systems are discussed and criticized. This work mainly focuses on the utilization of spatial subdivision technique for ray casting/tracing on a distributed-memory MIMD parallel computer. In this research, the reason for the use of parallel computers is not only the processing power but also the large memory space provided by them. The spatial subdivision technique has been adapted to parallel ray casting/tracing to decompose a three-dimensional complex scene that may not fit into the local memory of a single processor. The decomposition method achieves an even distribution of scene objects while allowing to exploit graphical coherence. Additionally, the decomposition method produces three-dimensional volumes which are mapped inexpensively to the processors so that the objects within adjacent volumes are stored in the local memories of close processors to decrease interprocessor communication cost. Then, the developeddecomposition and mapping methods have been parallelized efficiently to reduce the preprocessing overhead. Finally, a splitting plane concept (called ""jaggy splitting plane"") has been proposed to accomplish full utilization of the memory space of processors. Jaggy splitting plane avoids the shared objects which are the major sources of inefficient utilization of both memory and processing power. The proposed parallel algorithms have been implemented on the Intel iPSC/2 hypercube multicomputer (distributed- memory MIMD). Keywords: Ray Casting, Ray Tracing, Spatial Subdivision, Binary Spatial Partition ing (BSP), Splitting Plane, Hypercube Topology, Parallel Processing. u"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET FRAKTAL A?AÇ MODELLEME Ersin Ünal Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Prof. Bülent Özgüç Şubat, 1995 Kullanıcı etkileşimi söz konusu olduğunda, görüntü amaçlı bitki modellerinin gerçekleştirilmesi için çeşitli zorlukların aşılması gerekir. Bu tezde, yinele nen, zamana bağımlı, fraktal bir ağaç modeli öneriyoruz. Gerçekçi özellikleri arasında istatistiki eş benzerlik, rassal budama ve hüristik dal kesişimlerini sayabiliriz. Modelin odak noktası gerçekçi dal yapılarıdır. İstatistiki frak tal yöntemlerle oluşturulan model iskeleti hüristik hızlandırma teknikleri kul lanılarak kaplanmıştır. Modelin parametrik ayarlamalarına ve izlenmesine yakın gerçek zamanda cevap verebilen bir sistem geliştirdik. Sistemin uygulaması bir SGI Iris Indigo XS24 iş istasyonu üzerinde yapılmıştır. Anahtar Sözcükler: Fraktal Modelleme, Doğal Fenomen Modelleme, Ağaç Modelleme. iv","ABSTRACT FRACTAL TREE MODELING: GENERATION, RENDERING AND GROWTH ANIMATION Ersin Ünal M.S. in Computer Engineering and Information Science Advisor: Prof. Bülent Özgüç February, 1995 Plant modeling for visual purposes is a challenge, especially when user inter action is a concern. In this thesis, we propose a recursive, time-aware, fractal tree model with realistic features including statistical self-similarity, stochas tic pruning and heuristic branch intersections. The main focus is on realistic branch structures. The skeletal model constructed by statistically fractal meth ods is fleshed out using heuristic acceleration techniques. We develop a system with near real-time feedback for fast model manipulation and viewing. The system implementation is developed on an SGI Iris Indigo XS24 workstation. Keywords: Fractal Modeling, Natural Phenomena Modeling, Tree Modeling. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÇAKIŞIK ÖZELLİK ARALIKLARI İLE SINIFLANDIRMA Hakime Unsal Koç Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. H. Altay Güvenir Ocak 1995 Bu tezde örnek-tabanlı öğrenme için çakışık özellik aralıklarına dayalı yeni bir teknik sunulmuştur. Çakışık Özellik Aralıkları ile Sınıflandırma (COFI) bu yöntemin özel bir uygulamasıdır. Bu çıkarımsal, artımlı ve yönlendirilmiş öğrenme tekniğinde en temel gösterim birimi aralıktır. COFI algoritması tüm özelliklere ait her sınıf eksenindeki aralıkların izdüşümünü öğrenir. Aralıklar ilk olarak sınıf eksenlerinde birer noktadırlar, daha sonra tüm bir eksen boyunca genelleştirmeyle genişlerler. Bu algoritmada herhangi bir özelleştirme gerçek- leştirilmez. Öğrenme işleminden sonra, tahmin etme işlemi özelliklerin kendi adlarına yaptığı tahminler arasındaki oy çokluğuna dayanır. Anahtar Sözcükler: öğrenme, yönlendirilmiş öğrenme, çıkarımsal öğrenme, artımlı öğrenme, çakışık özellik aralıkları, sınıf tanımı iv","ABSTRACT CLASSIFICATION WITH OVERLAPPING FEATURE INTERVALS Hakime Unsal Koç M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. H. Altay Güvenir January, 1995 This thesis presents a new form of exemplar-based learning method, based on overlapping feature intervals. Classification with Overlapping Feature Intervals (COFI) is the particular implementation of this technique. In this incremental, inductive and supervised learning method, the basic unit of the representation is an interval. The COFI algorithm learns the projections of the intervals in each class dimension for each feature. An interval is initially a point on a class dimension, then it can be expanded through generalization. No specialization of intervals is done on class dimensions by this algorithm. Classification in the COFI algorithm is based on a majority voting among the local predictions that are made individually by each feature. Keywords: machine learning, supervised learning, inductive learning, incre mental learning, overlapping feature intervals, concept description 111"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ISTEMCI-SUNUCU SİSTEMLERDE GENEL BELLEK YÖNETİMİ Yasemin TÜRKAN Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Asst. Prof. Özgür Ulusoy Haziran, 1995 Bu tezde istemci-sunucu sistemlerin performansını arttırmak amacıyla iki tek nik tanıtılmıştır. Önerilen teknikler, ""Gönderilen Sayfaların Bellekten Atılması"" ve ""Bellekten Atılan Sayfaların Digger istemcilere Gönderilmesi"", daha önceden sunulmuş olan genel bellek yönetim tekniklerini (""Sayfa isteklerinin Digger istemcilere Yönlendirimesi"", ""Gönderilen Sayfaların işaretlenmesi"" ve ""Bellekten Atılan Sayfaların Sunucuya Geri Gönderilmesi"") geliştirmektedir. Tezde önerilen tekniklerin performansı istemci-sunucu özelliklerini sağlayan bir benzetim modeli kullanılarak incelenmiştir. Değişik iş yükleri altında toplanan sonuçlardan, önerilen tekniklerin bellekte bulunan veritabanı büyüklüğü nü arttırdığı gözlenmektedir. Böylece bu teknikler kullanılarak istemci-sunucu sistemlerde performans artışı sağlanmıştır. Anahtar Sözcükler: Istemci-Sunucu Sistemler, Genel Bellek Yönetimini, Bellek Tutarlılığı. iv","ABSTRACT GLOBAL MEMORY MANAGEMENT IN CLIENT-SERVER SYSTEMS Yasemin TÜRKAN M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Özgür Umsoy June, 1995 This thesis presents two techniques to improve the performance of the global memory management in client-server systems. The proposed memory manage ment techniques, called ""Dropping Sent Pages"" and ""Forwarding Sent Pages"", extend the previously proposed techniques called ""Forwarding"", ""Hate Hints"", and ""Sending Dropped Pages"". The aim of all these techniques is to increase the portion of the database available in the global memory, and thus to reduce disk I/O. The performance of the proposed techniques is experimented using a basic page-server client-server simulation model. The results obtained under different workloads show that the memory management algorithm applying the proposed techniques can exhibit better performance than the algorithms that are based on previous methods. Keywords: Client-Server Systems, Global Memory Management, Cache Con sistency. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET EKRAN UZAYINDA BÖLME YÖNTEMLERİNİN PARALEL HACİM GÖRÜNTÜLEME AMACIYLA KARŞILAŞTIRMALI İNCELENMESİ Egemen Tanin Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Cevdet Aykanat Temmuz, 1995 Birçok mühendislik uygulamalarında, elde edilen sonuçlar yapısal olmayan hacimsel veri kümeleri olarak saklanmaktadır. Doğrudan Hacim Görüntüleme (DUG) yöntemleri bu amaçla kullanılan en etkin görüntüleme tekniklerinden biridir. Ancak bu yöntemler oldukça yoğun işlemler sonucunda istenilen görüntüyü elde edebilmekte ve dolayısıyla animasyon ve benzeri uygulamalar için oldukça yavaş yöntemler olarak kabul edilmektedirler. Ayrıca hacim sel veriler çok büyük bilgisayar bellekleri kullanılarak saklanabilmekte ve bu açıdanda görüntüleme işlemini oldukça zorlaştırmaktadırlar. Bunlara ek olarak uygulamaların çoğunluğu çok işlemcili dağıtık hafızalı bilgisayarlarda yapılmaktadır. Dolayısıyla büyük veri kümelerinin görüntüleme amaçlı bil gisayarlara taşınması büyük sorunlar doğurabilmektedir, işte bütün bu ne denlerden dolayıdır ki Paralel-DHG (P-DHG) önemli bir araştırma konusu olmuştur. Fakat DHG yöntemlerinin yapısal olmayan hacimlerde uygulan ması nokta yeri tespiti ve bakış açısı sıralaması adı verilen iki problemin çözümünü gerektirmektedir. Bu tezde standart poligon boyama yöntemleri kullanılarak bu problemlere çözüm aranmış ve paralelleştirmek amacıyla üç yöntem ileri sürülmüştür. Önerilen ana paralel algoritma Işın Düşürme yönteminin kullanılması yoluyla görüntüleme yapılması esasını kullanmakta ve ileri sürülen bu üç yöntem gibi ekran uzayını baz olarak almaktadır. Dolayısıyla iş bölümü ekran uzayının daha küçük ekran parçacıklarına bölünmesi ile gerçekleştirilmektedir. İş dağılımı herbir paralel işlemci başına tek bir ekranVI bölümü düşecek şekilde statik dağılım yapılarak gerçekleştirilmiştir. Kullanılan poligon boyama yönteminin ana bölümleri göz önüne alınarak bir ekran bölümündeki iş hesaplanmış ve mümkün olduğunca işlemcilere eşit iş dağılımı yapılmaya çalışılmıştır. Geliştirilen programlar her türlü sistem ve veri kümesi kullanabilecek şekilde genelleştirilmiş ve bu şekilde gerçekleştirilmişlerdir. Yatay, dikdörtgene, ve özyineli adı verilen bölme yöntemlerinin IBM-SP2 sis teminde karşılaştırmalı incelenmesi yapılmış ve bu tezle birlikte sunulmuştur. Anahtar Sözcükler: Doğrudan hacim görüntüleme, bilgisayar grafikleri, paralel algoritmalar, dağıtık hafızalı çok işlemcili bilgisayarlar.","ABSTRACT COMPARISON OF IMAGE SRACE SUBDIVISION ALGORITHMS FOR PARALLEL VOLUME RENDERING Egemen Tanin M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Cevdet Aykanat July, 1995 In many scientific applications, results are presented as unstructured volumet ric data sets. Direct Volume Rendering (DVR) is a powerful way of visualizing these volumetric data sets. However, it involves intensive computations. In addition, most of the volumetric data sets also require huge memories. Hence, DVR is a good candidate for parallelization on distributed memory multicom puter. Also most of the engineering simulations are done on multicomputers. Therefore, visualization of these results on the same architectures where simu lations are done avoids the overhead of transporting large amount of data. In order to visualize unstructured volumetric data sets, the underlying algorithms should resolve the point location and the view sort problems of the 3D grid points. In this thesis, these problems are solved by using the well-known Scan- line Z-Buffer algorithm. Three image space subdivision algorithms, namely horizontal, rectangular, and recursive subdivisions, are utilized to distribute the computations evenly among the processors in the rendering phase. The main parallel algorithm uses Raycasting approach of DVR to visualize the data sets, which is also an image space method. Therefore, the divisions are made in order to obtain a set of sub-images. Static task decomposition is used where each processor is assigned to a single sub-image. The load balance among the processors is achieved by defining the overall work load with in a sub-image by using the milestone operations done in the Scanline Z-Buffer algorithm. The algorithms are developed in a way that they can handle any kind of polygonal, volumetric, and etc. data set where the underlying architecture is also kept iiiIV flexible in many aspects for the sake of generality and portability. The exper imental performance evaluation of the horizontal, rectangular, and recursive subdivision algorithms on an IBM-SP2 system are presented and discussed in a comparative way. Keywords: Direct volume rendering, computer graphics, parallel algorithms, distributed memory multicomputer."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BABY-SIT'LE HESAPSAL DURUM KURAMI Erkan Tın Bilgisayar ve Enformatik Mühendisliği, Doktora Danışman: Prof. Dr. Varol Akman Aralık, 1995 Dil günlük deneyimlerimizin bütünleşik bir parçasını oluşturmakta ve konuşma, dinleme, okuma ve yazma gibi durumsal etkinlikleri içermektedir. Bu etkin likler, durumlar içerisinde gerçekleştiklerinden ve durumları ilgilendirdik lerinden dolayı durumsaldırlar. Diğer yandan, bunların asıl işlevi bilgi taşımaktır. Bu görüş çerçevesinde, yaklaşık son on yıldır durum kuramı geliştirilmiş ve bu kuramın çeşitli uyarlamaları birtakım dilbilim sorunlarına uygulanmıştır. Fakat kuramın hesapsal yönleri ile ilgili pek bir çalışma yapılmamıştır. 'Hesapsal durum kuramı 'na halihazırda varolan yaklaşımlar durum kuramının özgün niteliklerinin sadece bir kısmını içermekte ve böylece kuramın varlıkbiliminden kavramsal ve felsefi uzaklaşma göstermektedir. Bu tez, durum kuramının temellerini ve varlıkbilimsel özelliklerini benimseyen hesapsal bir kuram sunmaktadır. Önerilen hesapsal temel üzerine kuru lan ve BABY-SIT adı verilen ortam tanımlanmakta ve bu ortamın yapıları biçimsel olarak tanımlanmaktadır. BABY-SIT'in özellikleri halihazırda varolan yaklaşımların özellikleri ile karşılaştırılmaktadır. BABY-SIT'in uygun bir or tam olduğunu göstermek amacı ile yapay zeka alanından bazı örnekler ver ilmektedir. BABY-SIT için dilbilimsel bir deney alanı olarak seçilen Türkçe'de anaforanın çözümlenmesi de gösterilmektedir. Anahtar Sözcükler: durum kuramı ve durum anlambilimi, durum şemaları, kalıtım, ileriye ve geriye doğru çıkarım, tekdüze olmayan çıkarım, anafora, sözdizimsel ve anlambilimsel alanlar, BABY-SIT, PROSIT, ASTL","ABSTRACT COMPUTATIONAL SITUATION THEORY WITH BABY-SIT Erkan Tın Ph.D. in Computer Engineering and Information Science Advisor: Prof. Varol Akman December, 1995 Language is an integral part of our everyday experience and encompasses situ ated activities such as talking, listening, reading, and writing. These activities are situated because they occur in situations and they are about situations. Their primary function, on the other hand, is to convey information. With this vision, situation theory has been developed over the last decade or so and various versions of the theory have been applied to a number of linguistic is sues. However, not much work has been done in regard to its computational aspects. Existing approaches towards 'computational situation theory' incor porate only some of the original features of situation theory and hence show conceptual and philosophical divergence from its ontology. This thesis presents a computational account of situation theory that embodies the essentials of the theory and adopts its ontological features. A medium (called BABY-SIT) which is based on the proposed computational foundation is described and its constructs are formally defined. The features of BABY-SIT are compared to those of the existing approaches. In order to demonstrate the appropriateness of BABY-SIT, some examples from the domain of artificial intelligence are given. Resolution of pronominal anaphora in Turkish, which has been chosen as a linguistic test-bed for BABY-SIT, is also demonstrated. Keywords: situation theory and situation semantics, situation schemata, inher itance, forward and backward reasoning, nonmonotonic reasoning, anaphora, syntactic and semantic domains, BABY-SIT, PROSIT, ASTL IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada, Karmarkar-tipi bir doğrusal programlama optimizasyon algoritması olan Mehrotra'nın predictor-corrector iç nokta algoritmasının paralelleştirilmesi sunulmaktadır. Algoritmanın içerdiği işlem tipleri belirlenmiş ve her işlem tipi için paralel algoritmalar sunulmuştur. Karmarkar-tipi algoritmaların işlem ağırlığını oluşturan büyük simetrik doğrusal denklem kümelerinin çözümü detaylı incelenmiştir. Birçok ileri ve geri çözüm algoritması test e- dilmiş, bir biriktirmeli geri çözüm algoritması geliştirilmiştir. Seyrek matris- vektör çarpımı ve faktörizasyon işlemlerinin dağıtımı için sezgisel bin-packing algoritmaları kullanılmıştır. Performans sonuçlan en iyi olan algoritmalar doğrusal programlama problemlerinin çoklu bilgisayarlarda paralel çözümü için bir sistem geliştirilmesinde kullanılmıştır. Dizayn kıstasları ve uygulama de tayları tartışılmış, bazı performans sonuçları sunulmuştur. Anahtar Kelimeler: Doğrusal Programlama, İç Nokta Algoritmaları, Dağı- tık Sistemler, Paralel İşleme.","In this study, we present the parallelization of Mehrotra's predictor-corrector interior point algorithm, which is a Karmarkar-type optimization method for linear programming. Computation types needed by the algorithm are identi fied and parallel algorithms for each type are presented. The repeated solution of large symmetric sets of linear equations, which constitutes the major com putational effort in Karmarkar-type algorithms, is studied in detail. Several forward and backward solution algorithms are tested, and buffered backward solution algorithm is developed. Heurustic bin-packing algorithms are used to schedule sparse matrix- vector product and factorization operations. Algo rithms having the best performance results are used to implement a system to solve linear programs in parallel on multicomputers. Design considerations and implementation details of the system are discussed, and performance results are presented from a number of real problems. Keywords: Linear Programming, Interior Point Algorithms, Distributed Systems, Parallel Processing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HACİMLİ DOKU KAPLAMA YÖNTEMİ Gürkan Saik Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Prof. Bülent Özgüç Şubat 1995 En. gerçekçi ve etkileyici bilgisayar çıktısı görüntüler, genellikle yeterince ayrıntılandırılmış ve görsel karmaşıklığa sahip olanlardır. Doku kaplama yöntemi bu görüntülere görsel karmaşıklık ve ayrıntı eklemenin en yaygın yöntemlerinden biridir. Eskiden doku kaplama sadece yüzey ayrıntısını artırmak için kullanılırdı. Bu tezde kaplanacak dokular üç boyutlu uzayda oluşturulup, geometrik tanımlarıyla birlikte karmaşık cisimler üzerine kaplanırlar. Doku kaplanmış cisim ışın izleme yöntemiyle tonlandırılır. Kullandığımız yöntemde doku cismin tanımım ve görünüşünü doğrudan etkilemektedir. Cisimlerin bulunduğu sahne bu yöntemle oluşturulurken ışığın soğurulması ve ışığın ayrışması gibi doğal olgular da dikkate alınmıştır. ııı","ABSTRACT VOLUME BASED TEXTURE MAPPING Gürkan Saik M.S. in Computer Engineering and Information Science Advisor: Prof. Bülent Özgüç February, 1995 The most realistic and attractive computer generated images are usually those that contain a large amount of visual complexity and detail. Texturing is a widely used way of adding visual complexity and detail to computer gener ated images. Traditionally surface texturing was only used to simulate sur face detail. In this thesis we generate textures defined throughout a region of three-dimensional space and map those textures together with their geometric definition onto complex objects. The textured object is rendered volume based with a backward mapping algorithm (ray tracing). Hence the texture affects the definition and the realism of the object. In rendering the scene, natural phenomena such as dispersion and absorption of light is also incorporated. Keywords: 3-D Texture Mapping, Ray Tracing, Dispersion 11"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bağlam konusu Yapay Us, Matematiksel Mantık ve Doğal Dil Ânlambiliminin çeşitli alanlarında karşımıza çıkar. Önemi değişik araştırmacılar tarafından fark edilmesine rağmen, kullanışlı bir formalizasyon konusunda fazla çalışma yapılmamıştır. Bu tezde sorunu tanımlamaya ve kabul edilebilir (formel) bir bağlam nosyonuna yönelik olarak neler yapılması gerektiğine karar vermeye çalışacağız. Durum Kuramına dayanan bir ön model önerdikten sonra, bağ lamın çeşitli konularda kullanımını ve önerilen modelin sağladığı avantajları gösteren örnekler vereceğiz. Anahtar Sözcükler: Bağlam, Bilgi Gösterimi, Sağduyusal Akıl Yürütme, Durum Kuramı ve Durum Anlambilimi. iv","The issue of context arises in assorted areas of Artificial Intelligence, Mathe matical Logic, and Natural Language Semantics. Although its importance is realized by various researchers, there is not much work towards a useful for malization. In this thesis, we will try to identify the problem, and decide what we need for an acceptable (formal) account of the notion of context. We will present a preliminary model (based on Situation Theory) and give examples to show the use of context in various fields, and the advantages gained by the acceptance of our proposal. Keywords: Context, Knowledge Representation, Commonsense Reasoning, Sit uation Theory and Situation Semantics. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ORTA ALAN TAVLAMA METODU KULLANILARAK EŞLEME VE FPGA'LERDEKİ KABA ROTALAMA PROBLEMLERİNİN ÇÖZÜMÜ İsmail Haritaoğlu Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. Cevdet Aykanat Eylül, 1994 Birleşimsel eniyileme problemlerini çözmek için önerilen Ortak Alan Tavlama (Mean Field Annealing) algoritması, sinir ağları ve benzetimsel tavlama (Sim ulated Annealing) yöntemlerinin özelliklerini taşır. Bu çalışmada, Ortak Alan Tavlama algoritması Alan Programlamalı Kapı Devrelerinin (Field Pro grammable Gate Arrays) kaba rotalama problemine (Global Routing) ve par alel programlamadaki eşleme (Mapping) problemlerine uyarlanmıştır. Tezin ilk bölümünde Ortak Alan Tavlama algoritması Alan Programlamalı Kapı Devrelerinin (Field Programmable Gate Arrays) kaba rotalama problemi- ninin çözümünde kullanılmıştır. Önerilen algoritmalarının başarımları Locus- Route kaba rotalama algoritması ile kıyaslanarak değerlendirilmiştir. Deneyler algoritmaları karşılaştırmak için kullanılan standart devreler (Benchmarks) üzerinde yapılmıştır. Elde edilen sonuçlar Ortak Alan Tavlama algoritmasının kaba rotalama problemini çözmek için iyi bir alternatif algoritma olarak kul lanılabileceğini göstermektedir. Tezin ikinci bölümünde Mesh ve Hiperküp tipindeki paralel bilgisayarlarındaki eşleme problemi için daha önce önerilen algoritmalardan daha hızlı olan bir algoritma geliştirilmiş ve bu önerilen algorit manın başarımları Kernighan-Lin, Simulated Annealing ve daha önce önerilen ortak alan tavlama metotları ile kıyaslanarak değerlendirilmiştir. Anahtar Sözcükler: Orta Alan tavlama algoritması, Eşleme problemi, Kaba rotalama algoritmaları, Alan programlamh kapı devreleri iv","Mean Field Annealing algorithm which was proposed for solving combinatorial optimization problems combines the properties of neural networks and Simu lated Annealing. In this thesis, MFA is formulated for mapping problem in parallel processing and global routing problem in physical design automation of Field Programmable Gate Array (FPGAs) A new Mean Field Annealing (MFA) formulation is proposed for the mapping problem for mesh-connected and hypercube architectures. The proposed MFA heuristic exploits the conven tional routing scheme used in mesh and hypercube interconnection topologies to introduce an efficient encoding scheme. An efficient implementation scheme which decreases the complexity of the proposed algorithm by asymptotical fac tors is also developed. Experimental results also show that the proposed MFA heuristic approaches the speed performance of the fast Kernighan-Lin heuris tic while approaching the solution quality of the powerful simulated annealing heuristic. Also, we propose an order-independent global routing algorithm for SRAM type FPGAs based on Mean Field Annealing. The performance of the proposed global routing algorithm is evaluated in comparison with LocusRoute global router on ACM/SIGDA Design Automation benchmarks. Experimen tal results indicate that the proposed MFA heuristic performs better than the LocusRoute. iiiKeywords: Mapping, Global Routing, Field Programmable Gate Arrays, Mean Field Annealing"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Sözcük türlerinin işaretlenmesi için kullanılan sistemler metin bilgilerini kullanarak o metinde bulunan her sözcüğü tek bir tür ile işaretlemeye çalışırlar. Otomatik olarak işaretleme, metinlerin üst düzey çözümlemesi açısından önemli bir adımdır ve bu adımın çıktıları pek çok doğal dil işleme uygulamasında kullanılabilir. Türkçe ve Fince gibi çekimli ve bitişken biçimbirimlere sahip dillerde, sözcükler çoğunlukla biçimbirimsel olarak çok yapılı olduğu için biçimbirimsel çok yapılılık çözümlemesi önemli bir işlemdir. Bu tez, Türkçe'nin tam kapsamlı iki aşamalı biçimbirimsel tanımlamasına dayanılarak geliştirilen bir sözcük türü işaretleyicisini sunmaktadır, işaretleyici aynı zamanda çok kelimeli ve deyimsel yapıları tanımlayabilmekte, daha önemlisi sözcüklerin komşularının biçimbirimsel bilgileri ve bir kısım sezgisel bilgiler (heuristics) kullanarak biçimbirimsel çok yapılılık çözümlemesi yapabilmektedir, işaretleyici istatistiksel bilgiler toplamak, biçimbirimsel çözümleyicinin bazı hatalarını düzeltmek gibi ek işlevlere de sahiptir. Deney sonuçları, işaretleyicinin metinlerin %97 ila %99'unu çok az kullanıcı yardımı alarak doğru işaretlediğini göstermiş, bir başka deneyde ise biçimbirimsel çok yapılılık çözümlemesi yapılan cümlelerin Türkçe için geliştirilen sözcüksel-işlevsel gramer (LPG) sözdizimsel çözümleyicisi tarafından işlenmesi sonucunda yarıya yakın daha az çözüm yapısı üretildiği ve bu işlemin 2.5 kez daha hızlı gerçekleştiği gözlenmiştir. Anahtar Sözcükler: işaretleme, Biçimbirimsel inceleme iv","A part-of-speech (POS) tagger is a system that uses various sources information to assign possibly unique POS to words. Automatic text tagging is an important component in higher level analysis of text corpora. Its output can also be used in many natural language processing applications. In languages like Turk ish or Finnish, with agglutinative morphology, morphological disambiguation is a very crucial process in tagging as the structures of many lexical forms are morphologically ambiguous. This thesis presents a POS tagger for Turkish text based on a full-scale two-level specification of Turkish morphology. The tag ger is augmented with a multi-word and idiomatic construct recognizer, and most importantly morphological disambiguator based on local lexical neigh borhood constraints, heuristics and limited amount of statistical information. The tagger also has additional functionality for statistics compilation and fine tuning of the morphological analyzer, such as logging erroneous morphological parses, commonly used roots, etc. Test results indicate that the tagger can tag about 97% to 99% of the texts accurately with very minimal user inter vention. Furthermore for sentences morphologically disambiguated with the tagger, an LFG parser developed for Turkish, on the average, generates 50% less ambiguous parses and parses almost 2.5 times faster. Keywords: Tagging, Morphological Analysis, Corpus Development m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET MINIMUM DERECE SIRALAMASINA DAYALI YAPICI ÇOK KISIMLI DEVRE PARÇALAMA ALGORİTMASI Ümit V. Çatalyürek Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd, Doç. Dr. Cevdet Aykanat Eylül, 1994 Devre parçalamanın geniş ölçekli tümleşik tasarımlarda bir çok önemli uygulaması vardır. Devre parçalama problemi en uygun şekilde hiperçizge parçalama olarak modellenebilir. Bu çalışmada, yoğunluğu çok seyrek olan simetrik matrislerin faktorizasyonunda yaratılan eleman sayısını azaltmada çokça kullanılan Minimum Derece (MD) sıralama sezgisel metodunu kullanarak yeni bir fc-kısımlı hiperçizge parçalama sezgisel algoritması öneriyoruz. Önerilen algoritma verilen hiperçizgenin karşıt çizgesi üzerinde çalışır. Önerilen algoritma karşıt çizgenin üzerinde çizge düğümlerini biraraya getirerek hiperçizgede yerel olarak minimum ağ-kesme miktarına sahip düğüm demetleri oluşturur. Algoritmanın daha hızlı çalışabilmesi için MD sıralamasında çokça kullanılan kümleştirilmiş çizge kavramı uygulanmıştır. Önerilen algoritma, bir çok standart test devrelerinde, elde edilen çözüm kalitesi açısından, Kernighan-Lin (KL) ve Simulated Annealing gibi çokça kullanılan sezgisel algoritmalardan çok daha iyi sonuçlar vermektedir. Algoritmamızın bir diğer önemli özelliği ise; daha önce önerilmiş metodların tersine, çalışma zamanının artan k değeriyle birlikte azalmasıdır. Hatta, önerilen algoritma hızlı olduğu bilinen KL-tipi algoritmalardan, k > 16 değeri için, standart test devrelerinde daha hızlı çalışmaktadır. ivAnahtar Sözcükler: Devre Parçalama, Hiperçizge Parçalama, Karşıt Çizge, Minimum Derece Sıralaması, Kümeleştirilmiş Çizge","ABSTRACT A CONSTRUCTIVE MULTI-WAY CIRCUIT PARTITIONING ALGORITHM BASED ON MINIMUM DEGREE ORDERING Ümit V. Çatalyürek M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Cevdet Aykanat September, 1994 Circuit partitioning has many important applications in VLSI. Circuit parti tioning problem can be most properly modeled as hypergraph partitioning. In this work, we propose a novel &-way hypergraph partitioning heuristic using the Minimum Degree (MD) ordering which is a well-known heuristic for re ducing the amount of fills in the factorization of symmetric sparse matrices. The proposed algorithm operates on the dual graph of the given hypergraph. The algorithm grows node-clusters on the dual graph which induce cell-clusters with locally minimum net-cut sizes. The quotient graph concept, widely used in MD ordering, is exploited for the sake of efficient implementation. The proposed algorithm outperforms well-known heuristics, such as Kernighan-Lin (KL) based algorithms and Simulated Annealing, in terms of solution quality on various VLSI benchmark circuits. A nice property of the proposed algo rithm is that its execution time reduces with increasing k as opposed to the existing iterative heuristics. It is even faster than the fast KL-based algorithms on the partitioning of the benchmark circuits for k > 16. Keywords: Circuit Partitioning, Hypergraph Partitioning, Dual Graph, Mini mum Degree Ordering, Quotient Graph m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Kinematik modelleme yöntemleri nesnelerin şekillerini tanımlamakta yeterli olmakla beraber gerçeğe uygun animasyon üretmek sözkonusu olduğunda yetersiz kalmaktadır. Fiziğe dayalı modelleme yöntemleri bu sorunu kuvvet, kütle, enerji, v.b. büyüklükleri kullanarak çözmektedir. Fiziğe dayalı modellerin hareketi rijit ve rijit olmayan dinamik yasaları ile belirlenmiştir. Hareket denklemleri bu modellerin dinamik hareketini tanımlar. Bu çalışmada rijit olmayan (deforme edilebilen) modellerin animasyonu için geliştirilmiş bir sistem anlatılmaktadır. Bu sistem, modellerin animasyonu için fiziğe ve elastisite kuramına dayanan yaklaşımları kullanmaktadır. Aynı zamanda, deforme edilebilen nesnelerin animasyonu için yeni bir yöntem (""yay kuvvet yöntemi"") geliştirilmiştir. Animasyon sisteminde ""primal"", ""hibrid"", ve ""yay kuvvet"" yöntemleri kullanılarak modeller hareket ettirilmektedir. Bu yolla kullanıcı yöntemlerin avantaj ve dezavantajlarına göre modele uygun olan yöntemi seçebilmektedir. Modellerin sabit engellerle çarpışması ve modeller üzerindeki bazı noktaların hareketinin kısıtlanması gibi seçenekler animasyonlarda kullanılabilmektedir.Anahtar sözcükler: Fiziğe dayalı modelleme, deforme olabilen modeller, animasyon, benzetim, kısıtlamalar, çarpışma tespiti, çarpışma sonrası hareket, kısmi türevsel denklemler, doğrusal denklem sistemi çözücüsü.","Although kinematic modeling methods are adequate for describing the shapes of static objects, they are insufficient when it comes to producing realistic an imation. Physically-based modeling remedies this problem by including forces, masses, strain energies, and other physical quantities. The behavior of physically- based models is governed by the laws of rigid and nonrigid dynamics expressed through a set of equations of motion. In this thesis, we describe a system for the animation of deformable models. A spring force formulation for animating deformable models is also presented. The animation system uses the physically- based modeling methods and the approaches from elasticity theory for animating the models. Three different formulations, namely the primal, hybrid, and the spring force formulations, are implemented so that the user could select the suit able one for an animation, considering the advantages and disadvantages of each formulation. Collision of the models with impenetrable obstacles and constrain ing model points to fixed positions in space are implemented.Keywords: Physically-based modeling, deformable models, animation, simulation, constraints, collision detection, collision response, partial differential equations, linear system solver."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET EPISTEMIK BULMACALARIN DURUMSAL MODELLENMESİ Murat Ersan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Varol Akman Temmuz 1993 Durum kuramı Jon Barwise ve John Perry tarafından önerilmiş ve anlam üzerine matematiksel bir kuramdır. Bu kuram kuramsal ve pratik ilgi uyandır mış ve bir takım 'hesapsal' sistemlerin çerçevesini güdülendirmiştir. PROSIT bu konuda yapılmış öncü bir çalışmadır. Ne yazık ki, bu sistemlerde gerçek yaşamdan uygulamalarda eksiklikler vardır ve bu çalışma bu eksikliği gider meyi hedeflemektedir. Burada, PROSIT' in durum kuramının kavramlarını ne denli yansıttığını inceliyor ve bu programlama dilini kullanarak bir dizi epis- temik bulmacayı çözüyoruz. Anahtar Sözcükler: Epistemik Bulmacalar, (Hesapsal) Durum Kuramı, PROSIT, Bilgi Gösterimi, Sağduyusal Akıl Yürütme. iv","ABSTRACT SITUATED MODELING OF EPISTEMIC PUZZLES Murat Ersan M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman July, 1994 Situation theory is a mathematical theory of meaning introduced by Jon Bar- wise and John Perry. It has evoked great theoretical and practical interest and motivated the framework of a few 'computational' systems. PROSIT is the pioneering work in this direction. Unfortunately, there is a lack of real- life applications on these systems and this study is a preliminary attempt to remedy this deficiency. Here, we examine how much PROSIT reflects situation- theoretic concepts and solve a group of epistemic puzzles, using the constructs provided by this programming language. Keywords: Epistemic Puzzles, (Computational) Situation Theory, PROSIT, Knowledge Representation, Commonsense Reasoning. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar sözlüğü özellikle bilgisayarlı çeviri gibi doğal dil işleme sistemlerinde önemli bir göreve sahiptir. Bu tezde biz türkçe için bir eylem belirleme sözlüğü ve eylem anlam çözümleyicisini tasarlayıp gerçekleştirdik. Eylemler olayları tümce içinde simgeleyip, özellikle sözdizimsel ayrıştırma ve bilgisayarlı çeviri gibi doğal dil işleme sistemlerinde en önemli göreve sahip olduklarından, sözlü ğümüzü yanlızca eylemlerden oluşturduk. Eylem anlam çözümleyicimiz oluştur duğumuz eylem sözlüğündeki bilgileri kullanır. Bu uygulamanın temel amacı çok anlamlı ya da deyimsel anlamlar içeren eylemlerin anlam çözümlemesini yapmaktır. Bununla birlikte sözlüğe kayıt ekleme, kayıtlara erişme, kayıtları güncelleme ve silme görevim yapan Lucid Common Lisp\e X- Windows altında geliştirilmiş bir yazılım ve Türkçede çok kullanılan adların özelliklerini içeren bir bilgi yapısını da sunacağız. Anahtar Sözcükler: Doğal dil işleme, bilgisayarlı çeviri, sözlük, sözcüksel çokanlamlılık, anlambilimsel bilgi yapısı.","The lexicon has a crucial role in all natural language processing systems and has special importance in machine translation systems. This thesis presents the design and implementation of a verb lexicon and a verb sense disambigua- tor for Turkish. The lexicon contains only verbs because verbs encode events in sentences and play the most important role in natural language processing systems, especially in parsing (syntactic analyzing) and machine translation. The verb sense disambiguator uses the information stored in the verb lexicon that we developed. The main purpose of this tool is to disambiguate senses of verbs having several meanings, some of which are idiomatic. We also present a tool implemented in Lucid Common Lisp under X- Windows for adding, access ing, modifying, and removing entries of the lexicon, and a semantic concept ontology containing semantic features of commonly used Turkish nouns. Keywords: Natural Language Processing, Machine Translation, Lexicon, Lex ical Ambiguity, Ontology."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET İNGİLİZCE' DE ZAMİRLERİN ÇÖZÜMÜ İÇİN ODAKLAMA: BİR GERÇEKLEŞTİRİM Ebru Ersan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Varol Akman Temmuz 1994 Anafora çözümü doğal dil işlemenin en etkin araştırma alanlarından biridir. Bu çalışma, anaforanın bir çeşidi olan zamirlerin çözümü için bir araç olarak önerilen odaklamayı inceler. Odaklama, anafora gibi, bir konuşma fenomenidir. Candy Sidner odaklamayı 1979 MİT doktora tezinde biçimlendirmiş ve zamirleri de içeren belirli anaforamn çözümü için çeşitli algoritmalar yaratmıştır. Sidner, kuramını hesapsal bir çerçeve içinde sunmuş fakat genelde algoritmaların uygulamasını gerçekleştirmemiştir. Sidner'in odaklama ve zamir çözümü ile ilgili algoritmaları bu tezde gerçekleştirilmiştir. Bu gerçekleştirim, kuramın hem hesapsal hem de kavramsal açıdan daha iyi kavranmasını sağlar. Sonuç olarak ortaya çıkan program değişik konuşma parçaları üzerinde de nenmiş ve deneylerin değerlendirme ve çözümlemesi istatistiksel sonuçlarla birlikte sunulmuştur. Anahtar Sözcükler: Anafora, Odaklama, Konuşma Çözümleme, Doğal Dil işleme. iv","ABSTRACT FOCUSING FOR PRONOUN RESOLUTION IN ENGLISH DISCOURSE: AN IMPLEMENTATION Ebru Ersan M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman July, 1994 Anaphora resolution is one of the most active research areas in natural lan guage processing. This study examines focusing as a tool for the resolution of pronouns which are a kind of anaphora. Focusing is a discourse phenomenon like anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and devised several algorithms to resolve definite anaphora including pronouns. She presented her theory in a computational framework but did not generally implement the algorithms. Her algorithms related to focusing and pronoun resolution are implemented in this thesis. This implementation provides a bet ter comprehension of the theory both from a conceptual and a computational point of view. The resulting program is tested on different discourse segments, and evaluation and analysis of the experiments are presented together with the statistical results. Keywords: Anaphora, Focusing, Discourse Analysis, Natural Language Pro cessing. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HİPERKÜMELER EVRENİNDE DENKLEM ÇÖZME Müjdat Pakkan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Varol Akman Şubat 1993 Peter Aczel'in (ZFC~/AFA diye de bilinen) Hiperküme Kuramı, klasik ZFC küme kuramının zenginleştirilmesiyle ortaya çıkmış ve kümeleri göstermek için çizgeler kullanan bir kuramdır. İyi-yapılanmamış kümeleri de içeren bu ku ram döngüsel birçok kavramın modellenmesi için uygun bir ortam yaratır. ZFC~/AFA kuramının Çözüm Teoremi olarak adlandırılan ve hiperkümeler evrenindeki denklem dizgelerinin çözülebilmesini sağlayan bir sonucu vardır. Bu teorem, hiperkümeler evreninde tanımlanmış bir denklem sisteminin tek bir çözümü olduğunu söyler, ve yapay zeka, veritabanı kuramı ve durum kuramı gibi alanlarda uygulama bulur. Bu tezde, Çözüm Teoremi 'nin uygulanabileceği türde denklem dizgelerini çözebilen ve çözümleri çizgeler şeklinde gösterebilen HYPERS OLVER adlı bir program tanıtılmaktadır. Anahtar Sözcükler: Küme Kuramı, ZFC, İyi-yapılanmamış Kümeler, Hiperkü me Kuramı (ZFC~/AFA), Denklem Çözme, Çözüm Teoremi","ABSTRACT SOLVING EQUATIONS IN THE UNIVERSE OF HYPERSETS Müjdat Pakkan M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman February, 1993 Hyperset Theory (a.k.a. ZFC~/AFA) of Peter Aczel is an enrichment of the classical ZFC set theory and uses a graphical representation for sets. By al lowing non-well-founded sets, the theory provides an appropriate framework for modeling various phenomena involving circularity. ZFC~/AFA has an im portant consequence that guarantees a solution to a set of equations in the universe of hypersets, viz. the Solution Lemma. This lemma asserts that a system of equations defined in the universe of hypersets has a unique solution, and has applications in areas like artificial intelligence, database theory, and situation theory. In this thesis, a program called HYPERSOLVER, which can solve systems of equations to which the Solution Lemma is applicable and which has built-in procedures to display the graphs depicting the solutions, is presented. Keywords: Set Theory, ZFC, Non-well-founded Sets, Hyperset Theory (ZFC~/AFA), Solving Equations, The Solution Lemma IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET LOTOS'DA TEST EDİLEBİLİRLİK İÇİN BELİRTİME DİLİMLEME YAKLAŞIMI Ahmet Feyzi ATEŞ Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Behçet Sarıkaya Ağustos, 1993 Yakın zamanlarda iletişim protokollarının belirtiminde biçimsel metodların kullanımının artmasıyla, protokol uyarlamalarının uygunluk testlerinin de biçimsel belirtimlere dayandırılması gereği doğmuştur. Bu durum, protokol belirtimlerinde, test üretmeyi kolaylaştıracak tasarım ilkeleri bulma problemini ortaya çıkarmıştır. Bu konuya test edilebilirlik için belirtim adı verilmektedir, ve bu çalışmada biçimsel bir tanımlama tekniği olan LOTOS için incelenmiştir. Test edilebilirlik için belirtim konusuna, temel protokol belirtimleri tasarlama ve daha sonra test üretmede kullanılmak üzere işlevsel belirtimleri temel belirtimlerden elde etme perspektifinde yaklaşılmıştır. İşlevsel belirtimleri elde etmede kullanılan yönteme dilimleme adı verilmektedir. Yazılım mühendisliği dalında daha önce yapılan çalışmalardan esinlenerek, protokol belirtimlerinin dilimleri, hiyerarşik biçimde tasarlanan test yapılarına göre, ve her dilim be lirli bir protokol işlevine karşılık gelecek şekilde sistematik olarak elde edilmiş ve daha sonraki test üretme safhası elde edilen dilimlere dayandırılmıştır. Geliştirilen teknikler, basit, sistem-durumuna yönelik INRES ve ACSE protokolları ile birlikte, kıstasa yönelik belirtim tarzında yazılmış gerçek bir temel belirtim olan OSI Transport Protokolü üzerinde gösterilmiştir. Sonuçlar şunu göstermektedir ki, işlevsel belirtimlerden çıkarılan testler, test durum analizi ve temsili açısından bazı dikkate değer özellikler taşımaktadır. ivAnahtar Sözcükler: Uygunluk testi, Test edilebilirlik için belirtim, LOTOS, Dilimleme.","ABSTRACT SLICING APPROACH TO SPECIFICATION FOR TESTABILITY IN LOTOS Ahmet Feyzi ATEŞ M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Behçet Sarıkaya August, 1993 With the recent increase in the use of formal methods in specification of communication protocols, there is a need to base the conformance testing of protocol implementations on formal specifications. This brings in the prob lem of finding out special design issues to be used in the specification of such systems that facilitate test generation. This aspect is called Specification For Testability, and it is investigated in this study for the particular formal de scription technique LOTOS. Specification for testability is approached from the perspective of designing formal base protocol specifications, and then de riving functional specifications from base specifications in order to use in test generation. The method utilized for the derivation of functional specifica tions is called slicing. As inspired from previous work in software engineering, slices of protocol specifications are obtained systematically according to the hierarchically designed test suite structures, where each slice corresponds to a particular function of the protocol, and subsequent test generation is based on the obtained slices. The techniques developed are demonstrated on the sim ple state-oriented specifications of INRES and ACSE protocols along with a real base specification of the OSI Transport Protocol written in the constraint- oriented specification style. The results indicate that tests derived from func tional specifications have some remarkable properties with respect to test case analysis and representation. inAnahtar Sözcükler: Uygunluk testi, Test edilebilirlik için belirtim, LOTOS, Dilimleme."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ABSTRACT GRAPH AND HYPERGRAPH PARTITIONING Ali Daşdan M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Cevdet Aykanat September, 1993 Graph and hypergraph partitioning have many important applications in var ious areas such as VLSI layout, mapping, and graph theory. For graph and hypergraph partitioning, there are very successful heuristics mainly based on Kernighan-Lin's minimization technique. We propose two novel approaches for multiple- way graph and hypergraph partitioning. The proposed algorithms drastically outperform the best multiple-way partitioning algorithm both on randomly generated graph instances and on benchmark circuits. The proposed algorithms convey all the advantages of the algorithms based on Kernighan- Lin's minimization technique such as their robustness. However, they do not convey many disadvantages of those algorithms such as their poor performance on sparse test cases. The proposed algorithms introduce very interesting ideas that are also applicable to the existing algorithms without very much effort. Keywords: Graph Partitioning, Hypergraph Partitioning, Circuit Partitioning, Local Search Heuristics, Partitioning Algorithms 111","ABSTRACT GRAPH AND HYPERGRAPH PARTITIONING Ali Daşdan M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Cevdet Aykanat September, 1993 Graph and hypergraph partitioning have many important applications in var ious areas such as VLSI layout, mapping, and graph theory. For graph and hypergraph partitioning, there are very successful heuristics mainly based on Kernighan-Lin's minimization technique. We propose two novel approaches for multiple- way graph and hypergraph partitioning. The proposed algorithms drastically outperform the best multiple-way partitioning algorithm both on randomly generated graph instances and on benchmark circuits. The proposed algorithms convey all the advantages of the algorithms based on Kernighan- Lin's minimization technique such as their robustness. However, they do not convey many disadvantages of those algorithms such as their poor performance on sparse test cases. The proposed algorithms introduce very interesting ideas that are also applicable to the existing algorithms without very much effort. Keywords: Graph Partitioning, Hypergraph Partitioning, Circuit Partitioning, Local Search Heuristics, Partitioning Algorithms 111"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE İÇİN BİR ATN GRAMERİ Coşkun Demir Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Dr. Kemal Of lazer Temmuz, 1993 Sözdizimsel dil çözümlemesi herhangi bir doğal dil işleme sistemindeki önemli aşamalardan biridir. Genişletilmiş geçiş ağları (ATNs) doğal dil çözümlemesi için kullanılan ilk ve en yaygın örneklerden biridir. ATNs bir Turing makinasının üretici gücüne sahiptir ve 1970 yılında Woods tarafından kullanılıp tanıtılmıştır. Bu tez Türkçe'nin basit ve girişik cümleleri kapsayan bir altkümesi için bir ATN grameri geliştirilmesi çalışmalarımızı sunmaktadaır. Gramerimizde beş tane ağ vardır: kapsamımızın içine giren cümle yapılarını kapsayan cümle (S) ağı, isim öbeği (NP) ağı, belirteç öbeği (ADVP) ağı ve son olarak girişik cümlelerin halledilmesi için tümcecik (CLAUSE) ve ulaç (GERUND) ağları. Sonuç olarak da yüksek sayıda Türkçe cümle çözümleme sonuçları sunulmaktadır. Anahtar Sözcükler: Doğal dil işleme, dilbilgisi, çözümleme, ATNs, Türkçe. iv","ABSTRACT AN ATN GRAMMAR FOR TURKISH Coşkun Demir M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Dr. Kemal Of lazer July, 1993 Syntactic parsing is an important step in any natural language processing system. Augmented Transition Networks (ATNs) are procedural mechanisms which have been one of the earliest and most common paradigms for parsing natural language. ATNs have the generative power of a Turing machine and were first popularized by Woods in 1970. This thesis presents our efforts in developing an ATN grammar for a subset of Turkish including simple and complex sentences. There are five networks in our grammar: the sentence (S) network, which includes the sentence structures that falls in our scope, the noun phrase (NP) network, the adverbial phrase (ADVP) network and finally the clause (CLAUSE) and gerund (GERUND) networks for handling complex sentences. We present results from parsing a large number of Turkish sentences. Keywords: Natural Language Processing, Syntax, Parsing, Augmented Tran sition Networks, Turkish. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BENZETİM VE YERLEŞTİRME İÇİN ÇOK GENİŞ ÖLÇEKLİ TÜMLEŞİK (VLSI) DEVRE PARÇALAMA R.adwan Talıboııb Bilgisayar Mühendisliği ve Eııformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Cevdet Aykanat Ocak 1993 Çok Geniş Ölçekli Tümleşik ( ÇGÜT ) devrelerin benzetim süresi devreyi daha küçük birçok all-devreye parçalayarak oldukça azaltılabilir. Bilgisayar benzetiminin etkinliğini arttırmak İçin geniş ölçekli devrelerin daha uygun ve bazen tie benzer birçok alt-devreye parçalanmasının temelinde Düğüm Bölme ( Node Splitting ) yatmakladır. Bu tezde, ÇGOT devrelerini etkin bir şekilde bölmek için Düğüm Bölme'ye dayalı bir yöntem kullanılmaktadır. Önerilen algoritmalar, Bilkent Üniversitesi Elektrik ve Elektronik Bölümü'nde tasarlanan ÇGOT analog devre benzetim sisteminin etkinliğini geliştirmek üzere bir ön-işlem adımı olarak kullanılacaklardır. Aynı algoritmalar, küçük değişikliklerle, transistor bağlantılarına göre transistor grupları oluşturmak üzere de kullanılmaktadır. Gruplanan devre daha sonra Tavlama Benzetimi ve Kernighan-Lin gibi bilinen algoritmalar kullanarak parçalanacaktır. Bu metodun sonuçları, geleneksel metotlarınkine göre daha iyi çıkmıştır. Bağlantı sayısındaki yaklaşık % 20 Tık azalma, ile birlikte GPU zamanında yaklaşık 4VI katlık bir hızlanma gözlenmiştir. Deneysel sonuçlar, önerilen algoritmaların benzetim ve yerleştirme için ÇCOT devre parçalamada etkin bir şekilde kul lanılabileceğini göstermektedir. Anahtar kelimeler : ÇCOT Devre Benzetimi, Yerleştirme, Düğüm Bölme, Parçalama, Tavlama Benzetimi, Kernighan-Lin.","ÖZET BENZETİM VE YERLEŞTİRME İÇİN ÇOK GENİŞ ÖLÇEKLİ TÜMLEŞİK (VLSI) DEVRE PARÇALAMA R.adwan Talıboııb Bilgisayar Mühendisliği ve Eııformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Cevdet Aykanat Ocak 1993 Çok Geniş Ölçekli Tümleşik ( ÇGÜT ) devrelerin benzetim süresi de vreyi daha küçük birçok all-devreye parçalayarak oldukça azaltılabilir. Bilgisayar benzetiminin etkinliğini arttırmak İçin geniş ölçekli devrelerin daha uygun ve bazen tie benzer birçok alt-devreye parçalanmasının temelinde Düğüm Bölme ( Node Splitting ) yatmakladır. Bu tezde, ÇGOT devrelerini etkin bir şekilde bölmek için Düğüm Bölme'ye dayalı bir yöntem kullanılmaktadır. Önerilen algoritmalar, Bilkent Üniversitesi Elektrik ve Elektronik Bölümü'nde tasarlanan ÇGOT analog devre benzetim sisteminin etkinliğini geliştirmek üzere bir ön-işlem adımı olarak kullanılacaklardır. Aynı algoritmalar, küçük değişikliklerle, transistor bağlantılarına göre transistor grupları oluşturmak üzere de kullanılmaktadır. Gruplanan devre daha sonra Tavlama Benzetimi ve Kernighan-Lin gibi bilinen algoritmalar kullanarak parçalanacaktır. Bu metodun sonuçları, geleneksel metotlarınkine göre daha iyi çıkmıştır. Bağlantı sayısındaki yaklaşık % 20 Tık azalma, ile birlikte GPU zamanında yaklaşık 4VI katlık bir hızlanma gözlenmiştir. Deneysel sonuçlar, önerilen algoritmaların benzetim ve yerleştirme için ÇCOT devre parçalamada etkin bir şekilde kullanılabileceğini göstermektedir. Anahtar kelimeler : ÇCOT Devre Benzetimi, Yerleştirme, Düğüm Bölme, Parçalama, Tavlama Benzetimi, Kernighan-Lin."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE İÇİN BİR SOZGUKSEL-IŞLEVSEL GRAMER Zelal Güngördü Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Dr. Kemal Of lazer Temmuz, 1993 Doğal dil işleme, hem akademik hem de ticari nedenlerden ötürü popülerliği her geçen gün daha da artan bir araştırma alanıdır. Tümcelerin sözdizimsel olarak çözümlenmesi, doğal dil işleme alanındaki birçok uygu lamanın temelini oluşturmaktadır. Türkçe sözdizimi üzerine dilbilimsel açıdan kapsamlı çalışmalar yapılmıştır. Ancak, bunların hiçbiri bilgisayar ile çözümlemeye yönelik değildir. Bu çalışma, Türkçe'nin sözdiziminin bilgisa yar ile çözümlenmesi konusunu etraflıca inceleyen ilk çalışmalardan biridir. Bu tezde, Türkçe için bir sözcüksel-işlevsel gramer sunulmaktadır. Şu andaki çalışmamız, yapı olarak basit veya girişik olan düz Türkçe tümcelerini ele alıp, başarı ile çözümlemektedir. Anahtar Sözcükler: Doğal dil işleme, bilgisayarla dil işleme, Türkçe sözdizimi, çözümleme, sözcüksel-işlevsel gramer. iv","ABSTRACT A LEXICAL-FUNCTIONAL GRAMMAR FOR TURKISH Zelal Güngördü M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Dr. Kemal Of lazer July, 1993 Natural language processing is a research area which is becoming increas ingly popular each day for both academic and commercial reasons. Syntactic parsing underlies most of the applications in natural language processing. Al though there have been comprehensive studies of Turkish syntax from a linguis tic perspective, this is one of the first attempts for investigating it extensively from a computational point of view. In this thesis, a lexical-functional grammar for Turkish syntax is presented. Our current work deals with regular Turkish sentences that are structurally simple or complex. Keywords: Natural language processing, computational linguistics, Turkish syntax, parsing, lexical-functional grammar. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET SIMLIB: NESNEYE- YÖNELİK BENZETİM İÇİN BİR SINIF KÜTÜPHANESİ Oğuz Işıklı Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışmanlar: Doç. Dr. Varol Akman ve Prof. Dr. Akif Eyler Temmuz 1993 Benzetim, karar verme sistemlerinde çok yaygın şekilde kullanılan teknikler den biridir. Bir gerçel dünya sisteminin matematiksel olarak modellenmesi, benzetim analistinin temel bir görevidir. Model gerçekleştiriminde hangi bil gisayar programlama dilinin kullanılacağı da önemli bir konudur. Bu alan daki son araştırmalar, benzetim uygulamaları ile nesneye yönelik program lama arasındaki yakın benzerlikler üzerinde yoğunlaşmıştır. Bu tezin amacı, kesikli-olay benzetim uygulamalarında nesneye yönelik yaklaşımın kullanımını araştırmaktır. Tezde, bir benzetim programının temel bileşenlerini sağlayan bir sınıf kütüphanesi tanıtılmaktadır. Yaklaşımın avantaj ve dezavantajları üç prototip uygulamanın yardımıyla tartışılmaktadır: tek-kuyruk/tek-işgören sistemi, üretim-hattı sistemi ve asansör sistemi. Anahtar Sözcükler: Kesikli-Olay Benzetimi, Nesneye- Yönelik Programlama, Nesneye- Yönelik Tasarım, Sınıf Kütüphaneleri, C++ Programlama Dili, Tek- Kuyruk/Tek-İşgören Sistemleri, Üretim-Hattı Sistemleri, Asansör Sistemleri.","ABSTRACT SIMLIB: A CLASS LIBRARY FOR OBJECT-ORIENTED SIMULATION Oğuz Işıklı M.S. in Computer Engineering and Information Science Advisors: Assoc. Prof. Varol Akman and Prof. Akif Eyler July, 1993 Simulation is one of the most widely used techniques in decision making. Math ematical modeling of a real world system is a major task of the simulation analyst. The selection of a computer language for implementing the model is also important. Recent research in this area has focused on the compatibility between simulation implementations and the object-oriented paradigm. It is the purpose of this thesis to explore the use of an object-oriented approach for the implementation of discrete event simulation applications. We present a class library which provides the skeletal elements of a simulation. The ad vantages and the disadvantages of the approach are discussed with the help of three prototype implementations: the single-queue/single-server system, the production-line system, and the elevator system. Keywords: Discrete Event Simulation, Object-Oriented Programming, Object- Oriented Design, Class Libraries, C++ Programming Language, Single-Queue/ Single-Server Systems, Production-Line Systems, Elevator Systems. IV"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ÖZNİTELİK BÖLÜNTÜLERİ İLE Ö?RENME İzzet Şirin Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Y. Doç. Dr. Halil Altay Güvenir Ağustos, 1993 Bu çalışmada öznitelik bölünmesine dayalı yeni bir mekanik öğrenme yöntemi sunulmuştur. Bu yöntem kullanılarak bir sınıflama algoritması olan Öznitelik Bölüntüleri ile Sınıflayım CFP'nin yazılımı hazırlanmıştır. CFP algoritması mekanik öğrenmeyi tümevarım ve artırımlı öğrenme yöntemlerini kullanarak sağlar. CFP algoritmasında bölütü elemanları temel gösterim unsurlarıdır. Başlangıçta bölüntü elemanları bir boyutlu uzayda bir noktayı ifade ederken, zaman içinde bu elemanlar genişleyerek bir aralığı ifade ederler. Bölüntü el emanları parçalanarak özelleştirilirler. CFP algoritmasının kuramsal analizi yaklaşık olarak doğru kuramına (PAC-model) göre yapılmıştır ve benzer sis temlerle uygulama sonuçları karşılaştırılmıştır. Anahtar Sözcükler: Mekanik öğrenme, tümevarımsal öğrenme, artırımsal öğrenme, denetimli öğrenme, öznitelik bölüntüleme. iv","ABSTRACT LEARNING WITH FEATURE PARTITIONS İzzet Şirin M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Halil Altay Güvenir August, 1993 This thesis presents a new methodology of learning from examples, based on feature ?partitioning. Classification by Feature Partitioning (CFP) is a particu lar implementation of this technique, which is an inductive, incremental, and supervised learning method. Learning in CFP is accomplished by storing the objects separately in each feature dimension as disjoint partitions of values. A partition, a basic unit of representation which is initially a point in the feature dimension, is expanded through generalization. The CFP algorithm special izes a partition by subdividing it into two subpartitions. Theoretical (with respect to PAC-model) and empirical evaluation of the CFP is presented and compared with some other similar techniques. Keywords: Machine learning, inductive learning, incremental learning, super vised learning, feature partitioning. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BİR RADYOLOJİK GÖRÜNTÜ İZLEME PROGRAMI TASARIMI VE YAZIMI Gürhan Keskin Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yard. Doç. Dr. Kemal Oflazer Haziran, 1993 Tıp alanında, görüntü arşivleme ve iletişim sistemlerinde son zamanlarda meydana gelen ilerlemeler, doktorlara ve radyologlara radyolojik görüntüleri iş istasyonları (görüntü istasyonları) yolu ile incelemelerine olanak tanımaktadir. Bu tezde, büyük hastanelerin radyoloji kliniklerinde kullanılmak üzere, iş is tasyonlarının görüntü istasyonları olarak kullanımına olanak tanıyan, bir rady olojik görüntü izleme programı tasarımı yapıldı ve gerçekleştirildi. RADVIEW2 ismi verilen programın ana kullanım amacı, tıbbi görüntülerin saklanmasını ve erişimini, radyologlara kullanıcı ile dost bir ortamda sun masıdır. Ayrıca bu program ile, bir hastanın radyolojik bilgilerine ve görüntülerine hızlı bir erişim de sağlanmaktadır. Anahtar Sözcükler: Görüntü Arşivleme ve İletişim Sistemi, Hastane Bilgi Sis temi, Radyoloji Bilgi Sistemi. 2 RADVIEW, Radyolojik görüntü izleme anlamına gelen, İngilizce "" Radiological Image Viewing"" kelimelerinden türetilmiştir. iv","ABSTRACT DESIGN AND IMPLEMENTATION OF A RADIOLOGICAL IMAGE VIEWING TOOL Gürhan Keskin M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Kemal Oflazer June, 1993 Recent developments in Picture Archiving and Communication Systems (PACS) in clinical environment allow physicians and radiologists to access and assess radiographic images directly through imaging workstations. In this thesis, an imaging workstation called RADVIEW1 has been designed and implemented for using in a PACS environment in radiology departments of large hospitals. The main function of RADVIEW is the archiving of and access to radiologi cal images by maintaining user friendly interactive environment to radiologists. The system can provide rapid access to any or all radiological information as sociated with a patient. Keywords: Picture Archiving and Communication Systems, Hospital Informa tion Systems, Radiology Information Systems. RADVIEW stands for "".Radiological Image Viewing Tool."" ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özet YARDIMLAŞAN AKILLI SİSTEMLER İÇİN BİR MODEL Faruk Polat Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticisi: Assist. Prof. Dr. H.Altay Güvenir 1993 Dağıtık yapay us alanındaki araştırmalar, tasarım, tıp, iş yönetimi gibi karmaşık alan problemlerini çözmek için bir araya gelen birden fazla akıllı problem çözücünün çalışmalarının birleştirilmesini ve koordine edilmesini amaçlar. Problem çözücülerin değişik amaç, bilgi ve bakış açılarına sahip olmaları, problem çözümü aşamalarında çelişkilerin ortaya çıkmasına sebep olur. Dağıtık bilgi yönetimi çok iyi organize edilmiş çelişki yönetimi modellerini gerektirir. Bu tez çalışmasında, çoklu çelişki bulumu ve çözümüne dayalı bir yardımlaşan akıllı sistemler modeli anlatılmaktadır. Bu modelde her problem çözücü alan bilgisinden ayrı olarak çelişki yönetimi bilgisine sahip olup bu bilgi diğer problem çözücüler tarafından bilinmemekte ve erişilememektedir. Böylece tüm problem çözücüler tarafından bili nen bir çelişki yönetimi bilgisi bulunmamaktadır. Her problem çözücü, çelişkinin giderilmesine kendi çelişki yönetim bilgisine dayanarak katkıda bulunur. Geliştirilen model, yeni bir problem çözücünün sisteme entegre olması veya sistemden ayrılmasına olanak verirken, modelin hiç bir şekilde değiştirilmesini gerektirmez. Böylece model açık bilgi sistemleri mantığına erişir. Anahtar sözcükler: Dağıtık Yapay Us, Çelişki Bulumu, Çelişki Çözümü, Çelişki Yönetimi Bilgisi, Açık Bilgi Sistemleri. ııı","Abstract A NEGOTIATION PLATFORM FOR COOPERATING MULTI-AGENT SYSTEMS Faruk Polat Ph. D. in Computer Engineering and Information Science Supervisor: Assist. Prof.Dr. H.Altay Güvenir 1993 Research in Distributed Artificial Intelligence attempts to integrate and coor dinate the activities of multiple, intelligent problem solvers that interact to solve complex tasks in domains such as design, medical diagnosis, business manage ment, and so on. Due to the different goals, knowledge and viewpoints of the agents, conflicts might arise at any phase of the problem-solving process. Man aging diverse knowledge requires well-organized models of conflict resolution. In this thesis, a computational model for cooperating intelligent agents which openly supports multi-agent conflict detection and resolution is described. The model is based on the insights that each agent has its own conflict management knowl edge which is separated from its domain level knowledge. Each agent has its own conflict management knowledge which is not accessible or visible to others. Fur thermore, there are no globally known conflict resolution strategies. Each agent involved in a conflict chooses a resolution scheme according to its self-interest. The problem-solving environment allows a new problem solver to be added or anexisting one to be removed, without requiring any modification of the rest of the model, and therefore achieves open information system semantics. Keywords: Distributed Artificial Intelligence, Conflict Detection, Conflict Resolution, Conflict Management Knowledge, Open Informa tion System 11"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET IŞIN İZLEME YÖNTEMİYLE DOKU KAPLAMA Uğur Akdemir Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Prof. Dr. Bülent Özgüç Ekim, 1993 Doku kaplama ve küresel ışıklandırma yöntemleri kullanılarak gerçeğe uygun bilgisayar görüntüleri oluşturmak olasıdır. Günümüzde, ışın izlemesi, ba sitliği, sonuçlarının güzelliği ve kolay uygulanabilirliğinden dolayı en yaygın küresel ışıklandırma yöntemlerinden birisidir. Bu tezde, yüksek kalitede görsel etkiler yaratabilmek için doku kaplama yöntemleri ışın izlemesi ile birlikte kullanılmıştır. Doku kaplama yöntemlerinin uygulanması ve ön- süzme yöntemlerinin ışın izleme yöntemiyle birlikte kullanılmasını sağlayan bir yaklaşım sunulmuştur. Modelleme için Topologybook tarafından üretilmiş süpürülmüş yüzeyler kullanılmıştır. Anahtar Sözcükler: doku kaplama, ışın izlenmesi, alan örnekleme, süzme, toplanmış alan tabloları, süpürülmüş yüzeyler, Topologybook iv","ABSTRACT TEXTURE MAPPING WITH RAY TRACING Uğur Akdemir M.S. in Computer Engineering and Information Science Advisor: Prof. Bülent Özgüç October, 1993 By using texture generation and global illumination techniques, it is possible to produce realistic computer images. Currently, ray tracing is one of the most popular global illumination techniques due to its simplicity, elegancy, and easy implementation. In this thesis, texture mapping techniques are used with ray tracing to generate high quality visual effects. The implementation of the mapping process is presented and an approach for combining prefiltering techniques with ray tracing is introduced. General sweep surfaces produced by Topologybook are used for modeling. Keywords: texture mapping, ray tracing, area sampling, filtering, summed area tables, sweep surfaces, Topologybook m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özet NESNESEL VERİ TABANLARI İÇİN SORGULAMA MODELİ VE NESNESEL CEBİR Reda ALHAJJ Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticisi: Prof. Dr. M.Erol ARKUN Şubat 1993 Sorgulama modeli, herhangi bir veri tabanının en önemli kısmıdır. Bu bağlamda, ilişkisel model, çok iyi tanımlanmış bir sorgulama modeline sahiptir. Buna karşılık nesnesel veri tabanları için iyi tanımlanmış bir sorgulama modeli henüz kabul edilmemiştir. Bu, nesnesel veri tabanlarına karşı getirilen en önemli eleştiridir. Böylece, formal bir nesnesel cebir tanımlanması, genel nesnesel veri tabanı teorisinin geliştirilmesinde en önemli basamaklardan biridir. Nesnesel veri tabanlarında, mesajlar veri tabanım kullanmaya olanak tanımalarına ragmen, hala karmaşık işlemlerin kolayca yapdabilmesi ve içerikle erişimin gerçekleştirilmesi için bir sorgulama modeline ihtiyaç vardır. Bu tez çalışmasında, nesnesel veri tabanları için nesnelerin davranışlarına ek olarak, yapdarının da gözönüne akndığı bir sorgulama modeli tanımlanmaktadır. Bu modelde, sadece hali hazırdaki nesnelerin işlenmesi değil, aynı zamanda yeni nesne ve bağıntıların yaratılması desteklenmiştir, ilişkisel modelin beş temel işlemine eşdeğer işlemlerin yanı sıra, ek olarak tek düzey izdüşüm, yuvalama ve bütünleme fonksiyonları tanımlanmıştır. Böylece, önerilen nesnesel cebir, ilişkisel cebiri kapsamaktadır. Aym zamanda, doğrusal özyineleme, hiç bir ek işlev gerektirmeksizin tanımlanmıştır. Hem işleçler hem de buna ek olarak işlevlerin sonuçları kümeler çifti -nesneler kümesi ve bunlara uygulanabilen mesaj terimleri kümesi (mesaj dizileri) olarak karakterize edilirler. Kapalı olma özelliğinin, işlemlerin sonuçlarının bir sorgudaki işleçler gibi aynı karakteristiğe sahip olması nedeni ile doğal olarak korunduğu gösterilmektedir. mHer sınıfın, bir nesne kümesi tanımlaması ve bunun için bir mesaj terim kümesi türetilmesiyle, bir işlecin özelliklelerine sahip olduğu gösterilmektedir. Bununla birlikte, bir sorgunun çıktısının bir sınıfın niteliklerine sahip olduğu gösterilmektedir. Ayrıca, bir sorgu çıktısı ile icsleçlerinin arasındaki alt /üst sınıf ilişkisinin nasıl oluştuğu ve sonucun şema yapısında kalıcı bir sınıf olarak nasıl yerleştirileceği gösterilmektedir. Böyle bir sınıf, tekrar kullanılabilirliği kalıtım vasıtası ile maksimuma ulaştıracak şekilde doğal ve uygun olarak şemada saklanabilmektedir. Ayrıca nesnesel cebir işlevlerinin eşdeğerleri tanımlanarak sorgu optimizasyonunda önemli bir özellik olan Cartesian- Çarpım işleminin birleşme özelliğinin doğruluğu kanıtlanmaktadır. Son olarak, şema evriminin, nesnesel veri tabanlarmca sağlanması gereken bir özellik olduğu anımsanırsa, önerilen nesnesel cebir işlemleri vasıtası ile şema evriminin sağlanması tezin bir başka katkısı olarak geliştirilmiştir. Anahtar sözcükler: veri tabam sistemi, nesnesel veri modeli, nesnesel veri tabam yönetimi sistemi, nesnesel sorgu modeli, nesnesel sorgu dili, nesne cebiri, tekrar kullanılabilirlik, yuvalanmış sorgu, şema evrimi, şema uyarlaması. iv","Özet NESNESEL VERİ TABANLARI İÇİN SORGULAMA MODELİ VE NESNESEL CEBİR Reda ALHAJJ Bilgisayar ve Enformatik Mühendisliği Doktora Tez Yöneticisi: Prof. Dr. M.Erol ARKUN Şubat 1993 Sorgulama modeli, herhangi bir veri tabanının en önemli kısmıdır. Bu bağlamda, ilişkisel model, çok iyi tanımlanmış bir sorgulama modeline sahiptir. Buna karşılık nesnesel veri tabanları için iyi tanımlanmış bir sorgulama modeli henüz kabul edilmemiştir. Bu, nesnesel veri tabanlarına karşı getirilen en önemli eleştiridir. Böylece, formal bir nesnesel cebir tanımlanması, genel nesnesel veri tabanı teorisinin geliştirilmesinde en önemli basamaklardan biridir. Nesnesel veri tabanlarında, mesajlar veri tabanım kullanmaya olanak tanımalarına ragmen, hala karmaşık işlemlerin kolayca yapdabilmesi ve içerikle erişimin gerçekleştirilmesi için bir sorgulama modeline ihtiyaç vardır. Bu tez çalışmasında, nesnesel veri tabanları için nesnelerin davranışlarına ek olarak, yapdarının da gözönüne akndığı bir sorgulama modeli tanımlanmaktadır. Bu modelde, sadece hali hazırdaki nesnelerin işlenmesi değil, aynı zamanda yeni nesne ve bağıntıların yaratılması desteklenmiştir, ilişkisel modelin beş temel işlemine eşdeğer işlemlerin yanı sıra, ek olarak tek düzey izdüşüm, yuvalama ve bütünleme fonksiyonları tanımlanmıştır. Böylece, önerilen nesnesel cebir, ilişkisel cebiri kapsamaktadır. Aym zamanda, doğrusal özyineleme, hiç bir ek işlev gerektirmeksizin tanımlanmıştır. Hem işleçler hem de buna ek olarak işlevlerin sonuçları kümeler çifti -nesneler kümesi ve bunlara uygulanabilen mesaj terimleri kümesi (mesaj dizileri) olarak karakterize edilirler. Kapalı olma özelliğinin, işlemlerin sonuçlarının bir sorgudaki işleçler gibi aynı karakteristiğe sahip olması nedeni ile doğal olarak korunduğu gösterilmektedir. mHer sınıfın, bir nesne kümesi tanımlaması ve bunun için bir mesaj terim kümesi türetilmesiyle, bir işlecin özelliklelerine sahip olduğu gösterilmektedir. Bununla birlikte, bir sorgunun çıktısının bir sınıfın niteliklerine sahip olduğu gösterilmektedir. Ayrıca, bir sorgu çıktısı ile icsleçlerinin arasındaki alt /üst sınıf ilişkisinin nasıl oluştuğu ve sonucun şema yapısında kalıcı bir sınıf olarak nasıl yerleştirileceği gösterilmektedir. Böyle bir sınıf, tekrar kullanılabilirliği kalıtım vasıtası ile maksimuma ulaştıracak şekilde doğal ve uygun olarak şemada saklanabilmektedir. Ayrıca nesnesel cebir işlevlerinin eşdeğerleri tanımlanarak sorgu optimizasyonunda önemli bir özellik olan Cartesian- Çarpım işleminin birleşme özelliğinin doğruluğu kanıtlanmaktadır. Son olarak, şema evriminin, nesnesel veri tabanlarmca sağlanması gereken bir özellik olduğu ammsamrsa, önerilen nesnesel cebir işlemleri vasıtası ile şema evriminin sağlanması tezin bir başka katkısı olarak geliştirilmiştir. Anahtar sözcükler: veri tabam sistemi, nesnesel veri modeli, nesnesel veri tabam yönetimi sistemi, nesnesel sorgu modeli, nesnesel sorgu dili, nesne cebiri, tekrar kullanılabilirlik, yuvalanmış sorgu, şema evrimi, şema uyarlaması. iv"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET içice ilişkiler için genişletilmiş bir ilişkisel CEBİR Eser Sükan Bilgisayar ve Enformatik Mühendisliği Bölümü, Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkun Ocak 1993 Bu çalışmada birinci normal biçimde olmayan ilişkileri formalize etmek için Roth-Korth-Silberschatz (RKS) [cf. ACM TODS 13(4): 389-417, 198% ve Abiteboul-Bidoit (AB) [cf. Journal of Computer System Sciences 33(4): 361- 393, 1986] tarafından geliştirilmiş veritabanı modelleri ve bu modeller için tanımlanmış bir ilişkisel cebir sunulmaktadır. Gerek RKS gerekse AB cebirleri içinde yer alan genişletilmiş küme operatörlerinden birleşim ve farkın, bilgi eşdeğer olmadığı gösterilmektedir. RKS'nin modeli kullanılarak, genişletilmiş küme operatörlerinden birleşim ve fark yeniden tanımlanmaktadır. Ayrıca yeni tanımlanan birleşim, fark ve RKS'nin genişletilmiş kesişim operatörlerinin bilgi eşdeğer olduğu gösterilmektedir. Anahtar Sözcükler: Veri modelleri, normal biçimler, genişletilmiş cebir, içice ilişkiler, birinci normal biçimde. olmayan ilişkiler, bölümlemeli normal biçim iv","ABSTRACT AN EXTENDED RELATIONAL ALGEBRA FOR NESTED RELATIONS Eser Sükan M.S. in Computer Engineering and Information Science Supervisor: Prof. Erol Arkun January 1993 In this study the database models of Roth-Korth-Silberschatz (RKS) [cf. ACM TODS 13(4): 389-417, 1988] and Abiteboul-Bidoit (AB) [cf. Journal of Computer and System Sciences 33(4)' 361-393, 1986] to formalize non-first- normal-form relations are presented along with their extended relational alge bra. We show that the extended set operators union and difference of RKS and AB are not information equivalent. Using the model of RKS and restricting ourselves to union and difference, we define our extended set operators and show that these two operators and the extended intersection of RKS are infor mation equivalent. Keywords: Data models, normal forms, extended algebra, nested relations, non-first-normal-form relations, partitioned normal form m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET MVL DİZGESİNİN FİZİKSEL DÜNYA HAKKINDA NİTEL USLAMLAMADA KULLANIMI Mine Ülkü Şencan Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Varol Akman Temmuz 1993 Matthew Ginsberg'ün MVL teorem tanıtlama dizgesinin çıkarım mekanizması kullanılarak deneysel bir program, QRM, gerçekleştirilmiştir. QRM dinamik dizgeler hakkında nitel terimler kullanarak uslamlama yapabilir. Kenneth For- bus'un Nitel Süreç Kuramı'na (QPT) göre tanımlanmış fiziksel bir dizgenin ve rilen başlangıç durumundan itibaren ulaşabileceği diğer durumları betimleyen ağacı öngörebilir. Bu tezde, bilgi gösterimi ve QPT'ye dayalı temel nitel us lamlama konuları üzerinde durulmaktadır. MVL'in Nitel Fizik programları yazılımında neler sağlayabileceğine dair bazı önerilerde bulunulmaktadır. Anahtar Sözcükler: Çok-değerli Mantıklar (MVL), Nitel Süreç Kuramı (QPT), Nitel Fizik, Öngörme, Sağduyusal Uslamlama. iv","ABSTRACT UTILIZATION OF THE MVL SYSTEM IN QUALITATIVE REASONING ABOUT THE PHYSICAL WORLD Mine Ülkü Şencan M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman July, 1993 An experimental program, QRM, has been implemented using the inference mechanism of the Multivalued Logics (MVL) Theorem Proving System of Matthew Ginsberg. QRM has suitable facilities to reason about dynamical systems in qualitative terms. It uses Kenneth Forbus's Qualitative Process Theory (QPT) to describe a physical system and constructs the envisionment tree for a given initial situation. In this thesis, we concentrate on knowledge representation issues, and basic qualitative reasoning tasks based on QPT. We offer some insights about what MVL can provide for writing Qualitative Physics programs. Keywords: Multivalued Logics (MVL), Qualitative Process Theory (QPT), Qualitative Physics, Envisioning, Commonsense Reasoning. in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BİR TEOREM İSPATLAYICININ BİR SIVI AKIŞ KONTROL PROGRAMINI DO?RULAMADA KULLANILMASI Erkan Uçar Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Doç. Dr. Varol Akman Şubat, 1993 Güvenilir yazılım ürettiğinden dolayı program doğrulama önemli bir iştir. Uygulamalarının gerçek dünyada çalıştırılmasından ötürü gerçek-zamanlı kon trol programlarının doğrulanması özel dikkat ister ve bunların matematiksel özelliklerini bulmak zordur. Ayrıca, büyük gerçek-zamanlı programlan elle doğrulamak imkansızdır. Bu nedenlerden dolayı, mekanik program doğrulama sistemleri kullanılması gerekir. Aslında genel-amaçlı bir otomatik teorem is- patlayıcı olan Boyer-Moore Teorem Ispatlayıcısı (NQTHM) böyle bir sistemdir. Biz NQTHM'i kullanarak basit bir gerçek-zamanlı sistemin, yani bir su-tank kompleksinin, kontrol programlarını doğruladık. Bu amaca yönelik olarak WA TERWORKS isimli kullanışlı bir benzetim sistemi gerçekleştirdik. Anahtar Sözcükler: Program Doğrulama, Boyer-Moore Teorem Ispatlayıcısı (NQTHM), Gerçek- Zamanlı Kontrol, Benzetim, Sağduyusal Akıl Yürütme, Sıvılar iv","ABSTRACT THE USE OF A THEOREM PROVER TO VERIFY A LIQUID FLOW CONTROL PROGRAM Erkan Uçar M.S. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman February, 1993 Program verification is an important task since it produces reliable software. Verification of real-time control programs needs special attention since these run in the real world and it is difficult to determine their mathematical prop erties. Besides, validating large real-time programs manually is impossible. Owing to these reasons, mechanical program verification systems have to be used. Boyer-Moore Theorem Prover (NQTHM) which, in fact, is a general- purpose automated theorem prover, is such a system. We corroborated the control programs of a simple real-time system, viz. a water-tank complex, using NQTHM. A useful simulator (called WATERWORKS) has been imple mented for this purpose. Keywords: Program Verification, Boyer-Moore Theorem Prover (NQTHM), Real-Time Control, Simulation, Commonsense Reasoning, Liquids m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNEYE YÖNELİK HAREKET SOYUTLAMASI Bilge Erkan Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Bülent Özgüç Eylül 1993 Bir animasyon dizisi üretiminde önemli bir problem hareketi tanımlamak ve denetlemek için verilmesi gereken çok yoğun bilgidir. Karmaşık animasyon dizilerinin üretimi, bu dizilerin bazı soyutlanmış diziler üzerine kurulmasıyla, daha az miktarda bilgi vererek olasıdır. Soyutlama, önemli özelliklerin ön plana çıkartılıp önemli olmayanların gizlenmesi şeklinde bir yapılaşma ile karmaşıklıkla başa çıkılmasını sağlar. Bizim çalışmamızda, nesneye yönelik kavramlar yardımıyla, karmaşık animasyon dizileri hazırlanması için hareket soyutlaması yapılmıştır. Bir animasyon dizisinin ara çerçevelerinin üretiminde parametrik anahtar-çerçeve interpolasyon tekniği kullanılmıştır. Bu teknikte, akıcı ara çerçeveler üretilmesi için modelin parametreleri interpole edilir. Çalışmamızda, bir modelin hareketini ifade eden parametreler, yer, yön, büyüklük, şekil ve renk tir. Yön değişimleri birim quaternionlax ile gerçekleştirilmektedir. Yeterli ve iyi bir kinetik denetim sayesinde dinamik hareketin iyi bir hayali görünümü ver ilebilir. Bu nedenle zamanlama ve yavaş giriş, yavaş çıkış denetimleri sağlanmışır. Anahtar kelimeler: Nesneye yönelik animasyon, parametrik anahtar-çerçeve interpolasyonu, hareket soyutlaması, quaternionlax","ABSTRACT OBJECT-ORIENTED MOTION ABSTRACTION Bilge Erkan M.S. in Computer Engineering and Information Science Supervisor: Prof. Bülent Ozgüç September 1993 An important problem in the production of an animation sequence is the great amount of information necessary to control and specify the motion. Specification of complex animation sequences with less amount of information is possible if they are built over some abstracted sequences. Abstraction supports dealing with complexity by structuring, so that the necessary features are made available while those that are not necessary are hidden. In our work, motion abstraction is used to build complex animation sequences by the help of object oriented concepts. Parametric key-frame interpolation method is used for producing the in-between frames of an animation sequence. In this technique, the parameters of the model are interpolated for smooth in-betweens. The parameters that define the motion of a model, in our work, are position, orientation, size, shape and color. Orientation transformations are implemented by unit quaternions. Sufficient and good kinetic control provides a good illusion of dynamics, so timing, slow-in and slow-out controls are being supported. Keywords: Object-oriented animation, parametric key-frame interpolation, motion abstraction, quaternions"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DERECELİ GELİŞEN IŞIMA İÇİN PARALEL İŞLEME Tolga K. Çapın Bilgisayar ve Enformatik Mühendisliği, Yüksek Lisans Danışman: Yrd. Doç. Dr. Cevdet Aykanat Eylül, 1993 Dereceli gelişen ışıma gerçeğe uygun görüntü üretmek için gittikçe daha fazla kullanılmakta olan bir yöntemdir. Yöntem, ışığın sahnede dağılımını başarılı bir şekilde hesaplamakta, ancak çok fazla işlem gerektirmektedir. Bu tezde, dereceli gelişen ışıma yönteminin zincir ve hiperküp bağlantılı dağıtık hafızalı çok işlemciler üzerinde paralel hesaplanması araştırılmaktadır. Işık dağılımının sırasının sağlanması ve basit topolojilerde iyi performans sağlanabilmesi için eşzamanlı paralel işlemeye dayalı iki yaklaşım geliştirilmiştir. Toplam iletişim miktarını asimtotik olarak azaltmak için verimli dolaştırma yöntemleri önerilmiştir. Önerilen ilk paralel yaklaşım, tek» işlemcili algoritmaya değişiklik getirmiştir, çünkü bu yaklaşımda aynı anda birden fazla yüzey ışık yayar, ikinci yaklaşım aynı anda sadece bir yüzey yayıcı yöntemine göre tasarlanmıştır. Önerilen yöntemler zincir ve hiper-küp bağlantılı dağıtık hafızalı çok işlemciler için hiperküp bağlantılı Intel iPSC/2 bilgisayarında gerçekleştirilmiştir. Önerilen yöntemlerin iş dağılımı kalitesi deneysel olarak gözlenmiştir. Anahtar Sözcükler: Gerçeğe Uygun Görüntü Üretme, Paralel işleme, Dağıtık Hafızalı Çok İşlemciler, Işıma, Dereceli Gelişen Işıma, Zincir Bağlantılı Topoloji, Hiperküp Bağlantılı Topoloji. iv","ABSTRACT PARALLEL PROCESSING FOR PROGRESSIVE REFINEMENT RADIOSITY Tolga K. Çapın M.S. in Computer Engineering and Information Science Advisor: Asst. Prof. Cevdet Aykanat September, 1993 Progressive refinement radiosity is an increasingly popular method for re alistic image synthesis of non-existing environments. The method successfully approximates the light distribution in an environment, however it requires excessive amount of computation. In this thesis, the progressive refinement method is investigated for parallelization on ring and hypercube-connected multicomputers. Two different approaches for parallelization, based on syn chronous parallelism with static task assignment, are proposed, in order to achieve better coherence in parallel light distributions and obtain good perfor mance on simple topologies. Efficient global circulation schemes are proposed in order to decrease the total volume of communication by asymptotical fac tors. The first scheme for parallelization is a modification of the sequential algorithm in that several patches shoot their energy at a time, while the sec ond scheme is based on the parallelism level of one shooting patch at a time. The proposed parallel algorithms are evaluated theoretically and implemented for ring and hypercube-connected topologies on Intel's iPSC/2 multicomputer. Load balance quality of the proposed schemes are evaluated experimentally. Keywords: Realistic Image Synthesis, Parallel Computing, Multicomputers, Radiosity, Progressive Refinement Radiosity, Ring Interconnection Topology, Hypercube Interconnection Topology. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ORTAK ALAN TAVLAMASINA DAYANAN PARALEL EŞLEME VE DEVRE PARÇALAMA ALGORİTMALARI Tevfik Bultan Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Cevdet Aykanat Ocak 1992 Birleşimsel eniyileme problemlerini çözmek için önerilen Ortak Alan Tavlama (OAT) algoritması, sinir ağları ve tavlama benzetimi yöntemlerinin özelliklerini taşır. Bu çalışmada, OAT algoritması eşleme ve devre parçalama problemlerine uyarlanmıştır. Önerilen algoritmaların karmaşıklığını asimtotik olarak azaltan verimli gerçekleme yöntemleri de geliştirilmiştir. Önerilen algoritmaların başaranları tavlama benzetimi ve Kernighan-Lin algoritmaları ile kıyaslıyarak değerlendirilmiştir. Elde edilen sonuçlar OAT'nin eşleme ve devre parçalama problemlerini çözmek için alternatif bir algoritma olarak kullanılabileceğini göstermektedir. Önerilen OAT algoritmaları verimli bir şekilde paralelleştirilmiştir. Devre parçalama problemi için önerilen paralel OAT algo ritmaları iPSC/2 hiperküp çok işlemcili bilgisayarında gerçeklenmiştir. Deney sel sonuçlar önerilen algoritmaların verimli bir şekilde paralelleştirilebildiklerini göstermektedir.VI Anahtar kelimeler : Ortak Alan Tavlaması, Sinir 'Ağları, Tavlama Benzetimi, Birleşimsel Eniyileme, Eşleme Problemi, Devre Parçalama Problemi, Paralel İşleme, Çok İşlemcili Bilgisayarlar.","ABSTRACT PARALLEL MAPPING AND CIRCUIT PARTITIONING HEURISTICS BASED ON MEAN FIELD ANNEALING Tevfik Bultan M. S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat January 1992 Mean Field Annealing (MFA) algorithm, recently proposed for solving com binatorial optimization problems, combines the characteristics of neural net works and simulated annealing. In this thesis, MFA is formulated for the mapping problem and the circuit partitioning problem. Efficient implemen tation schemes, which decrease the complexity of the proposed algorithms by asymptotical factors, are also given. Performances of the proposed MFA algo rithms are evaluated in comparison with two well-known heuristics: simulated annealing and Kernighan-Lin. Results of the experiments indicate that MFA can be used as an alternative heuristic for the mapping problem and the cir cuit partitioning problem. Inherent parallelism of the MFA is exploited by designing efficient parallel algorithms for the proposed MFA heuristics. Paral lel MFA algorithms proposed for solving the circuit partitioning problem are implemented on an iPSC/21 hypercube multicomputer. Experimental results show that the proposed heuristics can be efficiently parallelized, which is crucial for algorithms that solve such computationally hard problems. 'iPSC/2 is a registered trademark of Intel CorporationIV Keywords: Mean Field Annealing, Neural Networks, Simulated Annealing, Combinatorial Optimization, Mapping Problem, Circuit Partitioning Problem, Parallel Processing, Multicomputers."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Özet elektronik topoloji resim kitabi Ahmet Arslan Doktora, Bilgisayar ve Enformatik Mühendisliği Danışman: Doç. Dr. Varol Akman Ekim 1992 Elektronik topoloji resim kitabı, George K. Francis'in A Topological Picture- book [Springer- Verlag, New York (1987)] isimli kitabının bilgisayar ortamına aktarılmış hali olarak kabul edilebilir. Francis'in kitabı çoğunluğu elle çizilmiş karmaşık topolojik resimlerle doludur. Tezin genel amacı, çeşitli bilgisayarlı grafik tekniklerini kullanarak bu tür resimlerin çizimini otomatik hale getirmek ve basım kalitesinde kopyalarını üretmektir. Bu amaçla, genel bir yüzey modelleme aracı olarak süpürme yöntemi tartışılacaktır, ilginç yüzeyler elde etmek için bazı etkileşimli yöntemler verilecektir. Tb (Topologybook'un kısaltılmışı) programı açıklanacak ve bu yazılım tarafından üretilen değişik resimler sunulacaktır. Tb bir serbest-form yüzey modelleyici olup az bir çaba sarfederek çeşitli topolojik resimler çizmeyi mümkün kılar. Tb gerçekleştirimi temelde şu katı modelleme ilkesi üzerine kuruludur: Bir biçimin hesaplanması, bazı parametrik değişkenlerin kontrollü olarak süpürülmesine eşdeğerdir, yani biçim = süpürme + kontrol Anahtar Söscükle, Süpürme, yüzeyler, kullanıcı arabirim sistemleri, topoloji, katı modellleme, B-spline eğrileri, Bezier eğrileri, interpolasyon. ıı","Abstract AN ELECTRONIC TOPOLOGICAL PICTUREBOOK Ahmet Arslan Ph.D. in Computer Engineering and Information Science Advisor: Assoc. Prof. Varol Akman October, 1992 An electronic topological picturebook is envisaged as a computerized version of A Topological Picturebook by George K. Francis, Springer- Verlag, New York (1987). Francis' book is full of complicated topological figures, mostly drawn manually. The main goal of the thesis is to automate the production of such illustrations and to obtain publication-quality hardcopy using assorted tech niques of computer graphics. To that end, sweeping is discussed as a major surface modeling tool. Some interactive methods are given to produce interest ing topological surfaces. The program Tb (which stands for 'Topologybook') is described and various pictures generated by this software are presented. Tb is a free-form surface modeler and produces topological shapes with little ef fort. Central to the implementation of Tb is a paradigm of solid modeling in which computation of a shape is regarded as sweeping with some parametric variations, viz. shape = sweep + control Keywords Sweeping, surfaces, user interface systems, topology, solid model ing, B-spline curves, Bezier curves, interpolation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET MARS : BİR MODELLEME, CANLANDIRMA VE PARALEL BOYAMA SİSTEMİ Murat Aktıhanoğlu Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Dr. Bülent Özgüç Aralık, 1992 Bu araştırma, bir modelleme, canlandırma ve boyama sistemini tanımlamaktadır. Sistemin modelleme bölümünde, eklem ve parçalardan oluşan modeller yaratılmakta dır. Canlandırma sürecini oluşturan kişi daha sonra bu nesneleri anahtar çerçevelere yerleştirip, canlandırma sürecini oluşturmak için ara çerçevelerin ara değerlerini bulma işlemini başlatır. Boyama işlemi içinse model ve canlandırma bilgileri bir hiperküpe gönderilir. Tüm çerçeveler burada paralel bir şekilde dağıtımlı-işleme yöntemiyle ve çerçevelerin arasındaki benzerlikten faydalanılarak boyanır. Boyama işlemi bu şekilde önemli ölçüde kısaltılır. Bu araştırmanın ana amacı bir dizi çerçevenin boyanması üstüne ayrıntılı bir inceleme yapmaktır. Sonuçlar, bir canlandırma filminde var olan -çerçeveler arasındaki benzerlikten- yararlanarak geleneksel boyamadan daha etkili bir boyama yapılabileceğini göstermektedir. Anahtar kelimeler : Modelleme, Canlandırma, Dağıtımlı İşleme, Zamansa! Benzeşim, Anahtar Çerçeve, Hiperküp Topolojisi.","ABSTRACT MARS : A TOOL-BASED MODELING, ANIMATION AND PARALLEL RENDERING SYSTEM Murat Aktıhanoğlu M. S. in Computer Engineering and Information Science Supervisor: Prof. Dr. Bülent Ozgüç December, 1992 Abstract: This thesis describes a system for modeling, animating, previewing and rendering articulated objects. The system has a modeler which models objects, consisting of joints and segments. The animator interactively positions the articu lated object in its stick, control vertex or rectangular prism representation into the keyframes, interpolates inbetweens and previews the motion in real time. Then the data representing the motion and the models is sent to a multicomputer (iPSC/2 Hypercube1). The frames are rendered in parallel by distributed processing tech niques, exploiting the coherence between successive frames, thus cutting down the rendering time significantly. The main aim of this research has been to make a de tailed study on rendering of a sequence of 3D scenes. The results show that due to an inherent correlation between the 3D scenes, a much more efficient rendering than the conventional sequential one can be done. Keywords: 3D Modeling. Computer Animation, Rendering, Parallel Processing, Distributed Rendering, Temporal Coherence. !iPSC/2 is a trade mark of iNTEL Corporation"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HİPERKÜP ÇOK İŞLEMCİLİ BİLGİSAYARLARINDA VERİMLİ PARALEL SAYISAL İŞARET İŞLEME ALGORİTMALARI Argun Derviş Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Cevdet Aykanat Nisan 1992 Bu tez kapsamında, hiperküp bağlanti yapisini içeren çok işlemcili bilgisa yarlarda, tek boyutlu Sayısal işaret İşleme algoritmaları, FFT, FHT ve FCT araştirilmiştir. Önerilen algoritmalar, eşit yük dağilimi, minimum haberleşme, haberleşmeleri sayisal işlemlerle birleştirebilene ve düzgün algoritmik yapılar içermektedir. Önerilen paralel algoritmalar, Intel'in hiperküp bilgisayarinda, 32 işlemcisiyle denenmiştir. Küçük boyuttaki problemler için bile yüksek kazanç ve hizlar elde edilmiştir. Anahtar kelimeler : Sayisal İşaret İşleme, Hiperküp, Paralel İşleme, FFT, FHT, FCT. iv","ABSTRACT EFFICIENT PARALLEL DIGITAL SIGNAL PROCESSING ALGORITHMS FOR HYPERCUBE-CONNECTED MULTICOMPUTERS Argun Derviş M. S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Cevdet Aykanat April 1992 In this thesis, efficient parallelization of Digital Signal Processing (DSP) algorithms, (FFT, FHT and FCT), on multicomputer implementing the hy percube interconnection topology are investigated. The proposed algorithms, maintain perfect load-balance, minimize communication overhead, can overlap communications with computations and achieve regular computational pat terns. The proposed parallel algorithms are implemented on Intel's iPSC/21 hypercube multicomputer with 32 processors. High efficiency and almost linear speedup values are obtained for even small size problems. Keywords: Digital Signal Processing, Hypercube, Parallel Processing, FFT, FHT, FCT. 1iPSC/2 is a registered trademark of Intel Corporation iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GENEL AMAÇLI DÖNME, ÖLÇEKLENME VE ÖTELENME DEĞİŞİMSİZ ÖRÜNTÜ SINIFLANDIRMA SİSTEMİ Cem Yüceer Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Doçent Kemal Oflazer 1992 Yapay sinir ağları son çalışmalarda örüntü sınıflandırma amaçlan için kullanılmış tır. Bu çalışmada genel amaçlı dönme, ölçeklenme ve ötelenme değişimsiz örüntü sınıflandırma sistemi sunulmaktadır. Sistemin üç ana öbeği vardır; Karhunen- Loeve dönüşümü temelli önişlemci, yapay sinir ağı temelli sınıflandırın ve yo rumlayıcı, ingiliz abecesi, Japon Katakana abecesi ve bazı geometrik simgeler üzerindeki deneysel çalışmalarla sistemin değişimsizliği sağlama ve örüntü sınıf landırma gücü gösterilmiştir. Anahtar Kelimeler: Dönme değişimsizliği, ölçeklenme değişimsizliği, ötelenme değişimsizliği, genel amaçlı örüntü sınıflandırma, yapay sinir ağları, Karhunen- Loeve.","ABSTRACT A GENERAL PURPOSE ROTATION, SCALING, AND TRANSLATION INVARIANT PATTERN CLASSIFICATION SYSTEM Cem Yüceer M.S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Kemal Of lazer 1992 Artificial neural networks have recently been used for pattern classification pur poses. In this work, a general purpose pattern classification system which is rotation, scaling, and, translation invariant is introduced. The system has three main blocks; a Karhunen-Loeve transformation based preprocessor, an artificial neural network based classifier, and an interpreter. Through experimentation on the English alphabet, the Japanese Katakana alphabet, and some geometric sym bols the power of the system in maintaining invariancies and performing pattern classification has been shown. Keywords: Rotational invariancy, scaling invariancy, translational invariancy, general purpose pattern classification, artificial neural networks, Karhunen-Loeve."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET ZAMAN BOYUTLU İLİŞKİSEL BİR. VERİ TABANI YÖNETİM SİSTEMİNİN GERÇEKLEŞTİRİLMESİ Iqbal A. Goralwalla Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkım Haziran 1992 Bu çalışmada zaman boyutlu ilişkisel bir veritabanı yönetim sisteminin gerçekleştirilmesi anlatılmaktadır. Bu sistem küme değerli öznitelikleri olan ilişkileri kullanan mevcut bir sistem üzerine kurulmuştur. Zaman boyutlu ilişkisel model zaman boyutlu ilişkisel cebir ile birlikte tanımlanmaktadır. Zamansal verilere ulaşabilmek için ilişkisel cebire ait temel işlemler genişletilmiştir. Aynı zamanda geçmişe ait ilişkiselden bilgi çıkarımında yardımcı olması amacı ile yeni işlemler eklenmiştir. Bu işlemler bir öznitelik türünü diğerine çevirmekte ve zaman boyutu üzerinden seçim yapmaktadır. Ayrıca bir istatistik arabirimi de sisteme eklenmiştir. Basit istatistik fonksiyonlar ve zaman aralılarında seçme işlemi bu arabirimin ana öğeleridir. Zaman aralılarında seçme işlemi, verilen bir üç boyutlu geçmişe ait ilişkiselden bir zaman noktalar kümesi veya zaman aralıklar kümesi için geçerli bir veri tablosu oluşturmaktadır. Anlık, anlık/yuvalı ve tarihsel olmak üzere üç ayrı veritabanı türünün örnek sorularla sorgulanması ile sistemin başarım değerlendirilmesi yapılmıştır. Anahtar Kelimeler: zaman boyutlu veritabanı, genişletilmiş ilişkisel cebir, zaman aralılarında seçme işlemi, basit istatistik işlem, küme değerli bağıntılar, başarım değerlendirmesi.","ABSTRACT AN IMPLEMENTATION OF A TEMPORAL RELATIONAL DATABASE MANAGEMENT SYSTEM Iqbal A. Goralwalla M.S. in Computer Engineering and Information Science Supervisor: Prof. Erol Arkun June 1992 In this work, the implementation of a temporal database management system is reported. This system has been implemented on top of an existing database system that manipulates relations with set-valued attributes. The temporal relational mo'del together with the temporal algebra are described. The basic set of operations of the extended relational algebra have been modified to handle temporal attributes. New operations have been added to help in the extraction of information from historical relations. These operations convert one attribute type to another and do selection over the time dimension. Moreover, a statistical interface has been added to the system. This interface includes aggregate functions and a new operation, enumeration, which derives a table of uniform data for a set of time points or intervals, from a three dimensional historical relation. A performance evaluation of the system is carried out by executing sample queries against different types of databases: snapshot, snapshot/nested and historical. Keywords: temporal database, extended relational algebra, enumeration operation, ag gregation operation, set-valued relations, performance evaluation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET.. İNSAN HAREKETİNİN CANLANDIRILMASI: ETKİLEŞİMLİ BİR ARAÇ Syed Kamran Mahmud Bilgisayar Mühendisliği ve Enformatik Bilimi Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Bülent Özgüç Ocak, 1991 Bu çalışmada amaç, genel maksatlı, etkileşimli bir insan hareketin modelleme aracı geliştirmektir. Araç canlandırılma tekniği olarak parametrik anahtar çerçeve tekniğini kullanmaktadır. Anahtar çerçeveleri oluşturmanın değişik soyutlamaları araştırılmakta ve insan vücudunun anahtar çerçeve noktalarını bulmada yeni bir yaklaşım olan ""yarı amaç-y önlendirmek"" canlandırılma"" tanıtıl maktadır. Bu yeni yöntemle, hareket tanımlamının derecelendirmesinin yapılmasında animatörle makine arasındaki yük dağılımı sorunu çözümlenebilecektir. Anahtar Kelimeler: İnsan Canlandırılma, Geleneksel Canlandırılma, Bilgisayarlı Canlandırılma, Benzetişim, Hareket Tanımlama","ABSTRACT ANIMATION OF HUMAN MOTION: AN INTERACTIVE TOOL Syed Kamran Mahmud M.S. in Computer Engineering and Information Science Supervisor: Prof. Bülent Özgüç January, 1991 The goal of this work is the implementation of an interactive, general purpose, human motion animation tool. The tool uses parametric key-frame animation as the animation technique. Different abstractions of motion specification in key-frame generation are explored, and a new notion of semi goal-directed an imation for generating key-frame orientations of human body is introduced to resolve the tradeoff between animator and machine burden in choosing a level of motion specification. Keywords: Human Animation, Classical Animation, Computer Animation, Simulation, Motion Specification"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET BİLGİSAYARLARDA ÜÇ BOYUTLU CANLANDIRMA FİLM ÜRETİMİ İÇİN BİR ARAÇ Bu çalışmada, güzel sanatlar dalında eğitim veren bir okulda, öğrencilerin üç boyutlu canlandırmanın temellerini öğrenebilmelerini ve canlandırma filmi üretebilmelerini sağlamaya yönelik üç boyutlu. bir bilgisayar yazılımı tasarlanmıştır. Kukla Tiyatrosu adını verdiğimiz bu sistem, Targa 16T grafik kartı takılan düşük maliyetli bilgisayar sistemlerinin esnekliğini ve etkililiğini kullanmakta ve kullanıcı arabirimini olabildiğince anlaşılır kılmaktadır. Temel olarak bilgisayarın belleğinde oluşturulan üç boyutlu modellerin hareketlendirilip sonuçta da gerçeğe yakın, renkli bir görünüme büründürülmesini amaçlayan bir yazılımdır. Bilgisayar grafiği konusundaki son gelişmeleri yansıtan ve eldeki platformun sınırlarını zorlayan sistem, taşınabilirimi ve esnekliği sağlamak için C programlama diliyle geliştirilmiştir. Bu programa arka plan olarak da WODNIW adı verilen bir. pencereleme sistemi geliştirilmiştir. Bu açık pencereleme sistemi, en yeni kullanıcı arabirimi araçları olan pull-down menüleri, etkileşimli tuşları, boyu ayarlanabilir pencereleri ve nesneye yönelik operasyonları desteklemektedir. Anahtar sözcükler: Bilgisayar grafiği, Canlandırma, Üç boyutlu canlandırma film, Renklendirme.","ABSTRACT A TOOL FOR GENERATING THREE DIMENSIONAL ANIMATION ON COMPUTERS Cemil Sinasi Türün M.F.A. in Graphical Arts Supervisor: Prof. Dr. Bülent Özgüç 1991 In this work, a three dimensional computer animation system has been designed to be employed in schools, for the training of art students on basic three dimensional animation techniques. Puppet Theater, as we have called the system, utilizes the flexibility and effectiveness of the low-end hardware, namely IBM PCT computers supported with Targa 16T graphics board and gives special emphasis to user friendliness. It is basically a software to design three dimensional objects and choreograph the object data in the computer's memory, before rendering the resulting scenery with shading methods. The system is the result of reflecting the recent advances in the field of computer graphics and pushing the potentials of the existing platform. Software is implemented in C language, thus the code is transportable. A custom designed object oriented windowing system called WODNIW is used as the user interface. This open windowing system supports pull-down menus, interactive buttons, scalable windows and other popular user interface elements. Keywords: Computer graphics, Animation, Three-dimensional computer animation, Rendering."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu tez çalışmasında Küme Teorisi öğretmek için bir Akıllı Yardımcı Sistem (AYS) tasarlanmış ve nesneye dayalı yaklaşım kullanılarak gerçekleştirilmiştir. Bu program IBM PS/2 ve IBM uyumlu bilgisayarlarda Turbo Pascal sürüm 5.5 programlamlama dili ile geliştirilmiştir. Sistemin, öğretme modülü, uzman modülü ve öğrenci modeli olmak üzere üç ana bileşeni vardır. Sistemin akış kontrolü öğrenci, öğretmen ve öğrenci modeli arasında paylaştırılmıştır. Tasarlanan ve gerçekleştirilen sistemin en önemli özelliği örnek ve soruların sistem tarafından rassal olarak üretilmiştir. Sistemin bütün modülleri nesnesel bir yaklaşımla gerçekleştirilmiştir. Anahtar kelimeler: Akıllı Yardımcı Sistemler, Nesneye Dayalı Programlama, Akıllı Yardımcı Sistemlerde Kontrol Stratejisi","In this thesis an Intelligent Tutoring System (ITS) to teach set theory is designed and implemented using an object-oriented approach. The program is implemented on an IBM PS/2 and PC compatibles using the Turbo Pascal 5.5 programming language. The implemented system is an ITS that employs the features of set theory such as hierarchical structure, inheritably deductable operation and relations and set concept being the core of the theory to create a tutor that teaches the concept and monitors the user's state of knowledge. The system uses a distributed control strategy that allows four factors, namely student, teacher, student model and nondeterminism to possess the right to direct a session and its contents. Nondeterminism is used to generate the instructional content by randomly selecting different questions and examples each time the program is invoked. Finally, the system ends the tutorial session by giving a final examination to the user and monitoring any misconceived issues in order to repeat the related sections. Keywords : Intelligent tutoring systems, object oriented programming, control strategy"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bu çalışmada hiperküp çok işlemcileri için yeni bir indeksleme yöntemi önerildi. Önerilen indeksleme, dolaylı giriş-çıkış kaydedicisi eklenmiş SIMD hiperküp modeli üzerinde algoritmalar geliştirilerek kullanıldı. Bazı algoritmaların geliştirilmesi ve SIMD çalışma zamanlarının düşürülmesiyle, bu yeni indekslemenin alışılmış indekslemeden daha üstün olduğu gösterildi. Alışılmış indekslemede, içe konmuş halka ve ağlardaki en yakın komşu haberleşmesinde gereken işlemci indeksi hesaplamaları cü-boyutlu hiperküpte 0(d) zamanında yapılabilmektedir. Bu rota hesaplamaları eğer gray kodu çeviri tabloları kullanılırsa 0(1) zamanda yapılabilir. Bu rota hesaplamaları, önerilen indekslemede esnek bir paralel programlama ortamı sağlayacak şekilde sabit zamanda, basit ondalık aritmetik kullanarak ve hiçbir kod çeviri tablosuna ihtiyaç olmadan yapılabilmektedir. İçe konmuş halka ve ağ işlemlerinde, alışılmış indekslemedeki gray kodu sıralaması yerine önerilen indekslemedeki doğal ondalık sıralama yeterli olmaktadır. Geliştirilen çoğu SIMD algoritmalarında önceki en iyi MIMD zamanlarına ulaşılmış bulunulmaktadır. Son olarak, içe konmuş halka işlemlerinde algoritmik uyumluluk sağlayan, önerilen indeksle menin genelleştirilmiş hiperküp çok işlemcileri için genelleştirilmesi sunulmaktadır.","In this work, a new labeling scheme for hypercube multicomputers is proposed. The proposed labeling is exploited by developing algorithms on the SIMD hy percube model with the indirect I/O port register enhancement. Through the construction of some algorithms and reduction in their SIMD complexities, it is shown that this new labeling is superior to the common labeling used so far. In the common labeling, processor index computations required for nearest neighbor communications in ring and mesh embeddings can be performed in 0(d) time in a e?-dimensional hypercube. These routing computations can be performed in 0(1) time only if a number of gray code conversion tables are used. In the proposed labeling, these routing computations can be performed in 0(1) time, using simple decimal arithmetic and without the need of any code conversion tables, which provides a flexible parallel programming envi ronment. Instead of gray code ordering in the common labeling the natural decimal ordering of the processors in the proposed labeling suffices for the em bedded ring and mesh operations. In most of the SIMD algorithms developed, best previous MIMD complexities are reached. Finally, the generalization of the proposed labeling for the generalized hypercube architecture is presented which provides algorithmic compatibility in embedded ring operations. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET TÜRKÇE METİNLERDE SOZCUK YAZIMI KONTROLÜNÜN TASARIMI VE GERÇEKLEŞTİRİMİ Aysın Solak Bilgisayar ve Enformatik Mühendisliği Bölümü Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Dr. Kemal Of lazer Haziran 1991 Günümüzde, kişisel bilgisayarların ve iş istasyonlarının kullanımının gittikçe artması doküman hazırlamakta kullanılan yöntemleri de etkilemektedir. Ke lime işlemciler, dokümanları düzenlemek ve genel olarak kalitelerini arttırmak için pek çok işlev sunmaktadırlar. Bilgisayarların doküman hazırlamak için kullanımı Türkiye'de de gittikçe artmaktadır; ancak kullanılan kelime işlemcilerde Türkçe için sözcük yazımı kontrolü gibi bazı işlevler bulunmamaktadır. Türkçe pek çok dilden farklı bir dil olduğu ve bir takım zorluklar çıkardığı için, bu dilde sözcük yazımı kontrolü başlı başına ilginç bir problemdir. Bu tezde, Türkçe metinlerde sözcük yazımı kontrolü için gerçekleştirilen ve değişik kelime işlemcilere uyarlanabilecek bir yazılım ve tasarımı sunulmaktadır. iv","ABSTRACT DESIGN AND IMPLEMENTATION OF A SPELLING CHECKER FOR TURKISH Ayşm Solak M.S. in Computer Engineering and Information Sciences Supervisor: Assoc. Prof. Dr. Kemal Of lazer June 1991 Proliferation of personal computers and workstations that bring computing power to users of all levels has influenced how people prepare documents. Word processors offer numerous functionalities for formatting documents, and in general improving their presentation quality. In Turkey, computers are in creasingly being used for document production; but word processors used lack various tools like spelling checkers specific to Turkish. The problem of spelling checking is very interesting in itself, as Turkish, being very different from many languages, presents special challenges and problems. In this thesis, the design and implementation of a spelling checker for Turkish, which can be incorpo rated into word processing applications, is presented. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET HİPERKÜP ÇOK İŞLEMCİLİ BİLGİSAYARINDA PARALEL LABİRENT YOL BELİRLEME ALGORİTMALARI Tahsin Mertefe Kurç Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Doçent Dr. Cevdet Aykanat Kasım, 1991 Tümdevre tasarımında, devre bağlantılarının yapılması zaman alan bir iştir. Burada amaç bütün devre bağlantılarını en kısa yolları kullanarak yapmaktır. Eğer, her seferinde bir devre grubunun bağlantısı yapılırsa bunun adına labirent yol belirleme yöntemi denir. Bu yöntem için, hüristik algoritmalar vardır. Ancak, bu tip algoritmalar devre bağlantılarına getirdikleri kısıtlamalardan dolayı, bazen var olan bağlantıları bulamazlar. Bu yüzden, devrelerin bulunduğu alanın, tümden taranması gereke bilir. Lee'nin algoritması ve Lee benzeri algoritmalar bu tip algoritmalardır. Lee'nin algoritması ve Lee benzeri algoritmalar hesaplama bakımından pahalı ve devre yüzeyi için çok bilgisayar hafızası gerektiren algoritmalardır. Bu nedenle bu tip algoritmalar çok işlemcili bilgisayarlarda, paralel olarak çözmek için uygundur. Bu çalışmada, Lee benzeri labirent yol bulma algoritmalarının, hiperküp çok işlemcili bilgisayarında paralelleştirilmesi anlatılmaktadır.VI Anahtar kelimeler : Tümdevre tasarımı, labirent yol bulma yöntemi, Lee'nin labirent 3'ol bulma algoritması, Lee benzeri labirent algoritmaları, çok işlemcili bilgisayar, hiperküp topolojisi.","ABSTRACT PARALLEL MAZE ROUTING ALGORITHMS ON A HYPERCUBE MULTICOMPUTER Tahsin Mertefe Kurç M. S. in Computer Engineering and Information Science Supervisor: Assoc. Prof. Dr. Cevdet Aykanat November, 1991 Global routing phase is a time consuming task in VLSI layout. In global routing phase of the layout problem, the overall objective is to realize all the net interconnections using shortest paths. Efficient heuristics are used, for the global routing phase. However, due to the assumptions and constraints they impose, heuristics may fail to find a path for a net even if one exists. Re-routing is required for such nets. This re-routing phase requires the exhaustive search of the wiring area. Lee's maze routing algorithm and Lee type maze routing algorithms are exhaustive search algorithms used in re-routing phase. These algorithms are computationally expensive algorithms and consume large amounts of computer time for large grid sizes. Hence, these algorithms are good candidates for parallelization. Also, these algorithms require large memory space to hold the wiring grid. Therefore, the effective paralleliza tion of these algorithms require the partitioning of the computations and the grid among the processors. Hence, these algorithms can be parallelized on distributees-memory message passing multiprocessors (multicomputers).IV In this work, efficient parallel Lee type maze routing algorithms are devel oped for hypercube-connected multicomputers. These algorithms are imple mented on an Intel's iPSC/2 hypercube multicomputer. Keywords: VLSI layout, maze routing, Lee's maze routing algorithm, Lee type maze routing algorithms, multicomputer, hypercube topology."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NEDENSEL TEORİLERLE HESAPLAMA Erkan Tın Yüksek Lisans Tezi, Bilgisayar ve Enformatik Mühendisliği Bölümü Tez Yöneticisi: Doç. Dr. Varol Akman Ekim 1990 Zaman üzerine çıkarım yapılabilmesi için sağduyu bilgisinin formel hale sokulması uzun zamandır Yapay Zekâ'nm (YZ) merkezi meselesi olmuştur. Halihazırdaki formel sistemlerin YZ'nin çerçeve sorunu gibi bazı temel problemlerine tatmin edici çözümler getirmedikleri bilinmektedir. Dahası, bu sistemlerle yapılan çıkarımlar aksiyomlarla ifade edilmek istenenlerle daima uyuşmamaktadır. Bu meseleler zaman ve değişim üzerine çıkarım yapılabilmesi için iyi tanımlanmış bir formelizmi ve yararlı hesaplama metodlarını davet etmektedir. Stanford Üniversitesi'nden Yoav Shoham doktora tezinde (Yale, 1986) kronolojik bilgisizlik adım verdiği temporel, tekdüze olmayan cazip bir mantık ortaya koymuş ve nedensel teoriler olarak adlandırılan, hesaplaması basit model teorik özellikleri bulunan bir teori sınıfı tanımlamıştır. Bu sınıfın bazı sınırlamaları olduğu için bu tez Shoham'm nedensel teorileri üzerine yapılan bir geliştirme çalışmasıdır. Tez özellikle bu teorilerin hesapsal yönlerinin onların model teorik özelliklerim koruyarak iyileştirilmesi etrafında yoğunlaşmaktadır. Anahtar Kelimeler: Nedensellik, nedensel teoriler, çerçeve sorunu, kalifiye olma sorunu, kalıcılık sorunu, modal mantıklar, tekdüze olmayan mantıklar, temporel mantıklar, kronolojik bilgisizlik, model teorisi. iv","ABSTRACT COMPUTING WITH CAUSAL THEORIES Erkan Tin M. S. in Computer Engineering and Information Sciences Supervisor: Assoc. Prof. Varol Akman October 1990 Formalizing commonsense knowledge for reasoning about time has long been a central issue in Artificial Intelligence (AI). It has been recognized that the existing formalisms do not provide satisfactory solutions to some fundamental problems of AI, viz. the frame problem. Moreover, it has turned out that the inferences drawn by these systems do not always coincide with those one had intended when he wrote the axioms. These issues call for a well-defined formalism and useful computational utilities for reasoning about time and change. Yoav Shoham of Stanford University introduced in his 1986 Yale doctoral thesis ah appealing temporal nonmonotonic logic, the logic of chronological ignorance, and identified a class of theories, causal theories, which have computationally simple model-theoretic properties. This thesis is a study towards building upon Shoham' s work on causal theories for the latter are somewhat limited. The thesis mainly centers around improving computational aspects of causal theories while preserving their model-theoretic properties. Keywords: Causation, causal theories, the frame problem, the qualification problem, the persistence problem, modal logics, nonmonotonic logics, temporal logics, chronological ignorance, model theory. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL VERİ TABANI SİSTEMLERİNDE VERİ SAKLAMA VE INDEKSLEME Reda AL-HAJJ Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Prof.Dr. Erol Arkun Haziran 1990 Klasik veri tabam sistemlerinde kullanılmakta olan veri saklama ve indeksleme metotları nesne- sel veri tabanı sistemlerinde kullanılmaya uygun değildir. Bu tezde nesnesel veri tabanı sistem lerinde kullanılmaya uygun bir veri saklama modeli sunulmaktadır. Bu model nesne kimliği, çoklu sınıf sıradüzeni, bütünleşik nesneler, küçük granül olanağı ve çoklu sınıf sıradüzeni günlemesini içermektedir. Fazladan bellek kullanma ve güncelleme işlemlerindeki dezavantajlarına ragmen indeksleme, kütüklerde saklanan verilere hızlı bir şekilde erişimi sağlar. Bu çalışmada nesnesel veri taban ları için bir indeksleme metodu da önerilmektedir. Bu indeksleme metodu hem nesneleri, hem de her nesnenin bileşenlerini ayrı ayrı indeksleme olanağı sağlar. Böylece nesne kimliği ve bilgi gizlen mesi sağlanır. Çoklu smıf sıradüzeni üzerindeki değişiklikler oluşturulmuş indeksleri etkilemez. Bu metod, klasik veri tabanı yönetim sistemlerinde de kullanıma uygundur. Nesnesel veri tabanları için önerilen veri saklama ve indeksleme metotlarının bağıntısal veri tabanlarına dönüşümleri de sunulmaktadır. Anahtar Kelimeler: nesnesel veri tabanı sistemleri, yardımcı bellek, bilgi gizlenmesi, küçük granül olanağı, indeksleme, bütünleşik nesneler, çoklu sınıf sıradüzeni. iv","ABSTRACT STORAGE MANAGEMENT AND INDEXING IN OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEMS Reda AL-HAJJ M.S. in Computer Engineering and Information Sciences Supervisor : Prof.Dr. Erol Arkun June 1990 Storage management and indexing methods used in existing conventional database management systems are not appropriate for the object-oriented database management systems due to the distinctive features of the later systems. A model for storage management suitable for object- oriented database management systems is proposed in this thesis. It supports object identity, multiple inheritance, composite objects, a fine degree of granularity and schema evolution. An index provides fast access to data stored in files at the price of using additional storage space and an overhead in update operations. Work has been carried out on indexing and an indexing method for the object-oriented database systems is proposed. Identity and equality indexes are treated. Object identity and information hiding are provided. Schema changes are handled without affecting existing indexes. It is general enough to be applicable to most existing object-oriented database systems. The mapping of the proposed storage and indexing approaches into a relational database scheme is also presented. Keywords: object-oriented database management systems, storage management, inheritance, data encapsulation, identity, schema evolution, degree of granularity, composite objects, indexing, identity index, equality index. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET DERS ÇİZELGELERİ HAZIRLAYAN BİLGİSAYAR PROGRAMI TOPTAS Mehmet Yüksek Lisans Tezi, İşletme Enstitüsü Tez Yöneticisi Yd. Doç.Dr. Erdal Erel Haziran 1990 Bilgisayarın kullanımı ders çizelgelerinin hazırlanmasını kolaylaştırmış ve bir çok el ile yapılan işi ortadan kaldırmıştır. Bu tez çalışmasında Fortran IV bilgisayar dilinde yazılan ve Burroughs 9000 sisteminde çalıştıran ders çizelgesi hazırlayan bir bilgisayar programı Bilkent Üniversitesi Bilgisayar Merkezinde bulunan Data General sistemine uygunlaştırılmıştır. Bu program yardımıyla Bilkent Üniversitesi İşletme Bölümünün 1990-1991 öğretim yılı sonbahar dönemi ders çizelgeleri hazırlamıştır.","ABSTRACT A TIMETABLE SCHEDULING COMPUTER PROGRAM TOPTAS Mehmet MBA in Institute of Management Supervisor: Asst. Prof. Dr. Erdal Erel June 1990 The use of computers makes easy the preparation of timetable schedule and aliminates a lot of manual work.In this thesis, a timetable schdule generation program written in Fortran IV language and implemented in a Burroughs 9000 system is adapted to the Data General system at Bilkent University computing center. A case study on timetable schdule of Department of Management of Bilkent University is performed."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET iîLKENT ÜNİVERSİTESİ YURT YÖNETİMİ İÇİN BİR VERİTABANİ İŞLETİM SİSTEMİ OLUŞTURULMASI Kahraman Günaydın Tez Yöneticisi: Prof. Dr. ümit BERKMAN Bu çalışma. Bil kent üniversitesi yurt yönetimine yeni bir veri tabanı işletim sistemi inşa etmek amacıyla yapıldı. Buradaki hedef bilgilerin daha kolay ve daha çabuk kullanılmasıydı. Daha önceki elle dosyalama sistemine nazaran, bu yeni oluşturulan programla, veri girme, değiştirme, silme ve geri çağırma daha çabuk ve kolay yapılır hale geldi, üstelik bu programla verilerin sıralandırılması ve ayıklanması daha hızlı ve güvenilir oldu. Anahtar kelimeler: Veritabanı işletim Sistemi, Bilkent Üniversitesi Yurt Yönetimi. Bilgi, Elle Dosyalama Sistemi. iv","ABSTRACT ESTABLISHMENT OF A DBMS FOR BILKENT UNIVERSITY DORMITORY MANAGEMENT By Kahraman Gunaydin Supervisors Prof. Dr. Umit Berkman The purpose of this thesis is to establish a Database Management System for Bil ken t University Dormitory management. The aim of the program is to use information more easier and quicker than the present manual system. Activities such as, entering editing, deleting and retrieving data with this program are more quicker and easier. Sorting through large pieces of information is available and more efficient with this program. Key words s Database Management System, Bil kent University Dormitory management, Information, Manual System. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET Öğrenci dosyalama sistemi Hasırlayan Turgut Kemal oğlu Tez Yöneticisi : Yard. Doç. Erinç YELDAN Bu çalışma üniversite öğrencilerinin bilgilerini derleyen ve işleyen bir öğrenci dosyalama sisteminin tasarımıdır. Bu dosyalama programında amaç öğrencileri değişik parametrelere göre sıralamak s herhangi bir dersten verilen notların dağılımını çıkarmak, her öğrencinin ders ve not durumunu gösteren bir bilgi formunu hasırlamak ve öğrenciler hakkında gerekli her türlü bilgiyi, gerektiğinde hızla ulaşmak amacıyla saklamaktır.","ABSTRACT A STUDENT DATABASE By Turgut Kema loğlu Supervisors Assist. Prof. Erine YELDAN This thesis is a design of student database system which will manage the data of university students. The aim of the program is to obtain sorted lists of students according to several parameters, to obtain frequency of grades for the specified course, to design a suitable sheet which will summarize course and grade position of each student and to store all the related data about students for easy access whenever required."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GERİ YANSITMA ALGORİTMASININ İPSC/2 HYPERCUBE PARALEL İŞLEMCİSİNDE GERÇEKLEŞTİRİLMESİ Deniz Ercoşkun Bilgisayar ve Enformatik Mühendisliği Yüksek Lisans Tez Yöneticisi: Y. Doç. Dr. Kemal Of lazer Tarih 1990 Geri yansıtma, bazı yapay sinir ağı modelleri için geliştirilmiş bir öğrenme al goritmasıdır. Bu algoritma, özellikle bu tip sinir ağı modellerinin eğitilmesinde kullanılmaktadır. Temel geri yansıtma algoritmasının yavaş yakınsaması bu algoritmanın kullanımını küçük sinir ağlarıyla sınırlandırmıştır. Bu tez çalış masında geri yansıtma algoritması hypercube paralel işlemcisinde gerçekleştirilmiş ve bir dizi yapay sinir ağma uygulanmıştır. Bu çalışmanın diğer bir amacı, büyük yapay sinir ağları için bir simulasyon ve öğretim ortamı geliştirilmesidir. iv","ABSTRACT IMPLEMENTATION OF THE BACKPROPAGATION ALGORITHM ON iPSC/2 HYPERCUBE MULTICOMPUTER SYSTEM Deniz Ercoşkun M.S. in Computer Engineering and Information Science Supervisor: Assist. Prof. Dr. Kemal Of lazer December 1990 Backpropagation is a supervised learning procedure for a class of artificial neural networks. It has recently been widely used in training such neural networks to perform relatively nontrivial tasks like text-to-speech conversion or autonomous land vehicle control. However, the slow rate of convergence of the basic backpropagation algorithm has limited its application to rather small networks since the computational requirements grow significantly as the network size grows. This thesis work presents a parallel implementation of the backpropagation learning algorithm on a hypercube multicomputer system. The main motivation for this implementation is the construction of a parallel training and simulation utility for such networks, so that larger neural network applications can be experimented with. ni"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bugüne kadar yapılmış olan katı modelleme sistemlerinin en önemli sorun larından birisi de üretilebilen şekillerin kısıtlı olmasıdır. Alışılagelmiş bir katı modelleme sisteminde düzensiz yüzeyleri olan nesnelerin modellenmesi kolay değildir. Böyle nesneler, üzerindeki her nokta verilerek tanımlanabilir, fakat bu yöntem kullanıldığında bu nesneler üzerindeki işlemler belirgin olmamakta ve bazı zorluklar ortaya çıkmaktadır. Bu nesnelerin modellenmesinde etkili bir yöntem de düzenli şekilleri oluşturduktan sonra onlar üzerinde defor- masyon tekniklerini uygulamaktır. Bu yolla gerekli nesneler elde edilirken ortaya çıkan zorluklar da önlenmekte ve işlemlerde açıklık sağlanmaktadır. Bu araştırma bazı modelleme yöntemlerine deformasyon tekniklerinin uygu lanması ve deformasyon tekniklerinde bugüne kadar kullanılan değişik yakla şımların birleştirilmesi konulan ile ilgilidir. Kıvırma, bükme ve inceltme gibi düzenli deformasyonlar serbest deformasyon tekniği ile birleştirilerek yeni bir deformasyon yöntemi elde edilmiştir. Böylece bu yöntemlere özgü bazı kısıtlamalar yokedilmiş ve her iki yöntemin yeteneklerinden daha etkin bir şekilde yararlanılmıştır.Anahtar Kelimeler: Deformasyon, katıların geometrik modellenmesi, dü zensiz nesneler, kullanıcı arabirimleri, tarama, görünmeyen yüzeyleri yoketme, bilgisayar grafiği.","One of the most important problems of available solid modeling systems is that the range of shapes generated is limited. It is not easy to model ob jects with free-form surfaces in a conventional solid modeling system. Such objects can be defined arbitrarily but then operations on them are not trans parent and complications occur. A method for achieving free-form effect is to define regular objects or surfaces, then deform them. This keeps various properties of the model intact while achieving the required visual appear ance. This thesis explains a number of geometric modeling techniques with deformations applied to them in attempts to combine various approaches de veloped so far. Regular deformations, which include twisting, bending, and tapering, and free-form deformation technique are combined as a new defor mation method. This eliminates some of the disadvantages peculiar to each method and utilizes the advantages of both.Keywords: Deformations, geometric modeling of solids, free-form surfaces, user interface design, shading, hidden surface elimination, computer graphics. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL BİR VERİ TABANI İŞLETİM SİSTEMİ İÇİN ÇALIŞMA ORTAMI Can Yengül Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Dr. M. Erol Arkım 1989 Bu tez çalışmasında nesnesel bir sorgulama işlemcisi, nesnesel bir veri tabanı dili için çalıştırıcı ve sistemde bulunan sınıflar için gerekli iletişim protokolleri tasarımlanmış ve gerçekleştirilmiştir. Geliştirilen veri tabam dili nesnesel yaklaşımın tüm gereklerini yerine getirmektedir. Sorgu işleme fonksiyonları mesaj yollama yöntemi ile gerçekleştirildiğinden, veri kullanımı ve sorgu işlemleri benzer şekilde ifade edilebilmektedir. Ak tarım mekanizması, sınıf sıradüzeninin kurulması, nesne örneklerine ve sınıf tanımlamalarına ulaşım da program çalıştırma ortamı tarafından gerçekleş tirilmektedir. Anahtar sözcükler : nesnesel veri tabanı, sorgulama işlemcisi, nesnesel sorgulama işlemcisi, nesnesel sorgulama modeli, nesnesel dil, nesne, sınıf, nesne örneği, mesaj, metot, sınıf sıradüzeni, nesne kimliği iv","ABSTRACT A RUN-TIME ENVIRONMENT FOR AN OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEM Can Yengül M.S. in Computer Engineering and Information Sciences Supervisor: Prof. Dr. M. Erol Arkun 1989 In this thesis, an object-oriented query processor, a database language executer, and the protocols for the system- defined classes are designed and implemented. The designed and implemented database language completely fulfills the requirements of the object-oriented paradigm. Query processing functions are implemented through the message pass ing paradigm, which results in a uniform treatment of data manipulation and query processing functions. The run-time environment also supports the implementation of inheritance mechanism, class hierarchy maintenance, instance access and modification, and access to class definitions. Keywords : object-oriented database, query processor, object- oriented query model, object-oriented language, object, class, instance, message, met hod, class hierarchy, object identity. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GEOMETRİK MODELLER VE PARAMETRİK YÜZEYLER ÜZERİNDE IŞIN İZLEME « Veysi işler Bilgisayar Mühendisliği ve Enformatik Bilimleri Bölümü Yüksek Lisans Tez Yöneticisi: Prof. Dr. Bülent Özgüç 1989 Bilgisayarlı bir çok uygulamada gerçeğe uygun görüntüler sıkça kulla nılmaktadır. Bu nedenle, bilgisayarda gerçekçi görüntüler elde etmek için çeşitli yöntemler geliştirilmiştir. Işın izleme bunlar arasında en etkili ger çekçi görüntüler elde etmeye yarayan bir yöntemdir. Işm izlemede temel nokta, sunulacak sahnedeki ışık ve modellerin çevreleri ile etkileşimlerinin benzetimi yapılarak yansıma, gölgeleme ve kırılma gibi doğal olayları bilgisa yarda hesaplamaktır. Işın izleme metodu bu kadar yararlı olmasına rağmen, bu teknikle elde edilen görüntüler aşın hesaplama zamanı gerektirmektedir. Bu araştırmada ışın izleme metodu çalışılmış ve bu yöntemin sahip olduğu avantajlar ve dezavantajlar incelenmiştir. Ayrıca, ışın izlemedeki problemleri çözmek için geliştirilen metotlar araştırılıp geliştirilen ışın izleme sisteminde kullamlmıştır. Anahtar kelimeler : Işın izleme, Sekizli Ağaç, Tonlama, Gerçekçi Görüntü.","ABSTRACT RAY TRACING GEOMETRIC MODELS AND PARAMETRIC SURFACES Veysi İşler M.S. in Computer Engineering and Information Sciences Supervisor: Prof. Dr. Bülent Ozgüç 1989 In many computer graphics applications such as CAD, realistic displays have very important and positive effects on designers using the system. There are several techniques to generate realistic images with the computer. Ray tracing gives the most effective results by simulating the interaction of light with its environment. Furthermore, this technique can be easily adopted to many physical phenomena such as reflection, refraction, shadows, etc. by which the interaction of many different objects with each other could be realistically simulated. However, it may require excessive amount of time to generate an image. In this thesis, we studied the ray tracing algorithm and the speed problem associated with it and several methods developed to overcome this problem. We also implemented a ray tracer system that could be used to model a three dimensional scene and find out the lighting effects on the objects. m Keywords: Ray tracing, Octree, Shading, Realistic Images."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET UZMAN SİSTEM KABU?UNDA BİLGİ TABANI DO?RULANMASI Faruk Polat Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Halil Altay GÜVENİR Haziran 1989 Uzman sistemlerin önemli bir parçası da uygulamaya bağımlı bilgilerin saklandığı bilgi tabanıdır. Bilgi tabanının doğruluğunun kontrol edilmesi de bilgi toplama işleminin önemli bir bölümünü oluşturmaktadır. En basit de yimiyle bilgi tabanının bütünlük ve tutarlılık içinde olması kontrol edilmesi işlemidir. Verilen kuralların analizi bilgi tabanında bulunabilecek bir çok ha tanın önceden belirlenmesine yardımcı olacaktır. Uzman bilgi tabanı eksik, tutarsız, ve hatta yanlış olabilir. Bu problemler çıkarım makinasınıri tutarsız sonuçlar, örneğin çelişkili çıkarımlar, üretmesine ve hatta sonsuz döngülere girmesine neden olabilmektedir. Bu işlemin genel olması için bilgi kurallar şeklinde olup, her kural belli bir kesinlik değeri taşıyabilmektedir. Bunun nedeni ise bir çok uygulamada kurallar kullanılması ve bilginin doğruluğunun kesin olmayıp belli bir olasılık taşımasıdır. Doğruluk kontrolü sırasında sistem tarafından türetilen kurallar da gözönüne alınmaktadır. Bizim bu konuya yaklaşımımız uzman sistem kabuğuna herhangi bir bilgi tabanının doğruluk kontrolünü yapabilecek bir alt sistem eklemektir. Anahtar Kelimeler : bilgi tabanı, bilgi tabanı doğrulanması, bilgi toplama, çıkarım makinası, kesinlik değeri, uzman sistem, uzman sistem kabuğu. iv","ABSTRACT KNOWLEDGE BASE VERIFICATION IN AN EXPERT SYSTEM SHELL Faruk Polat M.S. in Computer Engineering and Information Sciences Supervisor: Asst. Prof. Dr. Halil Altay GÜVENİR June 1989 An important part of an expert system is its knowledge base which con tains domain dependent knowledge. Knowledge base verification is one of the important problems of knowledge acquisition. It is the process of checking that a knowledge base is complete and consistent. An analysis of the rules can detect many potential problems that may exist in a knowledge base. The knowledge base may be incomplete, inconsistent, or even partly erroneous. Those problems unless identified and corrected may cause the inference engine to produce inconsistent results such as conflicting conclusions and sometimes to enter infinite loops. In order to be general, rules with certainty factors are preferred for knowledge representation. This is partly because rules are used in many applications and certainty factors are necessary when knowledge has probabilistic characteristics. Our approach is to develop a knowledge base verification tool that can be used as a part of a rule-based expert system shell. Keywords : expert system, expert system shell, certainty factor, inference engine, knowledge acquisition, knowledge base, knowledge base verification. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET GEOMETRİK MODELLERİN D OKUL ANDIR ILM ASI Oktay Aydın Açıkgöz Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Prof. Dr. Bülent ÖZGÜÇ Temmuz 1989 Gerçeğe uygun görüntü elde edilmesinde, dokuların görsel etkilerinin ve rilmesinin önemi büyüktür. Doku eşlemesi bu bağlamda oldukça başarılı bir tekniktir. Doku eşlemesi iki temel başlık altında incelenebilir: geometrik eşleme ve filtreleme. Geliştirilen sistem değişik geometrik modellere uygu lanabilmektedir. Buna örnek olarak ""superquadric"" veya Bczier yüzeylerini verebiliriz. Geometrik modelleme ve doku sentezi alt sistemleri de bu amaç için geliştirilmiştir. Uygulamanmın hızı ve kolay kullanılırlığı önemli nokta lardır. Sistem karşılıklı etkileşimli olarak çalışmaktadır. Kullanıcı dokuyu ve geometrik modeli tanımlamakta ve sonucu kısa bir süre içinde alabilmektedir. Anahtar Kelimeler : Dokular, doku eşleme, karşıeşgörüngeleme, tarama, görüntü bireşimi, katlanma, renk, etkileşim sistemleri, görünmeyen yüzeyleri yoketme, bilgisayarlı çizim. iv","ABSTRACT TEXTURE MAPPING ON GEOMETRICAL MODELS Oktay Aydm Açıkgöz M.S. in Computer Engineering and Information Sciences Supervisor: Prof. Dr. Bülent ÖZGÜÇ July 1989 The contribution of the visual effects of textures is an important aspect in generating images of real objects. Texture mapping is a very success ful technique in this respect. Texture mapping can be subdivided into two fundamental topics: the geometric mapping and the filtering. The texture mapping system developed in this study is adaptable to different types of geometric models. Supcrquadric, Bezicr or b-spline surfaces can be mapped with textures. The geometric modeling and the texture synthesis subsystems were also implemented for this purpose. The system works in an interactive manner, the user describes the geometric model and the texture and gets the result in a reasonable amount of time. The speed and the usability of the system by a naive user are the keypoints of implementation. Keywords : Textures, texture mapping, antialiasing, shading, image syn thesis, convolution, color, user interface design, hidden-surface elimination, computer graphics. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL UZMAN SİSTEM KABU?U TASARIM VE GERÇEKLEŞTİRİMİ İsmail HakkıToroslu Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Yrd. Doç. Dr. Halil Altay GÜVENİR Haziran 1989 Uzman sistemler bilgisayar bilimlerinde yeni bir konu olarak ortaya çık mıştır. Uzman sistemler bilgi yoğun sistemlerin geliştirilmesinde kullanılan yazılım sistemleri olarak tanımlanabilir. Uzman sistem kabukları da uzman sistemlerin kolayca geliştirilebilmeleri için ortaya çıkmıştır. Son yıllarda yeni bir yazılım tekniği olarak ortaya çıkan nesnesel yaklaşım birçok üstünlüklere sahiptir. Bunlardan en önemlisi nesnesel yaklaşımın insanın düşünme tarzına yakın olmasıdır. Bu nedenle, bizim de geliştirmekte olduğumuz uzman sis tem kabuğunda bilgilerin saklanması için nesnesel yaklaşım teknikleri kul lanılmıştır. Sistemimizin bir parçası olan nesnesel veri tabanı büyük bilgi ta banını gerektiren uzman sistemlerin geliştirilmesine de olanak sağlamaktadır. Anahtar Kelimeler: Uzman sistem, uzman sistem kabuğu, bilgi gösterimi, nesnesel yaklaşım. iv","ABSTRACT DESIGN AND IMPLEMENTATION OF AN OBJECT-ORIENTED EXPERT SYSTEM SHELL Ismail HakkiToroslu M.S. in Computer Engineering and Information Sciences Supervisor: Asst.Trof. Dr. Halil Altay GÜVENİR June 1989 Expert systems represent a new opportunity in computing. An expert sys tem is a computing system capable of representing and reasoning about some knowledge-rich domain with a view to solving problems and giving advice. Expert system shells are developed to create expert systems in an easy way. In recent years the object-oriented paradigm has been developed. The object- oriented approach has many advantages such as data abstraction, program modularity, and structural data representation. Therefore, we are developing an expert system shell which stores knowledge and data in object-oriented style. Also, an object-oriented DBMS part of our shell satisfy the needs of several expert systems requiring large base of facts. Such shells can be used to build expert systems by only adding the domain-specific knowledge. Keywords: Expert system, expert system shell, knowledge representation, object-oriented approach. Ill"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"Bilgisayar ağları çok sayıda kullanıcı tarafından paylaşılan, kaynakları içermektedir. Bu kaynakların kapasitesi sınırlıdır ve kullanıcı istekleri bu kapasiteyi aşarsa, bilgisayar ağlarında aşırı bir yüklenme görülür. Aşırı yüklenme sistem performansının düşmesine sebep olur. Diğer bilgisayar ağlarıyla bağlantısı olan bir ağda iki grup mesaj trafiği görülür. İlk grup ağ içinde üretilen ve transfer edilen mesajlardan oluşur. Diğer grubu ise başka ağlara trans fer edilen veya başka ağlardan gelen mesajlar oluşturmaktadır. Bu tezde ikinci grup trafiğin bir ağın performansı üzerindeki etkisi incelenmektedir. Geliştirilen bir model üzerinde birbirine bağlı bilgisayar ağlarında ve geçitlerde görülen aşırı yüklenmenin önlenmesinde pencere metodunun etkinliği bilgisayar benzetimiyle incelenmektedir. Aynca pencere büyüklüğünün dinamik olarak değişebildiği bazı algoritmalar önerilmektedir. Bu algoritmalar pencere büyüklüğünün çeşitli ağ kaynaklarının o andaki yüküne bağlı olarak değişebilmesini sağlayarak pencere metodunun aşın yüklenme problemine daha etkin bir çözüm getirmesini sağlamaktadır. Dinamik algoritmalar, statik pencere kontrol metodu ile karşılaştırılmalı olarak incelenmektedir.Anahtar Kelimeler: Aşırı Yük Kontrolü, Bilgisayar Ağları, Birbirine Bağlı Bilgisayar Ağlan, Pencere Kontrolü, Performans Ölçümü.","A computer network has a collection of resources shared by multiple users. The capacity of the resources is limited, and if the user demands exceed the capacity, the network becomes 'congested'. The congestion causes a degrada tion in system performance. In interconnected networks there are two classes of traffic within a network. One class is the local traffic that is generated and transmitted within the network. The other class is the internetwork traffic transmitted to or from other networks. In this thesis, the effect of inter network traffic on the performance of a network is investigated. Computer simulation of an interconnected network model is provided in order to eval uate the effectiveness of a window-based congestion control mechanism on preventing congestion in gateways and in attached networks caused by the overload of internetwork traffic. Also two dynamic window congestion con trol algorithms are provided and studied. These algorithms provide further control to window mechanism by adjusting the window size in accordance with the availability of the network resources at the destination. Dynamic algorithms are evaluated comparing them with static window control.Keywords: Congestion Control, Computer Networks, Interconnected Com puter Networks, Window Control, Performance Evaluation."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET KULLANICININ ÇİZDİ?İ BLOKLARLA SÜREKLİ OLARAK GÖRÜNTÜ İŞLEME Aydın KAYA Bilgisayar Mühendisliği ve Enfomatik Bilimleri Yüksek Lisans Tez Yöneticisi: Doç. Dr. Bülent ÖZGÜÇ Haziran 1988 Görüntülerin işlemesi amacı ile nesneye yönelik bir bilgisayar, kullanıcı etkileşim sistemi geliştirilmektedir. Bu amaçla hazırlanan yazılım alçak düzeyde ve yüksek düzeyde kullanılabilen görüntü işleme foksiyonlarını ve dostça kullanıcı bilgisayar etkileşim elemanlarım içermektedir (pencereler, kontrol panoları, vs). Alçak düzeyde görüntü işleme fonksiyonları ile direk etkileşimli olarak çalışmak mümkündür. Yüksek düzeyde ise belli bir şekilde oluşturulan bir diagram düğümleri görüntü işleme fonksiyonları yerine ve düğümler arasındaki bağlar ise işlenen görüntülerin takip edeceği yolların yerine geçmektedir. Böyle bir diagram aracılığı ile karışık bir operasyonlar kümesi sürekli bir şekilde uygulanabilmektedir. Anahtar Kelimeler: Görüntü işleme, nesneye-yönelik programlama, çok pencereli iş düzeni, etkileşim sistemleri. iv","ABSTRACT CONTINUOUS PROCESSING OF IMAGES THROUGH USER SKETCHED FUNCTIONAL BLOCKS Aydın KAYA M.S. in Computer Engineering and Information Sciences Supervisor: Assoc. Prof. Dr. Bülent ÖZGÜÇ June 1988 An object oriented user interface is developed for interacting with and processing images. The software prepared for this purpose includes image processing functions as well as user friendly interaction tools both of a lower level such as menus, panels, windows and a higher level such as a schematics. The lower level utilities provide direct interface with the available image processing functions. At the higher level, the nodes of the schematics serve as image processing function instantiations and the arcs are the paths through which processed images flow. By constructing such a schematics, a complex set of operations can be applied to images continuously. Keywords : Image-processing, user interface design, window maneger, object-oriented programming. m"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET bilgisayar programlama eğitimi için bir YAZILIM Mesut Göktepe Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Assoc. Prof. Dr. Bülent Özgüç Eylül 1988 Bu tezde bilgisayar programlama öğretimi için uygulamalar, bazı grafiksel programlama sistemleri ve uygulandığı ortamlar araştırılmış, ve grafiksel bir Pascal programlama öğretim sistemi tasarlanıp uygulaması yapılmıştır. Grafiksel programlama sistemleri kullanıcıya imge ve semboller yoluyla çok az yazı kullanarak problem çözme olasılığı sağlar. Burada, bir Pascal programlama sistemi dost bir etkileşim ortamı içinde akış diagramları kulla narak görsel olarak programlamayı öğretmek amacıyla sunulmuştur. Anahtar kelimeler: grafiksel programlama, etkileşim sistemleri, pencere yönetim sistemi iv","ABSTRACT DESIGN AND IMPLEMENTATION OF A TOOL FOR TEACHING PROGRAMMING Mesut Göktepe M.S. in Computer Engineering and Information Sciences Supervisor: Assoc. Prof. Dr. Bülent Özgüç September 1988 In this thesis, a survey on computer applications for teaching program ming and some graphical programming tools together with their underlying environments has been carried out and a graphical Pascal teaching tool is designed and implemented. Graphical programming tools provide the user the ability to solve the problems through the use of icons and symbols allowing very little text. Here, a Pascal teaching tool is presented in a very user friendly environment to teach programming through the use of flowcharts in a visual manner. Keywords: graphical programming, user interface, window manager in"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET PARALEL PROLOG EMÜLATÖRÜ Attila Ciirsoy Bilgisayar Mîıhendisîiği ve Enformatik Biiimleri Yüksek Lisans Tez Yöneticisi: Prof. Dr. Mehmet Baray Temmnz 1988 Şimdiye kadar, çeşitli paralel Prolog işletme modelleri önerilmiş ve uygu lanmıştır. Bu çalışmada paralel Proiog işletme modeli PPEM için bir emulator geliştirilmiştir. Emülatörün amacı, modeli test etmek ve performans çalışmalarında kullanılmak üzere veri toplamaktır. Ayrıca, modelin tanımında gözükmeyen bazı uygulama zorlukları ve ön performans sonuçları sunulmuştur. Anahtar Kelimeier.Prolog,Mantıksal Programlama, Emulator, Paralel işlem, Paralel Prolog. iv","ABSTRACT A PARALLEL PROLOG EMULATOR Attila Giirsov M.S. in Computer Engineering and Information Sciences Supervisor: Prof.Dr.Mekmet Baray July 1988 There are various parallel Prolog execution models proposed so far. in. this study, an emulator has been developed to test the execution model PPEM. The emulator is used to collect data to evaluate the performance of the model. The underlying architecture is assumed to be a tightly coupled multiprocessor system. Some implementation difficulties faced, which are not apparent in the definition of PPEM are discussed, and performance results are presented. Keywords: Prolog, Logic Programming, Emulators, Parallel Processing, Parallel Prolog."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"tilt JL YAZI, RESİM, ÇİZİM İŞLEMCİSİ Ahmet Coşar Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Doç. Dr. Bülent ÖZGÜÇ Eylül 1988 Bu çalışmanın sonucunda geliştirilen sistem yazı, çizim ve görüntü verilerini tek tek ya da aralarında ilişkiler tanımlayarak bir bütün olarak işleyebiimek- tedir. Bu yöntemle herhangi bir veride yapılan bir değişiklik diğer verileri de etkileyebilmektedir. Sistemin geliştirilmesinde birçok yeni yazılım geliştirme teknikleri ve hazır yazılımlar kullanılmıştır. Bunlardan başiıcaiarı, nesne- sei yaklaşımlı programlama, çok pencereli iş istasyonları, imgesel kullanıcı arayüzeyidir. Veri saklama ve koruma amacıyla yedekieme olanakları da sağ lanmıştır. Bu yazılım geliştirilirken genel olarak kabul görmüş standartlara uyulmaya çalışılmış ancak gerekli olduğu zaman, özellikle değişik veri türleri arasında ilişki tanımlama amacıyla, bir tekim yeni teknikler kullanılmıştır. Anahtar Kelimeler: Yazı, çizim, görüntü, nesneye-yönelik programlama, etkileşim sistemleri, çok pencereli iş düzeni. iv","ABSTRACT TEXT, IMAGE, GRAPHICS EDITOR Ahmet Coşar M.S. in Computer Engineering and Information Sciences Supervisor: Assoc. Prof. Dr. Bülent OZGUÇ September 1988 The editor proposed in this study can manipulate textual, graphics and image data in a unified way. Each data type can be edited individually or dependencies can be set up between various data items so that modifying one might propagate its effects on others. The system is developed by us ing new software tools and techniques such as object oriented programming, multi window workstations running with event selection principles and iconic interfacing. Facilities for data protection, such as joumaling are provided. Data storage and editing principles are handled within guidelines of well es tablished standards. However, where such definitions fail short, proposals for new techniques are made especially with respect to relation sets binding various data types. Keywords : Text, image, graphics, user interface, window manager, object- oriented programming. iii"
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL BÎR VERİ TABANİ SİSTEMİNDE NESNE BELLE?İ F. Nikan Kesim Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkım Temm-ûz 1988 Çeşitü bilgisayar kullanım alanlarında uygulanabilen nesnesei yaklaşımda her bir varlık, kendi durumunu ve işlevlerini kapsayan bir nesne olarak mo- deilenir. Bu tezde nesnesei yaklaşım kavramı ve nesnesei veri tabanı işletim sistemleri üzerinde araştırma yapılmış ve bir nesnesei veri tabanı sistemi pro totipi için nesne belleği tasarlanıp gerçekleştirilmiştir. Nesne belleği, nes nelerin gösterimini, erişimini, kullanımını ve bütün prototip sistemin gelişti rilmesi için gereken temel fonksiyonları sağlar. Anahtar kelimeler : nesnesei veri tabanı sistemleri, nesne, sınıf, metod, mesaj, aktarım, veri soyutlaması, sınıf hiyerarşisi, nesne belleği, mesaj yol lama. iv","ABSTR. AN OBJECT MEMORY FOR AN OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEM F. Nihan Kesim. M.S. in Computer Engineering and Information Sciences Supervisor: Prof. Dr. Erol Arkım July 1988 Object-oriented paradigm is an approach that can be applied in various areas of computing. In this approach, each entity is represented by an object which captures the state and the behaviour of the entity. In this thesis, a focused survey of object-oriented paradigm in general and object-oriented database management systems in particular has been carried out and an object memory module is designed and implemented for an object-oriented database management system prototype. The object memory module handles the representation, access and manipulation of objects in the system and provides the primitive functions that are necessary in the development of the prototype. Keywords : object-oriented database management system, object, class, method, message, data abstraction, encapsulation, inheritance, class hierar chy, object, memory, message passing."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL BİR VERİ TABANI SİSTEMİNDE MESAJ YOLLAMA Sibel M. Özelçi Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Prof.Dr.Eroİ Arkım lemnLiiz 198ö 3u tez çakşmasında nesnesel yaklaşım ve nesnesel veri taban: sistemleri üzerinde bir araştırma yapılmıştır. Ayrıca tek kuliamoi: bir nesnesel veri tabanı sistemi prototipi için bir dil geliştirilmiş ve bir mesaj yollama yöntemi önerilmiş ve uygtdanmıştır. Geliştirilen dil için bir derleyici yazılmıştır. Geliştirilen dil hesapsal açıdan tamdır ve empedans uyumsuzluğu prob lemini çözmeyi amaçlamaktadır. Veri tanımlama ve veri kullanım: için ko mutlar içerir. Komutlar doğrudan doğruya veya metodlar halinde kullanılabilirler. Derleme sırasında komutlar baz: kodlara çevrilirler ve bu kodlar daha sonra gerekli işlemleri yapmak üzere kullanılırlar. Geliştirilen sistem tek kulianıiıcılı olduğundan önerilen mesaj yollama yöntemi verilere aynı anda erişimden doğan problemleri çözümleyecek mekanizmalar içermemektedir. Mesaj yol lama ve parametre gönderme yiğit kullanılarak gerçekleştirilmiştir. Anahtar kelimeler : nesnesei veri tabanı sistemleri, nesne, sınıf, eleman, metod, mesaj, mesaj yollama, aktarım, sınıf hiyerarşisi, nesne kimliği, veri soyutlaması. :v","ABSTRACT MESSAGE PASSING IN AN OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEM Sibel M. Ozelçi M.S. in Computer Engineering and Information Sciences Supervisor: Prof.Dr.Eroi Arknn July 1988 In this thesis, a focused survey on object-oriented database management systems and on object-orientation in general was carried out and a single- user object-oriented database management system prototype was designed and implemented. A command language was defined and a message passing scheme was proposed and implemented. A compiler for the language was developed. The developed language is computationally complete and aims at solving the impedance mismatch problem. It contains both data definition and data manipulation statements. The statements can be used interactively or in the form of methods. After compilation, the statements are translated into inte ger codes and these codes are used to perform the necessary operations. Since the developed prototype is a single-user system, the message passing passing scheme does not provide any concurrency control mechanisms and stacks are used to implement message passing and argument handling. Keywords : object-oriented database management systems, object, class, instance, method, message, message passing, inheritance, class hierarchy, ob ject identity, data abstraction."
İhsan Doğramacı Bilkent Üniversitesi,Bilgisayar Mühendisliği Bilimleri-Bilgisayar ve Kontrol = Computer Engineering and Computer Science and Control,"ÖZET NESNESEL BİR VEPJ TABANI SİSTEMİNDE YARDIMCI BELLEK Murat KaraGiman Bilgisayar Mühendisliği ve Enformatik Bilimleri Yüksek Lisans Tez Yöneticisi: Prof. Dr. Erol Arkım Temimiz 1988 Bu tezde nesnesel yaklaşım ve nesnesel veri taban: işletim sistemleri üze rinde araştırma yapılmış ve Bilkent üniversitesinde geliştirilen bir nesnesel veri tabanı sistemi prototipi için yardımcı bellek tasarlanmıştır. Tezin birinci kısmı yapılan araştırmanın sonuçlarım özetlemektedir. Nesnesel yaklaşımın başlıca kavramları, özellikleri ve uygulama alanları anlatıl maktadır, ikinci kısımda, tasarlanan prototip tanıtılmaktadır. Sistemin yardımcı belleği ayrıntılı olarak anlatılmakta, diğer bölümleri özetlenmektedir. Son olarak nesnesel veri tabam sistemlerindeki en son araştırma konulan sunulmaktadır. Anahtar kelimeler : nesnesel veri tabam sistemleri, nesne, sınıf, yardımcı bellek av","ABSTRACT SECONDARY STORAGE MANAGEMENT IN AN OBJECT-ORIENTED DATABASE MANAGEMENT SYSTEM Murat Karaorınan M.S. In Computer Engineering and Information Sciences Supervisor: Prof.Dr.Erol Arkım July 1988 In this thesis, a survey on object-orientation and object-oriented database management systems has been carried oat and a secondary storage manage ment and indexing module is implemented for an object-oriented database management system prototype developed at Billcent University. First, basic concepts, characteristics, and application areas of object- oriented approach are introduced, then, the designed prototype system is presented, the secondary storage management module is explained in detail and the functions of the other modules are summarized. Finally, the current research issues in the object-oriented database systems are introduced. Keywords: object, class, object-oriented databases, secondary storage, indexing 111"
