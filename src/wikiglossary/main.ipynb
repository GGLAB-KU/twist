{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T07:56:28.767572Z",
     "start_time": "2024-05-28T07:56:28.470775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to read and preprocess each glossary CSV file\n",
    "def read_and_preprocess(file_name, domain):\n",
    "    df = pd.read_csv(file_name, sep='delimiter', header=None, engine='python')\n",
    "    df = df[0].str.split(',', expand=True)[[0]]\n",
    "    df.columns = ['term']\n",
    "    df['domain'] = domain\n",
    "    return df\n",
    "\n",
    "# Function to fetch Wikipedia page details\n",
    "def get_wikipedia_page_details(title):\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts|info|categories|links\",\n",
    "        \"explaintext\": True,\n",
    "        \"inprop\": \"url\",\n",
    "        \"redirects\": True,\n",
    "        \"cllimit\": \"max\",\n",
    "        \"pllimit\": \"max\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return {\"page_id\": 'N/A', \"title\": title, \"summary\": 'Request failed', \"content\": '', \"lastrevid\": 'N/A', \"length\": 0, \"fullurl\": 'N/A', \"categories\": [], \"links\": []}\n",
    "\n",
    "    data = response.json()\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    page = next(iter(pages.values()))\n",
    "\n",
    "    if 'extract' not in page:\n",
    "        return {\"page_id\": 'N/A', \"title\": title, \"summary\": 'Not found', \"content\": '', \"lastrevid\": 'N/A', \"length\": 0, \"fullurl\": 'N/A', \"categories\": [], \"links\": []}\n",
    "\n",
    "    summary_params = params.copy()\n",
    "    summary_params['exintro'] = True\n",
    "    summary_response = requests.get(endpoint, params=summary_params)\n",
    "    summary_data = summary_response.json()\n",
    "    summary_page = next(iter(summary_data.get('query', {}).get('pages', {}).values()))\n",
    "\n",
    "    return {\n",
    "        \"page_id\": page.get('pageid', 'N/A'),\n",
    "        \"title\": page.get('title', title),\n",
    "        \"summary\": summary_page.get('extract', 'Summary not found'),\n",
    "        \"content\": page.get('extract', 'Content not found'),\n",
    "        \"lastrevid\": page.get('lastrevid', 'N/A'),\n",
    "        \"length\": len(page.get('extract', '')),\n",
    "        \"fullurl\": page.get('fullurl', 'N/A'),\n",
    "        \"categories\": [cat['title'] for cat in page.get('categories', [])],\n",
    "        \"links\": [link['title'] for link in page.get('links', [])]\n",
    "    }\n"
   ],
   "id": "1080aeae37a4b8c9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T08:12:44.285732Z",
     "start_time": "2024-05-28T07:56:28.769073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read and preprocess all the glossaries\n",
    "math = read_and_preprocess('glossary_of_areas_of_mathematics.csv', 'math')\n",
    "cs = read_and_preprocess('glossary_of_computer_science.csv', 'cs')\n",
    "physics = read_and_preprocess('glossary_of_physics.csv', 'physics')\n",
    "\n",
    "# Merge all dataframes into one\n",
    "all_data = pd.concat([math, cs, physics], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "all_data.to_csv('all_glossary.csv', index=False)\n",
    "\n",
    "# Apply the function and create new columns\n",
    "wikipedia_details = all_data['term'].apply(get_wikipedia_page_details)\n",
    "wikipedia_df = pd.DataFrame(wikipedia_details.tolist())\n",
    "\n",
    "# Combine the original dataframe with the new Wikipedia details dataframe\n",
    "all_data = pd.concat([all_data, wikipedia_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'wikipedia_details' column if it exists\n",
    "all_data.drop(columns=['wikipedia_details'], errors='ignore', inplace=True)\n",
    "\n",
    "# Save the final dataframe with Wikipedia details to a CSV file\n",
    "all_data.to_csv('all_glossary_with_wikipedia.csv', index=False)"
   ],
   "id": "490fd9408e3db185",
   "outputs": [],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
